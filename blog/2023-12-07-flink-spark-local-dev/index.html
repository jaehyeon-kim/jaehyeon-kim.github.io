<!doctype html><html class=position-relative itemscope itemtype=https://schema.org/WebPage lang=en data-bs-theme=light data-palette=blue-gray><head><script src=/assets/init/bundle.min.99cb2d6502b2c6f5d76f21079aa7b7ea5ad83125c684ac755e2a5af62cc7ad71.js integrity="sha256-mcstZQKyxvXXbyEHmqe36lrYMSXGhKx1Xipa9izHrXE=" crossorigin=anonymous></script><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>Setup Local Development Environment for Apache Flink and Spark Using EMR Container Images - Jaehyeon Kim</title>
<link rel=icon href=/favicon_huf63427acbec4fbe11db6a32164cc2763_6040_16x16_resize_q75_h2_box_2.webp sizes=16x16 type=image/webp><link rel=icon href=/favicon_huf63427acbec4fbe11db6a32164cc2763_6040_32x32_resize_q75_h2_box_2.webp sizes=32x32 type=image/webp><link rel=icon href=/favicon_huf63427acbec4fbe11db6a32164cc2763_6040_150x150_resize_q75_h2_box_2.webp sizes=150x150 type=image/webp><link rel=apple-touch-icon href=/favicon_huf63427acbec4fbe11db6a32164cc2763_6040_180x180_resize_q75_h2_box_2.webp sizes=180x180 type=image/webp><link rel=icon href=/favicon_huf63427acbec4fbe11db6a32164cc2763_6040_192x192_resize_q75_h2_box_2.webp sizes=192x192 type=image/webp><link rel=mask-icon href=/safari-pinned-tab.svg color=#6f42c1><meta name=keywords content="Analytics,Real-time Analytics,Data Engineering,Data Streaming,Architecture"><meta name=description content="Apache Flink became generally available for Amazon EMR on EKS from the EMR 6.15.0 releases. As it is integrated with the Glue Data Catalog, it can be particularly useful if we develop real time data ingestion/processing via Flink and build analytical queries using Spark (or any other tools or services that can access to the Glue Data Catalog). In this post, we will discuss how to set up a local development environment for Apache Flink and Spark using the EMR container images. After illustrating the environment setup, we will discuss a solution where data ingestion/processing is performed in real time using Apache Flink and the processed data is consumed by Apache Spark for analysis."><meta name=robots content="index, follow"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://jaehyeon.me/blog/2023-12-07-flink-spark-local-dev/featured.png"><meta name=twitter:title content="Setup Local Development Environment for Apache Flink and Spark Using EMR Container Images"><meta name=twitter:description content="Apache Flink became generally available for Amazon EMR on EKS from the EMR 6.15.0 releases. As it is integrated with the Glue Data Catalog, it can be particularly useful if we develop real time data ingestion/processing via Flink and build analytical queries using Spark (or any other tools or services that can access to the Glue Data Catalog). In this post, we will discuss how to set up a local development environment for Apache Flink and Spark using the EMR container images. After illustrating the environment setup, we will discuss a solution where data ingestion/processing is performed in real time using Apache Flink and the processed data is consumed by Apache Spark for analysis."><meta property="og:title" content="Setup Local Development Environment for Apache Flink and Spark Using EMR Container Images"><meta property="og:description" content="Apache Flink became generally available for Amazon EMR on EKS from the EMR 6.15.0 releases. As it is integrated with the Glue Data Catalog, it can be particularly useful if we develop real time data ingestion/processing via Flink and build analytical queries using Spark (or any other tools or services that can access to the Glue Data Catalog). In this post, we will discuss how to set up a local development environment for Apache Flink and Spark using the EMR container images. After illustrating the environment setup, we will discuss a solution where data ingestion/processing is performed in real time using Apache Flink and the processed data is consumed by Apache Spark for analysis."><meta property="og:type" content="article"><meta property="og:url" content="https://jaehyeon.me/blog/2023-12-07-flink-spark-local-dev/"><meta property="og:image" content="https://jaehyeon.me/blog/2023-12-07-flink-spark-local-dev/featured.png"><meta property="article:section" content="blog"><meta property="article:published_time" content="2023-12-07T00:00:00+00:00"><meta property="article:modified_time" content="2023-12-06T06:02:55+11:00"><meta itemprop=name content="Setup Local Development Environment for Apache Flink and Spark Using EMR Container Images"><meta itemprop=description content="Apache Flink became generally available for Amazon EMR on EKS from the EMR 6.15.0 releases. As it is integrated with the Glue Data Catalog, it can be particularly useful if we develop real time data ingestion/processing via Flink and build analytical queries using Spark (or any other tools or services that can access to the Glue Data Catalog). In this post, we will discuss how to set up a local development environment for Apache Flink and Spark using the EMR container images. After illustrating the environment setup, we will discuss a solution where data ingestion/processing is performed in real time using Apache Flink and the processed data is consumed by Apache Spark for analysis."><meta itemprop=datePublished content="2023-12-07T00:00:00+00:00"><meta itemprop=dateModified content="2023-12-06T06:02:55+11:00"><meta itemprop=wordCount content="3329"><meta itemprop=image content="https://jaehyeon.me/blog/2023-12-07-flink-spark-local-dev/featured.png"><meta itemprop=keywords content="Apache Flink,PyFlink,Apache Spark,PySpark,Apache Kafka,Amazon EMR,Docker,Docker Compose,Python,"><link rel=manifest href=/manifest.json><script async src="https://www.googletagmanager.com/gtag/js?id=G-076N8WCGWZ"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-076N8WCGWZ",{anonymize_ip:!1})}</script><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-2764466088407958"></script><link data-precache rel=stylesheet href="/assets/main/bundle.min.dc910a9364ba50e03d47ecf493e01f798a7ff46d1f793a172c8412e0c9867284.css" integrity="sha256-3JEKk2S6UOA9R+z0k+AfeYp/9G0feToXLIQS4MmGcoQ=" crossorigin=anonymous><link data-precache rel=stylesheet href=/assets/katex/bundle.min.a06a916255e286562e9c3684403a944bb85aebe51b00e2f1ef537987349ead12.css integrity="sha256-oGqRYlXihlYunDaEQDqUS7ha6+UbAOLx71N5hzSerRI=" crossorigin=anonymous><link data-precache rel=stylesheet href=/assets/viewer/bundle.min.8c1002839fa22c1350d6ae1eef6593120e108f973c41348be9b5065430566aaf.css integrity="sha256-jBACg5+iLBNQ1q4e72WTEg4Qj5c8QTSL6bUGVDBWaq8=" crossorigin=anonymous></head><body><header class="mb-4 sticky-top"><nav class="top-app-bar shadow navbar navbar-expand-xxl"><div class=container><a class="navbar-brand d-flex align-items-center flex-grow-1 flex-xxl-grow-0 justify-content-xxl-start ms-2 ms-xxl-0 mx-auto me-xxl-2" href=https://jaehyeon.me/>Jaehyeon Kim</a><div class="offcanvas-xxl offcanvas-end flex-grow-1" data-bs-scroll=true tabindex=-1 id=navbarMenus aria-labelledby=navbarMenusLabel><div class="offcanvas-header px-4 pb-0"><div class="offcanvas-title h5" id=navbarMenusLabel>Jaehyeon Kim</div><button type=button class="btn-close btn-close-white" data-bs-dismiss=offcanvas data-bs-target=#navbarMenus aria-label=Close></button></div><div class="offcanvas-body p-4 pt-0 p-xxl-0"><hr class=d-xxl-none><ul class="navbar-nav flex-row flex-wrap align-items-center me-auto"><li class="nav-item col-12 col-xxl-auto dropdown px-0"><a href=# class="nav-link dropdown-toggle" id=navbarDropdownBlog role=button data-bs-toggle=dropdown aria-expanded=false><span class="menu-icon me-1"><i class="fas fa-fw fa-blog text-warning"></i></span>Blog</a><ul class="dropdown-menu dropdown-menu-end" aria-labelledby=navbarDropdownBlog data-bs-popper=none><li><a class="dropdown-item d-flex align-items-center text-wrap text-xxl-nowrap" href=https://jaehyeon.me/archives/><span class="dropdown-item-icon me-2 p-2 rounded"><i class="fas fa-fw fa-file-archive text-primary"></i></span><div class=dropdown-item-content><p class="dropdown-item-title mb-0">Archives</p></div></a></li><li><a class="dropdown-item d-flex align-items-center text-wrap text-xxl-nowrap" href=https://jaehyeon.me/series/><span class="dropdown-item-icon me-2 p-2 rounded"><i class="fas fa-fw fa-columns"></i></span><div class=dropdown-item-content><p class="dropdown-item-title mb-1">Series</p><p class="dropdown-item-description mb-0 text-secondary">List of series.</p></div></a></li><li><a class="dropdown-item d-flex align-items-center text-wrap text-xxl-nowrap" href=https://jaehyeon.me/series/_index.zh-cn/><span class="dropdown-item-icon me-2 p-2 rounded"><i class="fas fa-fw fa-columns"></i></span><div class=dropdown-item-content><p class="dropdown-item-title mb-1">专栏</p><p class="dropdown-item-description mb-0 text-secondary">List of series.</p></div></a></li><li><a class="dropdown-item d-flex align-items-center text-wrap text-xxl-nowrap" href=https://jaehyeon.me/categories/><span class="dropdown-item-icon me-2 p-2 rounded"><i class="fas fa-fw fa-folder text-success"></i></span><div class=dropdown-item-content><p class="dropdown-item-title mb-1">Categories</p><p class="dropdown-item-description mb-0 text-secondary">List of categories.</p></div></a></li><li><a class="dropdown-item d-flex align-items-center text-wrap text-xxl-nowrap" href=https://jaehyeon.me/categories/_index.zh-cn/><span class="dropdown-item-icon me-2 p-2 rounded"><i class="fas fa-fw fa-folder text-success"></i></span><div class=dropdown-item-content><p class="dropdown-item-title mb-1">分类</p><p class="dropdown-item-description mb-0 text-secondary">List of categories.</p></div></a></li><li><a class="dropdown-item d-flex align-items-center text-wrap text-xxl-nowrap" href=https://jaehyeon.me/tags/><span class="dropdown-item-icon me-2 p-2 rounded"><i class="fas fa-fw fa-tags"></i></span><div class=dropdown-item-content><p class="dropdown-item-title mb-1">Tags</p><p class="dropdown-item-description mb-0 text-secondary">List of tags.</p></div></a></li><li><a class="dropdown-item d-flex align-items-center text-wrap text-xxl-nowrap" href=https://jaehyeon.me/tags/_index.zh-cn/><span class="dropdown-item-icon me-2 p-2 rounded"><i class="fas fa-fw fa-tags"></i></span><div class=dropdown-item-content><p class="dropdown-item-title mb-1">标签</p><p class="dropdown-item-description mb-0 text-secondary">List of tags.</p></div></a></li></ul></li></ul><hr class=d-xxl-none><form class="search-bar ms-auto my-auto" action=/search/ novalidate><div class="input-group align-items-center"><span class="btn btn-search disabled position-absolute left-0 border-0 px-1"><i class="fas fa-fw fa-search fa-lg"></i>
</span><input class="my-1 form-control border-white rounded-5 search-input bg-body" name=q type=search placeholder=Search aria-label=Search required>
<span class="search-shortcut position-absolute end-0 top-0 me-2"><kbd class="text-dark bg-white opacity-75 rounded-3 shadow border border-primary py-1 fw-bold">/</kbd></span></div></form><hr class=d-xxl-none><ul class="navbar-nav flex-row flex-wrap align-items-center ms-md-auto"><li class="nav-item py-2 py-xxl-1 col-12 col-xxl-auto"><nav class="social-links nav justify-content-center flex-row"><a class="nav-link social-link col-6 col-xxl-auto p-1" target=_blank href=https://github.com/jaehyeon-kim title=GitHub rel=me><i class="fa-fw fab fa-github"></i>
<span class="ms-1 d-xxl-none">Github</span>
</a><a class="nav-link social-link col-6 col-xxl-auto p-1" target=_blank href=https://linkedin.com/in/jaehyeon-kim-76b93429 title=LinkedIn rel=me><i class="fa-fw fab fa-linkedin-in"></i>
<span class="ms-1 d-xxl-none">Linkedin</span>
</a><a class="nav-link social-link col-6 col-xxl-auto p-1" target=_blank href=https://www.paypal.com/paypalme/dottami title=PayPal rel=me><i class="fa-fw fab fa-paypal"></i>
<span class="ms-1 d-xxl-none">Paypal</span>
</a><a class="nav-link social-link col-6 col-xxl-auto p-1" target=_blank href=/index.xml title=RSS rel=me><i class="fas fa-fw fa-rss"></i>
<span class="ms-1 d-xxl-none">RSS</span></a></nav></li><li class="nav-item py-2 py-xxl-1 col-12 col-xxl-auto"><div class="vr d-none d-xxl-flex h-100 mx-xxl-2 text-white"></div><hr class="d-xxl-none my-2"></li><li class="nav-item dropdown col-6 col-xxl-auto"><a class="nav-link px-0 py-2 px-xxl-1" href=# id=fontSizeDropdown role=button data-bs-toggle=dropdown aria-expanded=false><i class="fas fa-fw fa-font"></i>
<span class=d-xxl-none>Font Size</span></a><ul class="font-size-dropdown-menu dropdown-menu dropdown-menu-end" aria-labelledby=fontSizeDropdown><li><button class="font-size-item dropdown-item" data-size=xs>
Extra Small</button></li><li><button class="font-size-item dropdown-item" data-size=sm>
Small</button></li><li><button class="font-size-item dropdown-item active" data-size=md>
Medium</button></li><li><button class="font-size-item dropdown-item" data-size=lg>
Large</button></li><li><button class="font-size-item dropdown-item" data-size=xl>
Extra Large</button></li></ul></li><li class="nav-item dropdown col-6 col-xxl-auto"><a class="nav-link px-0 py-2 px-xxl-1" href=# id=paletteDropdown role=button data-bs-toggle=dropdown aria-expanded=false><i class="fas fa-fw fa-palette"></i>
<span class=d-xxl-none>Palette</span></a><ul class="palette-dropdown-menu dropdown-menu dropdown-menu-end px-2 row g-2" aria-labelledby=paletteDropdown><li class="col-4 my-1"><a role=button id=palette-blue aria-label=Blue class="btn btn-sm w-100 palette text-bg-blue" data-palette=blue></a></li><li class="col-4 my-1"><a role=button id=palette-blue-gray aria-label="Blue Gray" class="btn btn-sm w-100 palette text-bg-blue-gray" data-palette=blue-gray></a></li><li class="col-4 my-1"><a role=button id=palette-brown aria-label=Brown class="btn btn-sm w-100 palette text-bg-brown" data-palette=brown></a></li><li class="col-4 my-1"><a role=button id=palette-cyan aria-label=Cyan class="btn btn-sm w-100 palette text-bg-cyan" data-palette=cyan></a></li><li class="col-4 my-1"><a role=button id=palette-green aria-label=Green class="btn btn-sm w-100 palette text-bg-green" data-palette=green></a></li><li class="col-4 my-1"><a role=button id=palette-indigo aria-label=Indigo class="btn btn-sm w-100 palette text-bg-indigo" data-palette=indigo></a></li><li class="col-4 my-1"><a role=button id=palette-orange aria-label=Orange class="btn btn-sm w-100 palette text-bg-orange" data-palette=orange></a></li><li class="col-4 my-1"><a role=button id=palette-pink aria-label=Pink class="btn btn-sm w-100 palette text-bg-pink" data-palette=pink></a></li><li class="col-4 my-1"><a role=button id=palette-purple aria-label=Purple class="btn btn-sm w-100 palette text-bg-purple" data-palette=purple></a></li><li class="col-4 my-1"><a role=button id=palette-red aria-label=Red class="btn btn-sm w-100 palette text-bg-red" data-palette=red></a></li><li class="col-4 my-1"><a role=button id=palette-teal aria-label=Teal class="btn btn-sm w-100 palette text-bg-teal" data-palette=teal></a></li><li class="col-4 my-1"><a role=button id=palette-yellow aria-label=Yellow class="btn btn-sm w-100 palette text-bg-yellow" data-palette=yellow></a></li></ul></li><li class="nav-item dropdown col-6 col-xxl-auto"><a class="nav-link px-0 py-2 px-xxl-1" href=# id=modeDropdown role=button data-bs-toggle=dropdown aria-expanded=false><i class="mode-icon fas fa-fw fa-sun" id=modeIcon></i>
<span class=d-xxl-none>Mode</span></a><ul class="mode-dropdown-menu dropdown-menu dropdown-menu-end" aria-labelledby=modeDropdown><li class="mode-item active" data-color-mode=light data-icon=sun><button class=dropdown-item>
<i class="mode-icon fas fa-fw fa-sun"></i> Light</button></li><li class=mode-item data-color-mode=dark data-icon=moon><button class=dropdown-item>
<i class="mode-icon fas fa-fw fa-moon"></i> Dark</button></li><li class=mode-item data-color-mode=auto data-icon=adjust><button class=dropdown-item>
<i class="mode-icon fas fa-fw fa-adjust"></i> Auto</button></li></ul></li></ul></div></div><div class=d-flex><button class="navbar-toggler order-5 border-0" type=button data-bs-toggle=offcanvas data-bs-target=#navbarMenus aria-controls=navbarMenus aria-expanded=false aria-label="Toggle navigation">
<i class="fas fa-ellipsis-h"></i></button></div></div></nav></header><main class=container><div class="row content"><noscript><div class="alert alert-danger" role=alert>Your browser does not support JavaScript.</div></noscript><div class=col-xxl-8><div class=container><nav class="row card component" aria-label=breadcrumb><div class="card-body pb-0"><ol class="hbs-breadcrumb breadcrumb flex-nowrap"><li class="breadcrumb-item text-surface"><a href=/>Home</a></li><li class="breadcrumb-item text-surface"><a href=/blog/>Blogs</a></li><li class="breadcrumb-item active">Setup Local Development Environment for Apache Flink and Spark Using EMR Container Images</li></ol></div></nav><div class="post-panel-wrapper position-relative d-flex justify-content-center"><div class="d-flex flex-row justify-content-center rounded-5 border post-panel position-fixed px-3 py-1 surface shadow-1"><a class="action action-toc d-none d-xxl-block" href=#postTOC role=button title=Contents><i class="fas fa-fw fa-list-alt"></i>
</a><a class="action action-toc d-block d-xxl-none" href=#post-toc-container role=button title=Contents><i class="fas fa-fw fa-list-alt"></i>
</a><a class="action action-post-comments" href=#post-comments role=button aria-label=Comments title=Comments><i class="fas fa-fw fa-comments"></i>
</a><a id=sidebarToggler class="action action-sidebar-toggler d-none d-xxl-block" role=button title="Toggle sidebar"><i class="fas fa-fw fa-expand-alt" data-fa-transform=rotate-45></i></a></div></div><article class="row card component mb-4 post"><div class=card-header><h1 class="card-title post-title my-2">Setup Local Development Environment for Apache Flink and Spark Using EMR Container Images</h1></div><div class=card-body><div class="post-meta mb-3"><span class="post-date me-1 mb-1" title="created on 2023-12-07 00:00:00 +0000 UTC.">December 7, 2023</span><span class="post-reading-time me-1 mb-1">16 min read</span><a href=/categories/data-engineering/ class="btn btn-sm btn-secondary mb-1 me-2 py-0 pe-1 post-taxonomy post-taxonomy-sm post-category">
<i class="fas fa-fw fa-folder me-1"></i>Data Engineering</a><a href=/tags/amazon-emr/ class="btn btn-sm btn-secondary mb-1 me-2 py-0 pe-1 post-taxonomy post-taxonomy-sm post-tag">Amazon EMR</a><a href=/tags/apache-flink/ class="btn btn-sm btn-secondary mb-1 me-2 py-0 pe-1 post-taxonomy post-taxonomy-sm post-tag">Apache Flink</a><a href=/tags/apache-kafka/ class="btn btn-sm btn-secondary mb-1 me-2 py-0 pe-1 post-taxonomy post-taxonomy-sm post-tag">Apache Kafka</a><a href=/tags/apache-spark/ class="btn btn-sm btn-secondary mb-1 me-2 py-0 pe-1 post-taxonomy post-taxonomy-sm post-tag">Apache Spark</a><a href=/tags/docker/ class="btn btn-sm btn-secondary mb-1 me-2 py-0 pe-1 post-taxonomy post-taxonomy-sm post-tag">Docker</a><a href=/tags/docker-compose/ class="btn btn-sm btn-secondary mb-1 me-2 py-0 pe-1 post-taxonomy post-taxonomy-sm post-tag">Docker Compose</a><a href=/tags/pyflink/ class="btn btn-sm btn-secondary mb-1 me-2 py-0 pe-1 post-taxonomy post-taxonomy-sm post-tag">Pyflink</a><a href=/tags/pyspark/ class="btn btn-sm btn-secondary mb-1 me-2 py-0 pe-1 post-taxonomy post-taxonomy-sm post-tag">PySpark</a><a href=/tags/python/ class="btn btn-sm btn-secondary mb-1 me-2 py-0 pe-1 post-taxonomy post-taxonomy-sm post-tag">Python</a></div><picture class="d-flex justify-content-center"><source srcset=/blog/2023-12-07-flink-spark-local-dev/featured_hu7a73c8ca7d8b8a4ddf49d1a9b8fe40bd_133053_0x270_resize_box_3.png type=image/png media="(max-width: 576px)" width=362 height=270><img class="post-featured-img h-auto w-auto mb-3" alt="Setup Local Development Environment for Apache Flink and Spark Using EMR Container Images" src=/blog/2023-12-07-flink-spark-local-dev/featured_hu7a73c8ca7d8b8a4ddf49d1a9b8fe40bd_133053_0x480_resize_box_3.png width=643 height=480 data-src=/blog/2023-12-07-flink-spark-local-dev/featured.png></picture><p class="lead mb-3 text-body-emphasis">Apache Flink became generally available for Amazon EMR on EKS from the EMR 6.15.0 releases. As it is integrated with the Glue Data Catalog, it can be particularly useful if we develop real time data ingestion/processing via Flink and build analytical queries using Spark (or any other tools or services that can access to the Glue Data Catalog). In this post, we will discuss how to set up a local development environment for Apache Flink and Spark using the EMR container images. After illustrating the environment setup, we will discuss a solution where data ingestion/processing is performed in real time using Apache Flink and the processed data is consumed by Apache Spark for analysis.</p><div id=postTOC class="mb-3 text-surface"><h2 class=mb-3>Contents<a class="anchor ms-1" href=#postTOC></a></h2><nav id=TableOfContents><ul><li><a href=#architecture>Architecture</a></li><li><a href=#infrastructure>Infrastructure</a><ul><li><a href=#flink-setup>Flink Setup</a><ul><li><a href=#custom-docker-image>Custom Docker Image</a></li><li><a href=#docker-compose-services>Docker Compose Services</a></li></ul></li><li><a href=#spark-setup>Spark Setup</a><ul><li><a href=#configuration-updates>Configuration Updates</a></li><li><a href=#docker-compose-service>Docker Compose Service</a></li></ul></li><li><a href=#kafka-setup>Kafka Setup</a><ul><li><a href=#docker-compose-services-1>Docker Compose Services</a></li></ul></li></ul></li><li><a href=#applications>Applications</a><ul><li><a href=#flink-producer>Flink Producer</a></li><li><a href=#flink-processor>Flink Processor</a><ul><li><a href=#source-table-in-default-catalog>Source Table in Default Catalog</a></li><li><a href=#sink-table-in-glue-catalog>Sink Table in Glue Catalog</a></li><li><a href=#flink-job>Flink Job</a></li></ul></li><li><a href=#spark-consumer>Spark Consumer</a></li></ul></li><li><a href=#summary>Summary</a></li></ul></nav><hr class=text-secondary></div><div class="post-content mb-3" data-bs-spy=scroll data-bs-target=#TableOfContents tabindex=0><div id=post-content-body><p><a href=https://aws.amazon.com/about-aws/whats-new/2023/11/apache-flink-available-amazon-emr-eks/ target=_blank rel="noopener noreferrer">Apache Flink became generally available<i class="fas fa-external-link-square-alt ms-1"></i></a> for <a href=https://aws.amazon.com/emr/features/eks/ target=_blank rel="noopener noreferrer">Amazon EMR on EKS<i class="fas fa-external-link-square-alt ms-1"></i></a> from the <a href=https://docs.aws.amazon.com/emr/latest/EMR-on-EKS-DevelopmentGuide/emr-eks-6.15.0.html target=_blank rel="noopener noreferrer">EMR 6.15.0 releases<i class="fas fa-external-link-square-alt ms-1"></i></a>, and we are able to pull the Flink (as well as Spark) container images from the <a href=https://gallery.ecr.aws/emr-on-eks target=_blank rel="noopener noreferrer">ECR Public Gallery<i class="fas fa-external-link-square-alt ms-1"></i></a>. As both of them can be integrated with the <em>Glue Data Catalog</em>, it can be particularly useful if we develop real time data ingestion/processing via Flink and build analytical queries using Spark (or any other tools or services that can access to the Glue Data Catalog).</p><p>In this post, we will discuss how to set up a local development environment for Apache Flink and Spark using the EMR container images. For the former, a custom Docker image will be created, which downloads dependent connector Jar files into the Flink library folder, fixes process startup issues, and updates Hadoop configurations for Glue Data Catalog integration. For the latter, instead of creating a custom image, the EMR image is used to launch the Spark container where the required configuration updates are added at runtime via volume-mapping. After illustrating the environment setup, we will discuss a solution where data ingestion/processing is performed in real time using Apache Flink and the processed data is consumed by Apache Spark for analysis.</p><h2 id=architecture data-numberify>Architecture<a class="anchor ms-1" href=#architecture></a></h2><p>A PyFlink application produces messages into a Kafka topic and those messages are read and processed by another Flink application. For simplicity, the processor just buffers the messages and writes into S3 in Apache Hive style partitions. The sink (target) table is registered in the Glue Data Catalog for sharing the table details with other tools and services. A PySpark application is used to consume the processed data, which queries the Glue table using Spark SQL.</p><p><picture><img class="img-fluid mx-auto d-block" alt src=/blog/2023-12-07-flink-spark-local-dev/featured.png loading=lazy width=1009 height=753></picture></p><h2 id=infrastructure data-numberify>Infrastructure<a class="anchor ms-1" href=#infrastructure></a></h2><p>We create a Flink cluster, Spark container, Kafka cluster and Kafka management app. They are deployed using Docker Compose and the source can be found in the <a href=https://github.com/jaehyeon-kim/general-demos/tree/master/flink-spark-local-dev target=_blank rel="noopener noreferrer"><strong>GitHub repository</strong><i class="fas fa-external-link-square-alt ms-1"></i></a> of this post.</p><h3 id=flink-setup data-numberify>Flink Setup<a class="anchor ms-1" href=#flink-setup></a></h3><h4 id=custom-docker-image data-numberify>Custom Docker Image<a class="anchor ms-1" href=#custom-docker-image></a></h4><p>Flink 1.17.1 is installed in the EMR Flink image (<em>public.ecr.aws/emr-on-eks/flink/emr-6.15.0-flink:latest</em>) and a custom Docker image is created, which extends it. It begins with downloading dependent Jar files into the Flink library folder (<em>/usr/lib/flink/lib</em>), which are in relation to the <a href=https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/connectors/table/kafka/ target=_blank rel="noopener noreferrer">Kafka<i class="fas fa-external-link-square-alt ms-1"></i></a> and <a href=https://github.com/knaufk/flink-faker target=_blank rel="noopener noreferrer">Flink Faker<i class="fas fa-external-link-square-alt ms-1"></i></a> connectors.</p><p>When I started to run the <a href=https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/dev/table/sqlclient/ target=_blank rel="noopener noreferrer">Flink SQL client<i class="fas fa-external-link-square-alt ms-1"></i></a>, I encountered a number of process startup issues. First, I had a runtime exception whose error message is <em>java.lang.RuntimeException: Could not find a free permitted port on the machine</em>. When it gets started, it reserves a port and writes the port details into a folder via the <a href=https://github.com/apache/flink/blob/master/flink-core/src/main/java/org/apache/flink/util/NetUtils.java#L190 target=_blank rel="noopener noreferrer"><em>getAvailablePort</em><i class="fas fa-external-link-square-alt ms-1"></i></a> method of the <em>NetUtils</em> class. Unlike the official Flink Docker image where the details are written to the <em>/tmp</em> folder, the EMR image writes into the <em>/mnt/tmp</em> folder, and it throws an error due to insufficient permission. I was able to fix the issue by creating the <em>/mnt/tmp</em> folder beforehand. Secondly, I also had additional issues that were caused by the <em>NoClassDefFoundError</em>, and they were fixed by adding the <a href=https://mvnrepository.com/artifact/javax.inject/javax.inject/1 target=_blank rel="noopener noreferrer">Javax Inject<i class="fas fa-external-link-square-alt ms-1"></i></a> and <a href=https://mvnrepository.com/artifact/aopalliance/aopalliance/1.0 target=_blank rel="noopener noreferrer">AOP Alliance<i class="fas fa-external-link-square-alt ms-1"></i></a> Jar files into the Flink library folder.</p><p>For Glue Data Catalog integration, we need Hadoop configuration. The EMR image keeps <em>core-site.xml</em> in the <em>/glue/confs/hadoop/conf</em> folder, and I had to update the file. Specifically I updated the credentials providers from <em>WebIdentityTokenCredentialsProvider</em> to <em>EnvironmentVariableCredentialsProvider</em>. In this way, we are able to access AWS services with AWS credentials in environment variables - the updated Hadoop configuration file can be found below. I also created the <em>/mnt/s3</em> folder as it is specified as the S3 buffer directory in <em>core-site.xml</em>.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-docker data-lang=docker><span class=line><span class=ln> 1</span><span class=cl><span class=c># dockers/flink/Dockerfile</span><span class=err>
</span></span></span><span class=line><span class=ln> 2</span><span class=cl><span class=err></span><span class=k>FROM</span><span class=s> public.ecr.aws/emr-on-eks/flink/emr-6.15.0-flink:latest</span><span class=err>
</span></span></span><span class=line><span class=ln> 3</span><span class=cl><span class=err>
</span></span></span><span class=line><span class=ln> 4</span><span class=cl><span class=err></span><span class=k>ARG</span> FLINK_VERSION<span class=err>
</span></span></span><span class=line><span class=ln> 5</span><span class=cl><span class=err></span><span class=k>ENV</span> <span class=nv>FLINK_VERSION</span><span class=o>=</span><span class=si>${</span><span class=nv>FLINK_VERSION</span><span class=k>:-</span><span class=nv>1</span><span class=p>.17.1</span><span class=si>}</span><span class=err>
</span></span></span><span class=line><span class=ln> 6</span><span class=cl><span class=err></span><span class=k>ARG</span> KAFKA_VERSION<span class=err>
</span></span></span><span class=line><span class=ln> 7</span><span class=cl><span class=err></span><span class=k>ENV</span> <span class=nv>KAFKA_VERSION</span><span class=o>=</span><span class=si>${</span><span class=nv>KAFKA_VERSION</span><span class=k>:-</span><span class=nv>3</span><span class=p>.2.3</span><span class=si>}</span><span class=err>
</span></span></span><span class=line><span class=ln> 8</span><span class=cl><span class=err></span><span class=k>ARG</span> FAKER_VERSION<span class=err>
</span></span></span><span class=line><span class=ln> 9</span><span class=cl><span class=err></span><span class=k>ENV</span> <span class=nv>FAKER_VERSION</span><span class=o>=</span><span class=si>${</span><span class=nv>FAKER_VERSION</span><span class=k>:-</span><span class=nv>0</span><span class=p>.5.3</span><span class=si>}</span><span class=err>
</span></span></span><span class=line><span class=ln>10</span><span class=cl><span class=err>
</span></span></span><span class=line><span class=ln>11</span><span class=cl><span class=err></span><span class=c>##</span><span class=err>
</span></span></span><span class=line><span class=ln>12</span><span class=cl><span class=err></span><span class=c>## add connectors (Kafka and flink faker) and related dependencies</span><span class=err>
</span></span></span><span class=line><span class=ln>13</span><span class=cl><span class=err></span><span class=c>##</span><span class=err>
</span></span></span><span class=line><span class=ln>14</span><span class=cl><span class=err></span><span class=k>RUN</span> curl -o /usr/lib/flink/lib/flink-connector-kafka-<span class=si>${</span><span class=nv>FLINK_VERSION</span><span class=si>}</span>.jar <span class=se>\
</span></span></span><span class=line><span class=ln>15</span><span class=cl><span class=se></span>      https://repo.maven.apache.org/maven2/org/apache/flink/flink-connector-kafka/<span class=si>${</span><span class=nv>FLINK_VERSION</span><span class=si>}</span>/flink-connector-kafka-<span class=si>${</span><span class=nv>FLINK_VERSION</span><span class=si>}</span>.jar <span class=se>\
</span></span></span><span class=line><span class=ln>16</span><span class=cl><span class=se></span>  <span class=o>&amp;&amp;</span> curl -o /usr/lib/flink/lib/kafka-clients-<span class=si>${</span><span class=nv>KAFKA_VERSION</span><span class=si>}</span>.jar <span class=se>\
</span></span></span><span class=line><span class=ln>17</span><span class=cl><span class=se></span>      https://repo.maven.apache.org/maven2/org/apache/kafka/kafka-clients/<span class=si>${</span><span class=nv>KAFKA_VERSION</span><span class=si>}</span>/kafka-clients-<span class=si>${</span><span class=nv>KAFKA_VERSION</span><span class=si>}</span>.jar <span class=se>\
</span></span></span><span class=line><span class=ln>18</span><span class=cl><span class=se></span>  <span class=o>&amp;&amp;</span> curl -o /usr/lib/flink/lib/flink-sql-connector-kafka-<span class=si>${</span><span class=nv>FLINK_VERSION</span><span class=si>}</span>.jar <span class=se>\
</span></span></span><span class=line><span class=ln>19</span><span class=cl><span class=se></span>      https://repo.maven.apache.org/maven2/org/apache/flink/flink-sql-connector-kafka/<span class=si>${</span><span class=nv>FLINK_VERSION</span><span class=si>}</span>/flink-sql-connector-kafka-<span class=si>${</span><span class=nv>FLINK_VERSION</span><span class=si>}</span>.jar <span class=se>\
</span></span></span><span class=line><span class=ln>20</span><span class=cl><span class=se></span>  <span class=o>&amp;&amp;</span> curl -L -o /usr/lib/flink/lib/flink-faker-<span class=si>${</span><span class=nv>FAKER_VERSION</span><span class=si>}</span>.jar <span class=se>\
</span></span></span><span class=line><span class=ln>21</span><span class=cl><span class=se></span>      https://github.com/knaufk/flink-faker/releases/download/v<span class=si>${</span><span class=nv>FAKER_VERSION</span><span class=si>}</span>/flink-faker-<span class=si>${</span><span class=nv>FAKER_VERSION</span><span class=si>}</span>.jar<span class=err>
</span></span></span><span class=line><span class=ln>22</span><span class=cl><span class=err>
</span></span></span><span class=line><span class=ln>23</span><span class=cl><span class=err></span><span class=c>##</span><span class=err>
</span></span></span><span class=line><span class=ln>24</span><span class=cl><span class=err></span><span class=c>## fix process startup issues</span><span class=err>
</span></span></span><span class=line><span class=ln>25</span><span class=cl><span class=err></span><span class=c>##</span><span class=err>
</span></span></span><span class=line><span class=ln>26</span><span class=cl><span class=err></span><span class=c># should be able to write a file in /mnt/tmp as getAvailablePort() in NetUtils class writes to /mnt/tmp instead of /tmp</span><span class=err>
</span></span></span><span class=line><span class=ln>27</span><span class=cl><span class=err></span><span class=c>#   see https://stackoverflow.com/questions/77539526/fail-to-start-flink-sql-client-on-emr-on-eks-docker-image</span><span class=err>
</span></span></span><span class=line><span class=ln>28</span><span class=cl><span class=err></span><span class=k>RUN</span> mkdir -p /mnt/tmp<span class=err>
</span></span></span><span class=line><span class=ln>29</span><span class=cl><span class=err>
</span></span></span><span class=line><span class=ln>30</span><span class=cl><span class=err></span><span class=c>## add missing jar files</span><span class=err>
</span></span></span><span class=line><span class=ln>31</span><span class=cl><span class=err></span><span class=k>RUN</span> curl -L -o /usr/lib/flink/lib/javax.inject-1.jar <span class=se>\
</span></span></span><span class=line><span class=ln>32</span><span class=cl><span class=se></span>      https://repo1.maven.org/maven2/javax/inject/javax.inject/1/javax.inject-1.jar <span class=se>\
</span></span></span><span class=line><span class=ln>33</span><span class=cl><span class=se></span>  <span class=o>&amp;&amp;</span> curl -L -o /usr/lib/flink/lib/aopalliance-1.0.jar <span class=se>\
</span></span></span><span class=line><span class=ln>34</span><span class=cl><span class=se></span>      https://repo1.maven.org/maven2/aopalliance/aopalliance/1.0/aopalliance-1.0.jar<span class=err>
</span></span></span><span class=line><span class=ln>35</span><span class=cl><span class=err>
</span></span></span><span class=line><span class=ln>36</span><span class=cl><span class=err></span><span class=c>##</span><span class=err>
</span></span></span><span class=line><span class=ln>37</span><span class=cl><span class=err></span><span class=c>## update hadoop configuration for Glue data catalog integration</span><span class=err>
</span></span></span><span class=line><span class=ln>38</span><span class=cl><span class=err></span><span class=c>##</span><span class=err>
</span></span></span><span class=line><span class=ln>39</span><span class=cl><span class=err></span><span class=c>## create /mnt/s3 (value of fs.s3.buffer.dir) beforehand</span><span class=err>
</span></span></span><span class=line><span class=ln>40</span><span class=cl><span class=err></span><span class=k>RUN</span> mkdir -p /mnt/s3<span class=err>
</span></span></span><span class=line><span class=ln>41</span><span class=cl><span class=err>
</span></span></span><span class=line><span class=ln>42</span><span class=cl><span class=err></span><span class=c>## copy updated core-site.xml</span><span class=err>
</span></span></span><span class=line><span class=ln>43</span><span class=cl><span class=err></span><span class=c>## update credentials providers and value of fs.s3.buffer.dir to /mnt/s3 only</span><span class=err>
</span></span></span><span class=line><span class=ln>44</span><span class=cl><span class=err></span><span class=k>USER</span><span class=s> root</span><span class=err>
</span></span></span><span class=line><span class=ln>45</span><span class=cl><span class=err>
</span></span></span><span class=line><span class=ln>46</span><span class=cl><span class=err></span><span class=k>COPY</span> ./core-site.xml /glue/confs/hadoop/conf/core-site.xml<span class=err>
</span></span></span><span class=line><span class=ln>47</span><span class=cl><span class=err>
</span></span></span><span class=line><span class=ln>48</span><span class=cl><span class=err></span><span class=k>USER</span><span class=s> flink</span><span class=err>
</span></span></span></code></pre></div><p>Here is the updated Hadoop configuration file that is baked into the custom Docker image.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-xml data-lang=xml><span class=line><span class=ln> 1</span><span class=cl><span class=c>&lt;!-- dockers/flink/core-site.xml --&gt;</span>
</span></span><span class=line><span class=ln> 2</span><span class=cl><span class=nt>&lt;configuration&gt;</span>
</span></span><span class=line><span class=ln> 3</span><span class=cl>  <span class=nt>&lt;property&gt;</span>
</span></span><span class=line><span class=ln> 4</span><span class=cl>    <span class=nt>&lt;name&gt;</span>fs.s3a.aws.credentials.provider<span class=nt>&lt;/name&gt;</span>
</span></span><span class=line><span class=ln> 5</span><span class=cl>    <span class=nt>&lt;value&gt;</span>com.amazonaws.auth.EnvironmentVariableCredentialsProvider<span class=nt>&lt;/value&gt;</span>
</span></span><span class=line><span class=ln> 6</span><span class=cl>  <span class=nt>&lt;/property&gt;</span>
</span></span><span class=line><span class=ln> 7</span><span class=cl>
</span></span><span class=line><span class=ln> 8</span><span class=cl>  <span class=nt>&lt;property&gt;</span>
</span></span><span class=line><span class=ln> 9</span><span class=cl>    <span class=nt>&lt;name&gt;</span>fs.s3.customAWSCredentialsProvider<span class=nt>&lt;/name&gt;</span>
</span></span><span class=line><span class=ln>10</span><span class=cl>    <span class=nt>&lt;value&gt;</span>com.amazonaws.auth.EnvironmentVariableCredentialsProvider<span class=nt>&lt;/value&gt;</span>
</span></span><span class=line><span class=ln>11</span><span class=cl>  <span class=nt>&lt;/property&gt;</span>
</span></span><span class=line><span class=ln>12</span><span class=cl>
</span></span><span class=line><span class=ln>13</span><span class=cl>  <span class=nt>&lt;property&gt;</span>
</span></span><span class=line><span class=ln>14</span><span class=cl>    <span class=nt>&lt;name&gt;</span>fs.s3.impl<span class=nt>&lt;/name&gt;</span>
</span></span><span class=line><span class=ln>15</span><span class=cl>    <span class=nt>&lt;value&gt;</span>com.amazon.ws.emr.hadoop.fs.EmrFileSystem<span class=nt>&lt;/value&gt;</span>
</span></span><span class=line><span class=ln>16</span><span class=cl>  <span class=nt>&lt;/property&gt;</span>
</span></span><span class=line><span class=ln>17</span><span class=cl>
</span></span><span class=line><span class=ln>18</span><span class=cl>  <span class=nt>&lt;property&gt;</span>
</span></span><span class=line><span class=ln>19</span><span class=cl>    <span class=nt>&lt;name&gt;</span>fs.s3n.impl<span class=nt>&lt;/name&gt;</span>
</span></span><span class=line><span class=ln>20</span><span class=cl>    <span class=nt>&lt;value&gt;</span>com.amazon.ws.emr.hadoop.fs.EmrFileSystem<span class=nt>&lt;/value&gt;</span>
</span></span><span class=line><span class=ln>21</span><span class=cl>  <span class=nt>&lt;/property&gt;</span>
</span></span><span class=line><span class=ln>22</span><span class=cl>
</span></span><span class=line><span class=ln>23</span><span class=cl>  <span class=nt>&lt;property&gt;</span>
</span></span><span class=line><span class=ln>24</span><span class=cl>    <span class=nt>&lt;name&gt;</span>fs.AbstractFileSystem.s3.impl<span class=nt>&lt;/name&gt;</span>
</span></span><span class=line><span class=ln>25</span><span class=cl>    <span class=nt>&lt;value&gt;</span>org.apache.hadoop.fs.s3.EMRFSDelegate<span class=nt>&lt;/value&gt;</span>
</span></span><span class=line><span class=ln>26</span><span class=cl>  <span class=nt>&lt;/property&gt;</span>
</span></span><span class=line><span class=ln>27</span><span class=cl>
</span></span><span class=line><span class=ln>28</span><span class=cl>  <span class=nt>&lt;property&gt;</span>
</span></span><span class=line><span class=ln>29</span><span class=cl>    <span class=nt>&lt;name&gt;</span>fs.s3.buffer.dir<span class=nt>&lt;/name&gt;</span>
</span></span><span class=line><span class=ln>30</span><span class=cl>    <span class=nt>&lt;value&gt;</span>/mnt/s3<span class=nt>&lt;/value&gt;</span>
</span></span><span class=line><span class=ln>31</span><span class=cl>    <span class=nt>&lt;final&gt;</span>true<span class=nt>&lt;/final&gt;</span>
</span></span><span class=line><span class=ln>32</span><span class=cl>  <span class=nt>&lt;/property&gt;</span>
</span></span><span class=line><span class=ln>33</span><span class=cl><span class=nt>&lt;/configuration&gt;</span>
</span></span></code></pre></div><p>The Docker image can be built using the following command.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=ln>1</span><span class=cl>$ docker build -t emr-6.15.0-flink:local dockers/flink/.
</span></span></code></pre></div><h4 id=docker-compose-services data-numberify>Docker Compose Services<a class="anchor ms-1" href=#docker-compose-services></a></h4><p>The Flink cluster is made up of a single master container (<em>jobmanager</em>) and one task container (<em>taskmanager</em>). The master container opens up the port 8081, and we are able to access the Flink Web UI on <em>localhost:8081</em>. Also, the current folder is <em>volume-mapped</em> into the <em>/home/flink/project</em> folder, and it allows us to submit a Flink application in the host folder to the Flink cluster.</p><p>Other than the environment variables of the Kafka bootstrap server addresses and AWS credentials, the following environment variables are important for deploying the Flink cluster and running Flink jobs without an issue.</p><ul><li><em>K8S_FLINK_GLUE_ENABLED</em><ul><li>If this environment variable exists, the container entrypoint file (<em>docker-entrypoint.sh</em>) configures Apache Hive. It moves the Hive/Glue related dependencies into the Flink library folder (<em>/usr/lib/flink/lib</em>) for setting up Hive Catalog, which is integrated with the Glue Data Catalog.</li></ul></li><li><em>K8S_FLINK_LOG_URL_STDERR</em> and <em>K8S_FLINK_LOG_URL_STDOUT</em><ul><li>The container entrypoint file (<em>docker-entrypoint.sh</em>) creates these folders, but I had an error due to insufficient permission. Therefore, I changed the values of those folders within the <em>/tmp</em> folder.</li></ul></li><li><em>HADOOP_CONF_DIR</em><ul><li>This variable is required when setting up a Hive catalog, or we can add it as an option when creating a catalog (<em>hadoop-config-dir</em>).</li></ul></li><li><em>FLINK_PROPERTIES</em><ul><li>The properties will be appended into the Flink configuration file (<em>/usr/lib/flink/conf/flink-conf.yaml</em>). Among those, <em>jobmanager.memory.process.size</em> and <em>taskmanager.memory.process.size</em> are mandatory for the containers run without failure.</li></ul></li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=ln> 1</span><span class=cl><span class=c># docker-compose.yml</span><span class=w>
</span></span></span><span class=line><span class=ln> 2</span><span class=cl><span class=w></span><span class=nt>version</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;3.5&#34;</span><span class=w>
</span></span></span><span class=line><span class=ln> 3</span><span class=cl><span class=w>
</span></span></span><span class=line><span class=ln> 4</span><span class=cl><span class=w></span><span class=nt>services</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=ln> 5</span><span class=cl><span class=w>  </span><span class=nt>jobmanager</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=ln> 6</span><span class=cl><span class=w>    </span><span class=nt>image</span><span class=p>:</span><span class=w> </span><span class=l>emr-6.15.0-flink:local</span><span class=w>
</span></span></span><span class=line><span class=ln> 7</span><span class=cl><span class=w>    </span><span class=nt>container_name</span><span class=p>:</span><span class=w> </span><span class=l>jobmanager</span><span class=w>
</span></span></span><span class=line><span class=ln> 8</span><span class=cl><span class=w>    </span><span class=nt>command</span><span class=p>:</span><span class=w> </span><span class=l>jobmanager</span><span class=w>
</span></span></span><span class=line><span class=ln> 9</span><span class=cl><span class=w>    </span><span class=nt>ports</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=ln>10</span><span class=cl><span class=w>      </span>- <span class=s2>&#34;8081:8081&#34;</span><span class=w>
</span></span></span><span class=line><span class=ln>11</span><span class=cl><span class=w>    </span><span class=nt>networks</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=ln>12</span><span class=cl><span class=w>      </span>- <span class=l>appnet</span><span class=w>
</span></span></span><span class=line><span class=ln>13</span><span class=cl><span class=w>    </span><span class=nt>volumes</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=ln>14</span><span class=cl><span class=w>      </span>- <span class=l>./:/home/flink/project</span><span class=w>
</span></span></span><span class=line><span class=ln>15</span><span class=cl><span class=w>    </span><span class=nt>environment</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=ln>16</span><span class=cl><span class=w>      </span>- <span class=l>RUNTIME_ENV=DOCKER</span><span class=w>
</span></span></span><span class=line><span class=ln>17</span><span class=cl><span class=w>      </span>- <span class=l>BOOTSTRAP_SERVERS=kafka-0:9092</span><span class=w>
</span></span></span><span class=line><span class=ln>18</span><span class=cl><span class=w>      </span>- <span class=l>AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID:-not_set}</span><span class=w>
</span></span></span><span class=line><span class=ln>19</span><span class=cl><span class=w>      </span>- <span class=l>AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY:-not_set}</span><span class=w>
</span></span></span><span class=line><span class=ln>20</span><span class=cl><span class=w>      </span>- <span class=l>AWS_REGION=${AWS_REGION:-not_set}</span><span class=w>
</span></span></span><span class=line><span class=ln>21</span><span class=cl><span class=w>      </span>- <span class=l>K8S_FLINK_GLUE_ENABLED=true</span><span class=w>
</span></span></span><span class=line><span class=ln>22</span><span class=cl><span class=w>      </span>- <span class=l>K8S_FLINK_LOG_URL_STDERR=/tmp/stderr</span><span class=w>
</span></span></span><span class=line><span class=ln>23</span><span class=cl><span class=w>      </span>- <span class=l>K8S_FLINK_LOG_URL_STDOUT=/tmp/stdout</span><span class=w>
</span></span></span><span class=line><span class=ln>24</span><span class=cl><span class=w>      </span>- <span class=l>HADOOP_CONF_DIR=/glue/confs/hadoop/conf</span><span class=w>
</span></span></span><span class=line><span class=ln>25</span><span class=cl><span class=w>      </span>- <span class=p>|</span><span class=sd>
</span></span></span><span class=line><span class=ln>26</span><span class=cl><span class=sd>        FLINK_PROPERTIES=
</span></span></span><span class=line><span class=ln>27</span><span class=cl><span class=sd>        jobmanager.rpc.address: jobmanager
</span></span></span><span class=line><span class=ln>28</span><span class=cl><span class=sd>        state.backend: filesystem
</span></span></span><span class=line><span class=ln>29</span><span class=cl><span class=sd>        state.checkpoints.dir: file:///tmp/flink-checkpoints
</span></span></span><span class=line><span class=ln>30</span><span class=cl><span class=sd>        heartbeat.interval: 1000
</span></span></span><span class=line><span class=ln>31</span><span class=cl><span class=sd>        heartbeat.timeout: 5000
</span></span></span><span class=line><span class=ln>32</span><span class=cl><span class=sd>        rest.flamegraph.enabled: true
</span></span></span><span class=line><span class=ln>33</span><span class=cl><span class=sd>        web.backpressure.refresh-interval: 10000
</span></span></span><span class=line><span class=ln>34</span><span class=cl><span class=sd>        jobmanager.memory.process.size: 1600m
</span></span></span><span class=line><span class=ln>35</span><span class=cl><span class=sd>        taskmanager.memory.process.size: 1728m</span><span class=w>        
</span></span></span><span class=line><span class=ln>36</span><span class=cl><span class=w>  </span><span class=nt>taskmanager</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=ln>37</span><span class=cl><span class=w>    </span><span class=nt>image</span><span class=p>:</span><span class=w> </span><span class=l>emr-6.15.0-flink:local</span><span class=w>
</span></span></span><span class=line><span class=ln>38</span><span class=cl><span class=w>    </span><span class=nt>container_name</span><span class=p>:</span><span class=w> </span><span class=l>taskmanager</span><span class=w>
</span></span></span><span class=line><span class=ln>39</span><span class=cl><span class=w>    </span><span class=nt>command</span><span class=p>:</span><span class=w> </span><span class=l>taskmanager</span><span class=w>
</span></span></span><span class=line><span class=ln>40</span><span class=cl><span class=w>    </span><span class=nt>networks</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=ln>41</span><span class=cl><span class=w>      </span>- <span class=l>appnet</span><span class=w>
</span></span></span><span class=line><span class=ln>42</span><span class=cl><span class=w>    </span><span class=nt>volumes</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=ln>43</span><span class=cl><span class=w>      </span>- <span class=l>./:/home/flink/project</span><span class=w>
</span></span></span><span class=line><span class=ln>44</span><span class=cl><span class=w>      </span>- <span class=l>flink_data:/tmp/</span><span class=w>
</span></span></span><span class=line><span class=ln>45</span><span class=cl><span class=w>    </span><span class=nt>environment</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=ln>46</span><span class=cl><span class=w>      </span>- <span class=l>RUNTIME_ENV=DOCKER</span><span class=w>
</span></span></span><span class=line><span class=ln>47</span><span class=cl><span class=w>      </span>- <span class=l>BOOTSTRAP_SERVERS=kafka-0:9092</span><span class=w>
</span></span></span><span class=line><span class=ln>48</span><span class=cl><span class=w>      </span>- <span class=l>AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID:-not_set}</span><span class=w>
</span></span></span><span class=line><span class=ln>49</span><span class=cl><span class=w>      </span>- <span class=l>AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY:-not_set}</span><span class=w>
</span></span></span><span class=line><span class=ln>50</span><span class=cl><span class=w>      </span>- <span class=l>AWS_REGION=${AWS_REGION:-not_set}</span><span class=w>
</span></span></span><span class=line><span class=ln>51</span><span class=cl><span class=w>      </span>- <span class=l>K8S_FLINK_GLUE_ENABLED=true</span><span class=w>
</span></span></span><span class=line><span class=ln>52</span><span class=cl><span class=w>      </span>- <span class=l>K8S_FLINK_LOG_URL_STDERR=/tmp/stderr</span><span class=w>
</span></span></span><span class=line><span class=ln>53</span><span class=cl><span class=w>      </span>- <span class=l>K8S_FLINK_LOG_URL_STDOUT=/tmp/stdout</span><span class=w>
</span></span></span><span class=line><span class=ln>54</span><span class=cl><span class=w>      </span>- <span class=l>HADOOP_CONF_DIR=/glue/confs/hadoop/conf</span><span class=w>
</span></span></span><span class=line><span class=ln>55</span><span class=cl><span class=w>      </span>- <span class=p>|</span><span class=sd>
</span></span></span><span class=line><span class=ln>56</span><span class=cl><span class=sd>        FLINK_PROPERTIES=
</span></span></span><span class=line><span class=ln>57</span><span class=cl><span class=sd>        jobmanager.rpc.address: jobmanager
</span></span></span><span class=line><span class=ln>58</span><span class=cl><span class=sd>        taskmanager.numberOfTaskSlots: 5
</span></span></span><span class=line><span class=ln>59</span><span class=cl><span class=sd>        state.backend: filesystem
</span></span></span><span class=line><span class=ln>60</span><span class=cl><span class=sd>        state.checkpoints.dir: file:///tmp/flink-checkpoints
</span></span></span><span class=line><span class=ln>61</span><span class=cl><span class=sd>        heartbeat.interval: 1000
</span></span></span><span class=line><span class=ln>62</span><span class=cl><span class=sd>        heartbeat.timeout: 5000
</span></span></span><span class=line><span class=ln>63</span><span class=cl><span class=sd>        jobmanager.memory.process.size: 1600m
</span></span></span><span class=line><span class=ln>64</span><span class=cl><span class=sd>        taskmanager.memory.process.size: 1728m</span><span class=w>        
</span></span></span><span class=line><span class=ln>65</span><span class=cl><span class=w>    </span><span class=nt>depends_on</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=ln>66</span><span class=cl><span class=w>      </span>- <span class=l>jobmanager</span><span class=w>
</span></span></span><span class=line><span class=ln>67</span><span class=cl><span class=w>
</span></span></span><span class=line><span class=ln>68</span><span class=cl><span class=w>  </span><span class=l>...</span><span class=w>
</span></span></span><span class=line><span class=ln>69</span><span class=cl><span class=w>
</span></span></span><span class=line><span class=ln>70</span><span class=cl><span class=w></span><span class=nt>networks</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=ln>71</span><span class=cl><span class=w>  </span><span class=nt>appnet</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=ln>72</span><span class=cl><span class=w>    </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>app-network</span><span class=w>
</span></span></span><span class=line><span class=ln>73</span><span class=cl><span class=w>
</span></span></span><span class=line><span class=ln>74</span><span class=cl><span class=w></span><span class=nt>volumes</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=ln>75</span><span class=cl><span class=w>  </span><span class=nt>flink_data</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=ln>76</span><span class=cl><span class=w>    </span><span class=nt>driver</span><span class=p>:</span><span class=w> </span><span class=l>local</span><span class=w>
</span></span></span><span class=line><span class=ln>77</span><span class=cl><span class=w>    </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>flink_data</span><span class=w>
</span></span></span><span class=line><span class=ln>78</span><span class=cl><span class=w>  </span><span class=l>...</span><span class=w>
</span></span></span></code></pre></div><h3 id=spark-setup data-numberify>Spark Setup<a class="anchor ms-1" href=#spark-setup></a></h3><p>In an <a href=/blog/2022-05-08-emr-local-dev>earlier post</a>, I illustrated how to set up a local development environment using an EMR container image. That post is based the <a href="https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers" target=_blank rel="noopener noreferrer">Dev Containers<i class="fas fa-external-link-square-alt ms-1"></i></a> extension of VS Code, and it is assumed that development takes place after attaching the project folder into a Docker container. Thanks to the <a href="https://marketplace.visualstudio.com/items?itemName=ms-azuretools.vscode-docker" target=_blank rel="noopener noreferrer">Docker<i class="fas fa-external-link-square-alt ms-1"></i></a> extension, however, we no longer have to attach the project folder into a container always because the extension allows us to do so with just a few mouse clicks if necessary - see the screenshot below. Moreover, as Spark applications developed in the host folder can easily be submitted to the Spark container via volume-mapping, we can simplify Spark setup dramatically without creating a custom Docker image. Therefore, Spark will be set up using the EMR image where updated Spark configuration files and the project folder are volume-mapped to the container. Also, the Spark History Server will be running in the container, which allows us to monitor completed and running Spark applications.</p><p><picture><img class="img-fluid mx-auto d-block" alt src=/blog/2023-12-07-flink-spark-local-dev/vscode-attach.png loading=lazy width=640 height=747></picture></p><h4 id=configuration-updates data-numberify>Configuration Updates<a class="anchor ms-1" href=#configuration-updates></a></h4><p>Same to Flink Hadoop configuration updates, we need to update credentials providers from <em>WebIdentityTokenCredentialsProvider</em> to <em>EnvironmentVariableCredentialsProvider</em> to access AWS services with AWS credentials in environment variables. Also, we should specify the catalog implementation to <em>hive</em> and set <em>AWSGlueDataCatalogHiveClientFactory</em> as the Hive metastore factory class.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-properties data-lang=properties><span class=line><span class=ln> 1</span><span class=cl><span class=c1># dockers/spark/spark-defaults.conf</span>
</span></span><span class=line><span class=ln> 2</span><span class=cl>
</span></span><span class=line><span class=ln> 3</span><span class=cl><span class=err>...</span>
</span></span><span class=line><span class=ln> 4</span><span class=cl>
</span></span><span class=line><span class=ln> 5</span><span class=cl><span class=c1>##</span>
</span></span><span class=line><span class=ln> 6</span><span class=cl><span class=c1>## Update credentials providers</span>
</span></span><span class=line><span class=ln> 7</span><span class=cl><span class=c1>##</span>
</span></span><span class=line><span class=ln> 8</span><span class=cl><span class=na>spark.hadoop.fs.s3.customAWSCredentialsProvider</span>  <span class=s>com.amazonaws.auth.EnvironmentVariableCredentialsProvider</span>
</span></span><span class=line><span class=ln> 9</span><span class=cl><span class=na>spark.hadoop.dynamodb.customAWSCredentialsProvider</span>  <span class=s>com.amazonaws.auth.EnvironmentVariableCredentialsProvider</span>
</span></span><span class=line><span class=ln>10</span><span class=cl><span class=c1># spark.authenticate               true</span>
</span></span><span class=line><span class=ln>11</span><span class=cl>
</span></span><span class=line><span class=ln>12</span><span class=cl><span class=c1>##</span>
</span></span><span class=line><span class=ln>13</span><span class=cl><span class=c1>## Update to use Glue catalog</span>
</span></span><span class=line><span class=ln>14</span><span class=cl><span class=c1>##</span>
</span></span><span class=line><span class=ln>15</span><span class=cl><span class=na>spark.sql.catalogImplementation</span>  <span class=s>hive</span>
</span></span><span class=line><span class=ln>16</span><span class=cl><span class=na>spark.hadoop.hive.metastore.client.factory.class</span>  <span class=s>com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory</span>
</span></span></code></pre></div><p>Besides, I updated the log properties so that the log level of the root logger is set to be <em>warn</em> and warning messages due to EC2 metadata access failure are not logged.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-properties data-lang=properties><span class=line><span class=ln> 1</span><span class=cl><span class=c1># dockers/spark/log4j2.properties</span>
</span></span><span class=line><span class=ln> 2</span><span class=cl>
</span></span><span class=line><span class=ln> 3</span><span class=cl><span class=err>...</span>
</span></span><span class=line><span class=ln> 4</span><span class=cl>
</span></span><span class=line><span class=ln> 5</span><span class=cl><span class=c1># Set everything to be logged to the console</span>
</span></span><span class=line><span class=ln> 6</span><span class=cl><span class=na>rootLogger.level</span> <span class=o>=</span> <span class=s>warn</span>
</span></span><span class=line><span class=ln> 7</span><span class=cl>
</span></span><span class=line><span class=ln> 8</span><span class=cl><span class=err>...</span>
</span></span><span class=line><span class=ln> 9</span><span class=cl>
</span></span><span class=line><span class=ln>10</span><span class=cl><span class=c1>## Ignore warn messages related to EC2 metadata access failure</span>
</span></span><span class=line><span class=ln>11</span><span class=cl><span class=na>logger.InstanceMetadataServiceResourceFetcher.name</span> <span class=o>=</span> <span class=s>com.amazonaws.internal.InstanceMetadataServiceResourceFetcher</span>
</span></span><span class=line><span class=ln>12</span><span class=cl><span class=na>logger.InstanceMetadataServiceResourceFetcher.level</span> <span class=o>=</span> <span class=s>fatal</span>
</span></span><span class=line><span class=ln>13</span><span class=cl><span class=na>logger.EC2MetadataUtils.name</span> <span class=o>=</span> <span class=s>com.amazonaws.util.EC2MetadataUtils</span>
</span></span><span class=line><span class=ln>14</span><span class=cl><span class=na>logger.EC2MetadataUtils.level</span> <span class=o>=</span> <span class=s>fatal</span>
</span></span></code></pre></div><h4 id=docker-compose-service data-numberify>Docker Compose Service<a class="anchor ms-1" href=#docker-compose-service></a></h4><p>Spark 3.4.1 is installed in the EMR image (<em>public.ecr.aws/emr-on-eks/spark/emr-6.15.0:latest</em>) and the Spark container is created with it. It starts the Spark History Server, which provides an interface to debug and diagnose completed and running Spark applications. Note that the server is configured to run in foreground (<em>SPARK_NO_DAEMONIZE=true</em>) in order for the container to keep alive. As mentioned, the updated Spark configuration files and the project folder are volume-mapped.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=ln> 1</span><span class=cl><span class=c># docker-compose.yml</span><span class=w>
</span></span></span><span class=line><span class=ln> 2</span><span class=cl><span class=w></span><span class=nt>version</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;3.5&#34;</span><span class=w>
</span></span></span><span class=line><span class=ln> 3</span><span class=cl><span class=w>
</span></span></span><span class=line><span class=ln> 4</span><span class=cl><span class=w></span><span class=nt>services</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=ln> 5</span><span class=cl><span class=w>
</span></span></span><span class=line><span class=ln> 6</span><span class=cl><span class=w>  </span><span class=l>...</span><span class=w>
</span></span></span><span class=line><span class=ln> 7</span><span class=cl><span class=w>
</span></span></span><span class=line><span class=ln> 8</span><span class=cl><span class=w>  </span><span class=nt>spark</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=ln> 9</span><span class=cl><span class=w>    </span><span class=nt>image</span><span class=p>:</span><span class=w> </span><span class=l>public.ecr.aws/emr-on-eks/spark/emr-6.15.0:latest</span><span class=w>
</span></span></span><span class=line><span class=ln>10</span><span class=cl><span class=w>    </span><span class=nt>container_name</span><span class=p>:</span><span class=w> </span><span class=l>spark</span><span class=w>
</span></span></span><span class=line><span class=ln>11</span><span class=cl><span class=w>    </span><span class=nt>command</span><span class=p>:</span><span class=w> </span><span class=l>/usr/lib/spark/sbin/start-history-server.sh</span><span class=w>
</span></span></span><span class=line><span class=ln>12</span><span class=cl><span class=w>    </span><span class=nt>ports</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=ln>13</span><span class=cl><span class=w>      </span>- <span class=s2>&#34;18080:18080&#34;</span><span class=w>
</span></span></span><span class=line><span class=ln>14</span><span class=cl><span class=w>    </span><span class=nt>networks</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=ln>15</span><span class=cl><span class=w>      </span>- <span class=l>appnet</span><span class=w>
</span></span></span><span class=line><span class=ln>16</span><span class=cl><span class=w>    </span><span class=nt>environment</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=ln>17</span><span class=cl><span class=w>      </span>- <span class=l>BOOTSTRAP_SERVERS=kafka-0:9092</span><span class=w>
</span></span></span><span class=line><span class=ln>18</span><span class=cl><span class=w>      </span>- <span class=l>AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID:-not_set}</span><span class=w>
</span></span></span><span class=line><span class=ln>19</span><span class=cl><span class=w>      </span>- <span class=l>AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY:-not_set}</span><span class=w>
</span></span></span><span class=line><span class=ln>20</span><span class=cl><span class=w>      </span>- <span class=l>AWS_REGION=${AWS_REGION:-not_set}</span><span class=w>
</span></span></span><span class=line><span class=ln>21</span><span class=cl><span class=w>      </span>- <span class=l>SPARK_NO_DAEMONIZE=true</span><span class=w>
</span></span></span><span class=line><span class=ln>22</span><span class=cl><span class=w>    </span><span class=nt>volumes</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=ln>23</span><span class=cl><span class=w>      </span>- <span class=l>./:/home/hadoop/project</span><span class=w>
</span></span></span><span class=line><span class=ln>24</span><span class=cl><span class=w>      </span>- <span class=l>./dockers/spark/spark-defaults.conf:/usr/lib/spark/conf/spark-defaults.conf</span><span class=w>
</span></span></span><span class=line><span class=ln>25</span><span class=cl><span class=w>      </span>- <span class=l>./dockers/spark/log4j2.properties:/usr/lib/spark/conf/log4j2.properties</span><span class=w>
</span></span></span><span class=line><span class=ln>26</span><span class=cl><span class=w>
</span></span></span><span class=line><span class=ln>27</span><span class=cl><span class=w>  </span><span class=l>...</span><span class=w>
</span></span></span><span class=line><span class=ln>28</span><span class=cl><span class=w>
</span></span></span><span class=line><span class=ln>29</span><span class=cl><span class=w></span><span class=nt>networks</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=ln>30</span><span class=cl><span class=w>  </span><span class=nt>appnet</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=ln>31</span><span class=cl><span class=w>    </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>app-network</span><span class=w>
</span></span></span><span class=line><span class=ln>32</span><span class=cl><span class=w>
</span></span></span><span class=line><span class=ln>33</span><span class=cl><span class=w>  </span><span class=l>...</span><span class=w>
</span></span></span></code></pre></div><h3 id=kafka-setup data-numberify>Kafka Setup<a class="anchor ms-1" href=#kafka-setup></a></h3><h4 id=docker-compose-services-1 data-numberify>Docker Compose Services<a class="anchor ms-1" href=#docker-compose-services-1></a></h4><p>A Kafka cluster with a single broker and zookeeper node is used in this post. The broker has two listeners and the port 9092 and 29092 are used for internal and external communication respectively. The default number of topic partitions is set to 3. More details about Kafka cluster setup can be found in <a href=/blog/2023-05-04-kafka-development-with-docker-part-1/>this post</a>.</p><p>The <a href=https://docs.kpow.io/ce/ target=_blank rel="noopener noreferrer">UI for Apache Kafka (kafka-ui)<i class="fas fa-external-link-square-alt ms-1"></i></a> is used for monitoring Kafka topics and related resources. The bootstrap server address and zookeeper access url are added as environment variables. See <a href=/blog/2023-05-18-kafka-development-with-docker-part-2/>this post</a> for details about Kafka management apps.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=ln> 1</span><span class=cl><span class=c># docker-compose.yml</span><span class=w>
</span></span></span><span class=line><span class=ln> 2</span><span class=cl><span class=w></span><span class=nt>version</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;3.5&#34;</span><span class=w>
</span></span></span><span class=line><span class=ln> 3</span><span class=cl><span class=w>
</span></span></span><span class=line><span class=ln> 4</span><span class=cl><span class=w></span><span class=nt>services</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=ln> 5</span><span class=cl><span class=w>
</span></span></span><span class=line><span class=ln> 6</span><span class=cl><span class=w>  </span><span class=l>...</span><span class=w>
</span></span></span><span class=line><span class=ln> 7</span><span class=cl><span class=w>
</span></span></span><span class=line><span class=ln> 8</span><span class=cl><span class=w>  </span><span class=nt>zookeeper</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=ln> 9</span><span class=cl><span class=w>    </span><span class=nt>image</span><span class=p>:</span><span class=w> </span><span class=l>bitnami/zookeeper:3.5</span><span class=w>
</span></span></span><span class=line><span class=ln>10</span><span class=cl><span class=w>    </span><span class=nt>container_name</span><span class=p>:</span><span class=w> </span><span class=l>zookeeper</span><span class=w>
</span></span></span><span class=line><span class=ln>11</span><span class=cl><span class=w>    </span><span class=nt>ports</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=ln>12</span><span class=cl><span class=w>      </span>- <span class=s2>&#34;2181&#34;</span><span class=w>
</span></span></span><span class=line><span class=ln>13</span><span class=cl><span class=w>    </span><span class=nt>networks</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=ln>14</span><span class=cl><span class=w>      </span>- <span class=l>appnet</span><span class=w>
</span></span></span><span class=line><span class=ln>15</span><span class=cl><span class=w>    </span><span class=nt>environment</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=ln>16</span><span class=cl><span class=w>      </span>- <span class=l>ALLOW_ANONYMOUS_LOGIN=yes</span><span class=w>
</span></span></span><span class=line><span class=ln>17</span><span class=cl><span class=w>    </span><span class=nt>volumes</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=ln>18</span><span class=cl><span class=w>      </span>- <span class=l>zookeeper_data:/bitnami/zookeeper</span><span class=w>
</span></span></span><span class=line><span class=ln>19</span><span class=cl><span class=w>  </span><span class=nt>kafka-0</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=ln>20</span><span class=cl><span class=w>    </span><span class=nt>image</span><span class=p>:</span><span class=w> </span><span class=l>bitnami/kafka:2.8.1</span><span class=w>
</span></span></span><span class=line><span class=ln>21</span><span class=cl><span class=w>    </span><span class=nt>container_name</span><span class=p>:</span><span class=w> </span><span class=l>kafka-0</span><span class=w>
</span></span></span><span class=line><span class=ln>22</span><span class=cl><span class=w>    </span><span class=nt>expose</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=ln>23</span><span class=cl><span class=w>      </span>- <span class=m>9092</span><span class=w>
</span></span></span><span class=line><span class=ln>24</span><span class=cl><span class=w>    </span><span class=nt>ports</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=ln>25</span><span class=cl><span class=w>      </span>- <span class=s2>&#34;29092:29092&#34;</span><span class=w>
</span></span></span><span class=line><span class=ln>26</span><span class=cl><span class=w>    </span><span class=nt>networks</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=ln>27</span><span class=cl><span class=w>      </span>- <span class=l>appnet</span><span class=w>
</span></span></span><span class=line><span class=ln>28</span><span class=cl><span class=w>    </span><span class=nt>environment</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=ln>29</span><span class=cl><span class=w>      </span>- <span class=l>ALLOW_PLAINTEXT_LISTENER=yes</span><span class=w>
</span></span></span><span class=line><span class=ln>30</span><span class=cl><span class=w>      </span>- <span class=l>KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181</span><span class=w>
</span></span></span><span class=line><span class=ln>31</span><span class=cl><span class=w>      </span>- <span class=l>KAFKA_CFG_BROKER_ID=0</span><span class=w>
</span></span></span><span class=line><span class=ln>32</span><span class=cl><span class=w>      </span>- <span class=l>KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT</span><span class=w>
</span></span></span><span class=line><span class=ln>33</span><span class=cl><span class=w>      </span>- <span class=l>KAFKA_CFG_LISTENERS=INTERNAL://:9092,EXTERNAL://:29092</span><span class=w>
</span></span></span><span class=line><span class=ln>34</span><span class=cl><span class=w>      </span>- <span class=l>KAFKA_CFG_ADVERTISED_LISTENERS=INTERNAL://kafka-0:9092,EXTERNAL://localhost:29092</span><span class=w>
</span></span></span><span class=line><span class=ln>35</span><span class=cl><span class=w>      </span>- <span class=l>KAFKA_CFG_INTER_BROKER_LISTENER_NAME=INTERNAL</span><span class=w>
</span></span></span><span class=line><span class=ln>36</span><span class=cl><span class=w>      </span>- <span class=l>KAFKA_CFG_NUM_PARTITIONS=3</span><span class=w>
</span></span></span><span class=line><span class=ln>37</span><span class=cl><span class=w>      </span>- <span class=l>KAFKA_CFG_DEFAULT_REPLICATION_FACTOR=1</span><span class=w>
</span></span></span><span class=line><span class=ln>38</span><span class=cl><span class=w>    </span><span class=nt>volumes</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=ln>39</span><span class=cl><span class=w>      </span>- <span class=l>kafka_0_data:/bitnami/kafka</span><span class=w>
</span></span></span><span class=line><span class=ln>40</span><span class=cl><span class=w>    </span><span class=nt>depends_on</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=ln>41</span><span class=cl><span class=w>      </span>- <span class=l>zookeeper</span><span class=w>
</span></span></span><span class=line><span class=ln>42</span><span class=cl><span class=w>  </span><span class=nt>kafka-ui</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=ln>43</span><span class=cl><span class=w>    </span><span class=nt>image</span><span class=p>:</span><span class=w> </span><span class=l>provectuslabs/kafka-ui:v0.7.1</span><span class=w>
</span></span></span><span class=line><span class=ln>44</span><span class=cl><span class=w>    </span><span class=nt>container_name</span><span class=p>:</span><span class=w> </span><span class=l>kafka-ui</span><span class=w>
</span></span></span><span class=line><span class=ln>45</span><span class=cl><span class=w>    </span><span class=nt>ports</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=ln>46</span><span class=cl><span class=w>      </span>- <span class=s2>&#34;8080:8080&#34;</span><span class=w>
</span></span></span><span class=line><span class=ln>47</span><span class=cl><span class=w>    </span><span class=nt>networks</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=ln>48</span><span class=cl><span class=w>      </span>- <span class=l>appnet</span><span class=w>
</span></span></span><span class=line><span class=ln>49</span><span class=cl><span class=w>    </span><span class=nt>environment</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=ln>50</span><span class=cl><span class=w>      </span><span class=nt>KAFKA_CLUSTERS_0_NAME</span><span class=p>:</span><span class=w> </span><span class=l>local</span><span class=w>
</span></span></span><span class=line><span class=ln>51</span><span class=cl><span class=w>      </span><span class=nt>KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS</span><span class=p>:</span><span class=w> </span><span class=l>kafka-0:9092</span><span class=w>
</span></span></span><span class=line><span class=ln>52</span><span class=cl><span class=w>      </span><span class=nt>KAFKA_CLUSTERS_0_ZOOKEEPER</span><span class=p>:</span><span class=w> </span><span class=l>zookeeper:2181</span><span class=w>
</span></span></span><span class=line><span class=ln>53</span><span class=cl><span class=w>    </span><span class=nt>depends_on</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=ln>54</span><span class=cl><span class=w>      </span>- <span class=l>zookeeper</span><span class=w>
</span></span></span><span class=line><span class=ln>55</span><span class=cl><span class=w>      </span>- <span class=l>kafka-0</span><span class=w>
</span></span></span><span class=line><span class=ln>56</span><span class=cl><span class=w>
</span></span></span><span class=line><span class=ln>57</span><span class=cl><span class=w></span><span class=nt>networks</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=ln>58</span><span class=cl><span class=w>  </span><span class=nt>appnet</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=ln>59</span><span class=cl><span class=w>    </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>app-network</span><span class=w>
</span></span></span><span class=line><span class=ln>60</span><span class=cl><span class=w>
</span></span></span><span class=line><span class=ln>61</span><span class=cl><span class=w></span><span class=nt>volumes</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=ln>62</span><span class=cl><span class=w>  </span><span class=l>...</span><span class=w>
</span></span></span><span class=line><span class=ln>63</span><span class=cl><span class=w>  </span><span class=nt>zookeeper_data</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=ln>64</span><span class=cl><span class=w>    </span><span class=nt>driver</span><span class=p>:</span><span class=w> </span><span class=l>local</span><span class=w>
</span></span></span><span class=line><span class=ln>65</span><span class=cl><span class=w>    </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>zookeeper_data</span><span class=w>
</span></span></span><span class=line><span class=ln>66</span><span class=cl><span class=w>  </span><span class=nt>kafka_0_data</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=ln>67</span><span class=cl><span class=w>    </span><span class=nt>driver</span><span class=p>:</span><span class=w> </span><span class=l>local</span><span class=w>
</span></span></span><span class=line><span class=ln>68</span><span class=cl><span class=w>    </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>kafka_0_data</span><span class=w>
</span></span></span></code></pre></div><h2 id=applications data-numberify>Applications<a class="anchor ms-1" href=#applications></a></h2><h3 id=flink-producer data-numberify>Flink Producer<a class="anchor ms-1" href=#flink-producer></a></h3><p>A PyFlink application is created for data ingesting in real time. The app begins with generating timestamps using the <a href=https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/connectors/table/datagen/ target=_blank rel="noopener noreferrer">DataGen SQL Connector<i class="fas fa-external-link-square-alt ms-1"></i></a> where three records are generated per second. Then extra values (<em>id</em> and <em>value</em>) are added to the records using Python user defined functions and the updated records are ingested into a Kafka topic named <em>orders</em>. Note that the output records are printed in the terminal additionally when the app is running locally for ease of checking them.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=ln>  1</span><span class=cl><span class=c1># apps/flink/producer.py</span>
</span></span><span class=line><span class=ln>  2</span><span class=cl><span class=kn>import</span> <span class=nn>os</span>
</span></span><span class=line><span class=ln>  3</span><span class=cl><span class=kn>import</span> <span class=nn>uuid</span>
</span></span><span class=line><span class=ln>  4</span><span class=cl><span class=kn>import</span> <span class=nn>random</span>
</span></span><span class=line><span class=ln>  5</span><span class=cl>
</span></span><span class=line><span class=ln>  6</span><span class=cl><span class=kn>from</span> <span class=nn>pyflink.datastream</span> <span class=kn>import</span> <span class=n>StreamExecutionEnvironment</span><span class=p>,</span> <span class=n>RuntimeExecutionMode</span>
</span></span><span class=line><span class=ln>  7</span><span class=cl><span class=kn>from</span> <span class=nn>pyflink.table</span> <span class=kn>import</span> <span class=n>StreamTableEnvironment</span>
</span></span><span class=line><span class=ln>  8</span><span class=cl><span class=kn>from</span> <span class=nn>pyflink.table.udf</span> <span class=kn>import</span> <span class=n>udf</span>
</span></span><span class=line><span class=ln>  9</span><span class=cl>
</span></span><span class=line><span class=ln> 10</span><span class=cl><span class=k>def</span> <span class=nf>_qry_source_table</span><span class=p>():</span>
</span></span><span class=line><span class=ln> 11</span><span class=cl>    <span class=n>stmt</span> <span class=o>=</span> <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=ln> 12</span><span class=cl><span class=s2>    CREATE TABLE seeds (
</span></span></span><span class=line><span class=ln> 13</span><span class=cl><span class=s2>        ts  AS PROCTIME()
</span></span></span><span class=line><span class=ln> 14</span><span class=cl><span class=s2>    ) WITH (
</span></span></span><span class=line><span class=ln> 15</span><span class=cl><span class=s2>        &#39;connector&#39; = &#39;datagen&#39;,
</span></span></span><span class=line><span class=ln> 16</span><span class=cl><span class=s2>        &#39;rows-per-second&#39; = &#39;3&#39;
</span></span></span><span class=line><span class=ln> 17</span><span class=cl><span class=s2>    )
</span></span></span><span class=line><span class=ln> 18</span><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=ln> 19</span><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=n>stmt</span><span class=p>)</span>
</span></span><span class=line><span class=ln> 20</span><span class=cl>    <span class=k>return</span> <span class=n>stmt</span>
</span></span><span class=line><span class=ln> 21</span><span class=cl>
</span></span><span class=line><span class=ln> 22</span><span class=cl><span class=k>def</span> <span class=nf>_qry_sink_table</span><span class=p>(</span><span class=n>table_name</span><span class=p>:</span> <span class=nb>str</span><span class=p>,</span> <span class=n>topic_name</span><span class=p>:</span> <span class=nb>str</span><span class=p>,</span> <span class=n>bootstrap_servers</span><span class=p>:</span> <span class=nb>str</span><span class=p>):</span>
</span></span><span class=line><span class=ln> 23</span><span class=cl>    <span class=n>stmt</span> <span class=o>=</span> <span class=sa>f</span><span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=ln> 24</span><span class=cl><span class=s2>    CREATE TABLE </span><span class=si>{</span><span class=n>table_name</span><span class=si>}</span><span class=s2> (
</span></span></span><span class=line><span class=ln> 25</span><span class=cl><span class=s2>        `id`        VARCHAR,
</span></span></span><span class=line><span class=ln> 26</span><span class=cl><span class=s2>        `value`     INT,
</span></span></span><span class=line><span class=ln> 27</span><span class=cl><span class=s2>        `ts`        TIMESTAMP(3)
</span></span></span><span class=line><span class=ln> 28</span><span class=cl><span class=s2>    ) WITH (
</span></span></span><span class=line><span class=ln> 29</span><span class=cl><span class=s2>        &#39;connector&#39; = &#39;kafka&#39;,
</span></span></span><span class=line><span class=ln> 30</span><span class=cl><span class=s2>        &#39;topic&#39; = &#39;</span><span class=si>{</span><span class=n>topic_name</span><span class=si>}</span><span class=s2>&#39;,
</span></span></span><span class=line><span class=ln> 31</span><span class=cl><span class=s2>        &#39;properties.bootstrap.servers&#39; = &#39;</span><span class=si>{</span><span class=n>bootstrap_servers</span><span class=si>}</span><span class=s2>&#39;,        
</span></span></span><span class=line><span class=ln> 32</span><span class=cl><span class=s2>        &#39;format&#39; = &#39;json&#39;,
</span></span></span><span class=line><span class=ln> 33</span><span class=cl><span class=s2>        &#39;key.format&#39; = &#39;json&#39;,
</span></span></span><span class=line><span class=ln> 34</span><span class=cl><span class=s2>        &#39;key.fields&#39; = &#39;id&#39;,
</span></span></span><span class=line><span class=ln> 35</span><span class=cl><span class=s2>        &#39;properties.allow.auto.create.topics&#39; = &#39;true&#39;
</span></span></span><span class=line><span class=ln> 36</span><span class=cl><span class=s2>    )
</span></span></span><span class=line><span class=ln> 37</span><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=ln> 38</span><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=n>stmt</span><span class=p>)</span>
</span></span><span class=line><span class=ln> 39</span><span class=cl>    <span class=k>return</span> <span class=n>stmt</span>
</span></span><span class=line><span class=ln> 40</span><span class=cl>
</span></span><span class=line><span class=ln> 41</span><span class=cl>
</span></span><span class=line><span class=ln> 42</span><span class=cl><span class=k>def</span> <span class=nf>_qry_print_table</span><span class=p>():</span>
</span></span><span class=line><span class=ln> 43</span><span class=cl>    <span class=n>stmt</span> <span class=o>=</span> <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=ln> 44</span><span class=cl><span class=s2>    CREATE TABLE print (
</span></span></span><span class=line><span class=ln> 45</span><span class=cl><span class=s2>        `id`        VARCHAR,
</span></span></span><span class=line><span class=ln> 46</span><span class=cl><span class=s2>        `value`     INT,
</span></span></span><span class=line><span class=ln> 47</span><span class=cl><span class=s2>        `ts`        TIMESTAMP(3)  
</span></span></span><span class=line><span class=ln> 48</span><span class=cl><span class=s2>
</span></span></span><span class=line><span class=ln> 49</span><span class=cl><span class=s2>    ) WITH (
</span></span></span><span class=line><span class=ln> 50</span><span class=cl><span class=s2>        &#39;connector&#39; = &#39;print&#39;
</span></span></span><span class=line><span class=ln> 51</span><span class=cl><span class=s2>    )
</span></span></span><span class=line><span class=ln> 52</span><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=ln> 53</span><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=n>stmt</span><span class=p>)</span>
</span></span><span class=line><span class=ln> 54</span><span class=cl>    <span class=k>return</span> <span class=n>stmt</span>
</span></span><span class=line><span class=ln> 55</span><span class=cl>
</span></span><span class=line><span class=ln> 56</span><span class=cl><span class=k>def</span> <span class=nf>_qry_insert</span><span class=p>(</span><span class=n>target_table</span><span class=p>:</span> <span class=nb>str</span><span class=p>):</span>
</span></span><span class=line><span class=ln> 57</span><span class=cl>    <span class=n>stmt</span> <span class=o>=</span> <span class=sa>f</span><span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=ln> 58</span><span class=cl><span class=s2>    INSERT INTO </span><span class=si>{</span><span class=n>target_table</span><span class=si>}</span><span class=s2>
</span></span></span><span class=line><span class=ln> 59</span><span class=cl><span class=s2>    SELECT
</span></span></span><span class=line><span class=ln> 60</span><span class=cl><span class=s2>        add_id(),
</span></span></span><span class=line><span class=ln> 61</span><span class=cl><span class=s2>        add_value(),
</span></span></span><span class=line><span class=ln> 62</span><span class=cl><span class=s2>        ts    
</span></span></span><span class=line><span class=ln> 63</span><span class=cl><span class=s2>    FROM seeds
</span></span></span><span class=line><span class=ln> 64</span><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=ln> 65</span><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=n>stmt</span><span class=p>)</span>
</span></span><span class=line><span class=ln> 66</span><span class=cl>    <span class=k>return</span> <span class=n>stmt</span>
</span></span><span class=line><span class=ln> 67</span><span class=cl>
</span></span><span class=line><span class=ln> 68</span><span class=cl><span class=k>if</span> <span class=vm>__name__</span> <span class=o>==</span> <span class=s2>&#34;__main__&#34;</span><span class=p>:</span>
</span></span><span class=line><span class=ln> 69</span><span class=cl>    <span class=n>RUNTIME_ENV</span> <span class=o>=</span> <span class=n>os</span><span class=o>.</span><span class=n>environ</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;RUNTIME_ENV&#34;</span><span class=p>,</span> <span class=s2>&#34;LOCAL&#34;</span><span class=p>)</span>  <span class=c1># DOCKER or LOCAL</span>
</span></span><span class=line><span class=ln> 70</span><span class=cl>    <span class=n>BOOTSTRAP_SERVERS</span> <span class=o>=</span> <span class=n>os</span><span class=o>.</span><span class=n>environ</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;BOOTSTRAP_SERVERS&#34;</span><span class=p>,</span> <span class=s2>&#34;localhost:29092&#34;</span><span class=p>)</span>  <span class=c1># overwrite app config</span>
</span></span><span class=line><span class=ln> 71</span><span class=cl>
</span></span><span class=line><span class=ln> 72</span><span class=cl>    <span class=n>env</span> <span class=o>=</span> <span class=n>StreamExecutionEnvironment</span><span class=o>.</span><span class=n>get_execution_environment</span><span class=p>()</span>
</span></span><span class=line><span class=ln> 73</span><span class=cl>    <span class=n>env</span><span class=o>.</span><span class=n>set_runtime_mode</span><span class=p>(</span><span class=n>RuntimeExecutionMode</span><span class=o>.</span><span class=n>STREAMING</span><span class=p>)</span>
</span></span><span class=line><span class=ln> 74</span><span class=cl>    <span class=k>if</span> <span class=n>RUNTIME_ENV</span> <span class=o>==</span> <span class=s2>&#34;LOCAL&#34;</span><span class=p>:</span>
</span></span><span class=line><span class=ln> 75</span><span class=cl>        <span class=n>SRC_DIR</span> <span class=o>=</span> <span class=n>os</span><span class=o>.</span><span class=n>path</span><span class=o>.</span><span class=n>dirname</span><span class=p>(</span><span class=n>os</span><span class=o>.</span><span class=n>path</span><span class=o>.</span><span class=n>realpath</span><span class=p>(</span><span class=vm>__file__</span><span class=p>))</span>
</span></span><span class=line><span class=ln> 76</span><span class=cl>        <span class=n>JAR_FILES</span> <span class=o>=</span> <span class=p>[</span><span class=s2>&#34;flink-sql-connector-kafka-1.17.1.jar&#34;</span><span class=p>]</span> <span class=c1># should exist where producer.py exists</span>
</span></span><span class=line><span class=ln> 77</span><span class=cl>        <span class=n>JAR_PATHS</span> <span class=o>=</span> <span class=nb>tuple</span><span class=p>(</span>
</span></span><span class=line><span class=ln> 78</span><span class=cl>            <span class=p>[</span><span class=sa>f</span><span class=s2>&#34;file://</span><span class=si>{</span><span class=n>os</span><span class=o>.</span><span class=n>path</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=n>SRC_DIR</span><span class=p>,</span> <span class=n>name</span><span class=p>)</span><span class=si>}</span><span class=s2>&#34;</span> <span class=k>for</span> <span class=n>name</span> <span class=ow>in</span> <span class=n>JAR_FILES</span><span class=p>]</span>
</span></span><span class=line><span class=ln> 79</span><span class=cl>        <span class=p>)</span>        
</span></span><span class=line><span class=ln> 80</span><span class=cl>        <span class=n>env</span><span class=o>.</span><span class=n>add_jars</span><span class=p>(</span><span class=o>*</span><span class=n>JAR_PATHS</span><span class=p>)</span>
</span></span><span class=line><span class=ln> 81</span><span class=cl>        <span class=nb>print</span><span class=p>(</span><span class=n>JAR_PATHS</span><span class=p>)</span>
</span></span><span class=line><span class=ln> 82</span><span class=cl>
</span></span><span class=line><span class=ln> 83</span><span class=cl>    <span class=n>t_env</span> <span class=o>=</span> <span class=n>StreamTableEnvironment</span><span class=o>.</span><span class=n>create</span><span class=p>(</span><span class=n>stream_execution_environment</span><span class=o>=</span><span class=n>env</span><span class=p>)</span>
</span></span><span class=line><span class=ln> 84</span><span class=cl>    <span class=n>t_env</span><span class=o>.</span><span class=n>get_config</span><span class=p>()</span><span class=o>.</span><span class=n>set_local_timezone</span><span class=p>(</span><span class=s2>&#34;Australia/Sydney&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=ln> 85</span><span class=cl>    <span class=n>t_env</span><span class=o>.</span><span class=n>create_temporary_function</span><span class=p>(</span>
</span></span><span class=line><span class=ln> 86</span><span class=cl>        <span class=s2>&#34;add_id&#34;</span><span class=p>,</span> <span class=n>udf</span><span class=p>(</span><span class=k>lambda</span><span class=p>:</span> <span class=nb>str</span><span class=p>(</span><span class=n>uuid</span><span class=o>.</span><span class=n>uuid4</span><span class=p>()),</span> <span class=n>result_type</span><span class=o>=</span><span class=s2>&#34;STRING&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=ln> 87</span><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=ln> 88</span><span class=cl>    <span class=n>t_env</span><span class=o>.</span><span class=n>create_temporary_function</span><span class=p>(</span>
</span></span><span class=line><span class=ln> 89</span><span class=cl>        <span class=s2>&#34;add_value&#34;</span><span class=p>,</span> <span class=n>udf</span><span class=p>(</span><span class=k>lambda</span><span class=p>:</span> <span class=n>random</span><span class=o>.</span><span class=n>randrange</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1000</span><span class=p>),</span> <span class=n>result_type</span><span class=o>=</span><span class=s2>&#34;INT&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=ln> 90</span><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=ln> 91</span><span class=cl>    <span class=c1>## create source and sink tables</span>
</span></span><span class=line><span class=ln> 92</span><span class=cl>    <span class=n>SINK_TABLE_NAME</span> <span class=o>=</span> <span class=s2>&#34;orders_table&#34;</span>
</span></span><span class=line><span class=ln> 93</span><span class=cl>    <span class=n>t_env</span><span class=o>.</span><span class=n>execute_sql</span><span class=p>(</span><span class=n>_qry_source_table</span><span class=p>())</span>
</span></span><span class=line><span class=ln> 94</span><span class=cl>    <span class=n>t_env</span><span class=o>.</span><span class=n>execute_sql</span><span class=p>(</span><span class=n>_qry_sink_table</span><span class=p>(</span><span class=n>SINK_TABLE_NAME</span><span class=p>,</span> <span class=s2>&#34;orders&#34;</span><span class=p>,</span> <span class=n>BOOTSTRAP_SERVERS</span><span class=p>))</span>
</span></span><span class=line><span class=ln> 95</span><span class=cl>    <span class=n>t_env</span><span class=o>.</span><span class=n>execute_sql</span><span class=p>(</span><span class=n>_qry_print_table</span><span class=p>())</span>
</span></span><span class=line><span class=ln> 96</span><span class=cl>    <span class=c1>## insert into sink table</span>
</span></span><span class=line><span class=ln> 97</span><span class=cl>    <span class=k>if</span> <span class=n>RUNTIME_ENV</span> <span class=o>==</span> <span class=s2>&#34;LOCAL&#34;</span><span class=p>:</span>
</span></span><span class=line><span class=ln> 98</span><span class=cl>        <span class=n>statement_set</span> <span class=o>=</span> <span class=n>t_env</span><span class=o>.</span><span class=n>create_statement_set</span><span class=p>()</span>
</span></span><span class=line><span class=ln> 99</span><span class=cl>        <span class=n>statement_set</span><span class=o>.</span><span class=n>add_insert_sql</span><span class=p>(</span><span class=n>_qry_insert</span><span class=p>(</span><span class=n>SINK_TABLE_NAME</span><span class=p>))</span>
</span></span><span class=line><span class=ln>100</span><span class=cl>        <span class=n>statement_set</span><span class=o>.</span><span class=n>add_insert_sql</span><span class=p>(</span><span class=n>_qry_insert</span><span class=p>(</span><span class=s2>&#34;print&#34;</span><span class=p>))</span>
</span></span><span class=line><span class=ln>101</span><span class=cl>        <span class=n>statement_set</span><span class=o>.</span><span class=n>execute</span><span class=p>()</span><span class=o>.</span><span class=n>wait</span><span class=p>()</span>
</span></span><span class=line><span class=ln>102</span><span class=cl>    <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=ln>103</span><span class=cl>        <span class=n>table_result</span> <span class=o>=</span> <span class=n>t_env</span><span class=o>.</span><span class=n>execute_sql</span><span class=p>(</span><span class=n>_qry_insert</span><span class=p>(</span><span class=n>SINK_TABLE_NAME</span><span class=p>))</span>
</span></span><span class=line><span class=ln>104</span><span class=cl>        <span class=nb>print</span><span class=p>(</span><span class=n>table_result</span><span class=o>.</span><span class=n>get_job_client</span><span class=p>()</span><span class=o>.</span><span class=n>get_job_status</span><span class=p>())</span>
</span></span></code></pre></div><p>The application can be submitted into the Flink cluster as shown below. Note that the dependent Jar files for the Kafka connector exist in the Flink library folder (<em>/usr/lib/flink/lib</em>) and we don&rsquo;t have to specify them separately.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=ln>1</span><span class=cl>docker <span class=nb>exec</span> jobmanager /usr/lib/flink/bin/flink run <span class=se>\
</span></span></span><span class=line><span class=ln>2</span><span class=cl><span class=se></span>  --python /home/flink/project/apps/flink/producer.py <span class=se>\
</span></span></span><span class=line><span class=ln>3</span><span class=cl><span class=se></span>  -d
</span></span></code></pre></div><p>Once the app runs, we can see the status of the Flink job on the Flink Web UI (<em>localhost:8081</em>).</p><p><picture><img class="img-fluid mx-auto d-block" alt src=/blog/2023-12-07-flink-spark-local-dev/flink-producer.png loading=lazy width=1314 height=913></picture></p><p>Also, we can check the topic (<em>orders</em>) is created and messages are ingested on <em>kafka-ui</em> (<em>localhost:8080</em>).</p><p><picture><img class="img-fluid mx-auto d-block" alt src=/blog/2023-12-07-flink-spark-local-dev/kafka-topic.png loading=lazy width=1315 height=498></picture></p><h3 id=flink-processor data-numberify>Flink Processor<a class="anchor ms-1" href=#flink-processor></a></h3><p>The Flink processor application is created using Flink SQL on the <a href=https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/dev/table/sqlclient/ target=_blank rel="noopener noreferrer">Flink SQL client<i class="fas fa-external-link-square-alt ms-1"></i></a>. The SQL client can be started by executing <code>docker exec -it jobmanager ./bin/sql-client.sh</code>.</p><p><picture><img class="img-fluid mx-auto d-block" alt src=/blog/2023-12-07-flink-spark-local-dev/sql-client.png loading=lazy width=642 height=773></picture></p><h4 id=source-table-in-default-catalog data-numberify>Source Table in Default Catalog<a class="anchor ms-1" href=#source-table-in-default-catalog></a></h4><p>We first create a source table that reads messages from the <em>orders</em> topic of the Kafka cluster. As the table is not necessarily be shared by other tools or services, it is created on the default catalog, not on the Glue catalog.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-sql data-lang=sql><span class=line><span class=ln> 1</span><span class=cl><span class=c1>-- apps/flink/processor.sql
</span></span></span><span class=line><span class=ln> 2</span><span class=cl><span class=c1>-- // create the source table, metadata not registered in glue datalog
</span></span></span><span class=line><span class=ln> 3</span><span class=cl><span class=c1></span><span class=k>CREATE</span><span class=w> </span><span class=k>TABLE</span><span class=w> </span><span class=n>source_tbl</span><span class=p>(</span><span class=w>
</span></span></span><span class=line><span class=ln> 4</span><span class=cl><span class=w>  </span><span class=o>`</span><span class=n>id</span><span class=o>`</span><span class=w>      </span><span class=n>STRING</span><span class=p>,</span><span class=w>
</span></span></span><span class=line><span class=ln> 5</span><span class=cl><span class=w>  </span><span class=o>`</span><span class=n>value</span><span class=o>`</span><span class=w>   </span><span class=nb>INT</span><span class=p>,</span><span class=w>
</span></span></span><span class=line><span class=ln> 6</span><span class=cl><span class=w>  </span><span class=o>`</span><span class=n>ts</span><span class=o>`</span><span class=w>      </span><span class=k>TIMESTAMP</span><span class=p>(</span><span class=mi>3</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=ln> 7</span><span class=cl><span class=w></span><span class=p>)</span><span class=w> </span><span class=k>WITH</span><span class=w> </span><span class=p>(</span><span class=w>
</span></span></span><span class=line><span class=ln> 8</span><span class=cl><span class=w>  </span><span class=s1>&#39;connector&#39;</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=s1>&#39;kafka&#39;</span><span class=p>,</span><span class=w>
</span></span></span><span class=line><span class=ln> 9</span><span class=cl><span class=w>  </span><span class=s1>&#39;topic&#39;</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=s1>&#39;orders&#39;</span><span class=p>,</span><span class=w>
</span></span></span><span class=line><span class=ln>10</span><span class=cl><span class=w>  </span><span class=s1>&#39;properties.bootstrap.servers&#39;</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=s1>&#39;kafka-0:9092&#39;</span><span class=p>,</span><span class=w>
</span></span></span><span class=line><span class=ln>11</span><span class=cl><span class=w>  </span><span class=s1>&#39;properties.group.id&#39;</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=s1>&#39;orders-source&#39;</span><span class=p>,</span><span class=w>
</span></span></span><span class=line><span class=ln>12</span><span class=cl><span class=w>  </span><span class=s1>&#39;format&#39;</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=s1>&#39;json&#39;</span><span class=p>,</span><span class=w>
</span></span></span><span class=line><span class=ln>13</span><span class=cl><span class=w>  </span><span class=s1>&#39;scan.startup.mode&#39;</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=s1>&#39;latest-offset&#39;</span><span class=w>
</span></span></span><span class=line><span class=ln>14</span><span class=cl><span class=w></span><span class=p>);</span><span class=w>
</span></span></span></code></pre></div><h4 id=sink-table-in-glue-catalog data-numberify>Sink Table in Glue Catalog<a class="anchor ms-1" href=#sink-table-in-glue-catalog></a></h4><p>We create the sink table on the Glue Data Catalog because it needs to be accessed by a Spark application. First, we create a Hive catalog named <em>glue_catalog</em> with the Hive configuration that integrates with the Glue Data Catalog. The EMR Flink image includes the required Hive configuration file, and we can specify the corresponding path in the container (<em>/glue/confs/hive/conf</em>).</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-sql data-lang=sql><span class=line><span class=ln> 1</span><span class=cl><span class=c1>-- apps/flink/processor.sql
</span></span></span><span class=line><span class=ln> 2</span><span class=cl><span class=c1>--// create a hive catalogs that integrates with the glue catalog
</span></span></span><span class=line><span class=ln> 3</span><span class=cl><span class=c1></span><span class=k>CREATE</span><span class=w> </span><span class=k>CATALOG</span><span class=w> </span><span class=n>glue_catalog</span><span class=w> </span><span class=k>WITH</span><span class=w> </span><span class=p>(</span><span class=w>
</span></span></span><span class=line><span class=ln> 4</span><span class=cl><span class=w>  </span><span class=s1>&#39;type&#39;</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=s1>&#39;hive&#39;</span><span class=p>,</span><span class=w>
</span></span></span><span class=line><span class=ln> 5</span><span class=cl><span class=w>  </span><span class=s1>&#39;default-database&#39;</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=s1>&#39;default&#39;</span><span class=p>,</span><span class=w>
</span></span></span><span class=line><span class=ln> 6</span><span class=cl><span class=w>  </span><span class=s1>&#39;hive-conf-dir&#39;</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=s1>&#39;/glue/confs/hive/conf&#39;</span><span class=w>
</span></span></span><span class=line><span class=ln> 7</span><span class=cl><span class=w></span><span class=p>);</span><span class=w>
</span></span></span><span class=line><span class=ln> 8</span><span class=cl><span class=w>
</span></span></span><span class=line><span class=ln> 9</span><span class=cl><span class=w></span><span class=c1>-- Flink SQL&gt; show catalogs;
</span></span></span><span class=line><span class=ln>10</span><span class=cl><span class=c1>-- +-----------------+
</span></span></span><span class=line><span class=ln>11</span><span class=cl><span class=c1>-- |    catalog name |
</span></span></span><span class=line><span class=ln>12</span><span class=cl><span class=c1>-- +-----------------+
</span></span></span><span class=line><span class=ln>13</span><span class=cl><span class=c1>-- | default_catalog |
</span></span></span><span class=line><span class=ln>14</span><span class=cl><span class=c1>-- |    glue_catalog |
</span></span></span><span class=line><span class=ln>15</span><span class=cl><span class=c1>-- +-----------------+
</span></span></span></code></pre></div><p>Below shows the Hive configuration file (<em>/glue/confs/hive/conf/hive-site.xml</em>). Same as the Spark configuration, <em>AWSGlueDataCatalogHiveClientFactory</em> is specified as the Hive metastore factory class, which enables to use the Glue Data Catalog as the metastore of Hive databases and tables.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-xml data-lang=xml><span class=line><span class=ln> 1</span><span class=cl><span class=nt>&lt;configuration&gt;</span>
</span></span><span class=line><span class=ln> 2</span><span class=cl>    <span class=nt>&lt;property&gt;</span>
</span></span><span class=line><span class=ln> 3</span><span class=cl>        <span class=nt>&lt;name&gt;</span>hive.metastore.client.factory.class<span class=nt>&lt;/name&gt;</span>
</span></span><span class=line><span class=ln> 4</span><span class=cl>        <span class=nt>&lt;value&gt;</span>com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory<span class=nt>&lt;/value&gt;</span>
</span></span><span class=line><span class=ln> 5</span><span class=cl>    <span class=nt>&lt;/property&gt;</span>
</span></span><span class=line><span class=ln> 6</span><span class=cl>
</span></span><span class=line><span class=ln> 7</span><span class=cl>    <span class=nt>&lt;property&gt;</span>
</span></span><span class=line><span class=ln> 8</span><span class=cl>        <span class=nt>&lt;name&gt;</span>hive.metastore.uris<span class=nt>&lt;/name&gt;</span>
</span></span><span class=line><span class=ln> 9</span><span class=cl>        <span class=nt>&lt;value&gt;</span>thrift://dummy:9083<span class=nt>&lt;/value&gt;</span>
</span></span><span class=line><span class=ln>10</span><span class=cl>    <span class=nt>&lt;/property&gt;</span>
</span></span><span class=line><span class=ln>11</span><span class=cl><span class=nt>&lt;/configuration&gt;</span>
</span></span></code></pre></div><p>Secondly, we create a database named <em>demo</em> by specifying the S3 location URI - <em>s3://demo-ap-southeast-2/warehouse/</em>.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-sql data-lang=sql><span class=line><span class=ln>1</span><span class=cl><span class=c1>-- apps/flink/processor.sql
</span></span></span><span class=line><span class=ln>2</span><span class=cl><span class=c1>-- // create a database named demo
</span></span></span><span class=line><span class=ln>3</span><span class=cl><span class=c1></span><span class=k>CREATE</span><span class=w> </span><span class=k>DATABASE</span><span class=w> </span><span class=k>IF</span><span class=w> </span><span class=k>NOT</span><span class=w> </span><span class=k>EXISTS</span><span class=w> </span><span class=n>glue_catalog</span><span class=p>.</span><span class=n>demo</span><span class=w> 
</span></span></span><span class=line><span class=ln>4</span><span class=cl><span class=w>  </span><span class=k>WITH</span><span class=w> </span><span class=p>(</span><span class=s1>&#39;hive.database.location-uri&#39;</span><span class=o>=</span><span class=w> </span><span class=s1>&#39;s3://demo-ap-southeast-2/warehouse/&#39;</span><span class=p>);</span><span class=w>
</span></span></span></code></pre></div><p>Once succeeded, we are able to see the database is created in the Glue Data Catalog.</p><p><picture><img class="img-fluid mx-auto d-block" alt src=/blog/2023-12-07-flink-spark-local-dev/glue-database.png loading=lazy width=1273 height=264></picture></p><p>Finally, we create the sink table in the Glue database. The Hive SQL dialect is used to create the table, and it is partitioned by <em>year</em>, <em>month</em>, <em>date</em> and <em>hour</em>.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-sql data-lang=sql><span class=line><span class=ln> 1</span><span class=cl><span class=c1>-- apps/flink/processor.sql
</span></span></span><span class=line><span class=ln> 2</span><span class=cl><span class=c1>-- // create the sink table using hive dialect
</span></span></span><span class=line><span class=ln> 3</span><span class=cl><span class=c1></span><span class=k>SET</span><span class=w> </span><span class=k>table</span><span class=p>.</span><span class=k>sql</span><span class=o>-</span><span class=n>dialect</span><span class=o>=</span><span class=n>hive</span><span class=p>;</span><span class=w>
</span></span></span><span class=line><span class=ln> 4</span><span class=cl><span class=w></span><span class=k>CREATE</span><span class=w> </span><span class=k>TABLE</span><span class=w> </span><span class=n>glue_catalog</span><span class=p>.</span><span class=n>demo</span><span class=p>.</span><span class=n>sink_tbl</span><span class=p>(</span><span class=w>
</span></span></span><span class=line><span class=ln> 5</span><span class=cl><span class=w>  </span><span class=o>`</span><span class=n>id</span><span class=o>`</span><span class=w>      </span><span class=n>STRING</span><span class=p>,</span><span class=w>
</span></span></span><span class=line><span class=ln> 6</span><span class=cl><span class=w>  </span><span class=o>`</span><span class=n>value</span><span class=o>`</span><span class=w>   </span><span class=nb>INT</span><span class=p>,</span><span class=w>
</span></span></span><span class=line><span class=ln> 7</span><span class=cl><span class=w>  </span><span class=o>`</span><span class=n>ts</span><span class=o>`</span><span class=w>      </span><span class=k>TIMESTAMP</span><span class=p>(</span><span class=mi>9</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=ln> 8</span><span class=cl><span class=w></span><span class=p>)</span><span class=w> 
</span></span></span><span class=line><span class=ln> 9</span><span class=cl><span class=w></span><span class=n>PARTITIONED</span><span class=w> </span><span class=k>BY</span><span class=w> </span><span class=p>(</span><span class=o>`</span><span class=k>year</span><span class=o>`</span><span class=w> </span><span class=n>STRING</span><span class=p>,</span><span class=w> </span><span class=o>`</span><span class=k>month</span><span class=o>`</span><span class=w> </span><span class=n>STRING</span><span class=p>,</span><span class=w> </span><span class=o>`</span><span class=nb>date</span><span class=o>`</span><span class=w> </span><span class=n>STRING</span><span class=p>,</span><span class=w> </span><span class=o>`</span><span class=n>hour</span><span class=o>`</span><span class=w> </span><span class=n>STRING</span><span class=p>)</span><span class=w> 
</span></span></span><span class=line><span class=ln>10</span><span class=cl><span class=w></span><span class=n>STORED</span><span class=w> </span><span class=k>AS</span><span class=w> </span><span class=n>parquet</span><span class=w> 
</span></span></span><span class=line><span class=ln>11</span><span class=cl><span class=w></span><span class=n>TBLPROPERTIES</span><span class=w> </span><span class=p>(</span><span class=w>
</span></span></span><span class=line><span class=ln>12</span><span class=cl><span class=w>  </span><span class=s1>&#39;partition.time-extractor.timestamp-pattern&#39;</span><span class=o>=</span><span class=s1>&#39;$year-$month-$date $hour:00:00&#39;</span><span class=p>,</span><span class=w>
</span></span></span><span class=line><span class=ln>13</span><span class=cl><span class=w>  </span><span class=s1>&#39;sink.partition-commit.trigger&#39;</span><span class=o>=</span><span class=s1>&#39;partition-time&#39;</span><span class=p>,</span><span class=w>
</span></span></span><span class=line><span class=ln>14</span><span class=cl><span class=w>  </span><span class=s1>&#39;sink.partition-commit.delay&#39;</span><span class=o>=</span><span class=s1>&#39;1 h&#39;</span><span class=p>,</span><span class=w>
</span></span></span><span class=line><span class=ln>15</span><span class=cl><span class=w>  </span><span class=s1>&#39;sink.partition-commit.policy.kind&#39;</span><span class=o>=</span><span class=s1>&#39;metastore,success-file&#39;</span><span class=w>
</span></span></span><span class=line><span class=ln>16</span><span class=cl><span class=w></span><span class=p>);</span><span class=w>
</span></span></span></code></pre></div><p>We can check the sink table is created in the Glue database on AWS Console as shown below.</p><p><picture><img class="img-fluid mx-auto d-block" alt src=/blog/2023-12-07-flink-spark-local-dev/glue-table.png loading=lazy width=1277 height=827></picture></p><h4 id=flink-job data-numberify>Flink Job<a class="anchor ms-1" href=#flink-job></a></h4><p>The Flink processor job gets submitted when we execute the <em>INSERT</em> statement on the SQL client. Note that the checkpoint interval is set to 60 seconds to simplify monitoring, and it is expected that a new file is added per minute.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-sql data-lang=sql><span class=line><span class=ln> 1</span><span class=cl><span class=c1>-- apps/flink/processor.sql
</span></span></span><span class=line><span class=ln> 2</span><span class=cl><span class=c1></span><span class=k>SET</span><span class=w> </span><span class=s1>&#39;state.checkpoints.dir&#39;</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=s1>&#39;file:///tmp/checkpoints/&#39;</span><span class=p>;</span><span class=w>
</span></span></span><span class=line><span class=ln> 3</span><span class=cl><span class=w></span><span class=k>SET</span><span class=w> </span><span class=s1>&#39;execution.checkpointing.interval&#39;</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=s1>&#39;60000&#39;</span><span class=p>;</span><span class=w>
</span></span></span><span class=line><span class=ln> 4</span><span class=cl><span class=w>
</span></span></span><span class=line><span class=ln> 5</span><span class=cl><span class=w></span><span class=k>SET</span><span class=w> </span><span class=k>table</span><span class=p>.</span><span class=k>sql</span><span class=o>-</span><span class=n>dialect</span><span class=o>=</span><span class=n>hive</span><span class=p>;</span><span class=w>
</span></span></span><span class=line><span class=ln> 6</span><span class=cl><span class=w></span><span class=c1>-- // insert into the sink table
</span></span></span><span class=line><span class=ln> 7</span><span class=cl><span class=c1></span><span class=k>INSERT</span><span class=w> </span><span class=k>INTO</span><span class=w> </span><span class=k>TABLE</span><span class=w> </span><span class=n>glue_catalog</span><span class=p>.</span><span class=n>demo</span><span class=p>.</span><span class=n>sink_tbl</span><span class=w>
</span></span></span><span class=line><span class=ln> 8</span><span class=cl><span class=w></span><span class=k>SELECT</span><span class=w> 
</span></span></span><span class=line><span class=ln> 9</span><span class=cl><span class=w>  </span><span class=o>`</span><span class=n>id</span><span class=o>`</span><span class=p>,</span><span class=w> 
</span></span></span><span class=line><span class=ln>10</span><span class=cl><span class=w>  </span><span class=o>`</span><span class=n>value</span><span class=o>`</span><span class=p>,</span><span class=w> 
</span></span></span><span class=line><span class=ln>11</span><span class=cl><span class=w>  </span><span class=o>`</span><span class=n>ts</span><span class=o>`</span><span class=p>,</span><span class=w>
</span></span></span><span class=line><span class=ln>12</span><span class=cl><span class=w>  </span><span class=n>DATE_FORMAT</span><span class=p>(</span><span class=o>`</span><span class=n>ts</span><span class=o>`</span><span class=p>,</span><span class=w> </span><span class=s1>&#39;yyyy&#39;</span><span class=p>)</span><span class=w> </span><span class=k>AS</span><span class=w> </span><span class=o>`</span><span class=k>year</span><span class=o>`</span><span class=p>,</span><span class=w>
</span></span></span><span class=line><span class=ln>13</span><span class=cl><span class=w>  </span><span class=n>DATE_FORMAT</span><span class=p>(</span><span class=o>`</span><span class=n>ts</span><span class=o>`</span><span class=p>,</span><span class=w> </span><span class=s1>&#39;MM&#39;</span><span class=p>)</span><span class=w> </span><span class=k>AS</span><span class=w> </span><span class=o>`</span><span class=k>month</span><span class=o>`</span><span class=p>,</span><span class=w>
</span></span></span><span class=line><span class=ln>14</span><span class=cl><span class=w>  </span><span class=n>DATE_FORMAT</span><span class=p>(</span><span class=o>`</span><span class=n>ts</span><span class=o>`</span><span class=p>,</span><span class=w> </span><span class=s1>&#39;dd&#39;</span><span class=p>)</span><span class=w> </span><span class=k>AS</span><span class=w> </span><span class=o>`</span><span class=nb>date</span><span class=o>`</span><span class=p>,</span><span class=w>
</span></span></span><span class=line><span class=ln>15</span><span class=cl><span class=w>  </span><span class=n>DATE_FORMAT</span><span class=p>(</span><span class=o>`</span><span class=n>ts</span><span class=o>`</span><span class=p>,</span><span class=w> </span><span class=s1>&#39;HH&#39;</span><span class=p>)</span><span class=w> </span><span class=k>AS</span><span class=w> </span><span class=o>`</span><span class=n>hour</span><span class=o>`</span><span class=w>
</span></span></span><span class=line><span class=ln>16</span><span class=cl><span class=w></span><span class=k>FROM</span><span class=w> </span><span class=n>source_tbl</span><span class=p>;</span><span class=w>
</span></span></span></code></pre></div><p>Once the Flink app is submitted, we can check the Flink job on the Flink Web UI (<em>localhost:8081</em>).</p><p><picture><img class="img-fluid mx-auto d-block" alt src=/blog/2023-12-07-flink-spark-local-dev/flink-processor.png loading=lazy width=1313 height=896></picture></p><p>As expected, the output files are written into S3 in Apache Hive style partitions, and they are created in one minute interval.</p><p><picture><img class="img-fluid mx-auto d-block" alt src=/blog/2023-12-07-flink-spark-local-dev/s3-objects.png loading=lazy width=1237 height=762></picture></p><h3 id=spark-consumer data-numberify>Spark Consumer<a class="anchor ms-1" href=#spark-consumer></a></h3><p>A simple PySpark application is created to query the output table. Note that, as the partitions are not added dynamically, <a href=https://spark.apache.org/docs/3.0.0-preview/sql-ref-syntax-ddl-repair-table.html target=_blank rel="noopener noreferrer"><em>MSCK REPAIR TABLE</em><i class="fas fa-external-link-square-alt ms-1"></i></a> command is executed before querying the table.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=ln>1</span><span class=cl><span class=c1># apps/spark/consumer.py</span>
</span></span><span class=line><span class=ln>2</span><span class=cl><span class=kn>from</span> <span class=nn>pyspark.sql</span> <span class=kn>import</span> <span class=n>SparkSession</span>
</span></span><span class=line><span class=ln>3</span><span class=cl>
</span></span><span class=line><span class=ln>4</span><span class=cl><span class=k>if</span> <span class=vm>__name__</span> <span class=o>==</span> <span class=s2>&#34;__main__&#34;</span><span class=p>:</span>
</span></span><span class=line><span class=ln>5</span><span class=cl>    <span class=n>spark</span> <span class=o>=</span> <span class=n>SparkSession</span><span class=o>.</span><span class=n>builder</span><span class=o>.</span><span class=n>appName</span><span class=p>(</span><span class=s2>&#34;Consume Orders&#34;</span><span class=p>)</span><span class=o>.</span><span class=n>getOrCreate</span><span class=p>()</span>
</span></span><span class=line><span class=ln>6</span><span class=cl>    <span class=n>spark</span><span class=o>.</span><span class=n>sparkContext</span><span class=o>.</span><span class=n>setLogLevel</span><span class=p>(</span><span class=s2>&#34;FATAL&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=ln>7</span><span class=cl>    <span class=n>spark</span><span class=o>.</span><span class=n>sql</span><span class=p>(</span><span class=s2>&#34;MSCK REPAIR TABLE demo.sink_tbl&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=ln>8</span><span class=cl>    <span class=n>spark</span><span class=o>.</span><span class=n>sql</span><span class=p>(</span><span class=s2>&#34;SELECT * FROM demo.sink_tbl&#34;</span><span class=p>)</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</span></span></code></pre></div><p>The Spark app can be submitted as shown below. Note that the application is accessible in the container because the project folder is volume-mapped to the container folder (<em>/home/hadoop/project</em>).</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=ln>1</span><span class=cl><span class=c1>## spark submit</span>
</span></span><span class=line><span class=ln>2</span><span class=cl>docker <span class=nb>exec</span> spark spark-submit <span class=se>\
</span></span></span><span class=line><span class=ln>3</span><span class=cl><span class=se></span>    --master local<span class=o>[</span>*<span class=o>]</span> <span class=se>\
</span></span></span><span class=line><span class=ln>4</span><span class=cl><span class=se></span>    --deploy-mode client <span class=se>\
</span></span></span><span class=line><span class=ln>5</span><span class=cl><span class=se></span>    /home/hadoop/project/apps/spark/consumer.py
</span></span></code></pre></div><p>The app queries the output table successfully and shows the result as expected.</p><p><picture><img class="img-fluid mx-auto d-block" alt src=/blog/2023-12-07-flink-spark-local-dev/spark-consumer.png loading=lazy width=1497 height=525></picture></p><p>We can check the performance of the Spark application on the Spark History Server (<em>localhost:18080</em>).</p><p><picture><img class="img-fluid mx-auto d-block" alt src=/blog/2023-12-07-flink-spark-local-dev/spark-history-server.png loading=lazy width=1265 height=754></picture></p><h2 id=summary data-numberify>Summary<a class="anchor ms-1" href=#summary></a></h2><p>In this post, we discussed how to set up a local development environment for Apache Flink and Spark using the EMR container images. For the former, a custom Docker image was created, which downloads dependent connector Jar files into the Flink library folder, fixes process startup issues, and updates Hadoop configurations for Glue Data Catalog integration. For the latter, instead of creating a custom image, the EMR image was used to launch the Spark container where the required configuration updates are added at runtime via volume-mapping. After illustrating the environment setup, we discussed a solution where data ingestion/processing is performed in real time using Apache Flink and the processed data is consumed by Apache Spark for analysis.</p></div></div></div><div class=card-footer><div class="post-navs d-flex justify-content-evenly"><div class="post-nav post-prev"><i class="fas fa-fw fa-chevron-down post-prev-icon me-1" data-fa-transform=rotate-90></i>
<a href=/blog/2023-11-30-real-time-streaming-with-kafka-and-flink-6/>Real Time Streaming With Kafka and Flink - Lab 5 Write Data to DynamoDB Using Kafka Connect</a></div><div class="post-nav post-next"><a href=/blog/2023-12-14-real-time-streaming-with-kafka-and-flink-7/>Real Time Streaming With Kafka and Flink - Lab 6 Consume Data From Kafka Using Lambda</a>
<i class="fas fa-fw fa-chevron-down post-next-icon ms-1" data-fa-transform=rotate-270></i></div></div></div></article><section class="related-posts row card component"><div class=card-header><h2 class="card-title fs-4 my-2 text-surface">Related Posts</h2></div><div class="card-body slide px-1"><div class="slide-inner row gx-0"><div class="col-12 col-md-6 col-lg-4 me-2"><div class="post-card card card-body p-0 border-0 surface"><picture><source srcset=/blog/2023-11-23-real-time-streaming-with-kafka-and-flink-5/featured_hu934f3fa21a75bb8ccc480bfe067ce5a2_112340_500x0_resize_box_3.png media="(max-width: 576px)" height=301 width=500><img class=img-fluid height=108 width=180 alt=featured.png src=/blog/2023-11-23-real-time-streaming-with-kafka-and-flink-5/featured_hu934f3fa21a75bb8ccc480bfe067ce5a2_112340_180x0_resize_box_3.png data-src=/blog/2023-11-23-real-time-streaming-with-kafka-and-flink-5/featured.png loading=lazy>
</picture><a class=post-title href=/blog/2023-11-23-real-time-streaming-with-kafka-and-flink-5/>Real Time Streaming With Kafka and Flink - Lab 4 Clean, Aggregate, and Enrich Events With Flink</a><div class="post-meta mb-0">November 23, 2023</div></div></div><div class="col-12 col-md-6 col-lg-4 me-2"><div class="post-card card card-body p-0 border-0 surface"><picture><source srcset=/blog/2023-11-16-real-time-streaming-with-kafka-and-flink-4/featured_hua0f43666abbaff872f0e4bf2da3a3c1a_160359_500x0_resize_box_3.png media="(max-width: 576px)" height=301 width=500><img class=img-fluid height=108 width=180 alt=featured.png src=/blog/2023-11-16-real-time-streaming-with-kafka-and-flink-4/featured_hua0f43666abbaff872f0e4bf2da3a3c1a_160359_180x0_resize_box_3.png data-src=/blog/2023-11-16-real-time-streaming-with-kafka-and-flink-4/featured.png loading=lazy>
</picture><a class=post-title href=/blog/2023-11-16-real-time-streaming-with-kafka-and-flink-4/>Real Time Streaming With Kafka and Flink - Lab 3 Transform and Write Data to S3 From Kafka Using Flink</a><div class="post-meta mb-0">November 16, 2023</div></div></div><div class="col-12 col-md-6 col-lg-4 me-2"><div class="post-card card card-body p-0 border-0 surface"><picture><source srcset=/blog/2023-11-09-real-time-streaming-with-kafka-and-flink-3/featured_hucc8053169eb3042cf56156294e716152_139114_500x0_resize_box_3.png media="(max-width: 576px)" height=301 width=500><img class=img-fluid height=108 width=180 alt=featured.png src=/blog/2023-11-09-real-time-streaming-with-kafka-and-flink-3/featured_hucc8053169eb3042cf56156294e716152_139114_180x0_resize_box_3.png data-src=/blog/2023-11-09-real-time-streaming-with-kafka-and-flink-3/featured.png loading=lazy>
</picture><a class=post-title href=/blog/2023-11-09-real-time-streaming-with-kafka-and-flink-3/>Real Time Streaming With Kafka and Flink - Lab 2 Write Data to Kafka From S3 Using Flink</a><div class="post-meta mb-0">November 9, 2023</div></div></div><div class="col-12 col-md-6 col-lg-4 me-2"><div class="post-card card card-body p-0 border-0 surface"><picture><source srcset=/blog/2023-10-19-build-pyflink-apps/featured_hubbcf607e326a86791dcaeb23a1bfd08e_154736_500x0_resize_box_3.png media="(max-width: 576px)" height=241 width=500><img class=img-fluid height=87 width=180 alt=featured.png src=/blog/2023-10-19-build-pyflink-apps/featured_hubbcf607e326a86791dcaeb23a1bfd08e_154736_180x0_resize_box_3.png data-src=/blog/2023-10-19-build-pyflink-apps/featured.png loading=lazy>
</picture><a class=post-title href=/blog/2023-10-19-build-pyflink-apps/>Building Apache Flink Applications in Python</a><div class="post-meta mb-0">October 19, 2023</div></div></div><div class="col-12 col-md-6 col-lg-4 me-2"><div class="post-card card card-body p-0 border-0 surface"><picture><source srcset=/blog/2023-10-05-real-time-streaming-with-kafka-and-flink-1/featured_hua45c2e579ce8f6bc0318d15d62654514_138141_500x0_resize_box_3.png media="(max-width: 576px)" height=299 width=500><img class=img-fluid height=108 width=180 alt=featured.png src=/blog/2023-10-05-real-time-streaming-with-kafka-and-flink-1/featured_hua45c2e579ce8f6bc0318d15d62654514_138141_180x0_resize_box_3.png data-src=/blog/2023-10-05-real-time-streaming-with-kafka-and-flink-1/featured.png loading=lazy>
</picture><a class=post-title href=/blog/2023-10-05-real-time-streaming-with-kafka-and-flink-1/>Real Time Streaming With Kafka and Flink - Introduction</a><div class="post-meta mb-0">October 5, 2023</div></div></div><div class="col-12 col-md-6 col-lg-4 me-2"><div class="post-card card card-body p-0 border-0 surface"><picture><source srcset=/blog/2023-09-04-getting-started-with-pyflink-on-aws-part-3/featured_hue908d327a4e776f04c90ce3d9151d268_74618_500x0_resize_box_3.png media="(max-width: 576px)" height=256 width=500><img class=img-fluid height=92 width=180 alt=featured.png src=/blog/2023-09-04-getting-started-with-pyflink-on-aws-part-3/featured_hue908d327a4e776f04c90ce3d9151d268_74618_180x0_resize_box_3.png data-src=/blog/2023-09-04-getting-started-with-pyflink-on-aws-part-3/featured.png loading=lazy>
</picture><a class=post-title href=/blog/2023-09-04-getting-started-with-pyflink-on-aws-part-3/>Getting Started With Pyflink on AWS - Part 3 AWS Managed Flink and MSK</a><div class="post-meta mb-0">September 4, 2023</div></div></div><div class="col-12 col-md-6 col-lg-4 me-2"><div class="post-card card card-body p-0 border-0 surface"><picture><source srcset=/blog/2023-08-28-getting-started-with-pyflink-on-aws-part-2/featured_huff907a4f1fb46be4b7ff165486006a38_64005_500x0_resize_box_3.png media="(max-width: 576px)" height=328 width=500><img class=img-fluid height=118 width=180 alt=featured.png src=/blog/2023-08-28-getting-started-with-pyflink-on-aws-part-2/featured_huff907a4f1fb46be4b7ff165486006a38_64005_180x0_resize_box_3.png data-src=/blog/2023-08-28-getting-started-with-pyflink-on-aws-part-2/featured.png loading=lazy>
</picture><a class=post-title href=/blog/2023-08-28-getting-started-with-pyflink-on-aws-part-2/>Getting Started With Pyflink on AWS - Part 2 Local Flink and MSK</a><div class="post-meta mb-0">August 28, 2023</div></div></div><div class="col-12 col-md-6 col-lg-4 me-2"><div class="post-card card card-body p-0 border-0 surface"><picture><source srcset=/blog/2023-08-17-getting-started-with-pyflink-on-aws-part-1/featured_hu6470c0101df93a224de5159e363e8665_55960_500x0_resize_box_3.png media="(max-width: 576px)" height=289 width=500><img class=img-fluid height=104 width=180 alt=featured.png src=/blog/2023-08-17-getting-started-with-pyflink-on-aws-part-1/featured_hu6470c0101df93a224de5159e363e8665_55960_180x0_resize_box_3.png data-src=/blog/2023-08-17-getting-started-with-pyflink-on-aws-part-1/featured.png loading=lazy>
</picture><a class=post-title href=/blog/2023-08-17-getting-started-with-pyflink-on-aws-part-1/>Getting Started With Pyflink on AWS - Part 1 Local Flink and Local Kafka</a><div class="post-meta mb-0">August 17, 2023</div></div></div><div class="col-12 col-md-6 col-lg-4 me-2"><div class="post-card card card-body p-0 border-0 surface"><picture><source srcset=/blog/2023-08-10-fraud-detection-part-1/featured_hu0e6881a0b5c39bbfe5c0b72138bca889_72929_500x0_resize_box_3.png media="(max-width: 576px)" height=213 width=500><img class=img-fluid height=77 width=180 alt=featured.png src=/blog/2023-08-10-fraud-detection-part-1/featured_hu0e6881a0b5c39bbfe5c0b72138bca889_72929_180x0_resize_box_3.png data-src=/blog/2023-08-10-fraud-detection-part-1/featured.png loading=lazy>
</picture><a class=post-title href=/blog/2023-08-10-fraud-detection-part-1/>Kafka, Flink and DynamoDB for Real Time Fraud Detection - Part 1 Local Development</a><div class="post-meta mb-0">August 10, 2023</div></div></div><div class="col-12 col-md-6 col-lg-4 me-2"><div class="post-card card card-body p-0 border-0 surface"><picture><source srcset=/blog/2021-12-05-datalake-demo-part1/featured_hua25eccd3824300d3b1ed87f56797248c_164526_500x0_resize_box_3.png media="(max-width: 576px)" height=229 width=500><img class=img-fluid height=83 width=180 alt=featured.png src=/blog/2021-12-05-datalake-demo-part1/featured_hua25eccd3824300d3b1ed87f56797248c_164526_180x0_resize_box_3.png data-src=/blog/2021-12-05-datalake-demo-part1/featured.png loading=lazy>
</picture><a class=post-title href=/blog/2021-12-05-datalake-demo-part1/>Data Lake Demo Using Change Data Capture (CDC) on AWS – Part 1 Local Development</a><div class="post-meta mb-0">December 5, 2021</div></div></div></div><button class=slide-control-left>
<i class="fas fa-2x fa-chevron-circle-down" data-fa-transform=rotate-90></i>
<span class=visually-hidden>Left</span>
</button>
<button class=slide-control-right>
<i class="fas fa-2x fa-chevron-circle-down" data-fa-transform=rotate-270></i>
<span class=visually-hidden>Right</span></button></div></section><div class="card component row post-comments" id=post-comments><div class=card-header><h2 class="card-title my-2 fs-4 text-surface">Comments</h2></div><div class=card-body><div class=giscus></div></div></div></div></div><aside class="col-xxl-4 sidebar d-flex"><div class="container d-flex flex-column"><div class="accordion profile"><div class="accordion-item card row text-center component"><div class="accordion-header card-header border-0" id=profile-header><a class="accordion-button d-lg-none mb-2 shadow-none p-0 bg-transparent text-surface" role=button data-bs-toggle=collapse href=#profile aria-expanded=true aria-controls=profile>Profile</a></div><div class="card-body collapse accordion-collapse accordion-body d-lg-block show" id=profile aria-labelledby=profile-header><div class="col-12 d-flex align-items-center justify-content-center"><picture><img class="profile-avatar rounded-circle" alt="Jaehyeon Kim" src="https://jaehyeon.me/images/profile.png?v=def64c7ed1711a25afd88e36e3927ac7" loading=lazy data-viewer-invisible width=200 height=200></picture></div><div class="col-12 profile-meta"><div class="profile-name fw-fold fs-lg">Jaehyeon Kim</div><div class=profile-bio>Data Engineer 💡 Blogger ⚡ Data Streaming Enthusiast ☁ AWS Community Builder</div></div><nav class="social-links nav justify-content-center mt-1 justify-content-around"><a class="nav-link social-link" href=mailto:dottami@gmail.com title=Email><i class="fas fa-fw fa-2x fa-envelope" style=color:#0963ac></i>
</a><a class="nav-link social-link" target=_blank href=https://github.com/jaehyeon-kim title=GitHub rel=me><i class="fa-fw fa-2x fab fa-github"></i>
</a><a class="nav-link social-link" target=_blank href=https://linkedin.com/in/jaehyeon-kim-76b93429 title=LinkedIn rel=me><i class="fa-fw fa-2x fab fa-linkedin-in" style=color:#0a66c2></i>
</a><a class="nav-link social-link" target=_blank href=https://www.paypal.com/paypalme/dottami title=PayPal rel=me><i class="fa-fw fa-2x fab fa-paypal"></i></a></nav></div></div></div><div class="accordion taxonomies-toggle"><div class="row card component accordion-item"><div class="accordion-header card-header border-0"><a class="accordion-button d-lg-none mb-1 shadow-none p-0 bg-transparent" role=button data-bs-toggle=collapse href=#taxonomies-toggle aria-expanded=true aria-controls=taxonomies-toggle>Taxonomies</a></div><div class="card-body collapse accordion-collapse accordion-body d-lg-block show" id=taxonomies-toggle><ul class="nav nav-pills nav-fill" role=tablist><li class=nav-item role=presentation><button class="nav-link active" id=taxonomyCategoriesTab data-bs-toggle=tab data-bs-target=#taxonomyCategories type=button role=tab aria-controls=taxonomyCategories aria-selected=true>
Categories</button></li><li class=nav-item role=presentation><button class=nav-link id=taxonomyTagsTab data-bs-toggle=tab data-bs-target=#taxonomyTags type=button role=tab aria-controls=taxonomyTags aria-selected=true>
Tags</button></li><li class=nav-item role=presentation><button class=nav-link id=taxonomySeriesTab data-bs-toggle=tab data-bs-target=#taxonomySeries type=button role=tab aria-controls=taxonomySeries aria-selected=true>
Series</button></li><li class=nav-item role=presentation><button class=nav-link id=taxonomyArchivesTab data-bs-toggle=tab data-bs-target=#taxonomyArchives type=button role=tab aria-controls=taxonomyArchives aria-selected=true>
Archives</button></li></ul><div class="tab-content mt-3"><div class="tab-pane active" id=taxonomyCategories role=tabpanel aria-labelledby=taxonomyCategoriesTab tabindex=0><a href=/categories/data-streaming/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-category me-2 mb-2" title="Data Streaming">Data Streaming
<span class="badge badge-sm text-secondary bg-white ms-1">25</span>
</a><a href=/categories/data-engineering/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-category me-2 mb-2" title="Data Engineering">Data Engineering
<span class="badge badge-sm text-secondary bg-white ms-1">24</span>
</a><a href=/categories/development/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-category me-2 mb-2" title=Development>Development
<span class="badge badge-sm text-secondary bg-white ms-1">19</span>
</a><a href=/categories/r/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-category me-2 mb-2" title=R>R
<span class="badge badge-sm text-secondary bg-white ms-1">15</span>
</a><a href=/categories/apache-kafka/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-category me-2 mb-2" title="Apache Kafka">Apache Kafka
<span class="badge badge-sm text-secondary bg-white ms-1">14</span>
</a><a href=/categories/machine-learning/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-category me-2 mb-2" title="Machine Learning">Machine Learning
<span class="badge badge-sm text-secondary bg-white ms-1">7</span>
</a><a href=/categories/general/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-category me-2 mb-2" title=General>General
<span class="badge badge-sm text-secondary bg-white ms-1">5</span>
</a><a href=/categories/apache-beam/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-category me-2 mb-2" title="Apache Beam">Apache Beam
<span class="badge badge-sm text-secondary bg-white ms-1">2</span></a></div><div class=tab-pane id=taxonomyTags role=tabpanel aria-labelledby=taxonomyTagsTab tabindex=0><a href=/tags/docker/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-tag me-2 mb-2" title=Docker>Docker
<span class="badge badge-sm text-secondary bg-white ms-1">51</span>
</a><a href=/tags/python/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-tag me-2 mb-2" title=Python>Python
<span class="badge badge-sm text-secondary bg-white ms-1">47</span>
</a><a href=/tags/apache-kafka/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-tag me-2 mb-2" title="Apache Kafka">Apache Kafka
<span class="badge badge-sm text-secondary bg-white ms-1">44</span>
</a><a href=/tags/aws/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-tag me-2 mb-2" title=AWS>AWS
<span class="badge badge-sm text-secondary bg-white ms-1">43</span>
</a><a href=/tags/docker-compose/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-tag me-2 mb-2" title="Docker Compose">Docker Compose
<span class="badge badge-sm text-secondary bg-white ms-1">43</span>
</a><a href=/tags/r/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-tag me-2 mb-2" title=R>R
<span class="badge badge-sm text-secondary bg-white ms-1">36</span>
</a><a href=/tags/amazon-msk/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-tag me-2 mb-2" title="Amazon MSK">Amazon MSK
<span class="badge badge-sm text-secondary bg-white ms-1">21</span>
</a><a href=/tags/apache-spark/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-tag me-2 mb-2" title="Apache Spark">Apache Spark
<span class="badge badge-sm text-secondary bg-white ms-1">17</span>
</a><a href=/tags/kafka-connect/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-tag me-2 mb-2" title="Kafka Connect">Kafka Connect
<span class="badge badge-sm text-secondary bg-white ms-1">17</span>
</a><a href=/tags/aws-lambda/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-tag me-2 mb-2" title="AWS Lambda">AWS Lambda
<span class="badge badge-sm text-secondary bg-white ms-1">15</span>
</a><a href=/tags/apache-flink/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-tag me-2 mb-2" title="Apache Flink">Apache Flink
<span class="badge badge-sm text-secondary bg-white ms-1">14</span>
</a><a href=/tags/terraform/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-tag me-2 mb-2" title=Terraform>Terraform
<span class="badge badge-sm text-secondary bg-white ms-1">14</span>
</a><a href=/tags/amazon-emr/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-tag me-2 mb-2" title="Amazon EMR">Amazon EMR
<span class="badge badge-sm text-secondary bg-white ms-1">11</span>
</a><a href=/tags/data-build-tool-dbt/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-tag me-2 mb-2" title="Data Build Tool (DBT)">Data Build Tool (DBT)
<span class="badge badge-sm text-secondary bg-white ms-1">11</span>
</a><a href=/tags/pyflink/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-tag me-2 mb-2" title=PyFlink>PyFlink
<span class="badge badge-sm text-secondary bg-white ms-1">11</span>
</a><a href=/tags/amazon-msk-connect/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-tag me-2 mb-2" title="Amazon MSK Connect">Amazon MSK Connect
<span class="badge badge-sm text-secondary bg-white ms-1">9</span>
</a><a href=/tags/kubernetes/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-tag me-2 mb-2" title=Kubernetes>Kubernetes
<span class="badge badge-sm text-secondary bg-white ms-1">8</span>
</a><a href=/tags/amazon-athena/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-tag me-2 mb-2" title="Amazon Athena">Amazon Athena
<span class="badge badge-sm text-secondary bg-white ms-1">6</span>
</a><a href=/tags/pyspark/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-tag me-2 mb-2" title=PySpark>PySpark
<span class="badge badge-sm text-secondary bg-white ms-1">6</span>
</a><a href=/tags/amazon-dynamodb/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-tag me-2 mb-2" title="Amazon DynamoDB">Amazon DynamoDB
<span class="badge badge-sm text-secondary bg-white ms-1">5</span>
</a><a href=/tags/apache-airflow/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-tag me-2 mb-2" title="Apache Airflow">Apache Airflow
<span class="badge badge-sm text-secondary bg-white ms-1">5</span>
</a><a href=/tags/rserve/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-tag me-2 mb-2" title=Rserve>Rserve
<span class="badge badge-sm text-secondary bg-white ms-1">5</span>
</a><a href=/tags/visual-studio-code/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-tag me-2 mb-2" title="Visual Studio Code">Visual Studio Code
<span class="badge badge-sm text-secondary bg-white ms-1">5</span>
</a><a href=/tags/amazon-api-gateway/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-tag me-2 mb-2" title="Amazon API Gateway">Amazon API Gateway
<span class="badge badge-sm text-secondary bg-white ms-1">4</span>
</a><a href=/tags/amazon-managed-flink/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-tag me-2 mb-2" title="Amazon Managed Flink">Amazon Managed Flink
<span class="badge badge-sm text-secondary bg-white ms-1">4</span>
</a><a href=/tags/amazon-managed-service-for-apache-flink/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-tag me-2 mb-2" title="Amazon Managed Service for Apache Flink">Amazon Managed Service for Apache Flink
<span class="badge badge-sm text-secondary bg-white ms-1">4</span>
</a><a href=/tags/amazon-quicksight/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-tag me-2 mb-2" title="Amazon QuickSight">Amazon QuickSight
<span class="badge badge-sm text-secondary bg-white ms-1">4</span>
</a><a href=/tags/aws-glue/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-tag me-2 mb-2" title="AWS Glue">AWS Glue
<span class="badge badge-sm text-secondary bg-white ms-1">4</span>
</a><a href=/tags/minikube/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-tag me-2 mb-2" title=Minikube>Minikube
<span class="badge badge-sm text-secondary bg-white ms-1">4</span>
</a><a href=/tags/opensearch/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-tag me-2 mb-2" title=OpenSearch>OpenSearch
<span class="badge badge-sm text-secondary bg-white ms-1">4</span>
</a><a href=/tags/security/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-tag me-2 mb-2" title=Security>Security
<span class="badge badge-sm text-secondary bg-white ms-1">4</span>
</a><a href=/tags/shiny/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-tag me-2 mb-2" title=Shiny>Shiny
<span class="badge badge-sm text-secondary bg-white ms-1">4</span>
</a><a href=/tags/amazon-eks/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-tag me-2 mb-2" title="Amazon EKS">Amazon EKS
<span class="badge badge-sm text-secondary bg-white ms-1">3</span>
</a><a href=/tags/amazon-opensearch-service/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-tag me-2 mb-2" title="Amazon OpenSearch Service">Amazon OpenSearch Service
<span class="badge badge-sm text-secondary bg-white ms-1">3</span>
</a><a href=/tags/amazon-s3/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-tag me-2 mb-2" title="Amazon S3">Amazon S3
<span class="badge badge-sm text-secondary bg-white ms-1">3</span>
</a><a href=/tags/apache-camel/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-tag me-2 mb-2" title="Apache Camel">Apache Camel
<span class="badge badge-sm text-secondary bg-white ms-1">3</span>
</a><a href=/tags/apache-hudi/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-tag me-2 mb-2" title="Apache Hudi">Apache Hudi
<span class="badge badge-sm text-secondary bg-white ms-1">3</span>
</a><a href=/tags/apache-iceberg/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-tag me-2 mb-2" title="Apache Iceberg">Apache Iceberg
<span class="badge badge-sm text-secondary bg-white ms-1">3</span>
</a><a href=/tags/aws-sam/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-tag me-2 mb-2" title="AWS SAM">AWS SAM
<span class="badge badge-sm text-secondary bg-white ms-1">3</span>
</a><a href=/tags/change-data-capture/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-tag me-2 mb-2" title="Change Data Capture">Change Data Capture
<span class="badge badge-sm text-secondary bg-white ms-1">3</span>
</a><a href=https://jaehyeon.me/tags class="btn btn-sm btn-secondary post-taxonomy ps-3 post-tag me-2 mb-2" title=ALL>ALL
<span class="badge badge-sm text-secondary bg-white ms-1">97</span></a></div><div class=tab-pane id=taxonomySeries role=tabpanel aria-labelledby=taxonomySeriesTab tabindex=0><a href=/series/kafka-development-with-docker/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-series me-2 mb-2" title="Kafka Development With Docker">Kafka Development With Docker
<span class="badge badge-sm text-secondary bg-white ms-1">11</span>
</a><a href=/series/real-time-streaming-with-kafka-and-flink/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-series me-2 mb-2" title="Real Time Streaming With Kafka and Flink">Real Time Streaming With Kafka and Flink
<span class="badge badge-sm text-secondary bg-white ms-1">7</span>
</a><a href=/series/dbt-pizza-shop-demo/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-series me-2 mb-2" title="DBT Pizza Shop Demo">DBT Pizza Shop Demo
<span class="badge badge-sm text-secondary bg-white ms-1">6</span>
</a><a href=/series/tree-based-methods-in-r/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-series me-2 mb-2" title="Tree Based Methods in R">Tree Based Methods in R
<span class="badge badge-sm text-secondary bg-white ms-1">6</span>
</a><a href=/series/dbt-for-effective-data-transformation-on-aws/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-series me-2 mb-2" title="DBT for Effective Data Transformation on AWS">DBT for Effective Data Transformation on AWS
<span class="badge badge-sm text-secondary bg-white ms-1">5</span>
</a><a href=/series/kafka-connect-for-aws-services-integration/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-series me-2 mb-2" title="Kafka Connect for AWS Services Integration">Kafka Connect for AWS Services Integration
<span class="badge badge-sm text-secondary bg-white ms-1">5</span>
</a><a href=/series/serverless-data-product/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-series me-2 mb-2" title="Serverless Data Product">Serverless Data Product
<span class="badge badge-sm text-secondary bg-white ms-1">4</span>
</a><a href=/series/data-lake-demo-using-change-data-capture/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-series me-2 mb-2" title="Data Lake Demo Using Change Data Capture">Data Lake Demo Using Change Data Capture
<span class="badge badge-sm text-secondary bg-white ms-1">3</span>
</a><a href=/series/getting-started-with-pyflink-on-aws/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-series me-2 mb-2" title="Getting Started With Pyflink on AWS">Getting Started With Pyflink on AWS
<span class="badge badge-sm text-secondary bg-white ms-1">3</span>
</a><a href=/series/kafka-development-on-kubernetes/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-series me-2 mb-2" title="Kafka Development on Kubernetes">Kafka Development on Kubernetes
<span class="badge badge-sm text-secondary bg-white ms-1">3</span>
</a><a href=/series/parallel-processing-on-single-machine/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-series me-2 mb-2" title="Parallel Processing on Single Machine">Parallel Processing on Single Machine
<span class="badge badge-sm text-secondary bg-white ms-1">3</span>
</a><a href=/series/apache-beam-local-development-with-python/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-series me-2 mb-2" title="Apache Beam Local Development With Python">Apache Beam Local Development With Python
<span class="badge badge-sm text-secondary bg-white ms-1">2</span>
</a><a href=/series/api-development-with-r/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-series me-2 mb-2" title="API Development With R">API Development With R
<span class="badge badge-sm text-secondary bg-white ms-1">2</span>
</a><a href=/series/integrate-schema-registry-with-msk-connect/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-series me-2 mb-2" title="Integrate Schema Registry With MSK Connect">Integrate Schema Registry With MSK Connect
<span class="badge badge-sm text-secondary bg-white ms-1">2</span>
</a><a href=/series/kafka-flink-and-dynamodb-for-real-time-fraud-detection/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-series me-2 mb-2" title="Kafka, Flink and DynamoDB for Real Time Fraud Detection">Kafka, Flink and DynamoDB for Real Time Fraud Detection
<span class="badge badge-sm text-secondary bg-white ms-1">2</span>
</a><a href=/series/simplify-streaming-ingestion-on-aws/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-series me-2 mb-2" title="Simplify Streaming Ingestion on AWS">Simplify Streaming Ingestion on AWS
<span class="badge badge-sm text-secondary bg-white ms-1">2</span></a></div><div class=tab-pane id=taxonomyArchives role=tabpanel aria-labelledby=taxonomyArchivesTab tabindex=0><a href=/archives/2024/ class="btn btn-sm btn-secondary post-taxonomy ps-3 me-2 mb-2" title=2024>2024 <span class="badge badge-sm text-secondary bg-white ms-1">10</span>
</a><a href=/archives/2023/ class="btn btn-sm btn-secondary post-taxonomy ps-3 me-2 mb-2" title=2023>2023 <span class="badge badge-sm text-secondary bg-white ms-1">39</span>
</a><a href=/archives/2022/ class="btn btn-sm btn-secondary post-taxonomy ps-3 me-2 mb-2" title=2022>2022 <span class="badge badge-sm text-secondary bg-white ms-1">15</span>
</a><a href=/archives/2021/ class="btn btn-sm btn-secondary post-taxonomy ps-3 me-2 mb-2" title=2021>2021 <span class="badge badge-sm text-secondary bg-white ms-1">7</span>
</a><a href=/archives/2020/ class="btn btn-sm btn-secondary post-taxonomy ps-3 me-2 mb-2" title=2020>2020 <span class="badge badge-sm text-secondary bg-white ms-1">1</span>
</a><a href=/archives/2019/ class="btn btn-sm btn-secondary post-taxonomy ps-3 me-2 mb-2" title=2019>2019 <span class="badge badge-sm text-secondary bg-white ms-1">5</span>
</a><a href=/archives/2018/ class="btn btn-sm btn-secondary post-taxonomy ps-3 me-2 mb-2" title=2018>2018 <span class="badge badge-sm text-secondary bg-white ms-1">2</span>
</a><a href=/archives/2017/ class="btn btn-sm btn-secondary post-taxonomy ps-3 me-2 mb-2" title=2017>2017 <span class="badge badge-sm text-secondary bg-white ms-1">6</span>
</a><a href=/archives/2016/ class="btn btn-sm btn-secondary post-taxonomy ps-3 me-2 mb-2" title=2016>2016 <span class="badge badge-sm text-secondary bg-white ms-1">6</span>
</a><a href=/archives/2015/ class="btn btn-sm btn-secondary post-taxonomy ps-3 me-2 mb-2" title=2015>2015 <span class="badge badge-sm text-secondary bg-white ms-1">15</span>
</a><a href=/archives/2014/ class="btn btn-sm btn-secondary post-taxonomy ps-3 me-2 mb-2" title=2014>2014 <span class="badge badge-sm text-secondary bg-white ms-1">5</span></a></div></div></div></div></div><div class="accordion posts-toggle"><div class="row card component accordion-item"><div class="accordion-header card-header border-0"><a class="accordion-button d-lg-none mb-1 shadow-none p-0 bg-transparent" role=button data-bs-toggle=collapse href=#posts-toggle aria-expanded=true aria-controls=posts-toggle>Posts</a></div><div class="card-body collapse accordion-collapse accordion-body d-lg-block show" id=posts-toggle><ul class="nav nav-pills nav-fill" role=tablist><li class=nav-item role=presentation><button class="nav-link active" id=featured-posts-tab data-bs-toggle=tab data-bs-target=#featured-posts type=button role=tab aria-controls=featured-posts aria-selected=true>
Featured Posts</button></li><li class=nav-item role=presentation><button class=nav-link id=recent-posts-tab data-bs-toggle=tab data-bs-target=#recent-posts type=button role=tab aria-controls=recent-posts aria-selected=true>
Recent Posts</button></li></ul><div class="tab-content mt-3"><div class="tab-pane active" id=featured-posts role=tabpanel aria-labelledby=featured-posts-tab tabindex=0><ul class="post-list list-unstyled ms-1"><li class=mb-2><div class=d-flex><div class="flex-shrink-0 d-flex justify-content-center align-items-center" style=max-width:100px><picture><source srcset=/blog/2024-04-04-beam-local-dev-2/featured_huc60febc0cd72955054b95c3bf20650a7_55405_500x0_resize_box_3.png media="(max-width: 576px)" height=330 width=500><img class=img-fluid height=119 width=180 alt=featured.png src=/blog/2024-04-04-beam-local-dev-2/featured_huc60febc0cd72955054b95c3bf20650a7_55405_180x0_resize_box_3.png data-src=/blog/2024-04-04-beam-local-dev-2/featured.png loading=lazy></picture></div><div class="flex-grow-1 d-flex flex-column h-auto justify-content-center ms-3"><a class=post-title href=/blog/2024-04-04-beam-local-dev-2/>Apache Beam Local Development With Python - Part 2 Batch Pipelines</a><div class="post-meta mt-2"><span class=post-date>April 4, 2024</span></div></div></div></li><li class=mb-2><div class=d-flex><div class="flex-shrink-0 d-flex justify-content-center align-items-center" style=max-width:100px><picture><source srcset=/blog/2024-03-28-beam-local-dev-1/featured_hu8df3c1065468b257d73fe4ccb72b4c47_88260_500x0_resize_box_3.png media="(max-width: 576px)" height=338 width=500><img class=img-fluid height=122 width=180 alt=featured.png src=/blog/2024-03-28-beam-local-dev-1/featured_hu8df3c1065468b257d73fe4ccb72b4c47_88260_180x0_resize_box_3.png data-src=/blog/2024-03-28-beam-local-dev-1/featured.png loading=lazy></picture></div><div class="flex-grow-1 d-flex flex-column h-auto justify-content-center ms-3"><a class=post-title href=/blog/2024-03-28-beam-local-dev-1/>Apache Beam Local Development With Python - Part 1 Pipeline, Notebook, SQL and DataFrame</a><div class="post-meta mt-2"><span class=post-date>March 28, 2024</span></div></div></div></li><li class=mb-2><div class=d-flex><div class="flex-shrink-0 d-flex justify-content-center align-items-center" style=max-width:100px><picture><source srcset=/blog/2024-03-14-dbt-pizza-shop-6/featured_hu72fe52c30196d9a511f344ff548f2036_82921_500x0_resize_box_3.png media="(max-width: 576px)" height=256 width=500><img class=img-fluid height=92 width=180 alt=featured.png src=/blog/2024-03-14-dbt-pizza-shop-6/featured_hu72fe52c30196d9a511f344ff548f2036_82921_180x0_resize_box_3.png data-src=/blog/2024-03-14-dbt-pizza-shop-6/featured.png loading=lazy></picture></div><div class="flex-grow-1 d-flex flex-column h-auto justify-content-center ms-3"><a class=post-title href=/blog/2024-03-14-dbt-pizza-shop-6/>Data Build Tool (Dbt) Pizza Shop Demo - Part 6 ETL on Amazon Athena via Airflow</a><div class="post-meta mt-2"><span class=post-date>March 14, 2024</span></div></div></div></li><li class=mb-2><div class=d-flex><div class="flex-shrink-0 d-flex justify-content-center align-items-center" style=max-width:100px><picture><source srcset=/blog/2024-03-07-dbt-pizza-shop-5/featured_hu5822ed54c6b6e6cff286a9a560a4b806_61499_500x0_resize_box_3.png media="(max-width: 576px)" height=187 width=500><img class=img-fluid height=67 width=180 alt=featured.png src=/blog/2024-03-07-dbt-pizza-shop-5/featured_hu5822ed54c6b6e6cff286a9a560a4b806_61499_180x0_resize_box_3.png data-src=/blog/2024-03-07-dbt-pizza-shop-5/featured.png loading=lazy></picture></div><div class="flex-grow-1 d-flex flex-column h-auto justify-content-center ms-3"><a class=post-title href=/blog/2024-03-07-dbt-pizza-shop-5/>Data Build Tool (Dbt) Pizza Shop Demo - Part 5 Modelling on Amazon Athena</a><div class="post-meta mt-2"><span class=post-date>March 7, 2024</span></div></div></div></li><li class=mb-2><div class=d-flex><div class="flex-shrink-0 d-flex justify-content-center align-items-center" style=max-width:100px><picture><source srcset=/blog/2024-02-22-dbt-pizza-shop-4/featured_hu487187112b89f2568f433199221f000a_89588_500x0_resize_box_3.png media="(max-width: 576px)" height=327 width=500><img class=img-fluid height=118 width=180 alt=featured.png src=/blog/2024-02-22-dbt-pizza-shop-4/featured_hu487187112b89f2568f433199221f000a_89588_180x0_resize_box_3.png data-src=/blog/2024-02-22-dbt-pizza-shop-4/featured.png loading=lazy></picture></div><div class="flex-grow-1 d-flex flex-column h-auto justify-content-center ms-3"><a class=post-title href=/blog/2024-02-22-dbt-pizza-shop-4/>Data Build Tool (Dbt) Pizza Shop Demo - Part 4 ETL on BigQuery via Airflow</a><div class="post-meta mt-2"><span class=post-date>February 22, 2024</span></div></div></div></li><li class=mb-2><div class=d-flex><div class="flex-shrink-0 d-flex justify-content-center align-items-center" style=max-width:100px><picture><source srcset=/blog/2024-02-08-dbt-pizza-shop-3/featured_hu5082aedceaa438015dc31206ac8f27c2_70297_500x0_resize_box_3.png media="(max-width: 576px)" height=241 width=500><img class=img-fluid height=87 width=180 alt=featured.png src=/blog/2024-02-08-dbt-pizza-shop-3/featured_hu5082aedceaa438015dc31206ac8f27c2_70297_180x0_resize_box_3.png data-src=/blog/2024-02-08-dbt-pizza-shop-3/featured.png loading=lazy></picture></div><div class="flex-grow-1 d-flex flex-column h-auto justify-content-center ms-3"><a class=post-title href=/blog/2024-02-08-dbt-pizza-shop-3/>Data Build Tool (Dbt) Pizza Shop Demo - Part 3 Modelling on BigQuery</a><div class="post-meta mt-2"><span class=post-date>February 8, 2024</span></div></div></div></li><li class=mb-2><div class=d-flex><div class="flex-shrink-0 d-flex justify-content-center align-items-center" style=max-width:100px><picture><source srcset=/blog/2024-01-25-dbt-pizza-shop-2/featured_hu32c9a8b2df36b3a1704029fc725738dd_77355_500x0_resize_box_3.png media="(max-width: 576px)" height=328 width=500><img class=img-fluid height=118 width=180 alt=featured.png src=/blog/2024-01-25-dbt-pizza-shop-2/featured_hu32c9a8b2df36b3a1704029fc725738dd_77355_180x0_resize_box_3.png data-src=/blog/2024-01-25-dbt-pizza-shop-2/featured.png loading=lazy></picture></div><div class="flex-grow-1 d-flex flex-column h-auto justify-content-center ms-3"><a class=post-title href=/blog/2024-01-25-dbt-pizza-shop-2/>Data Build Tool (Dbt) Pizza Shop Demo - Part 2 ETL on PostgreSQL via Airflow</a><div class="post-meta mt-2"><span class=post-date>January 25, 2024</span></div></div></div></li><li class=mb-2><div class=d-flex><div class="flex-shrink-0 d-flex justify-content-center align-items-center" style=max-width:100px><picture><source srcset=/blog/2024-01-18-dbt-pizza-shop-1/featured_hua8fc52ecfe83e5abaf1872cc948e583f_85093_500x0_resize_box_3.png media="(max-width: 576px)" height=238 width=500><img class=img-fluid height=86 width=180 alt=featured.png src=/blog/2024-01-18-dbt-pizza-shop-1/featured_hua8fc52ecfe83e5abaf1872cc948e583f_85093_180x0_resize_box_3.png data-src=/blog/2024-01-18-dbt-pizza-shop-1/featured.png loading=lazy></picture></div><div class="flex-grow-1 d-flex flex-column h-auto justify-content-center ms-3"><a class=post-title href=/blog/2024-01-18-dbt-pizza-shop-1/>Data Build Tool (Dbt) Pizza Shop Demo - Part 1 Modelling on PostgreSQL</a><div class="post-meta mt-2"><span class=post-date>January 18, 2024</span></div></div></div></li><li class=mb-2><div class=d-flex><div class="flex-shrink-0 d-flex justify-content-center align-items-center" style=max-width:100px><picture><source srcset=/blog/2023-12-21-kafka-development-on-k8s-part-1/featured_hu216964ff6c7a4996550e95c3e0e5f209_108975_500x0_resize_box_3.png media="(max-width: 576px)" height=263 width=500><img class=img-fluid height=95 width=180 alt=featured.png src=/blog/2023-12-21-kafka-development-on-k8s-part-1/featured_hu216964ff6c7a4996550e95c3e0e5f209_108975_180x0_resize_box_3.png data-src=/blog/2023-12-21-kafka-development-on-k8s-part-1/featured.png loading=lazy></picture></div><div class="flex-grow-1 d-flex flex-column h-auto justify-content-center ms-3"><a class=post-title href=/blog/2023-12-21-kafka-development-on-k8s-part-1/>Kafka Development on Kubernetes - Part 1 Cluster Setup</a><div class="post-meta mt-2"><span class=post-date>December 21, 2023</span></div></div></div></li><li class=mb-2><div class=d-flex><div class="flex-shrink-0 d-flex justify-content-center align-items-center" style=max-width:100px><picture><source srcset=/blog/2023-12-07-flink-spark-local-dev/featured_hu7a73c8ca7d8b8a4ddf49d1a9b8fe40bd_133053_500x0_resize_box_3.png media="(max-width: 576px)" height=373 width=500><img class=img-fluid height=134 width=180 alt=featured.png src=/blog/2023-12-07-flink-spark-local-dev/featured_hu7a73c8ca7d8b8a4ddf49d1a9b8fe40bd_133053_180x0_resize_box_3.png data-src=/blog/2023-12-07-flink-spark-local-dev/featured.png loading=lazy></picture></div><div class="flex-grow-1 d-flex flex-column h-auto justify-content-center ms-3"><a class=post-title href=/blog/2023-12-07-flink-spark-local-dev/>Setup Local Development Environment for Apache Flink and Spark Using EMR Container Images</a><div class="post-meta mt-2"><span class=post-date>December 7, 2023</span></div></div></div></li></ul></div><div class=tab-pane id=recent-posts role=tabpanel aria-labelledby=recent-posts-tab tabindex=0><ul class="post-list list-unstyled ms-1"><li class=mb-2><div class=d-flex><div class="flex-shrink-0 d-flex justify-content-center align-items-center" style=max-width:100px><picture><source srcset=/blog/2024-04-04-beam-local-dev-2/featured_huc60febc0cd72955054b95c3bf20650a7_55405_500x0_resize_box_3.png media="(max-width: 576px)" height=330 width=500><img class=img-fluid height=119 width=180 alt=featured.png src=/blog/2024-04-04-beam-local-dev-2/featured_huc60febc0cd72955054b95c3bf20650a7_55405_180x0_resize_box_3.png data-src=/blog/2024-04-04-beam-local-dev-2/featured.png loading=lazy></picture></div><div class="flex-grow-1 d-flex flex-column h-auto justify-content-center ms-3"><a class=post-title href=/blog/2024-04-04-beam-local-dev-2/>Apache Beam Local Development With Python - Part 2 Batch Pipelines</a><div class="post-meta mt-2"><span class=post-date>April 4, 2024</span></div></div></div></li><li class=mb-2><div class=d-flex><div class="flex-shrink-0 d-flex justify-content-center align-items-center" style=max-width:100px><picture><source srcset=/blog/2024-03-28-beam-local-dev-1/featured_hu8df3c1065468b257d73fe4ccb72b4c47_88260_500x0_resize_box_3.png media="(max-width: 576px)" height=338 width=500><img class=img-fluid height=122 width=180 alt=featured.png src=/blog/2024-03-28-beam-local-dev-1/featured_hu8df3c1065468b257d73fe4ccb72b4c47_88260_180x0_resize_box_3.png data-src=/blog/2024-03-28-beam-local-dev-1/featured.png loading=lazy></picture></div><div class="flex-grow-1 d-flex flex-column h-auto justify-content-center ms-3"><a class=post-title href=/blog/2024-03-28-beam-local-dev-1/>Apache Beam Local Development With Python - Part 1 Pipeline, Notebook, SQL and DataFrame</a><div class="post-meta mt-2"><span class=post-date>March 28, 2024</span></div></div></div></li><li class=mb-2><div class=d-flex><div class="flex-shrink-0 d-flex justify-content-center align-items-center" style=max-width:100px><picture><source srcset=/blog/2024-03-14-dbt-pizza-shop-6/featured_hu72fe52c30196d9a511f344ff548f2036_82921_500x0_resize_box_3.png media="(max-width: 576px)" height=256 width=500><img class=img-fluid height=92 width=180 alt=featured.png src=/blog/2024-03-14-dbt-pizza-shop-6/featured_hu72fe52c30196d9a511f344ff548f2036_82921_180x0_resize_box_3.png data-src=/blog/2024-03-14-dbt-pizza-shop-6/featured.png loading=lazy></picture></div><div class="flex-grow-1 d-flex flex-column h-auto justify-content-center ms-3"><a class=post-title href=/blog/2024-03-14-dbt-pizza-shop-6/>Data Build Tool (Dbt) Pizza Shop Demo - Part 6 ETL on Amazon Athena via Airflow</a><div class="post-meta mt-2"><span class=post-date>March 14, 2024</span></div></div></div></li><li class=mb-2><div class=d-flex><div class="flex-shrink-0 d-flex justify-content-center align-items-center" style=max-width:100px><picture><source srcset=/blog/2024-03-07-dbt-pizza-shop-5/featured_hu5822ed54c6b6e6cff286a9a560a4b806_61499_500x0_resize_box_3.png media="(max-width: 576px)" height=187 width=500><img class=img-fluid height=67 width=180 alt=featured.png src=/blog/2024-03-07-dbt-pizza-shop-5/featured_hu5822ed54c6b6e6cff286a9a560a4b806_61499_180x0_resize_box_3.png data-src=/blog/2024-03-07-dbt-pizza-shop-5/featured.png loading=lazy></picture></div><div class="flex-grow-1 d-flex flex-column h-auto justify-content-center ms-3"><a class=post-title href=/blog/2024-03-07-dbt-pizza-shop-5/>Data Build Tool (Dbt) Pizza Shop Demo - Part 5 Modelling on Amazon Athena</a><div class="post-meta mt-2"><span class=post-date>March 7, 2024</span></div></div></div></li><li class=mb-2><div class=d-flex><div class="flex-shrink-0 d-flex justify-content-center align-items-center" style=max-width:100px><picture><source srcset=/blog/2024-02-22-dbt-pizza-shop-4/featured_hu487187112b89f2568f433199221f000a_89588_500x0_resize_box_3.png media="(max-width: 576px)" height=327 width=500><img class=img-fluid height=118 width=180 alt=featured.png src=/blog/2024-02-22-dbt-pizza-shop-4/featured_hu487187112b89f2568f433199221f000a_89588_180x0_resize_box_3.png data-src=/blog/2024-02-22-dbt-pizza-shop-4/featured.png loading=lazy></picture></div><div class="flex-grow-1 d-flex flex-column h-auto justify-content-center ms-3"><a class=post-title href=/blog/2024-02-22-dbt-pizza-shop-4/>Data Build Tool (Dbt) Pizza Shop Demo - Part 4 ETL on BigQuery via Airflow</a><div class="post-meta mt-2"><span class=post-date>February 22, 2024</span></div></div></div></li><li class=mb-2><div class=d-flex><div class="flex-shrink-0 d-flex justify-content-center align-items-center" style=max-width:100px><picture><source srcset=/blog/2024-02-08-dbt-pizza-shop-3/featured_hu5082aedceaa438015dc31206ac8f27c2_70297_500x0_resize_box_3.png media="(max-width: 576px)" height=241 width=500><img class=img-fluid height=87 width=180 alt=featured.png src=/blog/2024-02-08-dbt-pizza-shop-3/featured_hu5082aedceaa438015dc31206ac8f27c2_70297_180x0_resize_box_3.png data-src=/blog/2024-02-08-dbt-pizza-shop-3/featured.png loading=lazy></picture></div><div class="flex-grow-1 d-flex flex-column h-auto justify-content-center ms-3"><a class=post-title href=/blog/2024-02-08-dbt-pizza-shop-3/>Data Build Tool (Dbt) Pizza Shop Demo - Part 3 Modelling on BigQuery</a><div class="post-meta mt-2"><span class=post-date>February 8, 2024</span></div></div></div></li><li class=mb-2><div class=d-flex><div class="flex-shrink-0 d-flex justify-content-center align-items-center" style=max-width:100px><picture><source srcset=/blog/2024-01-25-dbt-pizza-shop-2/featured_hu32c9a8b2df36b3a1704029fc725738dd_77355_500x0_resize_box_3.png media="(max-width: 576px)" height=328 width=500><img class=img-fluid height=118 width=180 alt=featured.png src=/blog/2024-01-25-dbt-pizza-shop-2/featured_hu32c9a8b2df36b3a1704029fc725738dd_77355_180x0_resize_box_3.png data-src=/blog/2024-01-25-dbt-pizza-shop-2/featured.png loading=lazy></picture></div><div class="flex-grow-1 d-flex flex-column h-auto justify-content-center ms-3"><a class=post-title href=/blog/2024-01-25-dbt-pizza-shop-2/>Data Build Tool (Dbt) Pizza Shop Demo - Part 2 ETL on PostgreSQL via Airflow</a><div class="post-meta mt-2"><span class=post-date>January 25, 2024</span></div></div></div></li><li class=mb-2><div class=d-flex><div class="flex-shrink-0 d-flex justify-content-center align-items-center" style=max-width:100px><picture><source srcset=/blog/2024-01-18-dbt-pizza-shop-1/featured_hua8fc52ecfe83e5abaf1872cc948e583f_85093_500x0_resize_box_3.png media="(max-width: 576px)" height=238 width=500><img class=img-fluid height=86 width=180 alt=featured.png src=/blog/2024-01-18-dbt-pizza-shop-1/featured_hua8fc52ecfe83e5abaf1872cc948e583f_85093_180x0_resize_box_3.png data-src=/blog/2024-01-18-dbt-pizza-shop-1/featured.png loading=lazy></picture></div><div class="flex-grow-1 d-flex flex-column h-auto justify-content-center ms-3"><a class=post-title href=/blog/2024-01-18-dbt-pizza-shop-1/>Data Build Tool (Dbt) Pizza Shop Demo - Part 1 Modelling on PostgreSQL</a><div class="post-meta mt-2"><span class=post-date>January 18, 2024</span></div></div></div></li><li class=mb-2><div class=d-flex><div class="flex-shrink-0 d-flex justify-content-center align-items-center" style=max-width:100px><picture><source srcset=/blog/2024-01-11-kafka-development-on-k8s-part-3/featured_hu95d566cb28d7f91b2d6baddb7d8bb440_97270_500x0_resize_box_3.png media="(max-width: 576px)" height=216 width=500><img class=img-fluid height=78 width=180 alt=featured.png src=/blog/2024-01-11-kafka-development-on-k8s-part-3/featured_hu95d566cb28d7f91b2d6baddb7d8bb440_97270_180x0_resize_box_3.png data-src=/blog/2024-01-11-kafka-development-on-k8s-part-3/featured.png loading=lazy></picture></div><div class="flex-grow-1 d-flex flex-column h-auto justify-content-center ms-3"><a class=post-title href=/blog/2024-01-11-kafka-development-on-k8s-part-3/>Kafka Development on Kubernetes - Part 3 Kafka Connect</a><div class="post-meta mt-2"><span class=post-date>January 11, 2024</span></div></div></div></li><li class=mb-2><div class=d-flex><div class="flex-shrink-0 d-flex justify-content-center align-items-center" style=max-width:100px><picture><source srcset=/blog/2024-01-04-kafka-development-on-k8s-part-2/featured_hu866886d7082b1c38bf0c33066b35072f_75889_500x0_resize_box_3.png media="(max-width: 576px)" height=263 width=500><img class=img-fluid height=95 width=180 alt=featured.png src=/blog/2024-01-04-kafka-development-on-k8s-part-2/featured_hu866886d7082b1c38bf0c33066b35072f_75889_180x0_resize_box_3.png data-src=/blog/2024-01-04-kafka-development-on-k8s-part-2/featured.png loading=lazy></picture></div><div class="flex-grow-1 d-flex flex-column h-auto justify-content-center ms-3"><a class=post-title href=/blog/2024-01-04-kafka-development-on-k8s-part-2/>Kafka Development on Kubernetes - Part 2 Producer and Consumer</a><div class="post-meta mt-2"><span class=post-date>January 4, 2024</span></div></div></div></li></ul></div></div></div></div></div></div></aside></div></main><footer class="footer mt-auto py-3 text-center container"><div class="offcanvas offcanvas-bottom h-auto" tabindex=-1 id=offcanvasActionsPanel aria-labelledby=offcanvasActionsPanelLabel><div class=offcanvas-header><div class="offcanvas-title h5" id=offcanvasActionsPanelLabel><i class="fas fa-fw fa-th-large me-1"></i>
Actions</div><button type=button class="btn-close ms-auto" data-bs-dismiss=offcanvas data-bs-target=offcanvasActionsPanel aria-label=Close></button></div><div class="offcanvas-body mt-2"><div class="actions d-flex overflow-auto align-items-center"><a role=button class="action action-go-back d-flex flex-column align-items-center me-3" href="javascript: window.history.back();"><span class="action-icon mb-2"><i class="fas fa-2x fa-chevron-circle-down" data-fa-transform=rotate-90></i></span> Go back
</a><a role=button class="action action-reload-page d-flex flex-column align-items-center me-3"><span class="action-icon mb-2"><i class="fas fa-2x fa-redo-alt"></i></span> Reload
</a><a role=button class="action action-copy-url d-flex flex-column align-items-center me-3"><span class="action-icon mb-2"><i class="fas fa-2x fa-link"></i></span> Copy URL</a></div></div></div><div class="row text-center"><div class="col-12 mt-2"><p class=mb-2>Jaehyeon Kim</p><p class="text-secondary mb-2"><small>Data Engineer 💡 Blogger ⚡ Data Streaming Enthusiast ☁ AWS Community Builder</small></p><div class="copyright mb-2 text-secondary"><small>Copyright © 2023-2024 Jaehyeon Kim. All Rights Reserved.</small></div><nav class="social-links nav justify-content-center mb-2 mt-3"><a class="nav-link social-link p-0 me-1 mb-2" target=_blank href=/index.xml title=RSS rel=me><i class="fas fa-fw fa-2x fa-rss" style=color:#ea6221></i></a></nav></div><div class="col-12 col-lg-8 offset-0 offset-lg-1"></div></div></footer><script data-precache src=/assets/main/bundle.min.0ec8d79b95bd8d39246804f325aa5ba906dd898c752c661cb355262f78ecadcb.js integrity="sha256-DsjXm5W9jTkkaATzJapbqQbdiYx1LGYcs1UmL3jsrcs=" crossorigin=anonymous async></script><script data-precache src=/assets/icons/bundle.min.4f30d5267a9f2f9d45ed93969d45ec626100a969a8b91f71f753315a261e9034.js integrity="sha256-TzDVJnqfL51F7ZOWnUXsYmEAqWmouR9x91MxWiYekDQ=" crossorigin=anonymous defer></script><script data-precache src=/assets/viewer/bundle.min.06371891cfe6d10d36cba465c61c4d7cb17591a3be2fd9af4a38444d2074e709.js integrity="sha256-BjcYkc/m0Q02y6RlxhxNfLF1kaO+L9mvSjhETSB05wk=" crossorigin=anonymous defer></script><script data-precache defer src=/assets/katex/bundle.min.1a8cd88028fe1b600e81e2e780a8d8c388134c8237d5da89efed53ddfa28216e.js integrity="sha256-GozYgCj+G2AOgeLngKjYw4gTTII31dqJ7+1T3fooIW4=" crossorigin=anonymous></script><script data-precache defer src=/assets/mermaid/bundle.min.8d91e77f8b2cd5ebfd6d2af4e214fa37b555b15ad65dc1bdec5564ce709176f9.js integrity="sha256-jZHnf4ss1ev9bSr04hT6N7VVsVrWXcG97FVkznCRdvk=" crossorigin=anonymous></script><script src=/js/sw-register.js defer></script><script src=https://giscus.app/client.js data-repo=jaehyeon-kim/jaehyeon-kim.github.io data-repo-id="MDEwOlJlcG9zaXRvcnkyNjgwMjU0NQ==" data-category data-category-id=DIC_kwDOAZj5cc4CV2fp data-mapping=pathname data-reactions-enabled=1 data-emit-metadata=1 data-input-position=bottom data-theme=preferred_color_scheme data-lang=en data-loading=lazy crossorigin=anonymous defer></script></body></html>