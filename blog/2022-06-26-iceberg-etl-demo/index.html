<!doctype html><html class=position-relative itemscope itemtype=https://schema.org/WebPage lang=en data-bs-theme=light data-palette=blue-gray><head><script src=/assets/init/bundle.min.99cb2d6502b2c6f5d76f21079aa7b7ea5ad83125c684ac755e2a5af62cc7ad71.js integrity="sha256-mcstZQKyxvXXbyEHmqe36lrYMSXGhKx1Xipa9izHrXE=" crossorigin=anonymous></script><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>Data Warehousing ETL Demo with Apache Iceberg on EMR Local Environment - Jaehyeon Kim</title>
<link rel=icon href=/favicon_huf63427acbec4fbe11db6a32164cc2763_6040_16x16_resize_q75_h2_box_2.webp sizes=16x16 type=image/webp><link rel=icon href=/favicon_huf63427acbec4fbe11db6a32164cc2763_6040_32x32_resize_q75_h2_box_2.webp sizes=32x32 type=image/webp><link rel=icon href=/favicon_huf63427acbec4fbe11db6a32164cc2763_6040_150x150_resize_q75_h2_box_2.webp sizes=150x150 type=image/webp><link rel=apple-touch-icon href=/favicon_huf63427acbec4fbe11db6a32164cc2763_6040_180x180_resize_q75_h2_box_2.webp sizes=180x180 type=image/webp><link rel=icon href=/favicon_huf63427acbec4fbe11db6a32164cc2763_6040_192x192_resize_q75_h2_box_2.webp sizes=192x192 type=image/webp><link rel=mask-icon href=/safari-pinned-tab.svg color=#6f42c1><meta name=keywords content="Analytics,Real-time Analytics,Data Engineering,Data Streaming,Architecture"><meta name=description content="We'll discuss how to implement data warehousing ETL using Iceberg for data storage/management and Spark for data processing. A Pyspark ETL app will be used for demonstration in an EMR local environment. Finally the ETL results will be queried by Athena for verification."><meta name=robots content="index, follow"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://jaehyeon.me/blog/2022-06-26-iceberg-etl-demo/featured.png"><meta name=twitter:title content="Data Warehousing ETL Demo with Apache Iceberg on EMR Local Environment"><meta name=twitter:description content="We'll discuss how to implement data warehousing ETL using Iceberg for data storage/management and Spark for data processing. A Pyspark ETL app will be used for demonstration in an EMR local environment. Finally the ETL results will be queried by Athena for verification."><meta property="og:url" content="https://jaehyeon.me/blog/2022-06-26-iceberg-etl-demo/"><meta property="og:site_name" content="Jaehyeon Kim"><meta property="og:title" content="Data Warehousing ETL Demo with Apache Iceberg on EMR Local Environment"><meta property="og:description" content="We'll discuss how to implement data warehousing ETL using Iceberg for data storage/management and Spark for data processing. A Pyspark ETL app will be used for demonstration in an EMR local environment. Finally the ETL results will be queried by Athena for verification."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="blog"><meta property="article:published_time" content="2022-06-26T00:00:00+00:00"><meta property="article:modified_time" content="2023-11-08T18:49:11+11:00"><meta property="article:tag" content="AWS"><meta property="article:tag" content="Amazon EMR"><meta property="article:tag" content="Apache Spark"><meta property="article:tag" content="PySpark"><meta property="article:tag" content="Apache Iceberg"><meta property="article:tag" content="ETL"><meta property="og:image" content="https://jaehyeon.me/blog/2022-06-26-iceberg-etl-demo/featured.png"><meta itemprop=name content="Data Warehousing ETL Demo with Apache Iceberg on EMR Local Environment"><meta itemprop=description content="We'll discuss how to implement data warehousing ETL using Iceberg for data storage/management and Spark for data processing. A Pyspark ETL app will be used for demonstration in an EMR local environment. Finally the ETL results will be queried by Athena for verification."><meta itemprop=datePublished content="2022-06-26T00:00:00+00:00"><meta itemprop=dateModified content="2023-11-08T18:49:11+11:00"><meta itemprop=wordCount content="2498"><meta itemprop=image content="https://jaehyeon.me/blog/2022-06-26-iceberg-etl-demo/featured.png"><meta itemprop=keywords content="AWS,Amazon EMR,Apache Spark,PySpark,Apache Iceberg,ETL,SCD,Slowly Changing Dimension,Docker,Docker Compose,Visual Studio Code"><link rel=manifest href=/manifest.json><script async src="https://www.googletagmanager.com/gtag/js?id=G-076N8WCGWZ"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-076N8WCGWZ")}</script><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-2764466088407958"></script><link data-precache rel=stylesheet href="/assets/main/bundle.min.dc910a9364ba50e03d47ecf493e01f798a7ff46d1f793a172c8412e0c9867284.css" integrity="sha256-3JEKk2S6UOA9R+z0k+AfeYp/9G0feToXLIQS4MmGcoQ=" crossorigin=anonymous><link data-precache rel=stylesheet href=/assets/katex/bundle.min.a06a916255e286562e9c3684403a944bb85aebe51b00e2f1ef537987349ead12.css integrity="sha256-oGqRYlXihlYunDaEQDqUS7ha6+UbAOLx71N5hzSerRI=" crossorigin=anonymous><link data-precache rel=stylesheet href=/assets/viewer/bundle.min.8c1002839fa22c1350d6ae1eef6593120e108f973c41348be9b5065430566aaf.css integrity="sha256-jBACg5+iLBNQ1q4e72WTEg4Qj5c8QTSL6bUGVDBWaq8=" crossorigin=anonymous></head><body><header class="mb-4 sticky-top"><nav class="top-app-bar shadow navbar navbar-expand-xxl"><div class=container><a class="navbar-brand d-flex align-items-center flex-grow-1 flex-xxl-grow-0 justify-content-xxl-start ms-2 ms-xxl-0 mx-auto me-xxl-2" href=https://jaehyeon.me/>Jaehyeon Kim</a><div class="offcanvas-xxl offcanvas-end flex-grow-1" data-bs-scroll=true tabindex=-1 id=navbarMenus aria-labelledby=navbarMenusLabel><div class="offcanvas-header px-4 pb-0"><div class="offcanvas-title h5" id=navbarMenusLabel>Jaehyeon Kim</div><button type=button class="btn-close btn-close-white" data-bs-dismiss=offcanvas data-bs-target=#navbarMenus aria-label=Close></button></div><div class="offcanvas-body p-4 pt-0 p-xxl-0"><hr class=d-xxl-none><ul class="navbar-nav flex-row flex-wrap align-items-center me-auto"><li class="nav-item col-12 col-xxl-auto dropdown px-0"><a href=# class="nav-link dropdown-toggle" id=navbarDropdownBlog role=button data-bs-toggle=dropdown aria-expanded=false><span class="menu-icon me-1"><i class="fas fa-fw fa-blog text-warning"></i></span>Blog</a><ul class="dropdown-menu dropdown-menu-end" aria-labelledby=navbarDropdownBlog data-bs-popper=none><li><a class="dropdown-item d-flex align-items-center text-wrap text-xxl-nowrap" href=https://jaehyeon.me/archives/><span class="dropdown-item-icon me-2 p-2 rounded"><i class="fas fa-fw fa-file-archive text-primary"></i></span><div class=dropdown-item-content><p class="dropdown-item-title mb-0">Archives</p></div></a></li><li><a class="dropdown-item d-flex align-items-center text-wrap text-xxl-nowrap" href=https://jaehyeon.me/series/><span class="dropdown-item-icon me-2 p-2 rounded"><i class="fas fa-fw fa-columns"></i></span><div class=dropdown-item-content><p class="dropdown-item-title mb-1">Series</p><p class="dropdown-item-description mb-0 text-secondary">List of series.</p></div></a></li><li><a class="dropdown-item d-flex align-items-center text-wrap text-xxl-nowrap" href=https://jaehyeon.me/series/_index.zh-cn/><span class="dropdown-item-icon me-2 p-2 rounded"><i class="fas fa-fw fa-columns"></i></span><div class=dropdown-item-content><p class="dropdown-item-title mb-1">专栏</p><p class="dropdown-item-description mb-0 text-secondary">List of series.</p></div></a></li><li><a class="dropdown-item d-flex align-items-center text-wrap text-xxl-nowrap" href=https://jaehyeon.me/categories/><span class="dropdown-item-icon me-2 p-2 rounded"><i class="fas fa-fw fa-folder text-success"></i></span><div class=dropdown-item-content><p class="dropdown-item-title mb-1">Categories</p><p class="dropdown-item-description mb-0 text-secondary">List of categories.</p></div></a></li><li><a class="dropdown-item d-flex align-items-center text-wrap text-xxl-nowrap" href=https://jaehyeon.me/categories/_index.zh-cn/><span class="dropdown-item-icon me-2 p-2 rounded"><i class="fas fa-fw fa-folder text-success"></i></span><div class=dropdown-item-content><p class="dropdown-item-title mb-1">分类</p><p class="dropdown-item-description mb-0 text-secondary">List of categories.</p></div></a></li><li><a class="dropdown-item d-flex align-items-center text-wrap text-xxl-nowrap" href=https://jaehyeon.me/tags/><span class="dropdown-item-icon me-2 p-2 rounded"><i class="fas fa-fw fa-tags"></i></span><div class=dropdown-item-content><p class="dropdown-item-title mb-1">Tags</p><p class="dropdown-item-description mb-0 text-secondary">List of tags.</p></div></a></li><li><a class="dropdown-item d-flex align-items-center text-wrap text-xxl-nowrap" href=https://jaehyeon.me/tags/_index.zh-cn/><span class="dropdown-item-icon me-2 p-2 rounded"><i class="fas fa-fw fa-tags"></i></span><div class=dropdown-item-content><p class="dropdown-item-title mb-1">标签</p><p class="dropdown-item-description mb-0 text-secondary">List of tags.</p></div></a></li></ul></li></ul><hr class=d-xxl-none><form class="search-bar ms-auto my-auto" action=/search/ novalidate><div class="input-group align-items-center"><span class="btn btn-search disabled position-absolute left-0 border-0 px-1"><i class="fas fa-fw fa-search fa-lg"></i>
</span><input class="my-1 form-control border-white rounded-5 search-input bg-body" name=q type=search placeholder=Search aria-label=Search required>
<span class="search-shortcut position-absolute end-0 top-0 me-2"><kbd class="text-dark bg-white opacity-75 rounded-3 shadow border border-primary py-1 fw-bold">/</kbd></span></div></form><hr class=d-xxl-none><ul class="navbar-nav flex-row flex-wrap align-items-center ms-md-auto"><li class="nav-item py-2 py-xxl-1 col-12 col-xxl-auto"><nav class="social-links nav justify-content-center flex-row"><a class="nav-link social-link col-6 col-xxl-auto p-1" target=_blank href=https://github.com/jaehyeon-kim title=GitHub rel=me><i class="fa-fw fab fa-github"></i>
<span class="ms-1 d-xxl-none">Github</span>
</a><a class="nav-link social-link col-6 col-xxl-auto p-1" target=_blank href=https://linkedin.com/in/jaehyeon-kim-76b93429 title=LinkedIn rel=me><i class="fa-fw fab fa-linkedin-in"></i>
<span class="ms-1 d-xxl-none">Linkedin</span>
</a><a class="nav-link social-link col-6 col-xxl-auto p-1" target=_blank href=https://www.paypal.com/paypalme/dottami title=PayPal rel=me><i class="fa-fw fab fa-paypal"></i>
<span class="ms-1 d-xxl-none">Paypal</span>
</a><a class="nav-link social-link col-6 col-xxl-auto p-1" target=_blank href=/index.xml title=RSS rel=me><i class="fas fa-fw fa-rss"></i>
<span class="ms-1 d-xxl-none">RSS</span></a></nav></li><li class="nav-item py-2 py-xxl-1 col-12 col-xxl-auto"><div class="vr d-none d-xxl-flex h-100 mx-xxl-2 text-white"></div><hr class="d-xxl-none my-2"></li><li class="nav-item dropdown col-6 col-xxl-auto"><a class="nav-link px-0 py-2 px-xxl-1" href=# id=fontSizeDropdown role=button data-bs-toggle=dropdown aria-expanded=false><i class="fas fa-fw fa-font"></i>
<span class=d-xxl-none>Font Size</span></a><ul class="font-size-dropdown-menu dropdown-menu dropdown-menu-end" aria-labelledby=fontSizeDropdown><li><button class="font-size-item dropdown-item" data-size=xs>
Extra Small</button></li><li><button class="font-size-item dropdown-item" data-size=sm>
Small</button></li><li><button class="font-size-item dropdown-item active" data-size=md>
Medium</button></li><li><button class="font-size-item dropdown-item" data-size=lg>
Large</button></li><li><button class="font-size-item dropdown-item" data-size=xl>
Extra Large</button></li></ul></li><li class="nav-item dropdown col-6 col-xxl-auto"><a class="nav-link px-0 py-2 px-xxl-1" href=# id=paletteDropdown role=button data-bs-toggle=dropdown aria-expanded=false><i class="fas fa-fw fa-palette"></i>
<span class=d-xxl-none>Palette</span></a><ul class="palette-dropdown-menu dropdown-menu dropdown-menu-end px-2 row g-2" aria-labelledby=paletteDropdown><li class="col-4 my-1"><a role=button id=palette-blue aria-label=Blue class="btn btn-sm w-100 palette text-bg-blue" data-palette=blue></a></li><li class="col-4 my-1"><a role=button id=palette-blue-gray aria-label="Blue Gray" class="btn btn-sm w-100 palette text-bg-blue-gray" data-palette=blue-gray></a></li><li class="col-4 my-1"><a role=button id=palette-brown aria-label=Brown class="btn btn-sm w-100 palette text-bg-brown" data-palette=brown></a></li><li class="col-4 my-1"><a role=button id=palette-cyan aria-label=Cyan class="btn btn-sm w-100 palette text-bg-cyan" data-palette=cyan></a></li><li class="col-4 my-1"><a role=button id=palette-green aria-label=Green class="btn btn-sm w-100 palette text-bg-green" data-palette=green></a></li><li class="col-4 my-1"><a role=button id=palette-indigo aria-label=Indigo class="btn btn-sm w-100 palette text-bg-indigo" data-palette=indigo></a></li><li class="col-4 my-1"><a role=button id=palette-orange aria-label=Orange class="btn btn-sm w-100 palette text-bg-orange" data-palette=orange></a></li><li class="col-4 my-1"><a role=button id=palette-pink aria-label=Pink class="btn btn-sm w-100 palette text-bg-pink" data-palette=pink></a></li><li class="col-4 my-1"><a role=button id=palette-purple aria-label=Purple class="btn btn-sm w-100 palette text-bg-purple" data-palette=purple></a></li><li class="col-4 my-1"><a role=button id=palette-red aria-label=Red class="btn btn-sm w-100 palette text-bg-red" data-palette=red></a></li><li class="col-4 my-1"><a role=button id=palette-teal aria-label=Teal class="btn btn-sm w-100 palette text-bg-teal" data-palette=teal></a></li><li class="col-4 my-1"><a role=button id=palette-yellow aria-label=Yellow class="btn btn-sm w-100 palette text-bg-yellow" data-palette=yellow></a></li></ul></li><li class="nav-item dropdown col-6 col-xxl-auto"><a class="nav-link px-0 py-2 px-xxl-1" href=# id=modeDropdown role=button data-bs-toggle=dropdown aria-expanded=false><i class="mode-icon fas fa-fw fa-sun" id=modeIcon></i>
<span class=d-xxl-none>Mode</span></a><ul class="mode-dropdown-menu dropdown-menu dropdown-menu-end" aria-labelledby=modeDropdown><li class="mode-item active" data-color-mode=light data-icon=sun><button class=dropdown-item>
<i class="mode-icon fas fa-fw fa-sun"></i> Light</button></li><li class=mode-item data-color-mode=dark data-icon=moon><button class=dropdown-item>
<i class="mode-icon fas fa-fw fa-moon"></i> Dark</button></li><li class=mode-item data-color-mode=auto data-icon=adjust><button class=dropdown-item>
<i class="mode-icon fas fa-fw fa-adjust"></i> Auto</button></li></ul></li></ul></div></div><div class=d-flex><button class="navbar-toggler order-5 border-0" type=button data-bs-toggle=offcanvas data-bs-target=#navbarMenus aria-controls=navbarMenus aria-expanded=false aria-label="Toggle navigation">
<i class="fas fa-ellipsis-h"></i></button></div></div></nav></header><main class=container><div class="row content"><noscript><div class="alert alert-danger" role=alert>Your browser does not support JavaScript.</div></noscript><div class=col-xxl-8><div class=container><nav class="row card component" aria-label=breadcrumb><div class="card-body pb-0"><ol class="hbs-breadcrumb breadcrumb flex-nowrap"><li class="breadcrumb-item text-surface"><a href=/>Home</a></li><li class="breadcrumb-item text-surface"><a href=/blog/>Blogs</a></li><li class="breadcrumb-item active">Data Warehousing ETL Demo With Apache Iceberg on EMR Local Environment</li></ol></div></nav><div class="post-panel-wrapper position-relative d-flex justify-content-center"><div class="d-flex flex-row justify-content-center rounded-5 border post-panel position-fixed px-3 py-1 surface shadow-1"><a class="action action-toc d-none d-xxl-block" href=#postTOC role=button title=Contents><i class="fas fa-fw fa-list-alt"></i>
</a><a class="action action-toc d-block d-xxl-none" href=#post-toc-container role=button title=Contents><i class="fas fa-fw fa-list-alt"></i>
</a><a class="action action-post-comments" href=#post-comments role=button aria-label=Comments title=Comments><i class="fas fa-fw fa-comments"></i>
</a><a id=sidebarToggler class="action action-sidebar-toggler d-none d-xxl-block" role=button title="Toggle sidebar"><i class="fas fa-fw fa-expand-alt" data-fa-transform=rotate-45></i></a></div></div><article class="row card component mb-4 post"><div class=card-header><h1 class="card-title post-title my-2">Data Warehousing ETL Demo With Apache Iceberg on EMR Local Environment</h1></div><div class=card-body><div class="post-meta mb-3"><span class="post-date me-1 mb-1" title="created on 2022-06-26 00:00:00 +0000 UTC, updated on 2023-11-08 07:49:11 +0000 UTC.">June 26, 2022</span><span class="post-reading-time me-1 mb-1">12 min read</span><a href=/categories/data-engineering/ class="btn btn-sm btn-secondary mb-1 me-2 py-0 pe-1 post-taxonomy post-taxonomy-sm post-category">
<i class="fas fa-fw fa-folder me-1"></i>Data Engineering</a><a href=/tags/amazon-emr/ class="btn btn-sm btn-secondary mb-1 me-2 py-0 pe-1 post-taxonomy post-taxonomy-sm post-tag">Amazon EMR</a><a href=/tags/apache-iceberg/ class="btn btn-sm btn-secondary mb-1 me-2 py-0 pe-1 post-taxonomy post-taxonomy-sm post-tag">Apache Iceberg</a><a href=/tags/apache-spark/ class="btn btn-sm btn-secondary mb-1 me-2 py-0 pe-1 post-taxonomy post-taxonomy-sm post-tag">Apache Spark</a><a href=/tags/aws/ class="btn btn-sm btn-secondary mb-1 me-2 py-0 pe-1 post-taxonomy post-taxonomy-sm post-tag">AWS</a><a href=/tags/docker/ class="btn btn-sm btn-secondary mb-1 me-2 py-0 pe-1 post-taxonomy post-taxonomy-sm post-tag">Docker</a><a href=/tags/docker-compose/ class="btn btn-sm btn-secondary mb-1 me-2 py-0 pe-1 post-taxonomy post-taxonomy-sm post-tag">Docker Compose</a><a href=/tags/etl/ class="btn btn-sm btn-secondary mb-1 me-2 py-0 pe-1 post-taxonomy post-taxonomy-sm post-tag">ETL</a><a href=/tags/pyspark/ class="btn btn-sm btn-secondary mb-1 me-2 py-0 pe-1 post-taxonomy post-taxonomy-sm post-tag">PySpark</a><a href=/tags/scd/ class="btn btn-sm btn-secondary mb-1 me-2 py-0 pe-1 post-taxonomy post-taxonomy-sm post-tag">SCD</a><a href=/tags/slowly-changing-dimension/ class="btn btn-sm btn-secondary mb-1 me-2 py-0 pe-1 post-taxonomy post-taxonomy-sm post-tag">Slowly Changing Dimension</a><a href=/tags/visual-studio-code/ class="btn btn-sm btn-secondary mb-1 me-2 py-0 pe-1 post-taxonomy post-taxonomy-sm post-tag">Visual Studio Code</a></div><picture class="d-flex justify-content-center"><source srcset=/blog/2022-06-26-iceberg-etl-demo/featured_hu593bea0bbd3278708b1d2f8e8019b3ec_43604_0x270_resize_box_3.png type=image/png media="(max-width: 576px)" width=736 height=270><img class="post-featured-img h-auto w-auto mb-3" alt="Data Warehousing ETL Demo with Apache Iceberg on EMR Local Environment" src=/blog/2022-06-26-iceberg-etl-demo/featured_hu593bea0bbd3278708b1d2f8e8019b3ec_43604_0x480_resize_box_3.png width=1308 height=480 data-src=/blog/2022-06-26-iceberg-etl-demo/featured.png></picture><p class="lead mb-3 text-body-emphasis">We'll discuss how to implement data warehousing ETL using Iceberg for data storage/management and Spark for data processing. A Pyspark ETL app will be used for demonstration in an EMR local environment. Finally the ETL results will be queried by Athena for verification.</p><div id=postTOC class="mb-3 text-surface"><h2 class=mb-3>Contents<a class="anchor ms-1" href=#postTOC></a></h2><nav id=TableOfContents><ul><li><a href=#emr-local-environment>EMR Local Environment</a></li><li><a href=#etl-strategy>ETL Strategy</a><ul><li><a href=#sample-data>Sample Data</a></li><li><a href=#users>Users</a></li><li><a href=#products>Products</a></li><li><a href=#orders>Orders</a></li></ul></li><li><a href=#etl-implementation>ETL Implementation</a><ul><li><a href=#users-1>Users</a></li><li><a href=#products-1>Products</a></li><li><a href=#orders-1>Orders</a></li><li><a href=#run-etl>Run ETL</a></li></ul></li><li><a href=#summary>Summary</a></li></ul></nav><hr class=text-secondary></div><div class="post-content mb-3" data-bs-spy=scroll data-bs-target=#TableOfContents tabindex=0><div id=post-content-body><p>Unlike traditional Data Lake, new table formats (<a href=https://iceberg.apache.org/ target=_blank rel="noopener noreferrer">Iceberg<i class="fas fa-external-link-square-alt ms-1"></i></a>, <a href=https://hudi.apache.org/ target=_blank rel="noopener noreferrer">Hudi<i class="fas fa-external-link-square-alt ms-1"></i></a> and <a href=https://delta.io/ target=_blank rel="noopener noreferrer">Delta Lake<i class="fas fa-external-link-square-alt ms-1"></i></a>) support <a href=https://iceberg.apache.org/docs/latest/spark-writes/ target=_blank rel="noopener noreferrer">features<i class="fas fa-external-link-square-alt ms-1"></i></a> that can be used to apply data warehousing patterns, which can bring a way to be rescued from <a href=https://www.gartner.com/en/newsroom/press-releases/2014-07-28-gartner-says-beware-of-the-data-lake-fallacy target=_blank rel="noopener noreferrer">Data Swamp<i class="fas fa-external-link-square-alt ms-1"></i></a>. In this post, we&rsquo;ll discuss how to implement ETL using retail analytics data. It has two dimension data (user and product) and a single fact data (order). The dimension data sets have different ETL strategies depending on whether to track historical changes. For the fact data, the primary keys of the dimension data are added to facilitate later queries. We&rsquo;ll use Iceberg for data storage/management and Spark for data processing. Instead of provisioning an EMR cluster, a local development environment will be used. Finally, the ETL results will be queried by Athena for verification.</p><h2 id=emr-local-environment data-numberify>EMR Local Environment<a class="anchor ms-1" href=#emr-local-environment></a></h2><p>In <a href=/blog/2022-05-08-emr-local-dev>one of my earlier posts</a>, we discussed how to develop and test Apache Spark apps for EMR locally using Docker (and/or VSCode). Instead of provisioning an EMR cluster, we can quickly build an ETL app using the local environment. For this post, a new local environment is created based on the Docker image of the latest EMR 6.6.0 release. Check the <a href=https://github.com/jaehyeon-kim/iceberg-etl-demo target=_blank rel="noopener noreferrer"><strong>GitHub repository</strong><i class="fas fa-external-link-square-alt ms-1"></i></a> for this post for further details.</p><p>Apache Iceberg is <a href=https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-iceberg.html target=_blank rel="noopener noreferrer">supported by EMR 6.5.0 or later<i class="fas fa-external-link-square-alt ms-1"></i></a>, and it requires <em><a href=https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-iceberg-use-cluster.html target=_blank rel="noopener noreferrer">iceberg-defaults configuration classification<i class="fas fa-external-link-square-alt ms-1"></i></a></em> that enables Iceberg. The latest EMR Docker release (<a href=https://docs.aws.amazon.com/emr/latest/EMR-on-EKS-DevelopmentGuide/emr-eks-6.6.0.html target=_blank rel="noopener noreferrer">emr-6.6.0-20220411<i class="fas fa-external-link-square-alt ms-1"></i></a>), however, doesn&rsquo;t support that configuration classification and I didn&rsquo;t find the _iceberg _folder (<code>/usr/share/aws/iceberg</code>) within the Docker container. Therefore, the project&rsquo;s <a href=https://iceberg.apache.org/docs/latest/aws/#spark target=_blank rel="noopener noreferrer">AWS integration example<i class="fas fa-external-link-square-alt ms-1"></i></a> is used instead and the following script (<a href=https://github.com/jaehyeon-kim/iceberg-etl-demo/blob/main/run.sh target=_blank rel="noopener noreferrer">run.sh<i class="fas fa-external-link-square-alt ms-1"></i></a>) is an update of the example script that allows to launch the Pyspark shell or to submit a Spark application.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=ln> 1</span><span class=cl><span class=c1># run.sh</span>
</span></span><span class=line><span class=ln> 2</span><span class=cl><span class=c1>#!/usr/bin/env bash</span>
</span></span><span class=line><span class=ln> 3</span><span class=cl>
</span></span><span class=line><span class=ln> 4</span><span class=cl><span class=c1># add Iceberg dependency</span>
</span></span><span class=line><span class=ln> 5</span><span class=cl><span class=nv>ICEBERG_VERSION</span><span class=o>=</span>0.13.2
</span></span><span class=line><span class=ln> 6</span><span class=cl><span class=nv>DEPENDENCIES</span><span class=o>=</span><span class=s2>&#34;org.apache.iceberg:iceberg-spark3-runtime:</span><span class=nv>$ICEBERG_VERSION</span><span class=s2>&#34;</span>
</span></span><span class=line><span class=ln> 7</span><span class=cl>
</span></span><span class=line><span class=ln> 8</span><span class=cl><span class=c1># add AWS dependency</span>
</span></span><span class=line><span class=ln> 9</span><span class=cl><span class=nv>AWS_SDK_VERSION</span><span class=o>=</span>2.17.131
</span></span><span class=line><span class=ln>10</span><span class=cl><span class=nv>AWS_MAVEN_GROUP</span><span class=o>=</span>software.amazon.awssdk
</span></span><span class=line><span class=ln>11</span><span class=cl><span class=nv>AWS_PACKAGES</span><span class=o>=(</span>
</span></span><span class=line><span class=ln>12</span><span class=cl>    <span class=s2>&#34;bundle&#34;</span>
</span></span><span class=line><span class=ln>13</span><span class=cl>    <span class=s2>&#34;url-connection-client&#34;</span>
</span></span><span class=line><span class=ln>14</span><span class=cl><span class=o>)</span>
</span></span><span class=line><span class=ln>15</span><span class=cl><span class=k>for</span> pkg in <span class=s2>&#34;</span><span class=si>${</span><span class=nv>AWS_PACKAGES</span><span class=p>[@]</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>;</span> <span class=k>do</span>
</span></span><span class=line><span class=ln>16</span><span class=cl>    <span class=nv>DEPENDENCIES</span><span class=o>+=</span><span class=s2>&#34;,</span><span class=nv>$AWS_MAVEN_GROUP</span><span class=s2>:</span><span class=nv>$pkg</span><span class=s2>:</span><span class=nv>$AWS_SDK_VERSION</span><span class=s2>&#34;</span>
</span></span><span class=line><span class=ln>17</span><span class=cl><span class=k>done</span>
</span></span><span class=line><span class=ln>18</span><span class=cl>
</span></span><span class=line><span class=ln>19</span><span class=cl><span class=c1># execute pyspark or spark-submit</span>
</span></span><span class=line><span class=ln>20</span><span class=cl><span class=nv>execution</span><span class=o>=</span><span class=nv>$1</span>
</span></span><span class=line><span class=ln>21</span><span class=cl><span class=nv>app_path</span><span class=o>=</span><span class=nv>$2</span>
</span></span><span class=line><span class=ln>22</span><span class=cl><span class=k>if</span> <span class=o>[</span> -z <span class=nv>$execution</span> <span class=o>]</span><span class=p>;</span> <span class=k>then</span>
</span></span><span class=line><span class=ln>23</span><span class=cl>    <span class=nb>echo</span> <span class=s2>&#34;missing execution type. specify either pyspark or spark-submit&#34;</span>
</span></span><span class=line><span class=ln>24</span><span class=cl>    <span class=nb>exit</span> <span class=m>1</span>
</span></span><span class=line><span class=ln>25</span><span class=cl><span class=k>fi</span>
</span></span><span class=line><span class=ln>26</span><span class=cl>
</span></span><span class=line><span class=ln>27</span><span class=cl><span class=k>if</span> <span class=o>[</span> <span class=nv>$execution</span> <span class=o>==</span> <span class=s2>&#34;pyspark&#34;</span> <span class=o>]</span><span class=p>;</span> <span class=k>then</span>
</span></span><span class=line><span class=ln>28</span><span class=cl>    pyspark --packages <span class=nv>$DEPENDENCIES</span> <span class=se>\
</span></span></span><span class=line><span class=ln>29</span><span class=cl><span class=se></span>        --conf spark.sql.extensions<span class=o>=</span>org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions <span class=se>\
</span></span></span><span class=line><span class=ln>30</span><span class=cl><span class=se></span>        --conf spark.sql.catalog.demo<span class=o>=</span>org.apache.iceberg.spark.SparkCatalog <span class=se>\
</span></span></span><span class=line><span class=ln>31</span><span class=cl><span class=se></span>        --conf spark.sql.catalog.demo.warehouse<span class=o>=</span>s3://iceberg-etl-demo <span class=se>\
</span></span></span><span class=line><span class=ln>32</span><span class=cl><span class=se></span>        --conf spark.sql.catalog.demo.catalog-impl<span class=o>=</span>org.apache.iceberg.aws.glue.GlueCatalog <span class=se>\
</span></span></span><span class=line><span class=ln>33</span><span class=cl><span class=se></span>        --conf spark.sql.catalog.demo.io-impl<span class=o>=</span>org.apache.iceberg.aws.s3.S3FileIO
</span></span><span class=line><span class=ln>34</span><span class=cl><span class=k>elif</span> <span class=o>[</span> <span class=nv>$execution</span> <span class=o>==</span> <span class=s2>&#34;spark-submit&#34;</span> <span class=o>]</span><span class=p>;</span> <span class=k>then</span>
</span></span><span class=line><span class=ln>35</span><span class=cl>    <span class=k>if</span> <span class=o>[</span> -z <span class=nv>$app_path</span> <span class=o>]</span><span class=p>;</span> <span class=k>then</span>
</span></span><span class=line><span class=ln>36</span><span class=cl>        <span class=nb>echo</span> <span class=s2>&#34;pyspark application is mandatory&#34;</span>
</span></span><span class=line><span class=ln>37</span><span class=cl>        <span class=nb>exit</span> <span class=m>1</span>
</span></span><span class=line><span class=ln>38</span><span class=cl>    <span class=k>else</span>
</span></span><span class=line><span class=ln>39</span><span class=cl>        spark-submit --packages <span class=nv>$DEPENDENCIES</span> <span class=se>\
</span></span></span><span class=line><span class=ln>40</span><span class=cl><span class=se></span>            --deploy-mode client <span class=se>\
</span></span></span><span class=line><span class=ln>41</span><span class=cl><span class=se></span>            --master local<span class=o>[</span>*<span class=o>]</span> <span class=se>\
</span></span></span><span class=line><span class=ln>42</span><span class=cl><span class=se></span>            --conf spark.sql.extensions<span class=o>=</span>org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions <span class=se>\
</span></span></span><span class=line><span class=ln>43</span><span class=cl><span class=se></span>            --conf spark.sql.catalog.demo<span class=o>=</span>org.apache.iceberg.spark.SparkCatalog <span class=se>\
</span></span></span><span class=line><span class=ln>44</span><span class=cl><span class=se></span>            --conf spark.sql.catalog.demo.warehouse<span class=o>=</span>s3://iceberg-etl-demo <span class=se>\
</span></span></span><span class=line><span class=ln>45</span><span class=cl><span class=se></span>            --conf spark.sql.catalog.demo.catalog-impl<span class=o>=</span>org.apache.iceberg.aws.glue.GlueCatalog <span class=se>\
</span></span></span><span class=line><span class=ln>46</span><span class=cl><span class=se></span>            --conf spark.sql.catalog.demo.io-impl<span class=o>=</span>org.apache.iceberg.aws.s3.S3FileIO <span class=se>\
</span></span></span><span class=line><span class=ln>47</span><span class=cl><span class=se></span>            <span class=nv>$app_path</span>
</span></span><span class=line><span class=ln>48</span><span class=cl>    <span class=k>fi</span>
</span></span><span class=line><span class=ln>49</span><span class=cl><span class=k>fi</span>
</span></span></code></pre></div><p>Here is an example of using the script.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=ln>1</span><span class=cl><span class=c1># launch pyspark shell</span>
</span></span><span class=line><span class=ln>2</span><span class=cl>$ ./run.sh pyspark
</span></span><span class=line><span class=ln>3</span><span class=cl>
</span></span><span class=line><span class=ln>4</span><span class=cl><span class=c1># execute spark-submit, requires pyspark application (etl.py) as the second argument</span>
</span></span><span class=line><span class=ln>5</span><span class=cl>$ ./run.sh spark-submit path-to-app.py
</span></span></code></pre></div><h2 id=etl-strategy data-numberify>ETL Strategy<a class="anchor ms-1" href=#etl-strategy></a></h2><h3 id=sample-data data-numberify>Sample Data<a class="anchor ms-1" href=#sample-data></a></h3><p>We use the <a href=https://docs.yugabyte.com/preview/sample-data/retail-analytics/ target=_blank rel="noopener noreferrer">retail analytics sample database<i class="fas fa-external-link-square-alt ms-1"></i></a> from YugaByteDB to get the ETL sample data. Records from the following 3 tables are used to run ETL on its own ETL strategy.</p><p><picture><img class="img-fluid mx-auto d-block" alt src=/blog/2022-06-26-iceberg-etl-demo/03_data_diagram.png loading=lazy width=784 height=455></picture></p><p>The main focus of the demo ETL application is to show how to track product price changes over time and to apply those changes to the order data. Normally ETL is performed daily, but it&rsquo;ll be time-consuming to execute daily incremental ETL with the order data because it includes records spanning for 5 calendar years. Moreover, as it is related to the user and product data, splitting the corresponding dimension records will be quite difficult. Instead, I chose to run yearly incremental ETL. I first grouped orders in 4 groups where the first group (year 0) includes orders in 2016 and 2017. And each of the remaining groups (year 1 to 3) keeps records of a whole year from 2018 to 2020. Then I created 4 product groups in order to match the order groups and to execute incremental ETL together with the order data. The first group (year 0) keeps the original data and the product price is set to be increased by 5% in the following years until the last group (year 3). Note, with this setup, it is expected that orders for a given product tend to be mapped to a higher product price over time. On the other hand, the ETL strategy of the user data is not to track historical data so that it is used as it is. The sample data files used for the ETL app are listed below, and they can be found in the <a href=https://github.com/jaehyeon-kim/iceberg-etl-demo/tree/main/data target=_blank rel="noopener noreferrer">data folder<i class="fas fa-external-link-square-alt ms-1"></i></a> of the <strong>GitHub repository</strong>.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=ln> 1</span><span class=cl>$ tree data
</span></span><span class=line><span class=ln> 2</span><span class=cl>data
</span></span><span class=line><span class=ln> 3</span><span class=cl>├── orders_year_0.csv
</span></span><span class=line><span class=ln> 4</span><span class=cl>├── orders_year_1.csv
</span></span><span class=line><span class=ln> 5</span><span class=cl>├── orders_year_2.csv
</span></span><span class=line><span class=ln> 6</span><span class=cl>├── orders_year_3.csv
</span></span><span class=line><span class=ln> 7</span><span class=cl>├── products_year_0.csv
</span></span><span class=line><span class=ln> 8</span><span class=cl>├── products_year_1.csv
</span></span><span class=line><span class=ln> 9</span><span class=cl>├── products_year_2.csv
</span></span><span class=line><span class=ln>10</span><span class=cl>├── products_year_3.csv
</span></span><span class=line><span class=ln>11</span><span class=cl>└── users.csv
</span></span></code></pre></div><h3 id=users data-numberify>Users<a class="anchor ms-1" href=#users></a></h3><p><a href=https://en.wikipedia.org/wiki/Slowly_changing_dimension target=_blank rel="noopener noreferrer">Slowly changing dimension (SCD) type 1<i class="fas fa-external-link-square-alt ms-1"></i></a> is implemented for the user data. This method basically _upsert_s records by comparing the primary key values and therefore doesn&rsquo;t track historical data. The data has the <a href=https://en.wikipedia.org/wiki/Natural_key target=_blank rel="noopener noreferrer">natural key<i class="fas fa-external-link-square-alt ms-1"></i></a> of _id _and its <a href=https://en.wikipedia.org/wiki/MD5 target=_blank rel="noopener noreferrer">md5 hash<i class="fas fa-external-link-square-alt ms-1"></i></a> is used as the <a href=https://en.wikipedia.org/wiki/Surrogate_key target=_blank rel="noopener noreferrer">surrogate key<i class="fas fa-external-link-square-alt ms-1"></i></a> named <em>user_sk</em> - this column is used as the primary key of the table. The table is configured to be partitioned by its surrogate key in 20 buckets. The table creation statement can be found below.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-sql data-lang=sql><span class=line><span class=ln> 1</span><span class=cl><span class=k>CREATE</span><span class=w> </span><span class=k>TABLE</span><span class=w> </span><span class=n>demo</span><span class=p>.</span><span class=n>dwh</span><span class=p>.</span><span class=n>users</span><span class=w> </span><span class=p>(</span><span class=w>
</span></span></span><span class=line><span class=ln> 2</span><span class=cl><span class=w>	</span><span class=n>user_sk</span><span class=w>     </span><span class=n>string</span><span class=p>,</span><span class=w>
</span></span></span><span class=line><span class=ln> 3</span><span class=cl><span class=w>	</span><span class=n>id</span><span class=w>          </span><span class=nb>bigint</span><span class=p>,</span><span class=w>
</span></span></span><span class=line><span class=ln> 4</span><span class=cl><span class=w>	</span><span class=n>name</span><span class=w>        </span><span class=n>string</span><span class=p>,</span><span class=w>
</span></span></span><span class=line><span class=ln> 5</span><span class=cl><span class=w>	</span><span class=n>email</span><span class=w>       </span><span class=n>string</span><span class=p>,</span><span class=w>
</span></span></span><span class=line><span class=ln> 6</span><span class=cl><span class=w>	</span><span class=n>address</span><span class=w>     </span><span class=n>string</span><span class=p>,</span><span class=w>
</span></span></span><span class=line><span class=ln> 7</span><span class=cl><span class=w>	</span><span class=n>city</span><span class=w>        </span><span class=n>string</span><span class=p>,</span><span class=w>
</span></span></span><span class=line><span class=ln> 8</span><span class=cl><span class=w>	</span><span class=k>state</span><span class=w>       </span><span class=n>string</span><span class=p>,</span><span class=w>
</span></span></span><span class=line><span class=ln> 9</span><span class=cl><span class=w>	</span><span class=n>zip</span><span class=w>         </span><span class=n>string</span><span class=p>,</span><span class=w>
</span></span></span><span class=line><span class=ln>10</span><span class=cl><span class=w>	</span><span class=n>birth_date</span><span class=w>  </span><span class=nb>date</span><span class=p>,</span><span class=w>
</span></span></span><span class=line><span class=ln>11</span><span class=cl><span class=w>	</span><span class=k>source</span><span class=w>      </span><span class=n>string</span><span class=p>,</span><span class=w>
</span></span></span><span class=line><span class=ln>12</span><span class=cl><span class=w>	</span><span class=n>created_at</span><span class=w>  </span><span class=k>timestamp</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=ln>13</span><span class=cl><span class=w></span><span class=k>USING</span><span class=w> </span><span class=n>iceberg</span><span class=w>
</span></span></span><span class=line><span class=ln>14</span><span class=cl><span class=w></span><span class=n>PARTITIONED</span><span class=w> </span><span class=k>BY</span><span class=w> </span><span class=p>(</span><span class=n>bucket</span><span class=p>(</span><span class=mi>20</span><span class=p>,</span><span class=w> </span><span class=n>user_sk</span><span class=p>))</span><span class=w>
</span></span></span></code></pre></div><h3 id=products data-numberify>Products<a class="anchor ms-1" href=#products></a></h3><p><a href=https://en.wikipedia.org/wiki/Slowly_changing_dimension target=_blank rel="noopener noreferrer">Slowly changing dimension (SCD) type 2<i class="fas fa-external-link-square-alt ms-1"></i></a> is taken for product data. This method tracks historical data by adding multiple records for a given natural key. Same as the user data, the <em>id</em> column is the natural key. Each record for the same natural key will be given a different surrogate key and the md5 hash of a combination of the <em>id</em> and _created_at _columns is used as the surrogate key named <em>prod_sk</em>. Each record has its own effect period and it is determined by the <em>eff_from</em> and _eff_to _columns and the latest record is marked as 1 for its <em>curr_flag</em> value. The table is also configured to be partitioned by its surrogate key in 20 buckets. The table creation statement is shown below.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-sql data-lang=sql><span class=line><span class=ln> 1</span><span class=cl><span class=k>CREATE</span><span class=w> </span><span class=k>TABLE</span><span class=w> </span><span class=n>demo</span><span class=p>.</span><span class=n>dwh</span><span class=p>.</span><span class=n>products</span><span class=w> </span><span class=p>(</span><span class=w>
</span></span></span><span class=line><span class=ln> 2</span><span class=cl><span class=w>	</span><span class=n>prod_sk</span><span class=w>     </span><span class=n>string</span><span class=p>,</span><span class=w>
</span></span></span><span class=line><span class=ln> 3</span><span class=cl><span class=w>	</span><span class=n>id</span><span class=w>          </span><span class=nb>bigint</span><span class=p>,</span><span class=w>
</span></span></span><span class=line><span class=ln> 4</span><span class=cl><span class=w>	</span><span class=n>category</span><span class=w>    </span><span class=n>string</span><span class=p>,</span><span class=w>
</span></span></span><span class=line><span class=ln> 5</span><span class=cl><span class=w>	</span><span class=n>price</span><span class=w>       </span><span class=nb>decimal</span><span class=p>(</span><span class=mi>6</span><span class=p>,</span><span class=mi>3</span><span class=p>),</span><span class=w>
</span></span></span><span class=line><span class=ln> 6</span><span class=cl><span class=w>	</span><span class=n>title</span><span class=w>       </span><span class=n>string</span><span class=p>,</span><span class=w>
</span></span></span><span class=line><span class=ln> 7</span><span class=cl><span class=w>	</span><span class=n>vendor</span><span class=w>      </span><span class=n>string</span><span class=p>,</span><span class=w>
</span></span></span><span class=line><span class=ln> 8</span><span class=cl><span class=w>	</span><span class=n>curr_flag</span><span class=w>   </span><span class=nb>int</span><span class=p>,</span><span class=w>
</span></span></span><span class=line><span class=ln> 9</span><span class=cl><span class=w>	</span><span class=n>eff_from</span><span class=w>    </span><span class=k>timestamp</span><span class=p>,</span><span class=w>
</span></span></span><span class=line><span class=ln>10</span><span class=cl><span class=w>	</span><span class=n>eff_to</span><span class=w>      </span><span class=k>timestamp</span><span class=p>,</span><span class=w>
</span></span></span><span class=line><span class=ln>11</span><span class=cl><span class=w>	</span><span class=n>created_at</span><span class=w>  </span><span class=k>timestamp</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=ln>12</span><span class=cl><span class=w></span><span class=k>USING</span><span class=w> </span><span class=n>iceberg</span><span class=w>
</span></span></span><span class=line><span class=ln>13</span><span class=cl><span class=w></span><span class=n>PARTITIONED</span><span class=w> </span><span class=k>BY</span><span class=w> </span><span class=p>(</span><span class=n>bucket</span><span class=p>(</span><span class=mi>20</span><span class=p>,</span><span class=w> </span><span class=n>prod_sk</span><span class=p>))</span><span class=w>
</span></span></span></code></pre></div><h3 id=orders data-numberify>Orders<a class="anchor ms-1" href=#orders></a></h3><p>The orders table has a composite primary key of the surrogate keys of the dimension tables - <em>users_sk</em> and <em>prod_sk</em>. Those columns don&rsquo;t exist in the source data and are added during transformation. The table is configured to be partitioned by the date part of the <em>created_at</em> column. The table creation statement can be found below.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-sql data-lang=sql><span class=line><span class=ln>1</span><span class=cl><span class=k>CREATE</span><span class=w> </span><span class=k>TABLE</span><span class=w> </span><span class=n>demo</span><span class=p>.</span><span class=n>dwh</span><span class=p>.</span><span class=n>orders</span><span class=w> </span><span class=p>(</span><span class=w>
</span></span></span><span class=line><span class=ln>2</span><span class=cl><span class=w>	</span><span class=n>user_sk</span><span class=w>     </span><span class=n>string</span><span class=p>,</span><span class=w>
</span></span></span><span class=line><span class=ln>3</span><span class=cl><span class=w>	</span><span class=n>prod_sk</span><span class=w>     </span><span class=n>string</span><span class=p>,</span><span class=w>
</span></span></span><span class=line><span class=ln>4</span><span class=cl><span class=w>	</span><span class=n>id</span><span class=w>          </span><span class=nb>bigint</span><span class=p>,</span><span class=w>
</span></span></span><span class=line><span class=ln>5</span><span class=cl><span class=w>	</span><span class=n>discount</span><span class=w>    </span><span class=nb>decimal</span><span class=p>(</span><span class=mi>4</span><span class=p>,</span><span class=mi>2</span><span class=p>),</span><span class=w>
</span></span></span><span class=line><span class=ln>6</span><span class=cl><span class=w>	</span><span class=n>quantity</span><span class=w>    </span><span class=nb>integer</span><span class=p>,</span><span class=w>
</span></span></span><span class=line><span class=ln>7</span><span class=cl><span class=w>	</span><span class=n>created_at</span><span class=w>  </span><span class=k>timestamp</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=ln>8</span><span class=cl><span class=w></span><span class=k>USING</span><span class=w> </span><span class=n>iceberg</span><span class=w>
</span></span></span><span class=line><span class=ln>9</span><span class=cl><span class=w></span><span class=n>PARTITIONED</span><span class=w> </span><span class=k>BY</span><span class=w> </span><span class=p>(</span><span class=n>days</span><span class=p>(</span><span class=n>created_at</span><span class=p>))</span><span class=w>
</span></span></span></code></pre></div><h2 id=etl-implementation data-numberify>ETL Implementation<a class="anchor ms-1" href=#etl-implementation></a></h2><h3 id=users-1 data-numberify>Users<a class="anchor ms-1" href=#users-1></a></h3><p>In the transformation phase, a source dataframe is created by creating the surrogate key (<em>user_sk</em>), changing data types of relevant columns and selecting columns in the same order as the table is created. Then a view (<em>users_tbl</em>) is created from the source dataframe and it is used to execute MERGE operation by comparing the surrogate key values of the source view with those of the target users table.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=ln> 1</span><span class=cl><span class=c1># src.py</span>
</span></span><span class=line><span class=ln> 2</span><span class=cl><span class=k>def</span> <span class=nf>etl_users</span><span class=p>(</span><span class=n>file_path</span><span class=p>:</span> <span class=nb>str</span><span class=p>,</span> <span class=n>spark_session</span><span class=p>:</span> <span class=n>SparkSession</span><span class=p>):</span>
</span></span><span class=line><span class=ln> 3</span><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;users - transform records...&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=ln> 4</span><span class=cl>    <span class=n>src_df</span> <span class=o>=</span> <span class=p>(</span>
</span></span><span class=line><span class=ln> 5</span><span class=cl>        <span class=n>spark_session</span><span class=o>.</span><span class=n>read</span><span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>&#34;header&#34;</span><span class=p>,</span> <span class=s2>&#34;true&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=ln> 6</span><span class=cl>        <span class=o>.</span><span class=n>csv</span><span class=p>(</span><span class=n>file_path</span><span class=p>)</span>
</span></span><span class=line><span class=ln> 7</span><span class=cl>        <span class=o>.</span><span class=n>withColumn</span><span class=p>(</span><span class=s2>&#34;user_sk&#34;</span><span class=p>,</span> <span class=n>md5</span><span class=p>(</span><span class=s2>&#34;id&#34;</span><span class=p>))</span>
</span></span><span class=line><span class=ln> 8</span><span class=cl>        <span class=o>.</span><span class=n>withColumn</span><span class=p>(</span><span class=s2>&#34;id&#34;</span><span class=p>,</span> <span class=n>expr</span><span class=p>(</span><span class=s2>&#34;CAST(id AS bigint)&#34;</span><span class=p>))</span>
</span></span><span class=line><span class=ln> 9</span><span class=cl>        <span class=o>.</span><span class=n>withColumn</span><span class=p>(</span><span class=s2>&#34;created_at&#34;</span><span class=p>,</span> <span class=n>to_timestamp</span><span class=p>(</span><span class=s2>&#34;created_at&#34;</span><span class=p>))</span>
</span></span><span class=line><span class=ln>10</span><span class=cl>        <span class=o>.</span><span class=n>withColumn</span><span class=p>(</span><span class=s2>&#34;birth_date&#34;</span><span class=p>,</span> <span class=n>to_date</span><span class=p>(</span><span class=s2>&#34;birth_date&#34;</span><span class=p>))</span>
</span></span><span class=line><span class=ln>11</span><span class=cl>        <span class=o>.</span><span class=n>select</span><span class=p>(</span>
</span></span><span class=line><span class=ln>12</span><span class=cl>            <span class=s2>&#34;user_sk&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=ln>13</span><span class=cl>            <span class=s2>&#34;id&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=ln>14</span><span class=cl>            <span class=s2>&#34;name&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=ln>15</span><span class=cl>            <span class=s2>&#34;email&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=ln>16</span><span class=cl>            <span class=s2>&#34;address&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=ln>17</span><span class=cl>            <span class=s2>&#34;city&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=ln>18</span><span class=cl>            <span class=s2>&#34;state&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=ln>19</span><span class=cl>            <span class=s2>&#34;zip&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=ln>20</span><span class=cl>            <span class=s2>&#34;birth_date&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=ln>21</span><span class=cl>            <span class=s2>&#34;source&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=ln>22</span><span class=cl>            <span class=s2>&#34;created_at&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=ln>23</span><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=ln>24</span><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=ln>25</span><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;users - upsert records...&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=ln>26</span><span class=cl>    <span class=n>src_df</span><span class=o>.</span><span class=n>createOrReplaceTempView</span><span class=p>(</span><span class=s2>&#34;users_tbl&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=ln>27</span><span class=cl>    <span class=n>spark_session</span><span class=o>.</span><span class=n>sql</span><span class=p>(</span>
</span></span><span class=line><span class=ln>28</span><span class=cl>        <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=ln>29</span><span class=cl><span class=s2>        MERGE INTO demo.dwh.users t
</span></span></span><span class=line><span class=ln>30</span><span class=cl><span class=s2>        USING (SELECT * FROM users_tbl ORDER BY user_sk) s
</span></span></span><span class=line><span class=ln>31</span><span class=cl><span class=s2>        ON s.user_sk = t.user_sk
</span></span></span><span class=line><span class=ln>32</span><span class=cl><span class=s2>        WHEN MATCHED THEN UPDATE SET *
</span></span></span><span class=line><span class=ln>33</span><span class=cl><span class=s2>        WHEN NOT MATCHED THEN INSERT *
</span></span></span><span class=line><span class=ln>34</span><span class=cl><span class=s2>        &#34;&#34;&#34;</span>
</span></span><span class=line><span class=ln>35</span><span class=cl>    <span class=p>)</span>
</span></span></code></pre></div><h3 id=products-1 data-numberify>Products<a class="anchor ms-1" href=#products-1></a></h3><p>The source dataframe is created by adding the surrogate key while concatenating the <em>id</em> and _created_at _columns, followed by changing data types of relevant columns and selecting columns in the same order as the table is created. The view (<em>products_tbl</em>) that is created from the source dataframe is used to query all the records that have the product ids in the source table - see <em>products_to_update</em>. Note we need data from the products table in order to update <em>eff_from</em>, _eff_to _and _current_flag _column values. Then <em>eff_lead</em> is added to the result set, which is the next record&rsquo;s created_at value for a given product id - see <em>products_updated</em>. The final result set is created by determining the _curr_flag _and _eff_to _column value. Note that the _eff_to _value of the last record for a product is set to ‘<em>9999-12-31 00:00:00</em>&rsquo; in order to make it easy to query the relevant records. The updated records are updated/inserted by executing MERGE operation by comparing the surrogate key values to those of the target products table</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=ln> 1</span><span class=cl><span class=c1># src.py</span>
</span></span><span class=line><span class=ln> 2</span><span class=cl><span class=k>def</span> <span class=nf>etl_products</span><span class=p>(</span><span class=n>file_path</span><span class=p>:</span> <span class=nb>str</span><span class=p>,</span> <span class=n>spark_session</span><span class=p>:</span> <span class=n>SparkSession</span><span class=p>):</span>
</span></span><span class=line><span class=ln> 3</span><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;products - transform records...&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=ln> 4</span><span class=cl>    <span class=n>src_df</span> <span class=o>=</span> <span class=p>(</span>
</span></span><span class=line><span class=ln> 5</span><span class=cl>        <span class=n>spark_session</span><span class=o>.</span><span class=n>read</span><span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>&#34;header&#34;</span><span class=p>,</span> <span class=s2>&#34;true&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=ln> 6</span><span class=cl>        <span class=o>.</span><span class=n>csv</span><span class=p>(</span><span class=n>file_path</span><span class=p>)</span>
</span></span><span class=line><span class=ln> 7</span><span class=cl>        <span class=o>.</span><span class=n>withColumn</span><span class=p>(</span><span class=s2>&#34;prod_sk&#34;</span><span class=p>,</span> <span class=n>md5</span><span class=p>(</span><span class=n>concat</span><span class=p>(</span><span class=s2>&#34;id&#34;</span><span class=p>,</span> <span class=s2>&#34;created_at&#34;</span><span class=p>)))</span>
</span></span><span class=line><span class=ln> 8</span><span class=cl>        <span class=o>.</span><span class=n>withColumn</span><span class=p>(</span><span class=s2>&#34;id&#34;</span><span class=p>,</span> <span class=n>expr</span><span class=p>(</span><span class=s2>&#34;CAST(id AS bigint)&#34;</span><span class=p>))</span>
</span></span><span class=line><span class=ln> 9</span><span class=cl>        <span class=o>.</span><span class=n>withColumn</span><span class=p>(</span><span class=s2>&#34;price&#34;</span><span class=p>,</span> <span class=n>expr</span><span class=p>(</span><span class=s2>&#34;CAST(price AS decimal(6,3))&#34;</span><span class=p>))</span>
</span></span><span class=line><span class=ln>10</span><span class=cl>        <span class=o>.</span><span class=n>withColumn</span><span class=p>(</span><span class=s2>&#34;created_at&#34;</span><span class=p>,</span> <span class=n>to_timestamp</span><span class=p>(</span><span class=s2>&#34;created_at&#34;</span><span class=p>))</span>
</span></span><span class=line><span class=ln>11</span><span class=cl>        <span class=o>.</span><span class=n>withColumn</span><span class=p>(</span><span class=s2>&#34;curr_flag&#34;</span><span class=p>,</span> <span class=n>expr</span><span class=p>(</span><span class=s2>&#34;CAST(NULL AS int)&#34;</span><span class=p>))</span>
</span></span><span class=line><span class=ln>12</span><span class=cl>        <span class=o>.</span><span class=n>withColumn</span><span class=p>(</span><span class=s2>&#34;eff_from&#34;</span><span class=p>,</span> <span class=n>col</span><span class=p>(</span><span class=s2>&#34;created_at&#34;</span><span class=p>))</span>
</span></span><span class=line><span class=ln>13</span><span class=cl>        <span class=o>.</span><span class=n>withColumn</span><span class=p>(</span><span class=s2>&#34;eff_to&#34;</span><span class=p>,</span> <span class=n>expr</span><span class=p>(</span><span class=s2>&#34;CAST(NULL AS timestamp)&#34;</span><span class=p>))</span>
</span></span><span class=line><span class=ln>14</span><span class=cl>        <span class=o>.</span><span class=n>select</span><span class=p>(</span>
</span></span><span class=line><span class=ln>15</span><span class=cl>            <span class=s2>&#34;prod_sk&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=ln>16</span><span class=cl>            <span class=s2>&#34;id&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=ln>17</span><span class=cl>            <span class=s2>&#34;category&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=ln>18</span><span class=cl>            <span class=s2>&#34;price&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=ln>19</span><span class=cl>            <span class=s2>&#34;title&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=ln>20</span><span class=cl>            <span class=s2>&#34;vendor&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=ln>21</span><span class=cl>            <span class=s2>&#34;curr_flag&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=ln>22</span><span class=cl>            <span class=s2>&#34;eff_from&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=ln>23</span><span class=cl>            <span class=s2>&#34;eff_to&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=ln>24</span><span class=cl>            <span class=s2>&#34;created_at&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=ln>25</span><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=ln>26</span><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=ln>27</span><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;products - upsert records...&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=ln>28</span><span class=cl>    <span class=n>src_df</span><span class=o>.</span><span class=n>createOrReplaceTempView</span><span class=p>(</span><span class=s2>&#34;products_tbl&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=ln>29</span><span class=cl>    <span class=n>products_update_qry</span> <span class=o>=</span> <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=ln>30</span><span class=cl><span class=s2>    WITH products_to_update AS (
</span></span></span><span class=line><span class=ln>31</span><span class=cl><span class=s2>        SELECT l.*
</span></span></span><span class=line><span class=ln>32</span><span class=cl><span class=s2>        FROM demo.dwh.products AS l
</span></span></span><span class=line><span class=ln>33</span><span class=cl><span class=s2>        JOIN products_tbl AS r ON l.id = r.id
</span></span></span><span class=line><span class=ln>34</span><span class=cl><span class=s2>        UNION
</span></span></span><span class=line><span class=ln>35</span><span class=cl><span class=s2>        SELECT *
</span></span></span><span class=line><span class=ln>36</span><span class=cl><span class=s2>        FROM products_tbl
</span></span></span><span class=line><span class=ln>37</span><span class=cl><span class=s2>    ), products_updated AS (
</span></span></span><span class=line><span class=ln>38</span><span class=cl><span class=s2>        SELECT *,
</span></span></span><span class=line><span class=ln>39</span><span class=cl><span class=s2>                LEAD(created_at) OVER (PARTITION BY id ORDER BY created_at) AS eff_lead
</span></span></span><span class=line><span class=ln>40</span><span class=cl><span class=s2>        FROM products_to_update
</span></span></span><span class=line><span class=ln>41</span><span class=cl><span class=s2>    )
</span></span></span><span class=line><span class=ln>42</span><span class=cl><span class=s2>    SELECT prod_sk,
</span></span></span><span class=line><span class=ln>43</span><span class=cl><span class=s2>            id,
</span></span></span><span class=line><span class=ln>44</span><span class=cl><span class=s2>            category,
</span></span></span><span class=line><span class=ln>45</span><span class=cl><span class=s2>            price,
</span></span></span><span class=line><span class=ln>46</span><span class=cl><span class=s2>            title,
</span></span></span><span class=line><span class=ln>47</span><span class=cl><span class=s2>            vendor,
</span></span></span><span class=line><span class=ln>48</span><span class=cl><span class=s2>            (CASE WHEN eff_lead IS NULL THEN 1 ELSE 0 END) AS curr_flag,
</span></span></span><span class=line><span class=ln>49</span><span class=cl><span class=s2>            eff_from,
</span></span></span><span class=line><span class=ln>50</span><span class=cl><span class=s2>            COALESCE(eff_lead, to_timestamp(&#39;9999-12-31 00:00:00&#39;)) AS eff_to,
</span></span></span><span class=line><span class=ln>51</span><span class=cl><span class=s2>            created_at
</span></span></span><span class=line><span class=ln>52</span><span class=cl><span class=s2>    FROM products_updated
</span></span></span><span class=line><span class=ln>53</span><span class=cl><span class=s2>    ORDER BY prod_sk
</span></span></span><span class=line><span class=ln>54</span><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=ln>55</span><span class=cl>    <span class=n>spark_session</span><span class=o>.</span><span class=n>sql</span><span class=p>(</span>
</span></span><span class=line><span class=ln>56</span><span class=cl>        <span class=sa>f</span><span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=ln>57</span><span class=cl><span class=s2>        MERGE INTO demo.dwh.products t
</span></span></span><span class=line><span class=ln>58</span><span class=cl><span class=s2>        USING (</span><span class=si>{</span><span class=n>products_update_qry</span><span class=si>}</span><span class=s2>) s
</span></span></span><span class=line><span class=ln>59</span><span class=cl><span class=s2>        ON s.prod_sk = t.prod_sk
</span></span></span><span class=line><span class=ln>60</span><span class=cl><span class=s2>        WHEN MATCHED THEN UPDATE SET *
</span></span></span><span class=line><span class=ln>61</span><span class=cl><span class=s2>        WHEN NOT MATCHED THEN INSERT *
</span></span></span><span class=line><span class=ln>62</span><span class=cl><span class=s2>        &#34;&#34;&#34;</span>
</span></span><span class=line><span class=ln>63</span><span class=cl>    <span class=p>)</span>
</span></span></code></pre></div><h3 id=orders-1 data-numberify>Orders<a class="anchor ms-1" href=#orders-1></a></h3><p>After transformation, a view (<em>orders_tbl</em>) is created from the source dataframe. The relevant user (<em>user_sk</em>) and product (<em>prod_sk</em>) surrogate keys are added to source data by joining the users and products dimension tables. The users table is SCD type 1 so matching the _user_id _alone is enough for the join condition. On the other hand, additional join condition based on the _eff_from _and _eff_to _columns is necessary for the products table as it is SCD type 2 and records in that table have their own effective periods. Note that ideally we should be able to apply INNER JOIN but the sample data is not clean and some product records are not matched by that operation. For example, an order whose id is 15 is made at 2018-06-26 02:24:38 with a product whose id is 116. However the earliest record of that product is created at 2018-09-12 15:23:05 and it&rsquo;ll be missed by INNER JOIN. Therefore LEFT JOIN is applied to create the initial result set (<em>orders_updated</em>) and, for those products that are not matched, the surrogate keys of the earliest records are added instead. Finally the updated order records are appended using the <a href=https://iceberg.apache.org/docs/latest/spark-writes/#appending-data target=_blank rel="noopener noreferrer">DataFrameWriterV2 API<i class="fas fa-external-link-square-alt ms-1"></i></a>.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=ln> 1</span><span class=cl><span class=c1># src.py</span>
</span></span><span class=line><span class=ln> 2</span><span class=cl><span class=k>def</span> <span class=nf>etl_orders</span><span class=p>(</span><span class=n>file_path</span><span class=p>:</span> <span class=nb>str</span><span class=p>,</span> <span class=n>spark_session</span><span class=p>:</span> <span class=n>SparkSession</span><span class=p>):</span>
</span></span><span class=line><span class=ln> 3</span><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;orders - transform records...&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=ln> 4</span><span class=cl>    <span class=n>src_df</span> <span class=o>=</span> <span class=p>(</span>
</span></span><span class=line><span class=ln> 5</span><span class=cl>        <span class=n>spark_session</span><span class=o>.</span><span class=n>read</span><span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>&#34;header&#34;</span><span class=p>,</span> <span class=s2>&#34;true&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=ln> 6</span><span class=cl>        <span class=o>.</span><span class=n>csv</span><span class=p>(</span><span class=n>file_path</span><span class=p>)</span>
</span></span><span class=line><span class=ln> 7</span><span class=cl>        <span class=o>.</span><span class=n>withColumn</span><span class=p>(</span><span class=s2>&#34;id&#34;</span><span class=p>,</span> <span class=n>expr</span><span class=p>(</span><span class=s2>&#34;CAST(id AS bigint)&#34;</span><span class=p>))</span>
</span></span><span class=line><span class=ln> 8</span><span class=cl>        <span class=o>.</span><span class=n>withColumn</span><span class=p>(</span><span class=s2>&#34;user_id&#34;</span><span class=p>,</span> <span class=n>expr</span><span class=p>(</span><span class=s2>&#34;CAST(user_id AS bigint)&#34;</span><span class=p>))</span>
</span></span><span class=line><span class=ln> 9</span><span class=cl>        <span class=o>.</span><span class=n>withColumn</span><span class=p>(</span><span class=s2>&#34;product_id&#34;</span><span class=p>,</span> <span class=n>expr</span><span class=p>(</span><span class=s2>&#34;CAST(product_id AS bigint)&#34;</span><span class=p>))</span>
</span></span><span class=line><span class=ln>10</span><span class=cl>        <span class=o>.</span><span class=n>withColumn</span><span class=p>(</span><span class=s2>&#34;discount&#34;</span><span class=p>,</span> <span class=n>expr</span><span class=p>(</span><span class=s2>&#34;CAST(discount AS decimal(4,2))&#34;</span><span class=p>))</span>
</span></span><span class=line><span class=ln>11</span><span class=cl>        <span class=o>.</span><span class=n>withColumn</span><span class=p>(</span><span class=s2>&#34;quantity&#34;</span><span class=p>,</span> <span class=n>expr</span><span class=p>(</span><span class=s2>&#34;CAST(quantity AS int)&#34;</span><span class=p>))</span>
</span></span><span class=line><span class=ln>12</span><span class=cl>        <span class=o>.</span><span class=n>withColumn</span><span class=p>(</span><span class=s2>&#34;created_at&#34;</span><span class=p>,</span> <span class=n>to_timestamp</span><span class=p>(</span><span class=s2>&#34;created_at&#34;</span><span class=p>))</span>
</span></span><span class=line><span class=ln>13</span><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=ln>14</span><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;orders - append records...&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=ln>15</span><span class=cl>    <span class=n>src_df</span><span class=o>.</span><span class=n>createOrReplaceTempView</span><span class=p>(</span><span class=s2>&#34;orders_tbl&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=ln>16</span><span class=cl>    <span class=n>spark_session</span><span class=o>.</span><span class=n>sql</span><span class=p>(</span>
</span></span><span class=line><span class=ln>17</span><span class=cl>        <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=ln>18</span><span class=cl><span class=s2>        WITH src_products AS (
</span></span></span><span class=line><span class=ln>19</span><span class=cl><span class=s2>            SELECT * FROM demo.dwh.products
</span></span></span><span class=line><span class=ln>20</span><span class=cl><span class=s2>        ), orders_updated AS (
</span></span></span><span class=line><span class=ln>21</span><span class=cl><span class=s2>            SELECT o.*, u.user_sk, p.prod_sk
</span></span></span><span class=line><span class=ln>22</span><span class=cl><span class=s2>            FROM orders_tbl o
</span></span></span><span class=line><span class=ln>23</span><span class=cl><span class=s2>            LEFT JOIN demo.dwh.users u
</span></span></span><span class=line><span class=ln>24</span><span class=cl><span class=s2>                ON o.user_id = u.id
</span></span></span><span class=line><span class=ln>25</span><span class=cl><span class=s2>            LEFT JOIN src_products p
</span></span></span><span class=line><span class=ln>26</span><span class=cl><span class=s2>                ON o.product_id = p.id
</span></span></span><span class=line><span class=ln>27</span><span class=cl><span class=s2>                AND o.created_at &gt;= p.eff_from
</span></span></span><span class=line><span class=ln>28</span><span class=cl><span class=s2>                AND o.created_at &lt; p.eff_to
</span></span></span><span class=line><span class=ln>29</span><span class=cl><span class=s2>        ), products_tbl AS (
</span></span></span><span class=line><span class=ln>30</span><span class=cl><span class=s2>            SELECT prod_sk,
</span></span></span><span class=line><span class=ln>31</span><span class=cl><span class=s2>                   id,
</span></span></span><span class=line><span class=ln>32</span><span class=cl><span class=s2>                   ROW_NUMBER() OVER (PARTITION BY id ORDER BY eff_from) AS rn
</span></span></span><span class=line><span class=ln>33</span><span class=cl><span class=s2>            FROM src_products
</span></span></span><span class=line><span class=ln>34</span><span class=cl><span class=s2>        )
</span></span></span><span class=line><span class=ln>35</span><span class=cl><span class=s2>        SELECT o.user_sk,
</span></span></span><span class=line><span class=ln>36</span><span class=cl><span class=s2>               COALESCE(o.prod_sk, p.prod_sk) AS prod_sk,
</span></span></span><span class=line><span class=ln>37</span><span class=cl><span class=s2>               o.id,
</span></span></span><span class=line><span class=ln>38</span><span class=cl><span class=s2>               o.discount,
</span></span></span><span class=line><span class=ln>39</span><span class=cl><span class=s2>               o.quantity,
</span></span></span><span class=line><span class=ln>40</span><span class=cl><span class=s2>               o.created_at
</span></span></span><span class=line><span class=ln>41</span><span class=cl><span class=s2>        FROM orders_updated AS o
</span></span></span><span class=line><span class=ln>42</span><span class=cl><span class=s2>        JOIN products_tbl AS p ON o.product_id = p.id
</span></span></span><span class=line><span class=ln>43</span><span class=cl><span class=s2>        WHERE p.rn = 1
</span></span></span><span class=line><span class=ln>44</span><span class=cl><span class=s2>        ORDER BY o.created_at
</span></span></span><span class=line><span class=ln>45</span><span class=cl><span class=s2>        &#34;&#34;&#34;</span>
</span></span><span class=line><span class=ln>46</span><span class=cl>    <span class=p>)</span><span class=o>.</span><span class=n>writeTo</span><span class=p>(</span><span class=s2>&#34;demo.dwh.orders&#34;</span><span class=p>)</span><span class=o>.</span><span class=n>append</span><span class=p>()</span>
</span></span></code></pre></div><h3 id=run-etl data-numberify>Run ETL<a class="anchor ms-1" href=#run-etl></a></h3><p>The ETL script begins with creating all the tables - users, products and orders. Then the ETL for the users table is executed. Note that, although it is executed as initial loading, the code can also be applied to incremental ETL. Finally, incremental ETL is executed for the products and orders tables. The application can be submitted by <code>./run.sh spark-submit etl.py</code>. Note to create the following environment variables before submitting the application.</p><ul><li><em>AWS_ACCESS_KEY_ID</em></li><li><em>AWS_SECRET_ACCESS_KEY</em></li><li><em>AWS_SESSION_TOKEN</em><ul><li>Note it is optional and required if authentication is made via assume role</li></ul></li><li><em>AWS_REGION</em><ul><li>Note it is NOT <em>AWS_DEFAULT_REGION</em></li></ul></li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=ln> 1</span><span class=cl><span class=c1># etl.py</span>
</span></span><span class=line><span class=ln> 2</span><span class=cl><span class=kn>from</span> <span class=nn>pyspark.sql</span> <span class=kn>import</span> <span class=n>SparkSession</span>
</span></span><span class=line><span class=ln> 3</span><span class=cl><span class=kn>from</span> <span class=nn>src</span> <span class=kn>import</span> <span class=n>create_tables</span><span class=p>,</span> <span class=n>etl_users</span><span class=p>,</span> <span class=n>etl_products</span><span class=p>,</span> <span class=n>etl_orders</span>
</span></span><span class=line><span class=ln> 4</span><span class=cl>
</span></span><span class=line><span class=ln> 5</span><span class=cl><span class=n>spark</span> <span class=o>=</span> <span class=n>SparkSession</span><span class=o>.</span><span class=n>builder</span><span class=o>.</span><span class=n>appName</span><span class=p>(</span><span class=s2>&#34;Iceberg ETL Demo&#34;</span><span class=p>)</span><span class=o>.</span><span class=n>getOrCreate</span><span class=p>()</span>
</span></span><span class=line><span class=ln> 6</span><span class=cl>
</span></span><span class=line><span class=ln> 7</span><span class=cl><span class=c1>## create all tables - demo.dwh.users, demo.dwh.products and demo.dwh.orders</span>
</span></span><span class=line><span class=ln> 8</span><span class=cl><span class=n>create_tables</span><span class=p>(</span><span class=n>spark_session</span><span class=o>=</span><span class=n>spark</span><span class=p>)</span>
</span></span><span class=line><span class=ln> 9</span><span class=cl>
</span></span><span class=line><span class=ln>10</span><span class=cl><span class=c1>## users etl - assuming SCD type 1</span>
</span></span><span class=line><span class=ln>11</span><span class=cl><span class=n>etl_users</span><span class=p>(</span><span class=s2>&#34;./data/users.csv&#34;</span><span class=p>,</span> <span class=n>spark</span><span class=p>)</span>
</span></span><span class=line><span class=ln>12</span><span class=cl>
</span></span><span class=line><span class=ln>13</span><span class=cl><span class=c1>## incremental ETL</span>
</span></span><span class=line><span class=ln>14</span><span class=cl><span class=k>for</span> <span class=n>yr</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=mi>4</span><span class=p>):</span>
</span></span><span class=line><span class=ln>15</span><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;processing year </span><span class=si>{</span><span class=n>yr</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=ln>16</span><span class=cl>    <span class=c1>## products etl - assuming SCD type 2</span>
</span></span><span class=line><span class=ln>17</span><span class=cl>    <span class=n>etl_products</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;./data/products_year_</span><span class=si>{</span><span class=n>yr</span><span class=si>}</span><span class=s2>.csv&#34;</span><span class=p>,</span> <span class=n>spark</span><span class=p>)</span>
</span></span><span class=line><span class=ln>18</span><span class=cl>    <span class=c1>## orders etl - relevant user_sk and prod_sk are added during transformation</span>
</span></span><span class=line><span class=ln>19</span><span class=cl>    <span class=n>etl_orders</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;./data/orders_year_</span><span class=si>{</span><span class=n>yr</span><span class=si>}</span><span class=s2>.csv&#34;</span><span class=p>,</span> <span class=n>spark</span><span class=p>)</span>
</span></span></code></pre></div><p>Once the application completes, we&rsquo;re able to query the iceberg tables on Athena. The following query returns all products whose id is 1. It is shown that the price increases over time and the relevant columns (<em>curr_flag, eff_from and eff_to</em>) for SCD type 2 are created as expected.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-sql data-lang=sql><span class=line><span class=ln>1</span><span class=cl><span class=k>SELECT</span><span class=w> </span><span class=o>*</span><span class=w>
</span></span></span><span class=line><span class=ln>2</span><span class=cl><span class=w></span><span class=k>FROM</span><span class=w> </span><span class=n>dwh</span><span class=p>.</span><span class=n>products</span><span class=w>
</span></span></span><span class=line><span class=ln>3</span><span class=cl><span class=w></span><span class=k>WHERE</span><span class=w> </span><span class=n>id</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=mi>1</span><span class=w>
</span></span></span><span class=line><span class=ln>4</span><span class=cl><span class=w></span><span class=k>ORDER</span><span class=w> </span><span class=k>BY</span><span class=w> </span><span class=n>eff_from</span><span class=w>
</span></span></span></code></pre></div><p><picture><img class="img-fluid mx-auto d-block" alt src=/blog/2022-06-26-iceberg-etl-demo/01_products.png loading=lazy width=1599 height=313></picture></p><p>The following query returns sample order records that bought the product. It can be checked that the product surrogate key matches the products dimension records.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-sql data-lang=sql><span class=line><span class=ln> 1</span><span class=cl><span class=k>WITH</span><span class=w> </span><span class=n>src_orders</span><span class=w> </span><span class=k>AS</span><span class=w> </span><span class=p>(</span><span class=w>
</span></span></span><span class=line><span class=ln> 2</span><span class=cl><span class=w>    </span><span class=k>SELECT</span><span class=w> </span><span class=n>o</span><span class=p>.</span><span class=n>user_sk</span><span class=p>,</span><span class=w> </span><span class=n>o</span><span class=p>.</span><span class=n>prod_sk</span><span class=p>,</span><span class=w> </span><span class=n>o</span><span class=p>.</span><span class=n>id</span><span class=p>,</span><span class=w> </span><span class=n>p</span><span class=p>.</span><span class=n>title</span><span class=p>,</span><span class=w> </span><span class=n>p</span><span class=p>.</span><span class=n>price</span><span class=p>,</span><span class=w> </span><span class=n>o</span><span class=p>.</span><span class=n>discount</span><span class=p>,</span><span class=w> </span><span class=n>o</span><span class=p>.</span><span class=n>quantity</span><span class=p>,</span><span class=w> </span><span class=n>o</span><span class=p>.</span><span class=n>created_at</span><span class=p>,</span><span class=w>
</span></span></span><span class=line><span class=ln> 3</span><span class=cl><span class=w>           </span><span class=n>ROW_NUMBER</span><span class=p>()</span><span class=w> </span><span class=n>OVER</span><span class=w> </span><span class=p>(</span><span class=n>PARTITION</span><span class=w> </span><span class=k>BY</span><span class=w> </span><span class=n>p</span><span class=p>.</span><span class=n>price</span><span class=w> </span><span class=k>ORDER</span><span class=w> </span><span class=k>BY</span><span class=w> </span><span class=n>o</span><span class=p>.</span><span class=n>created_at</span><span class=p>)</span><span class=w> </span><span class=k>AS</span><span class=w> </span><span class=n>rn</span><span class=w>
</span></span></span><span class=line><span class=ln> 4</span><span class=cl><span class=w>    </span><span class=k>FROM</span><span class=w> </span><span class=n>dwh</span><span class=p>.</span><span class=n>orders</span><span class=w> </span><span class=k>AS</span><span class=w> </span><span class=n>o</span><span class=w>
</span></span></span><span class=line><span class=ln> 5</span><span class=cl><span class=w>    </span><span class=k>JOIN</span><span class=w> </span><span class=n>dwh</span><span class=p>.</span><span class=n>products</span><span class=w> </span><span class=k>AS</span><span class=w> </span><span class=n>p</span><span class=w> </span><span class=k>ON</span><span class=w> </span><span class=n>o</span><span class=p>.</span><span class=n>prod_sk</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=n>p</span><span class=p>.</span><span class=n>prod_sk</span><span class=w>
</span></span></span><span class=line><span class=ln> 6</span><span class=cl><span class=w>    </span><span class=k>WHERE</span><span class=w> </span><span class=n>p</span><span class=p>.</span><span class=n>id</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=mi>1</span><span class=w>
</span></span></span><span class=line><span class=ln> 7</span><span class=cl><span class=w></span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=ln> 8</span><span class=cl><span class=w></span><span class=k>SELECT</span><span class=w> </span><span class=o>*</span><span class=w>
</span></span></span><span class=line><span class=ln> 9</span><span class=cl><span class=w></span><span class=k>FROM</span><span class=w> </span><span class=n>src_orders</span><span class=w>
</span></span></span><span class=line><span class=ln>10</span><span class=cl><span class=w></span><span class=k>WHERE</span><span class=w> </span><span class=n>rn</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=mi>1</span><span class=w>
</span></span></span><span class=line><span class=ln>11</span><span class=cl><span class=w></span><span class=k>ORDER</span><span class=w> </span><span class=k>BY</span><span class=w> </span><span class=n>created_at</span><span class=w>
</span></span></span></code></pre></div><p><picture><img class="img-fluid mx-auto d-block" alt src=/blog/2022-06-26-iceberg-etl-demo/02_orders.png loading=lazy width=1599 height=252></picture></p><h2 id=summary data-numberify>Summary<a class="anchor ms-1" href=#summary></a></h2><p>In this post, we discussed how to implement ETL using retail analytics data. In transformation, SCD type 1 and SCD type 2 are applied to the user and product data respectively. For the order data, the corresponding surrogate keys of the user and product data are added. A Pyspark application that implements ETL against Iceberg tables is used for demonstration in an EMR location environment. Finally, the ETL results will be queried by Athena for verification.</p></div></div></div><div class=card-footer><div class="post-navs d-flex justify-content-evenly"><div class="post-nav post-prev"><i class="fas fa-fw fa-chevron-down post-prev-icon me-1" data-fa-transform=rotate-90></i>
<a href=/blog/2022-05-08-emr-local-dev/>Develop and Test Apache Spark Apps for EMR Locally Using Docker</a></div><div class="post-nav post-next"><a href=/blog/2022-07-18-sam-for-data-professionals/>Serverless Application Model (SAM) for Data Professionals</a>
<i class="fas fa-fw fa-chevron-down post-next-icon ms-1" data-fa-transform=rotate-270></i></div></div></div></article><section class="related-posts row card component"><div class=card-header><h2 class="card-title fs-4 my-2 text-surface">Related Posts</h2></div><div class="card-body slide px-1"><div class="slide-inner row gx-0"><div class="col-12 col-md-6 col-lg-4 me-2"><div class="post-card card card-body p-0 border-0 surface"><picture><source srcset=/blog/2022-05-08-emr-local-dev/featured_hu653aa93a98a5139fededca231658be70_25693_500x0_resize_box_3.png media="(max-width: 576px)" height=291 width=500><img class=img-fluid height=105 width=180 alt=featured.png src=/blog/2022-05-08-emr-local-dev/featured_hu653aa93a98a5139fededca231658be70_25693_180x0_resize_box_3.png data-src=/blog/2022-05-08-emr-local-dev/featured.png loading=lazy>
</picture><a class=post-title href=/blog/2022-05-08-emr-local-dev/>Develop and Test Apache Spark Apps for EMR Locally Using Docker</a><div class="post-meta mb-0">May 8, 2022</div></div></div><div class="col-12 col-md-6 col-lg-4 me-2"><div class="post-card card card-body p-0 border-0 surface"><picture><source srcset=/blog/2021-12-05-datalake-demo-part1/featured_hua25eccd3824300d3b1ed87f56797248c_164526_500x0_resize_box_3.png media="(max-width: 576px)" height=229 width=500><img class=img-fluid height=83 width=180 alt=featured.png src=/blog/2021-12-05-datalake-demo-part1/featured_hua25eccd3824300d3b1ed87f56797248c_164526_180x0_resize_box_3.png data-src=/blog/2021-12-05-datalake-demo-part1/featured.png loading=lazy>
</picture><a class=post-title href=/blog/2021-12-05-datalake-demo-part1/>Data Lake Demo Using Change Data Capture (CDC) on AWS – Part 1 Local Development</a><div class="post-meta mb-0">December 5, 2021</div></div></div><div class="col-12 col-md-6 col-lg-4 me-2"><div class="post-card card card-body p-0 border-0 surface"><picture><source srcset=/blog/2021-12-19-datalake-demo-part3/featured_hua25eccd3824300d3b1ed87f56797248c_164526_500x0_resize_box_3.png media="(max-width: 576px)" height=229 width=500><img class=img-fluid height=83 width=180 alt=featured.png src=/blog/2021-12-19-datalake-demo-part3/featured_hua25eccd3824300d3b1ed87f56797248c_164526_180x0_resize_box_3.png data-src=/blog/2021-12-19-datalake-demo-part3/featured.png loading=lazy>
</picture><a class=post-title href=/blog/2021-12-19-datalake-demo-part3/>Data Lake Demo Using Change Data Capture (CDC) on AWS – Part 3 Implement Data Lake</a><div class="post-meta mb-0">December 19, 2021</div></div></div><div class="col-12 col-md-6 col-lg-4 me-2"><div class="post-card card card-body p-0 border-0 surface"><picture><source srcset=/blog/2021-12-12-datalake-demo-part2/featured_hua25eccd3824300d3b1ed87f56797248c_164526_500x0_resize_box_3.png media="(max-width: 576px)" height=229 width=500><img class=img-fluid height=83 width=180 alt=featured.png src=/blog/2021-12-12-datalake-demo-part2/featured_hua25eccd3824300d3b1ed87f56797248c_164526_180x0_resize_box_3.png data-src=/blog/2021-12-12-datalake-demo-part2/featured.png loading=lazy>
</picture><a class=post-title href=/blog/2021-12-12-datalake-demo-part2/>Data Lake Demo Using Change Data Capture (CDC) on AWS – Part 2 Implement CDC</a><div class="post-meta mb-0">December 12, 2021</div></div></div><div class="col-12 col-md-6 col-lg-4 me-2"><div class="post-card card card-body p-0 border-0 surface"><picture><source srcset=/blog/2021-11-14-glue-3-local-development/featured_hu689859cb443a98b1c98384fedaa00395_30923_500x0_resize_box_3.png media="(max-width: 576px)" height=296 width=500><img class=img-fluid height=107 width=180 alt=featured.png src=/blog/2021-11-14-glue-3-local-development/featured_hu689859cb443a98b1c98384fedaa00395_30923_180x0_resize_box_3.png data-src=/blog/2021-11-14-glue-3-local-development/featured.png loading=lazy>
</picture><a class=post-title href=/blog/2021-11-14-glue-3-local-development/>Local Development of AWS Glue 3.0 and Later</a><div class="post-meta mb-0">November 14, 2021</div></div></div><div class="col-12 col-md-6 col-lg-4 me-2"><div class="post-card card card-body p-0 border-0 surface"><picture><source srcset=/blog/2021-08-20-glue-local-development/featured_hu248b4052e45f408d4fe80445a9d59f15_19535_500x0_resize_box_3.png media="(max-width: 576px)" height=141 width=500><img class=img-fluid height=51 width=180 alt=featured.png src=/blog/2021-08-20-glue-local-development/featured_hu248b4052e45f408d4fe80445a9d59f15_19535_180x0_resize_box_3.png data-src=/blog/2021-08-20-glue-local-development/featured.png loading=lazy>
</picture><a class=post-title href=/blog/2021-08-20-glue-local-development/>AWS Glue Local Development With Docker and Visual Studio Code</a><div class="post-meta mb-0">August 20, 2021</div></div></div><div class="col-12 col-md-6 col-lg-4 me-2"><div class="post-card card card-body p-0 border-0 surface"><picture><source srcset=/blog/2022-04-03-schema-registry-part2/featured_hu55ef0aeef397b55768e961367d004bbc_59689_500x0_resize_box_3.png media="(max-width: 576px)" height=252 width=500><img class=img-fluid height=91 width=180 alt=featured.png src=/blog/2022-04-03-schema-registry-part2/featured_hu55ef0aeef397b55768e961367d004bbc_59689_180x0_resize_box_3.png data-src=/blog/2022-04-03-schema-registry-part2/featured.png loading=lazy>
</picture><a class=post-title href=/blog/2022-04-03-schema-registry-part2/>Use External Schema Registry With MSK Connect – Part 2 MSK Deployment</a><div class="post-meta mb-0">April 3, 2022</div></div></div><div class="col-12 col-md-6 col-lg-4 me-2"><div class="post-card card card-body p-0 border-0 surface"><picture><source srcset=/blog/2022-03-07-schema-registry-part1/featured_hu55ef0aeef397b55768e961367d004bbc_59689_500x0_resize_box_3.png media="(max-width: 576px)" height=252 width=500><img class=img-fluid height=91 width=180 alt=featured.png src=/blog/2022-03-07-schema-registry-part1/featured_hu55ef0aeef397b55768e961367d004bbc_59689_180x0_resize_box_3.png data-src=/blog/2022-03-07-schema-registry-part1/featured.png loading=lazy>
</picture><a class=post-title href=/blog/2022-03-07-schema-registry-part1/>Use External Schema Registry With MSK Connect – Part 1 Local Development</a><div class="post-meta mb-0">March 7, 2022</div></div></div><div class="col-12 col-md-6 col-lg-4 me-2"><div class="post-card card card-body p-0 border-0 surface"><picture><source srcset=/blog/2022-01-17-emr-on-eks-by-example/featured_hu7346e7fea9bf42a33719962b2d46c84d_76740_500x0_resize_box_3.png media="(max-width: 576px)" height=292 width=500><img class=img-fluid height=105 width=180 alt=featured.png src=/blog/2022-01-17-emr-on-eks-by-example/featured_hu7346e7fea9bf42a33719962b2d46c84d_76740_180x0_resize_box_3.png data-src=/blog/2022-01-17-emr-on-eks-by-example/featured.png loading=lazy>
</picture><a class=post-title href=/blog/2022-01-17-emr-on-eks-by-example/>EMR on EKS by Example</a><div class="post-meta mb-0">January 17, 2022</div></div></div><div class="col-12 col-md-6 col-lg-4 me-2"><div class="post-card card card-body p-0 border-0 surface"><picture><source srcset=/blog/2020-04-13-airflow-lambda-operator/featured_hu1fb366ed8468cd95c289e8b53adf2b4c_44994_500x0_resize_box_3.png media="(max-width: 576px)" height=144 width=500><img class=img-fluid height=52 width=180 alt=featured.png src=/blog/2020-04-13-airflow-lambda-operator/featured_hu1fb366ed8468cd95c289e8b53adf2b4c_44994_180x0_resize_box_3.png data-src=/blog/2020-04-13-airflow-lambda-operator/featured.png loading=lazy>
</picture><a class=post-title href=/blog/2020-04-13-airflow-lambda-operator/>Thoughts on Apache Airflow AWS Lambda Operator</a><div class="post-meta mb-0">April 13, 2020</div></div></div></div><button class=slide-control-left>
<i class="fas fa-2x fa-chevron-circle-down" data-fa-transform=rotate-90></i>
<span class=visually-hidden>Left</span>
</button>
<button class=slide-control-right>
<i class="fas fa-2x fa-chevron-circle-down" data-fa-transform=rotate-270></i>
<span class=visually-hidden>Right</span></button></div></section><div class="card component row post-comments" id=post-comments><div class=card-header><h2 class="card-title my-2 fs-4 text-surface">Comments</h2></div><div class=card-body><div class=giscus></div></div></div></div></div><aside class="col-xxl-4 sidebar d-flex"><div class="container d-flex flex-column"><div class="accordion profile"><div class="accordion-item card row text-center component"><div class="accordion-header card-header border-0" id=profile-header><a class="accordion-button d-lg-none mb-2 shadow-none p-0 bg-transparent text-surface" role=button data-bs-toggle=collapse href=#profile aria-expanded=true aria-controls=profile>Profile</a></div><div class="card-body collapse accordion-collapse accordion-body d-lg-block show" id=profile aria-labelledby=profile-header><div class="col-12 d-flex align-items-center justify-content-center"><picture><img class="profile-avatar rounded-circle" alt="Jaehyeon Kim" src="https://jaehyeon.me/images/profile.png?v=90502f53ec5660e987702f4625fb0d18" loading=lazy data-viewer-invisible width=200 height=200></picture></div><div class="col-12 profile-meta"><div class="profile-name fw-fold fs-lg">Jaehyeon Kim</div><div class=profile-bio>Data Engineer 💡 Blogger ⚡ Data Streaming Enthusiast ☁ AWS Community Builder</div></div><nav class="social-links nav justify-content-center mt-1 justify-content-around"><a class="nav-link social-link" href=mailto:dottami@gmail.com title=Email><i class="fas fa-fw fa-2x fa-envelope" style=color:#0963ac></i>
</a><a class="nav-link social-link" target=_blank href=https://github.com/jaehyeon-kim title=GitHub rel=me><i class="fa-fw fa-2x fab fa-github"></i>
</a><a class="nav-link social-link" target=_blank href=https://linkedin.com/in/jaehyeon-kim-76b93429 title=LinkedIn rel=me><i class="fa-fw fa-2x fab fa-linkedin-in" style=color:#0a66c2></i>
</a><a class="nav-link social-link" target=_blank href=https://www.paypal.com/paypalme/dottami title=PayPal rel=me><i class="fa-fw fa-2x fab fa-paypal"></i></a></nav></div></div></div><div class="accordion taxonomies-toggle"><div class="row card component accordion-item"><div class="accordion-header card-header border-0"><a class="accordion-button d-lg-none mb-1 shadow-none p-0 bg-transparent" role=button data-bs-toggle=collapse href=#taxonomies-toggle aria-expanded=true aria-controls=taxonomies-toggle>Taxonomies</a></div><div class="card-body collapse accordion-collapse accordion-body d-lg-block show" id=taxonomies-toggle><ul class="nav nav-pills nav-fill" role=tablist><li class=nav-item role=presentation><button class="nav-link active" id=taxonomyCategoriesTab data-bs-toggle=tab data-bs-target=#taxonomyCategories type=button role=tab aria-controls=taxonomyCategories aria-selected=true>
Categories</button></li><li class=nav-item role=presentation><button class=nav-link id=taxonomyTagsTab data-bs-toggle=tab data-bs-target=#taxonomyTags type=button role=tab aria-controls=taxonomyTags aria-selected=true>
Tags</button></li><li class=nav-item role=presentation><button class=nav-link id=taxonomySeriesTab data-bs-toggle=tab data-bs-target=#taxonomySeries type=button role=tab aria-controls=taxonomySeries aria-selected=true>
Series</button></li><li class=nav-item role=presentation><button class=nav-link id=taxonomyArchivesTab data-bs-toggle=tab data-bs-target=#taxonomyArchives type=button role=tab aria-controls=taxonomyArchives aria-selected=true>
Archives</button></li></ul><div class="tab-content mt-3"><div class="tab-pane active" id=taxonomyCategories role=tabpanel aria-labelledby=taxonomyCategoriesTab tabindex=0><a href=/categories/data-streaming/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-category me-2 mb-2" title="Data Streaming">Data Streaming
<span class="badge badge-sm text-secondary bg-white ms-1">25</span>
</a><a href=/categories/data-engineering/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-category me-2 mb-2" title="Data Engineering">Data Engineering
<span class="badge badge-sm text-secondary bg-white ms-1">24</span>
</a><a href=/categories/development/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-category me-2 mb-2" title=Development>Development
<span class="badge badge-sm text-secondary bg-white ms-1">19</span>
</a><a href=/categories/r/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-category me-2 mb-2" title=R>R
<span class="badge badge-sm text-secondary bg-white ms-1">15</span>
</a><a href=/categories/apache-kafka/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-category me-2 mb-2" title="Apache Kafka">Apache Kafka
<span class="badge badge-sm text-secondary bg-white ms-1">14</span>
</a><a href=/categories/machine-learning/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-category me-2 mb-2" title="Machine Learning">Machine Learning
<span class="badge badge-sm text-secondary bg-white ms-1">7</span>
</a><a href=/categories/general/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-category me-2 mb-2" title=General>General
<span class="badge badge-sm text-secondary bg-white ms-1">5</span>
</a><a href=/categories/apache-beam/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-category me-2 mb-2" title="Apache Beam">Apache Beam
<span class="badge badge-sm text-secondary bg-white ms-1">4</span></a></div><div class=tab-pane id=taxonomyTags role=tabpanel aria-labelledby=taxonomyTagsTab tabindex=0><a href=/tags/docker/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-tag me-2 mb-2" title=Docker>Docker
<span class="badge badge-sm text-secondary bg-white ms-1">51</span>
</a><a href=/tags/python/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-tag me-2 mb-2" title=Python>Python
<span class="badge badge-sm text-secondary bg-white ms-1">49</span>
</a><a href=/tags/apache-kafka/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-tag me-2 mb-2" title="Apache Kafka">Apache Kafka
<span class="badge badge-sm text-secondary bg-white ms-1">44</span>
</a><a href=/tags/aws/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-tag me-2 mb-2" title=AWS>AWS
<span class="badge badge-sm text-secondary bg-white ms-1">43</span>
</a><a href=/tags/docker-compose/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-tag me-2 mb-2" title="Docker Compose">Docker Compose
<span class="badge badge-sm text-secondary bg-white ms-1">43</span>
</a><a href=/tags/r/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-tag me-2 mb-2" title=R>R
<span class="badge badge-sm text-secondary bg-white ms-1">36</span>
</a><a href=/tags/amazon-msk/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-tag me-2 mb-2" title="Amazon MSK">Amazon MSK
<span class="badge badge-sm text-secondary bg-white ms-1">21</span>
</a><a href=/tags/apache-spark/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-tag me-2 mb-2" title="Apache Spark">Apache Spark
<span class="badge badge-sm text-secondary bg-white ms-1">17</span>
</a><a href=/tags/kafka-connect/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-tag me-2 mb-2" title="Kafka Connect">Kafka Connect
<span class="badge badge-sm text-secondary bg-white ms-1">17</span>
</a><a href=/tags/apache-flink/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-tag me-2 mb-2" title="Apache Flink">Apache Flink
<span class="badge badge-sm text-secondary bg-white ms-1">16</span>
</a><a href=/tags/aws-lambda/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-tag me-2 mb-2" title="AWS Lambda">AWS Lambda
<span class="badge badge-sm text-secondary bg-white ms-1">15</span>
</a><a href=/tags/terraform/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-tag me-2 mb-2" title=Terraform>Terraform
<span class="badge badge-sm text-secondary bg-white ms-1">14</span>
</a><a href=/tags/amazon-emr/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-tag me-2 mb-2" title="Amazon EMR">Amazon EMR
<span class="badge badge-sm text-secondary bg-white ms-1">11</span>
</a><a href=/tags/data-build-tool-dbt/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-tag me-2 mb-2" title="Data Build Tool (DBT)">Data Build Tool (DBT)
<span class="badge badge-sm text-secondary bg-white ms-1">11</span>
</a><a href=/tags/pyflink/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-tag me-2 mb-2" title=PyFlink>PyFlink
<span class="badge badge-sm text-secondary bg-white ms-1">11</span>
</a><a href=/tags/amazon-msk-connect/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-tag me-2 mb-2" title="Amazon MSK Connect">Amazon MSK Connect
<span class="badge badge-sm text-secondary bg-white ms-1">9</span>
</a><a href=/tags/kubernetes/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-tag me-2 mb-2" title=Kubernetes>Kubernetes
<span class="badge badge-sm text-secondary bg-white ms-1">8</span>
</a><a href=/tags/amazon-athena/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-tag me-2 mb-2" title="Amazon Athena">Amazon Athena
<span class="badge badge-sm text-secondary bg-white ms-1">6</span>
</a><a href=/tags/pyspark/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-tag me-2 mb-2" title=PySpark>PySpark
<span class="badge badge-sm text-secondary bg-white ms-1">6</span>
</a><a href=/tags/amazon-dynamodb/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-tag me-2 mb-2" title="Amazon DynamoDB">Amazon DynamoDB
<span class="badge badge-sm text-secondary bg-white ms-1">5</span>
</a><a href=/tags/apache-airflow/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-tag me-2 mb-2" title="Apache Airflow">Apache Airflow
<span class="badge badge-sm text-secondary bg-white ms-1">5</span>
</a><a href=/tags/rserve/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-tag me-2 mb-2" title=Rserve>Rserve
<span class="badge badge-sm text-secondary bg-white ms-1">5</span>
</a><a href=/tags/visual-studio-code/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-tag me-2 mb-2" title="Visual Studio Code">Visual Studio Code
<span class="badge badge-sm text-secondary bg-white ms-1">5</span>
</a><a href=/tags/amazon-api-gateway/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-tag me-2 mb-2" title="Amazon API Gateway">Amazon API Gateway
<span class="badge badge-sm text-secondary bg-white ms-1">4</span>
</a><a href=/tags/amazon-managed-flink/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-tag me-2 mb-2" title="Amazon Managed Flink">Amazon Managed Flink
<span class="badge badge-sm text-secondary bg-white ms-1">4</span>
</a><a href=/tags/amazon-managed-service-for-apache-flink/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-tag me-2 mb-2" title="Amazon Managed Service for Apache Flink">Amazon Managed Service for Apache Flink
<span class="badge badge-sm text-secondary bg-white ms-1">4</span>
</a><a href=/tags/amazon-quicksight/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-tag me-2 mb-2" title="Amazon QuickSight">Amazon QuickSight
<span class="badge badge-sm text-secondary bg-white ms-1">4</span>
</a><a href=/tags/apache-beam/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-tag me-2 mb-2" title="Apache Beam">Apache Beam
<span class="badge badge-sm text-secondary bg-white ms-1">4</span>
</a><a href=/tags/aws-glue/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-tag me-2 mb-2" title="AWS Glue">AWS Glue
<span class="badge badge-sm text-secondary bg-white ms-1">4</span>
</a><a href=/tags/minikube/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-tag me-2 mb-2" title=Minikube>Minikube
<span class="badge badge-sm text-secondary bg-white ms-1">4</span>
</a><a href=/tags/opensearch/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-tag me-2 mb-2" title=OpenSearch>OpenSearch
<span class="badge badge-sm text-secondary bg-white ms-1">4</span>
</a><a href=/tags/security/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-tag me-2 mb-2" title=Security>Security
<span class="badge badge-sm text-secondary bg-white ms-1">4</span>
</a><a href=/tags/shiny/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-tag me-2 mb-2" title=Shiny>Shiny
<span class="badge badge-sm text-secondary bg-white ms-1">4</span>
</a><a href=/tags/amazon-eks/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-tag me-2 mb-2" title="Amazon EKS">Amazon EKS
<span class="badge badge-sm text-secondary bg-white ms-1">3</span>
</a><a href=/tags/amazon-opensearch-service/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-tag me-2 mb-2" title="Amazon OpenSearch Service">Amazon OpenSearch Service
<span class="badge badge-sm text-secondary bg-white ms-1">3</span>
</a><a href=/tags/amazon-s3/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-tag me-2 mb-2" title="Amazon S3">Amazon S3
<span class="badge badge-sm text-secondary bg-white ms-1">3</span>
</a><a href=/tags/apache-camel/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-tag me-2 mb-2" title="Apache Camel">Apache Camel
<span class="badge badge-sm text-secondary bg-white ms-1">3</span>
</a><a href=/tags/apache-hudi/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-tag me-2 mb-2" title="Apache Hudi">Apache Hudi
<span class="badge badge-sm text-secondary bg-white ms-1">3</span>
</a><a href=/tags/apache-iceberg/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-tag me-2 mb-2" title="Apache Iceberg">Apache Iceberg
<span class="badge badge-sm text-secondary bg-white ms-1">3</span>
</a><a href=/tags/aws-sam/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-tag me-2 mb-2" title="AWS SAM">AWS SAM
<span class="badge badge-sm text-secondary bg-white ms-1">3</span>
</a><a href=https://jaehyeon.me/tags class="btn btn-sm btn-secondary post-taxonomy ps-3 post-tag me-2 mb-2" title=ALL>ALL
<span class="badge badge-sm text-secondary bg-white ms-1">97</span></a></div><div class=tab-pane id=taxonomySeries role=tabpanel aria-labelledby=taxonomySeriesTab tabindex=0><a href=/series/kafka-development-with-docker/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-series me-2 mb-2" title="Kafka Development With Docker">Kafka Development With Docker
<span class="badge badge-sm text-secondary bg-white ms-1">11</span>
</a><a href=/series/real-time-streaming-with-kafka-and-flink/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-series me-2 mb-2" title="Real Time Streaming With Kafka and Flink">Real Time Streaming With Kafka and Flink
<span class="badge badge-sm text-secondary bg-white ms-1">7</span>
</a><a href=/series/dbt-pizza-shop-demo/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-series me-2 mb-2" title="DBT Pizza Shop Demo">DBT Pizza Shop Demo
<span class="badge badge-sm text-secondary bg-white ms-1">6</span>
</a><a href=/series/tree-based-methods-in-r/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-series me-2 mb-2" title="Tree Based Methods in R">Tree Based Methods in R
<span class="badge badge-sm text-secondary bg-white ms-1">6</span>
</a><a href=/series/dbt-for-effective-data-transformation-on-aws/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-series me-2 mb-2" title="DBT for Effective Data Transformation on AWS">DBT for Effective Data Transformation on AWS
<span class="badge badge-sm text-secondary bg-white ms-1">5</span>
</a><a href=/series/kafka-connect-for-aws-services-integration/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-series me-2 mb-2" title="Kafka Connect for AWS Services Integration">Kafka Connect for AWS Services Integration
<span class="badge badge-sm text-secondary bg-white ms-1">5</span>
</a><a href=/series/apache-beam-local-development-with-python/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-series me-2 mb-2" title="Apache Beam Local Development With Python">Apache Beam Local Development With Python
<span class="badge badge-sm text-secondary bg-white ms-1">4</span>
</a><a href=/series/serverless-data-product/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-series me-2 mb-2" title="Serverless Data Product">Serverless Data Product
<span class="badge badge-sm text-secondary bg-white ms-1">4</span>
</a><a href=/series/data-lake-demo-using-change-data-capture/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-series me-2 mb-2" title="Data Lake Demo Using Change Data Capture">Data Lake Demo Using Change Data Capture
<span class="badge badge-sm text-secondary bg-white ms-1">3</span>
</a><a href=/series/getting-started-with-pyflink-on-aws/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-series me-2 mb-2" title="Getting Started With Pyflink on AWS">Getting Started With Pyflink on AWS
<span class="badge badge-sm text-secondary bg-white ms-1">3</span>
</a><a href=/series/kafka-development-on-kubernetes/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-series me-2 mb-2" title="Kafka Development on Kubernetes">Kafka Development on Kubernetes
<span class="badge badge-sm text-secondary bg-white ms-1">3</span>
</a><a href=/series/parallel-processing-on-single-machine/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-series me-2 mb-2" title="Parallel Processing on Single Machine">Parallel Processing on Single Machine
<span class="badge badge-sm text-secondary bg-white ms-1">3</span>
</a><a href=/series/api-development-with-r/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-series me-2 mb-2" title="API Development With R">API Development With R
<span class="badge badge-sm text-secondary bg-white ms-1">2</span>
</a><a href=/series/integrate-schema-registry-with-msk-connect/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-series me-2 mb-2" title="Integrate Schema Registry With MSK Connect">Integrate Schema Registry With MSK Connect
<span class="badge badge-sm text-secondary bg-white ms-1">2</span>
</a><a href=/series/kafka-flink-and-dynamodb-for-real-time-fraud-detection/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-series me-2 mb-2" title="Kafka, Flink and DynamoDB for Real Time Fraud Detection">Kafka, Flink and DynamoDB for Real Time Fraud Detection
<span class="badge badge-sm text-secondary bg-white ms-1">2</span>
</a><a href=/series/simplify-streaming-ingestion-on-aws/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-series me-2 mb-2" title="Simplify Streaming Ingestion on AWS">Simplify Streaming Ingestion on AWS
<span class="badge badge-sm text-secondary bg-white ms-1">2</span></a></div><div class=tab-pane id=taxonomyArchives role=tabpanel aria-labelledby=taxonomyArchivesTab tabindex=0><a href=/archives/2024/ class="btn btn-sm btn-secondary post-taxonomy ps-3 me-2 mb-2" title=2024>2024 <span class="badge badge-sm text-secondary bg-white ms-1">12</span>
</a><a href=/archives/2023/ class="btn btn-sm btn-secondary post-taxonomy ps-3 me-2 mb-2" title=2023>2023 <span class="badge badge-sm text-secondary bg-white ms-1">39</span>
</a><a href=/archives/2022/ class="btn btn-sm btn-secondary post-taxonomy ps-3 me-2 mb-2" title=2022>2022 <span class="badge badge-sm text-secondary bg-white ms-1">15</span>
</a><a href=/archives/2021/ class="btn btn-sm btn-secondary post-taxonomy ps-3 me-2 mb-2" title=2021>2021 <span class="badge badge-sm text-secondary bg-white ms-1">7</span>
</a><a href=/archives/2020/ class="btn btn-sm btn-secondary post-taxonomy ps-3 me-2 mb-2" title=2020>2020 <span class="badge badge-sm text-secondary bg-white ms-1">1</span>
</a><a href=/archives/2019/ class="btn btn-sm btn-secondary post-taxonomy ps-3 me-2 mb-2" title=2019>2019 <span class="badge badge-sm text-secondary bg-white ms-1">5</span>
</a><a href=/archives/2018/ class="btn btn-sm btn-secondary post-taxonomy ps-3 me-2 mb-2" title=2018>2018 <span class="badge badge-sm text-secondary bg-white ms-1">2</span>
</a><a href=/archives/2017/ class="btn btn-sm btn-secondary post-taxonomy ps-3 me-2 mb-2" title=2017>2017 <span class="badge badge-sm text-secondary bg-white ms-1">6</span>
</a><a href=/archives/2016/ class="btn btn-sm btn-secondary post-taxonomy ps-3 me-2 mb-2" title=2016>2016 <span class="badge badge-sm text-secondary bg-white ms-1">6</span>
</a><a href=/archives/2015/ class="btn btn-sm btn-secondary post-taxonomy ps-3 me-2 mb-2" title=2015>2015 <span class="badge badge-sm text-secondary bg-white ms-1">15</span>
</a><a href=/archives/2014/ class="btn btn-sm btn-secondary post-taxonomy ps-3 me-2 mb-2" title=2014>2014 <span class="badge badge-sm text-secondary bg-white ms-1">5</span></a></div></div></div></div></div><div class="accordion posts-toggle"><div class="row card component accordion-item"><div class="accordion-header card-header border-0"><a class="accordion-button d-lg-none mb-1 shadow-none p-0 bg-transparent" role=button data-bs-toggle=collapse href=#posts-toggle aria-expanded=true aria-controls=posts-toggle>Posts</a></div><div class="card-body collapse accordion-collapse accordion-body d-lg-block show" id=posts-toggle><ul class="nav nav-pills nav-fill" role=tablist><li class=nav-item role=presentation><button class="nav-link active" id=featured-posts-tab data-bs-toggle=tab data-bs-target=#featured-posts type=button role=tab aria-controls=featured-posts aria-selected=true>
Featured Posts</button></li><li class=nav-item role=presentation><button class=nav-link id=recent-posts-tab data-bs-toggle=tab data-bs-target=#recent-posts type=button role=tab aria-controls=recent-posts aria-selected=true>
Recent Posts</button></li></ul><div class="tab-content mt-3"><div class="tab-pane active" id=featured-posts role=tabpanel aria-labelledby=featured-posts-tab tabindex=0><ul class="post-list list-unstyled ms-1"><li class=mb-2><div class=d-flex><div class="flex-shrink-0 d-flex justify-content-center align-items-center" style=max-width:100px><picture><source srcset=/blog/2024-03-28-beam-local-dev-1/featured_hu8df3c1065468b257d73fe4ccb72b4c47_88260_500x0_resize_box_3.png media="(max-width: 576px)" height=338 width=500><img class=img-fluid height=122 width=180 alt=featured.png src=/blog/2024-03-28-beam-local-dev-1/featured_hu8df3c1065468b257d73fe4ccb72b4c47_88260_180x0_resize_box_3.png data-src=/blog/2024-03-28-beam-local-dev-1/featured.png loading=lazy></picture></div><div class="flex-grow-1 d-flex flex-column h-auto justify-content-center ms-3"><a class=post-title href=/blog/2024-03-28-beam-local-dev-1/>Apache Beam Local Development With Python - Part 1 Pipeline, Notebook, SQL and DataFrame</a><div class="post-meta mt-2"><span class=post-date>March 28, 2024</span></div></div></div></li><li class=mb-2><div class=d-flex><div class="flex-shrink-0 d-flex justify-content-center align-items-center" style=max-width:100px><picture><source srcset=/blog/2024-01-18-dbt-pizza-shop-1/featured_hua8fc52ecfe83e5abaf1872cc948e583f_85093_500x0_resize_box_3.png media="(max-width: 576px)" height=238 width=500><img class=img-fluid height=86 width=180 alt=featured.png src=/blog/2024-01-18-dbt-pizza-shop-1/featured_hua8fc52ecfe83e5abaf1872cc948e583f_85093_180x0_resize_box_3.png data-src=/blog/2024-01-18-dbt-pizza-shop-1/featured.png loading=lazy></picture></div><div class="flex-grow-1 d-flex flex-column h-auto justify-content-center ms-3"><a class=post-title href=/blog/2024-01-18-dbt-pizza-shop-1/>Data Build Tool (Dbt) Pizza Shop Demo - Part 1 Modelling on PostgreSQL</a><div class="post-meta mt-2"><span class=post-date>January 18, 2024</span></div></div></div></li><li class=mb-2><div class=d-flex><div class="flex-shrink-0 d-flex justify-content-center align-items-center" style=max-width:100px><picture><source srcset=/blog/2023-12-21-kafka-development-on-k8s-part-1/featured_hu216964ff6c7a4996550e95c3e0e5f209_108975_500x0_resize_box_3.png media="(max-width: 576px)" height=263 width=500><img class=img-fluid height=95 width=180 alt=featured.png src=/blog/2023-12-21-kafka-development-on-k8s-part-1/featured_hu216964ff6c7a4996550e95c3e0e5f209_108975_180x0_resize_box_3.png data-src=/blog/2023-12-21-kafka-development-on-k8s-part-1/featured.png loading=lazy></picture></div><div class="flex-grow-1 d-flex flex-column h-auto justify-content-center ms-3"><a class=post-title href=/blog/2023-12-21-kafka-development-on-k8s-part-1/>Kafka Development on Kubernetes - Part 1 Cluster Setup</a><div class="post-meta mt-2"><span class=post-date>December 21, 2023</span></div></div></div></li><li class=mb-2><div class=d-flex><div class="flex-shrink-0 d-flex justify-content-center align-items-center" style=max-width:100px><picture><source srcset=/blog/2023-12-07-flink-spark-local-dev/featured_hu7a73c8ca7d8b8a4ddf49d1a9b8fe40bd_133053_500x0_resize_box_3.png media="(max-width: 576px)" height=373 width=500><img class=img-fluid height=134 width=180 alt=featured.png src=/blog/2023-12-07-flink-spark-local-dev/featured_hu7a73c8ca7d8b8a4ddf49d1a9b8fe40bd_133053_180x0_resize_box_3.png data-src=/blog/2023-12-07-flink-spark-local-dev/featured.png loading=lazy></picture></div><div class="flex-grow-1 d-flex flex-column h-auto justify-content-center ms-3"><a class=post-title href=/blog/2023-12-07-flink-spark-local-dev/>Setup Local Development Environment for Apache Flink and Spark Using EMR Container Images</a><div class="post-meta mt-2"><span class=post-date>December 7, 2023</span></div></div></div></li><li class=mb-2><div class=d-flex><div class="flex-shrink-0 d-flex justify-content-center align-items-center" style=max-width:100px><picture><source srcset=/blog/2023-10-19-build-pyflink-apps/featured_hubbcf607e326a86791dcaeb23a1bfd08e_154736_500x0_resize_box_3.png media="(max-width: 576px)" height=241 width=500><img class=img-fluid height=87 width=180 alt=featured.png src=/blog/2023-10-19-build-pyflink-apps/featured_hubbcf607e326a86791dcaeb23a1bfd08e_154736_180x0_resize_box_3.png data-src=/blog/2023-10-19-build-pyflink-apps/featured.png loading=lazy></picture></div><div class="flex-grow-1 d-flex flex-column h-auto justify-content-center ms-3"><a class=post-title href=/blog/2023-10-19-build-pyflink-apps/>Building Apache Flink Applications in Python</a><div class="post-meta mt-2"><span class=post-date>October 19, 2023</span></div></div></div></li><li class=mb-2><div class=d-flex><div class="flex-shrink-0 d-flex justify-content-center align-items-center" style=max-width:100px><picture><source srcset=/blog/2023-10-05-real-time-streaming-with-kafka-and-flink-1/featured_hua45c2e579ce8f6bc0318d15d62654514_138141_500x0_resize_box_3.png media="(max-width: 576px)" height=299 width=500><img class=img-fluid height=108 width=180 alt=featured.png src=/blog/2023-10-05-real-time-streaming-with-kafka-and-flink-1/featured_hua45c2e579ce8f6bc0318d15d62654514_138141_180x0_resize_box_3.png data-src=/blog/2023-10-05-real-time-streaming-with-kafka-and-flink-1/featured.png loading=lazy></picture></div><div class="flex-grow-1 d-flex flex-column h-auto justify-content-center ms-3"><a class=post-title href=/blog/2023-10-05-real-time-streaming-with-kafka-and-flink-1/>Real Time Streaming With Kafka and Flink - Introduction</a><div class="post-meta mt-2"><span class=post-date>October 5, 2023</span></div></div></div></li><li class=mb-2><div class=d-flex><div class="flex-shrink-0 d-flex justify-content-center align-items-center" style=max-width:100px><picture><source srcset=/blog/2023-05-04-kafka-development-with-docker-part-1/featured_hu351ee075bb995e8f69034dc54fe8364b_98355_500x0_resize_box_3.png media="(max-width: 576px)" height=322 width=500><img class=img-fluid height=116 width=180 alt=featured.png src=/blog/2023-05-04-kafka-development-with-docker-part-1/featured_hu351ee075bb995e8f69034dc54fe8364b_98355_180x0_resize_box_3.png data-src=/blog/2023-05-04-kafka-development-with-docker-part-1/featured.png loading=lazy></picture></div><div class="flex-grow-1 d-flex flex-column h-auto justify-content-center ms-3"><a class=post-title href=/blog/2023-05-04-kafka-development-with-docker-part-1/>Kafka Development With Docker - Part 1 Cluster Setup</a><div class="post-meta mt-2"><span class=post-date>May 4, 2023</span></div></div></div></li><li class=mb-2><div class=d-flex><div class="flex-shrink-0 d-flex justify-content-center align-items-center" style=max-width:100px><picture><source srcset=/blog/2023-04-12-integrate-glue-schema-registry/featured_hu57e44328735e0754f0de2b0f6335f8bb_46040_500x0_resize_box_3.png media="(max-width: 576px)" height=305 width=500><img class=img-fluid height=110 width=180 alt=featured.png src=/blog/2023-04-12-integrate-glue-schema-registry/featured_hu57e44328735e0754f0de2b0f6335f8bb_46040_180x0_resize_box_3.png data-src=/blog/2023-04-12-integrate-glue-schema-registry/featured.png loading=lazy></picture></div><div class="flex-grow-1 d-flex flex-column h-auto justify-content-center ms-3"><a class=post-title href=/blog/2023-04-12-integrate-glue-schema-registry/>Integrate Glue Schema Registry With Your Python Kafka App</a><div class="post-meta mt-2"><span class=post-date>April 12, 2023</span></div></div></div></li><li class=mb-2><div class=d-flex><div class="flex-shrink-0 d-flex justify-content-center align-items-center" style=max-width:100px><picture><source srcset=/blog/2023-02-08-simplify-streaming-ingestion-redshift/featured_hu77abd5c8ed84cebc51bde5ecaefc7320_32864_500x0_resize_box_3.png media="(max-width: 576px)" height=239 width=500><img class=img-fluid height=86 width=180 alt=featured.png src=/blog/2023-02-08-simplify-streaming-ingestion-redshift/featured_hu77abd5c8ed84cebc51bde5ecaefc7320_32864_180x0_resize_box_3.png data-src=/blog/2023-02-08-simplify-streaming-ingestion-redshift/featured.png loading=lazy></picture></div><div class="flex-grow-1 d-flex flex-column h-auto justify-content-center ms-3"><a class=post-title href=/blog/2023-02-08-simplify-streaming-ingestion-redshift/>Simplify Streaming Ingestion on AWS – Part 1 MSK and Redshift</a><div class="post-meta mt-2"><span class=post-date>February 8, 2023</span></div></div></div></li><li class=mb-2><div class=d-flex><div class="flex-shrink-0 d-flex justify-content-center align-items-center" style=max-width:100px><picture><source srcset=/blog/2022-09-28-dbt-on-aws-part-1-redshift/featured_hu35d67be5daab492ffba00e48de3bbd29_97234_500x0_resize_box_3.png media="(max-width: 576px)" height=244 width=500><img class=img-fluid height=88 width=180 alt=featured.png src=/blog/2022-09-28-dbt-on-aws-part-1-redshift/featured_hu35d67be5daab492ffba00e48de3bbd29_97234_180x0_resize_box_3.png data-src=/blog/2022-09-28-dbt-on-aws-part-1-redshift/featured.png loading=lazy></picture></div><div class="flex-grow-1 d-flex flex-column h-auto justify-content-center ms-3"><a class=post-title href=/blog/2022-09-28-dbt-on-aws-part-1-redshift/>Data Build Tool (Dbt) for Effective Data Transformation on AWS – Part 1 Redshift</a><div class="post-meta mt-2"><span class=post-date>September 28, 2022</span></div></div></div></li></ul></div><div class=tab-pane id=recent-posts role=tabpanel aria-labelledby=recent-posts-tab tabindex=0><ul class="post-list list-unstyled ms-1"><li class=mb-2><div class=d-flex><div class="flex-shrink-0 d-flex justify-content-center align-items-center" style=max-width:100px><picture><source srcset=/blog/2024-05-02-beam-local-dev-4/featured_hu03c27d6b9ebae8125b322f8d9fec30d2_54556_500x0_resize_box_3.png media="(max-width: 576px)" height=332 width=500><img class=img-fluid height=120 width=180 alt=featured.png src=/blog/2024-05-02-beam-local-dev-4/featured_hu03c27d6b9ebae8125b322f8d9fec30d2_54556_180x0_resize_box_3.png data-src=/blog/2024-05-02-beam-local-dev-4/featured.png loading=lazy></picture></div><div class="flex-grow-1 d-flex flex-column h-auto justify-content-center ms-3"><a class=post-title href=/blog/2024-05-02-beam-local-dev-4/>Apache Beam Local Development With Python - Part 4 Streaming Pipelines</a><div class="post-meta mt-2"><span class=post-date>May 2, 2024</span></div></div></div></li><li class=mb-2><div class=d-flex><div class="flex-shrink-0 d-flex justify-content-center align-items-center" style=max-width:100px><picture><source srcset=/blog/2024-04-18-beam-local-dev-3/featured_hua9988bbf060f67942c8c24ced892e1f1_262307_500x0_resize_box_3.png media="(max-width: 576px)" height=232 width=500><img class=img-fluid height=83 width=180 alt=featured.png src=/blog/2024-04-18-beam-local-dev-3/featured_hua9988bbf060f67942c8c24ced892e1f1_262307_180x0_resize_box_3.png data-src=/blog/2024-04-18-beam-local-dev-3/featured.png loading=lazy></picture></div><div class="flex-grow-1 d-flex flex-column h-auto justify-content-center ms-3"><a class=post-title href=/blog/2024-04-18-beam-local-dev-3/>Apache Beam Local Development With Python - Part 3 Flink Runner</a><div class="post-meta mt-2"><span class=post-date>April 18, 2024</span></div></div></div></li><li class=mb-2><div class=d-flex><div class="flex-shrink-0 d-flex justify-content-center align-items-center" style=max-width:100px><picture><source srcset=/blog/2024-04-04-beam-local-dev-2/featured_huc60febc0cd72955054b95c3bf20650a7_55405_500x0_resize_box_3.png media="(max-width: 576px)" height=330 width=500><img class=img-fluid height=119 width=180 alt=featured.png src=/blog/2024-04-04-beam-local-dev-2/featured_huc60febc0cd72955054b95c3bf20650a7_55405_180x0_resize_box_3.png data-src=/blog/2024-04-04-beam-local-dev-2/featured.png loading=lazy></picture></div><div class="flex-grow-1 d-flex flex-column h-auto justify-content-center ms-3"><a class=post-title href=/blog/2024-04-04-beam-local-dev-2/>Apache Beam Local Development With Python - Part 2 Batch Pipelines</a><div class="post-meta mt-2"><span class=post-date>April 4, 2024</span></div></div></div></li><li class=mb-2><div class=d-flex><div class="flex-shrink-0 d-flex justify-content-center align-items-center" style=max-width:100px><picture><source srcset=/blog/2024-03-28-beam-local-dev-1/featured_hu8df3c1065468b257d73fe4ccb72b4c47_88260_500x0_resize_box_3.png media="(max-width: 576px)" height=338 width=500><img class=img-fluid height=122 width=180 alt=featured.png src=/blog/2024-03-28-beam-local-dev-1/featured_hu8df3c1065468b257d73fe4ccb72b4c47_88260_180x0_resize_box_3.png data-src=/blog/2024-03-28-beam-local-dev-1/featured.png loading=lazy></picture></div><div class="flex-grow-1 d-flex flex-column h-auto justify-content-center ms-3"><a class=post-title href=/blog/2024-03-28-beam-local-dev-1/>Apache Beam Local Development With Python - Part 1 Pipeline, Notebook, SQL and DataFrame</a><div class="post-meta mt-2"><span class=post-date>March 28, 2024</span></div></div></div></li><li class=mb-2><div class=d-flex><div class="flex-shrink-0 d-flex justify-content-center align-items-center" style=max-width:100px><picture><source srcset=/blog/2024-03-14-dbt-pizza-shop-6/featured_hu72fe52c30196d9a511f344ff548f2036_82921_500x0_resize_box_3.png media="(max-width: 576px)" height=256 width=500><img class=img-fluid height=92 width=180 alt=featured.png src=/blog/2024-03-14-dbt-pizza-shop-6/featured_hu72fe52c30196d9a511f344ff548f2036_82921_180x0_resize_box_3.png data-src=/blog/2024-03-14-dbt-pizza-shop-6/featured.png loading=lazy></picture></div><div class="flex-grow-1 d-flex flex-column h-auto justify-content-center ms-3"><a class=post-title href=/blog/2024-03-14-dbt-pizza-shop-6/>Data Build Tool (Dbt) Pizza Shop Demo - Part 6 ETL on Amazon Athena via Airflow</a><div class="post-meta mt-2"><span class=post-date>March 14, 2024</span></div></div></div></li><li class=mb-2><div class=d-flex><div class="flex-shrink-0 d-flex justify-content-center align-items-center" style=max-width:100px><picture><source srcset=/blog/2024-03-07-dbt-pizza-shop-5/featured_hu5822ed54c6b6e6cff286a9a560a4b806_61499_500x0_resize_box_3.png media="(max-width: 576px)" height=187 width=500><img class=img-fluid height=67 width=180 alt=featured.png src=/blog/2024-03-07-dbt-pizza-shop-5/featured_hu5822ed54c6b6e6cff286a9a560a4b806_61499_180x0_resize_box_3.png data-src=/blog/2024-03-07-dbt-pizza-shop-5/featured.png loading=lazy></picture></div><div class="flex-grow-1 d-flex flex-column h-auto justify-content-center ms-3"><a class=post-title href=/blog/2024-03-07-dbt-pizza-shop-5/>Data Build Tool (Dbt) Pizza Shop Demo - Part 5 Modelling on Amazon Athena</a><div class="post-meta mt-2"><span class=post-date>March 7, 2024</span></div></div></div></li><li class=mb-2><div class=d-flex><div class="flex-shrink-0 d-flex justify-content-center align-items-center" style=max-width:100px><picture><source srcset=/blog/2024-02-22-dbt-pizza-shop-4/featured_hu487187112b89f2568f433199221f000a_89588_500x0_resize_box_3.png media="(max-width: 576px)" height=327 width=500><img class=img-fluid height=118 width=180 alt=featured.png src=/blog/2024-02-22-dbt-pizza-shop-4/featured_hu487187112b89f2568f433199221f000a_89588_180x0_resize_box_3.png data-src=/blog/2024-02-22-dbt-pizza-shop-4/featured.png loading=lazy></picture></div><div class="flex-grow-1 d-flex flex-column h-auto justify-content-center ms-3"><a class=post-title href=/blog/2024-02-22-dbt-pizza-shop-4/>Data Build Tool (Dbt) Pizza Shop Demo - Part 4 ETL on BigQuery via Airflow</a><div class="post-meta mt-2"><span class=post-date>February 22, 2024</span></div></div></div></li><li class=mb-2><div class=d-flex><div class="flex-shrink-0 d-flex justify-content-center align-items-center" style=max-width:100px><picture><source srcset=/blog/2024-02-08-dbt-pizza-shop-3/featured_hu5082aedceaa438015dc31206ac8f27c2_70297_500x0_resize_box_3.png media="(max-width: 576px)" height=241 width=500><img class=img-fluid height=87 width=180 alt=featured.png src=/blog/2024-02-08-dbt-pizza-shop-3/featured_hu5082aedceaa438015dc31206ac8f27c2_70297_180x0_resize_box_3.png data-src=/blog/2024-02-08-dbt-pizza-shop-3/featured.png loading=lazy></picture></div><div class="flex-grow-1 d-flex flex-column h-auto justify-content-center ms-3"><a class=post-title href=/blog/2024-02-08-dbt-pizza-shop-3/>Data Build Tool (Dbt) Pizza Shop Demo - Part 3 Modelling on BigQuery</a><div class="post-meta mt-2"><span class=post-date>February 8, 2024</span></div></div></div></li><li class=mb-2><div class=d-flex><div class="flex-shrink-0 d-flex justify-content-center align-items-center" style=max-width:100px><picture><source srcset=/blog/2024-01-25-dbt-pizza-shop-2/featured_hu32c9a8b2df36b3a1704029fc725738dd_77355_500x0_resize_box_3.png media="(max-width: 576px)" height=328 width=500><img class=img-fluid height=118 width=180 alt=featured.png src=/blog/2024-01-25-dbt-pizza-shop-2/featured_hu32c9a8b2df36b3a1704029fc725738dd_77355_180x0_resize_box_3.png data-src=/blog/2024-01-25-dbt-pizza-shop-2/featured.png loading=lazy></picture></div><div class="flex-grow-1 d-flex flex-column h-auto justify-content-center ms-3"><a class=post-title href=/blog/2024-01-25-dbt-pizza-shop-2/>Data Build Tool (Dbt) Pizza Shop Demo - Part 2 ETL on PostgreSQL via Airflow</a><div class="post-meta mt-2"><span class=post-date>January 25, 2024</span></div></div></div></li><li class=mb-2><div class=d-flex><div class="flex-shrink-0 d-flex justify-content-center align-items-center" style=max-width:100px><picture><source srcset=/blog/2024-01-18-dbt-pizza-shop-1/featured_hua8fc52ecfe83e5abaf1872cc948e583f_85093_500x0_resize_box_3.png media="(max-width: 576px)" height=238 width=500><img class=img-fluid height=86 width=180 alt=featured.png src=/blog/2024-01-18-dbt-pizza-shop-1/featured_hua8fc52ecfe83e5abaf1872cc948e583f_85093_180x0_resize_box_3.png data-src=/blog/2024-01-18-dbt-pizza-shop-1/featured.png loading=lazy></picture></div><div class="flex-grow-1 d-flex flex-column h-auto justify-content-center ms-3"><a class=post-title href=/blog/2024-01-18-dbt-pizza-shop-1/>Data Build Tool (Dbt) Pizza Shop Demo - Part 1 Modelling on PostgreSQL</a><div class="post-meta mt-2"><span class=post-date>January 18, 2024</span></div></div></div></li></ul></div></div></div></div></div></div></aside></div></main><footer class="footer mt-auto py-3 text-center container"><div class="offcanvas offcanvas-bottom h-auto" tabindex=-1 id=offcanvasActionsPanel aria-labelledby=offcanvasActionsPanelLabel><div class=offcanvas-header><div class="offcanvas-title h5" id=offcanvasActionsPanelLabel><i class="fas fa-fw fa-th-large me-1"></i>
Actions</div><button type=button class="btn-close ms-auto" data-bs-dismiss=offcanvas data-bs-target=offcanvasActionsPanel aria-label=Close></button></div><div class="offcanvas-body mt-2"><div class="actions d-flex overflow-auto align-items-center"><a role=button class="action action-go-back d-flex flex-column align-items-center me-3" href="javascript: window.history.back();"><span class="action-icon mb-2"><i class="fas fa-2x fa-chevron-circle-down" data-fa-transform=rotate-90></i></span> Go back
</a><a role=button class="action action-reload-page d-flex flex-column align-items-center me-3"><span class="action-icon mb-2"><i class="fas fa-2x fa-redo-alt"></i></span> Reload
</a><a role=button class="action action-copy-url d-flex flex-column align-items-center me-3"><span class="action-icon mb-2"><i class="fas fa-2x fa-link"></i></span> Copy URL</a></div></div></div><div class="row text-center"><div class="col-12 mt-2"><p class=mb-2>Jaehyeon Kim</p><p class="text-secondary mb-2"><small>Data Engineer 💡 Blogger ⚡ Data Streaming Enthusiast ☁ AWS Community Builder</small></p><div class="copyright mb-2 text-secondary"><small>Copyright © 2023-2024 Jaehyeon Kim. All Rights Reserved.</small></div><nav class="social-links nav justify-content-center mb-2 mt-3"><a class="nav-link social-link p-0 me-1 mb-2" target=_blank href=/index.xml title=RSS rel=me><i class="fas fa-fw fa-2x fa-rss" style=color:#ea6221></i></a></nav></div><div class="col-12 col-lg-8 offset-0 offset-lg-1"></div></div></footer><script data-precache src=/assets/main/bundle.min.0ec8d79b95bd8d39246804f325aa5ba906dd898c752c661cb355262f78ecadcb.js integrity="sha256-DsjXm5W9jTkkaATzJapbqQbdiYx1LGYcs1UmL3jsrcs=" crossorigin=anonymous async></script><script data-precache src=/assets/icons/bundle.min.4f30d5267a9f2f9d45ed93969d45ec626100a969a8b91f71f753315a261e9034.js integrity="sha256-TzDVJnqfL51F7ZOWnUXsYmEAqWmouR9x91MxWiYekDQ=" crossorigin=anonymous defer></script><script data-precache src=/assets/viewer/bundle.min.06371891cfe6d10d36cba465c61c4d7cb17591a3be2fd9af4a38444d2074e709.js integrity="sha256-BjcYkc/m0Q02y6RlxhxNfLF1kaO+L9mvSjhETSB05wk=" crossorigin=anonymous defer></script><script data-precache defer src=/assets/katex/bundle.min.1a8cd88028fe1b600e81e2e780a8d8c388134c8237d5da89efed53ddfa28216e.js integrity="sha256-GozYgCj+G2AOgeLngKjYw4gTTII31dqJ7+1T3fooIW4=" crossorigin=anonymous></script><script data-precache defer src=/assets/mermaid/bundle.min.8d91e77f8b2cd5ebfd6d2af4e214fa37b555b15ad65dc1bdec5564ce709176f9.js integrity="sha256-jZHnf4ss1ev9bSr04hT6N7VVsVrWXcG97FVkznCRdvk=" crossorigin=anonymous></script><script src=/js/sw-register.js defer></script><script src=https://giscus.app/client.js data-repo=jaehyeon-kim/jaehyeon-kim.github.io data-repo-id="MDEwOlJlcG9zaXRvcnkyNjgwMjU0NQ==" data-category data-category-id=DIC_kwDOAZj5cc4CV2fp data-mapping=pathname data-reactions-enabled=1 data-emit-metadata=1 data-input-position=bottom data-theme=preferred_color_scheme data-lang=en data-loading=lazy crossorigin=anonymous defer></script></body></html>