<!doctype html><html class=position-relative itemscope itemtype=http://schema.org/WebPage lang=en data-bs-theme=light data-palette=blue-gray><head><script src=/assets/init/bundle.min.0e3ade1b3737d3bc6d3db7b1ddd75ff949ecea2db23eedda3ac32b80472c3dbe.js integrity="sha256-DjreGzc307xtPbex3ddf+Uns6i2yPu3aOsMrgEcsPb4=" crossorigin=anonymous></script><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>Develop and Test Apache Spark Apps for EMR Locally Using Docker - Jaehyeon's Personal Site</title><link rel=icon href=/favicon_huf63427acbec4fbe11db6a32164cc2763_6040_16x16_resize_q75_h2_box_2.webp sizes=16x16 type=image/webp><link rel=icon href=/favicon_huf63427acbec4fbe11db6a32164cc2763_6040_32x32_resize_q75_h2_box_2.webp sizes=32x32 type=image/webp><link rel=icon href=/favicon_huf63427acbec4fbe11db6a32164cc2763_6040_150x150_resize_q75_h2_box_2.webp sizes=150x150 type=image/webp><link rel=apple-touch-icon href=/favicon_huf63427acbec4fbe11db6a32164cc2763_6040_180x180_resize_q75_h2_box_2.webp sizes=180x180 type=image/webp><link rel=icon href=/favicon_huf63427acbec4fbe11db6a32164cc2763_6040_192x192_resize_q75_h2_box_2.webp sizes=192x192 type=image/webp><link rel=mask-icon href=/safari-pinned-tab.svg color=#6f42c1><meta name=keywords content="AWS,Data,Analytics,Containers,Serverless"><meta name=description content="We'll discuss how to create a Spark local dev environment for EMR using Docker and/or VSCode. A range of Spark development examples are demonstrated and Glue Catalog integration is illustrated as well."><meta name=robots content="index, follow"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://jaehyeon.me/blog/2022-05-08-emr-local-dev/featured.png"><meta name=twitter:title content="Develop and Test Apache Spark Apps for EMR Locally Using Docker"><meta name=twitter:description content="We'll discuss how to create a Spark local dev environment for EMR using Docker and/or VSCode. A range of Spark development examples are demonstrated and Glue Catalog integration is illustrated as well."><meta property="og:title" content="Develop and Test Apache Spark Apps for EMR Locally Using Docker"><meta property="og:description" content="We'll discuss how to create a Spark local dev environment for EMR using Docker and/or VSCode. A range of Spark development examples are demonstrated and Glue Catalog integration is illustrated as well."><meta property="og:type" content="article"><meta property="og:url" content="https://jaehyeon.me/blog/2022-05-08-emr-local-dev/"><meta property="og:image" content="https://jaehyeon.me/blog/2022-05-08-emr-local-dev/featured.png"><meta property="article:section" content="blog"><meta property="article:published_time" content="2022-05-08T00:00:00+00:00"><meta property="article:modified_time" content="2023-04-14T10:32:29+10:00"><meta itemprop=name content="Develop and Test Apache Spark Apps for EMR Locally Using Docker"><meta itemprop=description content="We'll discuss how to create a Spark local dev environment for EMR using Docker and/or VSCode. A range of Spark development examples are demonstrated and Glue Catalog integration is illustrated as well."><meta itemprop=datePublished content="2022-05-08T00:00:00+00:00"><meta itemprop=dateModified content="2023-04-14T10:32:29+10:00"><meta itemprop=wordCount content="3462"><meta itemprop=image content="https://jaehyeon.me/blog/2022-05-08-emr-local-dev/featured.png"><meta itemprop=keywords content="AWS,Amazon EMR,Apache Spark,Pyspark,Docker,Docker Compose,Visual Studio Code,"><link rel=manifest href=/manifest.json><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-2764466088407958"></script>
<link data-precache rel=stylesheet href="/assets/main/bundle.min.58b5d95a736352dbba074d789c4e2dc9bfbc282c6ba3309e9225471726917ee1.css" integrity="sha256-WLXZWnNjUtu6B014nE4tyb+8KCxrozCekiVHFyaRfuE=" crossorigin=anonymous><link data-precache rel=stylesheet href=/assets/katex/bundle.min.222aac0b0b2e29944e5e468ecbd302b4ece09ac5ee29a0c811086b03659edd76.css integrity="sha256-IiqsCwsuKZROXkaOy9MCtOzgmsXuKaDIEQhrA2We3XY=" crossorigin=anonymous><link data-precache rel=stylesheet href=/assets/viewer/bundle.min.eb914844636cd41f221f109e99c887bbc3b6b5ffb2af7c664b284cea2d1b54b7.css integrity="sha256-65FIRGNs1B8iHxCemciHu8O2tf+yr3xmSyhM6i0bVLc=" crossorigin=anonymous></head><body><header><nav class="top-app-bar shadow navbar navbar-expand-xxl fixed-top"><div class=container><a class="navbar-brand d-flex align-items-center flex-grow-1 flex-xxl-grow-0 justify-content-xxl-start ms-2 ms-xxl-0 mx-auto me-xxl-2" href=https://jaehyeon.me/>Jaehyeon Kim</a><div class="offcanvas-xxl offcanvas-end flex-grow-1" data-bs-scroll=true tabindex=-1 id=navbarMenus aria-labelledby=navbarMenusLabel><div class="offcanvas-header px-4 pb-0"><div class="offcanvas-title h5" id=navbarMenusLabel>Jaehyeon Kim</div><button type=button class="btn-close btn-close-white" data-bs-dismiss=offcanvas data-bs-target=#navbarMenus aria-label=Close></button></div><div class="offcanvas-body p-4 pt-0 p-xxl-0"><hr class=d-xxl-none><ul class="navbar-nav flex-row flex-wrap align-items-center me-auto"><li class="nav-item col-12 col-xxl-auto dropdown px-0"><a href=# class="nav-link dropdown-toggle" id=navbarDropdownBlog role=button data-bs-toggle=dropdown aria-expanded=false><span class="menu-icon me-1"><i class="fas fa-fw fa-blog text-warning"></i></span>Blog</a><ul class="dropdown-menu dropdown-menu-end" aria-labelledby=navbarDropdownBlog data-bs-popper=none><li><a class="dropdown-item d-flex align-items-center text-wrap text-xxl-nowrap" href=https://jaehyeon.me/archives/><span class="dropdown-item-icon me-2 p-2 rounded"><i class="fas fa-fw fa-file-archive text-primary"></i></span><div class=dropdown-item-content><p class="dropdown-item-title mb-0">Archives</p></div></a></li><li><a class="dropdown-item d-flex align-items-center text-wrap text-xxl-nowrap" href=https://jaehyeon.me/series/><span class="dropdown-item-icon me-2 p-2 rounded"><i class="fas fa-fw fa-columns"></i></span><div class=dropdown-item-content><p class="dropdown-item-title mb-1">Series</p><p class="dropdown-item-description mb-0 text-secondary">List of series.</p></div></a></li><li><a class="dropdown-item d-flex align-items-center text-wrap text-xxl-nowrap" href=https://jaehyeon.me/categories/><span class="dropdown-item-icon me-2 p-2 rounded"><i class="fas fa-fw fa-folder text-success"></i></span><div class=dropdown-item-content><p class="dropdown-item-title mb-1">Categories</p><p class="dropdown-item-description mb-0 text-secondary">List of categories.</p></div></a></li><li><a class="dropdown-item d-flex align-items-center text-wrap text-xxl-nowrap" href=https://jaehyeon.me/tags/><span class="dropdown-item-icon me-2 p-2 rounded"><i class="fas fa-fw fa-tags"></i></span><div class=dropdown-item-content><p class="dropdown-item-title mb-1">Tags</p><p class="dropdown-item-description mb-0 text-secondary">List of tags.</p></div></a></li></ul></li></ul><hr class=d-xxl-none><form class="search-bar ms-auto my-1" action=/search/ novalidate><div class="input-group input-group-sm align-items-center"><span class="btn btn-search disabled position-absolute left-0 border-0 px-1"><i class="fas fa-fw fa-search fa-lg"></i></span>
<input class="my-1 form-control border-white rounded search-input bg-body" name=q type=search placeholder="Press / to search" aria-label=Search required></div></form><hr class=d-xxl-none><ul class="navbar-nav flex-row flex-wrap align-items-center ms-md-auto"><li class="nav-item py-2 py-xxl-1 col-12 col-xxl-auto"><nav class="social-links nav justify-content-center flex-row"><a class="nav-link social-link col-6 col-xxl-auto p-1" target=_blank href=https://github.com/jaehyeon-kim title=GitHub rel=me><i class="fa-fw fab fa-github"></i>
<span class="ms-1 d-xxl-none">Github</span></a>
<a class="nav-link social-link col-6 col-xxl-auto p-1" target=_blank href=https://linkedin.com/in/jaehyeon-kim-76b93429 title=LinkedIn rel=me><i class="fa-fw fab fa-linkedin-in"></i>
<span class="ms-1 d-xxl-none">Linkedin</span></a>
<a class="nav-link social-link col-6 col-xxl-auto p-1" target=_blank href=/index.xml title=RSS rel=me><i class="fas fa-fw fa-rss"></i>
<span class="ms-1 d-xxl-none">RSS</span></a></nav></li><li class="nav-item py-2 py-xxl-1 col-12 col-xxl-auto"><div class="vr d-none d-xxl-flex h-100 mx-xxl-2 text-white"></div><hr class="d-xxl-none my-2"></li><li class="nav-item dropdown py-1 py-xxl-1 col-6 col-xxl-auto"><a class="nav-link px-0 px-xxl-1" href=# id=languageDropdown role=button data-bs-toggle=dropdown aria-expanded=false><i class="fas fa-fw fa-globe"></i>
<span class=d-xxl-none>Language</span></a><ul class="dropdown-menu dropdown-menu-end" aria-labelledby=languageDropdown><li><a class="dropdown-item active" href=/>English</a></li><li><a class=dropdown-item href=/zh-cn/></a></li></ul></li><li class="nav-item py-1 col-12 col-xxl-auto"><div class="vr d-none d-xxl-flex h-100 mx-xxl-2 text-white"></div><hr class="d-xxl-none my-2"></li><li class="nav-item dropdown col-6 col-xxl-auto"><a class="nav-link px-0 py-2 px-xxl-1" href=# id=fontSizeDropdown role=button data-bs-toggle=dropdown aria-expanded=false><i class="fas fa-fw fa-font"></i>
<span class=d-xxl-none>Font Size</span></a><ul class="dropdown-menu dropdown-menu-end" aria-labelledby=fontSizeDropdown><li><button class="font-size-item dropdown-item" data-size=xs>
Extra Small</button></li><li><button class="font-size-item dropdown-item" data-size=sm>
Small</button></li><li><button class="font-size-item dropdown-item active" data-size=md>
Medium</button></li><li><button class="font-size-item dropdown-item" data-size=lg>
Large</button></li><li><button class="font-size-item dropdown-item" data-size=xl>
Extra Large</button></li></ul></li><li class="nav-item dropdown col-6 col-xxl-auto"><a class="nav-link px-0 py-2 px-xxl-1" href=# id=paletteDropdown role=button data-bs-toggle=dropdown aria-expanded=false><i class="fas fa-fw fa-palette"></i>
<span class=d-xxl-none>Palette</span></a><ul class="palette-dropdown-menu dropdown-menu dropdown-menu-end px-2 row g-2" aria-labelledby=paletteDropdown><li class="col-4 my-1"><a role=button id=palette-blue aria-label=Blue class="btn btn-sm w-100 palette text-bg-blue" data-palette=blue></a></li><li class="col-4 my-1"><a role=button id=palette-blue-gray aria-label="Blue Gray" class="btn btn-sm w-100 palette text-bg-blue-gray" data-palette=blue-gray></a></li><li class="col-4 my-1"><a role=button id=palette-brown aria-label=Brown class="btn btn-sm w-100 palette text-bg-brown" data-palette=brown></a></li><li class="col-4 my-1"><a role=button id=palette-cyan aria-label=Cyan class="btn btn-sm w-100 palette text-bg-cyan" data-palette=cyan></a></li><li class="col-4 my-1"><a role=button id=palette-green aria-label=Green class="btn btn-sm w-100 palette text-bg-green" data-palette=green></a></li><li class="col-4 my-1"><a role=button id=palette-indigo aria-label=Indigo class="btn btn-sm w-100 palette text-bg-indigo" data-palette=indigo></a></li><li class="col-4 my-1"><a role=button id=palette-orange aria-label=Orange class="btn btn-sm w-100 palette text-bg-orange" data-palette=orange></a></li><li class="col-4 my-1"><a role=button id=palette-pink aria-label=Pink class="btn btn-sm w-100 palette text-bg-pink" data-palette=pink></a></li><li class="col-4 my-1"><a role=button id=palette-purple aria-label=Purple class="btn btn-sm w-100 palette text-bg-purple" data-palette=purple></a></li><li class="col-4 my-1"><a role=button id=palette-red aria-label=Red class="btn btn-sm w-100 palette text-bg-red" data-palette=red></a></li><li class="col-4 my-1"><a role=button id=palette-teal aria-label=Teal class="btn btn-sm w-100 palette text-bg-teal" data-palette=teal></a></li><li class="col-4 my-1"><a role=button id=palette-yellow aria-label=Yellow class="btn btn-sm w-100 palette text-bg-yellow" data-palette=yellow></a></li></ul></li><li class="nav-item dropdown col-6 col-xxl-auto"><a class="nav-link px-0 py-2 px-xxl-1" href=# id=modeDropdown role=button data-bs-toggle=dropdown aria-expanded=false><i class="mode-icon fas fa-fw fa-sun" id=modeIcon></i>
<span class=d-xxl-none>Mode</span></a><ul class="dropdown-menu dropdown-menu-end" aria-labelledby=modeDropdown><li class="mode-item active" data-color-mode=light data-icon=sun><button class=dropdown-item>
<i class="mode-icon fas fa-fw fa-sun"></i> Light</button></li><li class=mode-item data-color-mode=dark data-icon=moon><button class=dropdown-item>
<i class="mode-icon fas fa-fw fa-moon"></i> Dark</button></li><li class=mode-item data-color-mode=auto data-icon=adjust><button class=dropdown-item>
<i class="mode-icon fas fa-fw fa-adjust"></i> Auto</button></li></ul></li></ul></div></div><div class=d-flex><button class="navbar-toggler order-5 border-0" type=button data-bs-toggle=offcanvas data-bs-target=#navbarMenus aria-controls=navbarMenus aria-expanded=false aria-label="Toggle navigation">
<i class="fas fa-ellipsis-h"></i></button></div></div></nav></header><main class=container><div class="row content"><noscript><div class="alert alert-danger" role=alert>Your browser does not support JavaScript.</div></noscript><div class=col-xxl-8><div class=container><nav class="row card component" aria-label=breadcrumb><div class="card-body pb-0"><ol class="hbs-breadcrumb breadcrumb flex-nowrap"><li class="breadcrumb-item text-surface"><a href=/>Home</a></li><li class="breadcrumb-item text-surface"><a href=/blog/>Blogs</a></li><li class="breadcrumb-item active">Develop and Test Apache Spark Apps for EMR Locally Using Docker</li></ol></div></nav><div class="post-panel-wrapper position-relative d-flex justify-content-center"><div class="d-flex flex-row justify-content-center rounded rounded-5 border border-primary post-panel position-fixed px-3 py-1 surface shadow"><a class="action action-toc d-none d-xxl-block" href=#postTOC role=button title="Table of contents"><i class="fas fa-fw fa-list-alt"></i></a>
<a class="action action-toc d-block d-xxl-none" href=#post-toc-container role=button title="Table of contents"><i class="fas fa-fw fa-list-alt"></i></a>
<a class="action action-post-comments" href=#post-comments role=button aria-label=Comments title=Comments><i class="fas fa-fw fa-comments"></i></a>
<a id=sidebarToggler class="action action-sidebar-toggler d-none d-xxl-block" role=button title="Sidebar toggler"><i class="fas fa-fw fa-expand-alt" data-fa-transform=rotate-45></i></a></div></div><article class="row card component mb-4 post"><div class=card-header><h1 class="card-title post-title my-2">Develop and Test Apache Spark Apps for EMR Locally Using Docker</h1></div><div class=card-body><div class="post-meta mb-3"><span class="post-date me-1 mb-1" title="created on 2022-05-08 00:00:00 +0000 UTC, updated on 2023-04-14 00:32:29 +0000 UTC.">May 8, 2022</span><span class="post-reading-time me-1 mb-1">17 min read</span><a href=/categories/data-engineering/ class="btn btn-sm btn-secondary mb-1 me-2 py-0 pe-1 post-taxonomy post-taxonomy-sm post-category">
<i class="fas fa-fw fa-folder me-1"></i>Data Engineering</a><a href=/tags/amazon-emr/ class="btn btn-sm btn-secondary mb-1 me-2 py-0 pe-1 post-taxonomy post-taxonomy-sm post-tag">Amazon EMR</a><a href=/tags/apache-spark/ class="btn btn-sm btn-secondary mb-1 me-2 py-0 pe-1 post-taxonomy post-taxonomy-sm post-tag">Apache Spark</a><a href=/tags/aws/ class="btn btn-sm btn-secondary mb-1 me-2 py-0 pe-1 post-taxonomy post-taxonomy-sm post-tag">AWS</a><a href=/tags/docker/ class="btn btn-sm btn-secondary mb-1 me-2 py-0 pe-1 post-taxonomy post-taxonomy-sm post-tag">Docker</a><a href=/tags/docker-compose/ class="btn btn-sm btn-secondary mb-1 me-2 py-0 pe-1 post-taxonomy post-taxonomy-sm post-tag">Docker Compose</a><a href=/tags/pyspark/ class="btn btn-sm btn-secondary mb-1 me-2 py-0 pe-1 post-taxonomy post-taxonomy-sm post-tag">PySpark</a><a href=/tags/visual-studio-code/ class="btn btn-sm btn-secondary mb-1 me-2 py-0 pe-1 post-taxonomy post-taxonomy-sm post-tag">Visual Studio Code</a></div><picture class="d-flex justify-content-center"><source srcset=/blog/2022-05-08-emr-local-dev/featured_hu653aa93a98a5139fededca231658be70_25693_0x270_resize_box_3.png type=image/png media="(max-width: 576px)" width=464 height=270><img class="post-featured-img h-auto w-auto mb-3" alt="Develop and Test Apache Spark Apps for EMR Locally Using Docker" src=/blog/2022-05-08-emr-local-dev/featured_hu653aa93a98a5139fededca231658be70_25693_0x480_resize_box_3.png width=826 height=480 data-src=/blog/2022-05-08-emr-local-dev/featured.png></picture><div id=postTOC class="mb-3 text-surface"><h2 class=mb-3>Contents<a class="anchor ms-1" href=#postTOC></a></h2><nav id=TableOfContents><ul><li><a href=#custom-docker-image>Custom Docker Image</a></li><li><a href=#vscode-development-container>VSCode Development Container</a><ul><li><a href=#docker-compose>Docker Compose</a></li><li><a href=#development-container>Development Container</a></li><li><a href=#file-permission-management>File Permission Management</a></li></ul></li><li><a href=#examples>Examples</a><ul><li><a href=#spark-submit>Spark Submit</a></li><li><a href=#pytest>Pytest</a></li><li><a href=#pyspark-shell>PySpark Shell</a></li><li><a href=#jupyter-notebook>Jupyter Notebook</a></li><li><a href=#spark-streaming>Spark Streaming</a></li></ul></li><li><a href=#summary>Summary</a></li></ul></nav><hr class=text-secondary></div><div class="post-content mb-3" data-bs-spy=scroll data-bs-target=#TableOfContents tabindex=0><div id=post-content-body><p><strong><a href=https://cevo.com.au/post/develop-and-test-apache-spark-apps-for-emr-locally-using-docker/ target=_blank rel="noopener noreferrer">This article<i class="fas fa-external-link-square-alt ms-1"></i></a> was originally posted on Tech Insights of <a href=https://cevo.com.au/ target=_blank rel="noopener noreferrer">Cevo Australia<i class="fas fa-external-link-square-alt ms-1"></i></a>.</strong></p><p><a href=https://aws.amazon.com/emr/ target=_blank rel="noopener noreferrer">Amazon EMR<i class="fas fa-external-link-square-alt ms-1"></i></a> is a managed service that simplifies running Apache Spark on AWS. It has multiple deployment options that cover EC2, <a href=https://aws.amazon.com/emr/features/eks/ target=_blank rel="noopener noreferrer">EKS<i class="fas fa-external-link-square-alt ms-1"></i></a>, <a href=https://aws.amazon.com/emr/features/outposts/ target=_blank rel="noopener noreferrer">Outposts<i class="fas fa-external-link-square-alt ms-1"></i></a> and <a href=https://aws.amazon.com/emr/serverless/ target=_blank rel="noopener noreferrer">Serverless<i class="fas fa-external-link-square-alt ms-1"></i></a>. For development and testing, <a href=https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-managed-notebooks.html target=_blank rel="noopener noreferrer">EMR Notebooks<i class="fas fa-external-link-square-alt ms-1"></i></a> or <a href=https://aws.amazon.com/emr/features/studio/ target=_blank rel="noopener noreferrer">EMR Studio<i class="fas fa-external-link-square-alt ms-1"></i></a> can be an option. Both provide a Jupyter Notebook environment and the former is only available for EMR on EC2. There are cases, however, that development (and learning) is performed in a local environment more efficiently. The AWS Glue team understands this demand, and they illustrate how to make use of a custom Docker image for Glue in a <a href=https://aws.amazon.com/blogs/big-data/develop-and-test-aws-glue-version-3-0-jobs-locally-using-a-docker-container/ target=_blank rel="noopener noreferrer">recent blog post<i class="fas fa-external-link-square-alt ms-1"></i></a>. However, we don’t hear similar news from the EMR team. In order to fill the gap, we’ll discuss how to create a Spark local development environment for EMR using Docker and/or VSCode. Typical Spark development examples will be demonstrated, which covers Spark Submit, Pytest, PySpark shell, Jupyter Notebook and Spark Structured Streaming. For the Spark Submit and Jupyter Notebook examples, <a href=https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-spark-glue.html target=_blank rel="noopener noreferrer">Glue Catalog integration<i class="fas fa-external-link-square-alt ms-1"></i></a> will be illustrated as well. And both the cases of utilising <a href=https://code.visualstudio.com/docs/remote/containers target=_blank rel="noopener noreferrer">Visual Studio Code Remote - Containers<i class="fas fa-external-link-square-alt ms-1"></i></a> extension and running as an isolated container will be covered in some key examples.</p><h2 id=custom-docker-image data-numberify>Custom Docker Image<a class="anchor ms-1" href=#custom-docker-image></a></h2><p>While we may build a custom Spark Docker image from scratch, it’ll be tricky to configure the <a href=https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-spark-glue.html target=_blank rel="noopener noreferrer">AWS Glue Data Catalog as the metastore for Spark SQL<i class="fas fa-external-link-square-alt ms-1"></i></a>. Note that it is important to set up this feature because it can be used to integrate other AWS services such as Athena, Glue, Redshift Spectrum and so on. For example, with this feature, we can create a Glue table using a Spark application and the table can be queried by Athena or Redshift Spectrum.</p><p>Instead, we can use one of the <a href=https://docs.aws.amazon.com/emr/latest/EMR-on-EKS-DevelopmentGuide/emr-eks-releases.html target=_blank rel="noopener noreferrer">Docker images for EMR on EKS<i class="fas fa-external-link-square-alt ms-1"></i></a> as a base image and build a custom image from it. As indicated in the <a href=https://docs.aws.amazon.com/emr/latest/EMR-on-EKS-DevelopmentGuide/docker-custom-images.html target=_blank rel="noopener noreferrer">EMR on EKS document<i class="fas fa-external-link-square-alt ms-1"></i></a>, we can pull an EMR release image from ECR. Note to select the right AWS account ID as it is different from one region to another. After authenticating to the ECR repository, I pulled the latest EMR 6.5.0 release image.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=ln>1</span><span class=cl><span class=c1>## different aws region has a different account id </span>
</span></span><span class=line><span class=ln>2</span><span class=cl>$ aws ecr get-login-password --region ap-southeast-2 <span class=se>\
</span></span></span><span class=line><span class=ln>3</span><span class=cl><span class=se></span>  <span class=p>|</span> docker login --username AWS --password-stdin 038297999601.dkr.ecr.ap-southeast-2.amazonaws.com
</span></span><span class=line><span class=ln>4</span><span class=cl><span class=c1>## download the latest release (6.5.0)</span>
</span></span><span class=line><span class=ln>5</span><span class=cl>$ docker pull 038297999601.dkr.ecr.ap-southeast-2.amazonaws.com/spark/emr-6.5.0:20211119
</span></span></code></pre></div><p>In the <a href=https://github.com/jaehyeon-kim/emr-local-dev/blob/main/.devcontainer/Dockerfile target=_blank rel="noopener noreferrer">Dockerfile<i class="fas fa-external-link-square-alt ms-1"></i></a>, I updated the default user (<em>hadoop</em>) to have the admin privilege as it can be handy to modify system configuration if necessary. Then <a href=https://github.com/jaehyeon-kim/emr-local-dev/blob/main/.devcontainer/spark/spark-defaults.conf target=_blank rel="noopener noreferrer">spark-defaults.conf<i class="fas fa-external-link-square-alt ms-1"></i></a> and <a href=https://github.com/jaehyeon-kim/emr-local-dev/blob/main/.devcontainer/spark/log4j.properties target=_blank rel="noopener noreferrer">log4j.properties<i class="fas fa-external-link-square-alt ms-1"></i></a> are copied to the Spark configuration folder - they’ll be discussed in detail below. Finally a number of python packages are installed. Among those, the <a href=https://pypi.org/project/ipykernel/ target=_blank rel="noopener noreferrer">ipykernel<i class="fas fa-external-link-square-alt ms-1"></i></a> and <a href=https://pypi.org/project/python-dotenv/ target=_blank rel="noopener noreferrer">python-dotenv<i class="fas fa-external-link-square-alt ms-1"></i></a> packages are installed to work on Jupyter Notebooks and the <a href=https://pypi.org/project/pytest/ target=_blank rel="noopener noreferrer">pytest<i class="fas fa-external-link-square-alt ms-1"></i></a> and <a href=https://pypi.org/project/pytest-cov/ target=_blank rel="noopener noreferrer">pytest-cov<i class="fas fa-external-link-square-alt ms-1"></i></a> packages are for testing. The custom Docker image is built with the following command: <code>docker build -t=emr-6.5.0:20211119 .devcontainer/</code>.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-Dockerfile data-lang=Dockerfile><span class=line><span class=ln> 1</span><span class=cl><span class=c># .devcontainer/Dockerfile</span><span class=err>
</span></span></span><span class=line><span class=ln> 2</span><span class=cl><span class=err></span><span class=k>FROM</span><span class=s> 038297999601.dkr.ecr.ap-southeast-2.amazonaws.com/spark/emr-6.5.0:20211119</span><span class=err>
</span></span></span><span class=line><span class=ln> 3</span><span class=cl><span class=err>
</span></span></span><span class=line><span class=ln> 4</span><span class=cl><span class=err></span><span class=k>USER</span><span class=s> root</span><span class=err>
</span></span></span><span class=line><span class=ln> 5</span><span class=cl><span class=err>
</span></span></span><span class=line><span class=ln> 6</span><span class=cl><span class=err></span><span class=c>## Add hadoop to sudo</span><span class=err>
</span></span></span><span class=line><span class=ln> 7</span><span class=cl><span class=err></span><span class=k>RUN</span> yum install -y sudo git <span class=se>\
</span></span></span><span class=line><span class=ln> 8</span><span class=cl><span class=se></span>  <span class=o>&amp;&amp;</span> <span class=nb>echo</span> <span class=s2>&#34;hadoop ALL=(ALL) NOPASSWD:ALL&#34;</span> &gt;&gt; /etc/sudoers<span class=err>
</span></span></span><span class=line><span class=ln> 9</span><span class=cl><span class=err>
</span></span></span><span class=line><span class=ln>10</span><span class=cl><span class=err></span><span class=c>## Update spark config and log4j properties</span><span class=err>
</span></span></span><span class=line><span class=ln>11</span><span class=cl><span class=err></span><span class=k>COPY</span> ./spark/spark-defaults.conf /usr/lib/spark/conf/spark-defaults.conf<span class=err>
</span></span></span><span class=line><span class=ln>12</span><span class=cl><span class=err></span><span class=k>COPY</span> ./spark/log4j.properties /usr/lib/spark/conf/log4j.properties<span class=err>
</span></span></span><span class=line><span class=ln>13</span><span class=cl><span class=err>
</span></span></span><span class=line><span class=ln>14</span><span class=cl><span class=err></span><span class=c>## Install python packages</span><span class=err>
</span></span></span><span class=line><span class=ln>15</span><span class=cl><span class=err></span><span class=k>COPY</span> ./pkgs /tmp/pkgs<span class=err>
</span></span></span><span class=line><span class=ln>16</span><span class=cl><span class=err></span><span class=k>RUN</span> pip3 install -r /tmp/pkgs/requirements.txt<span class=err>
</span></span></span><span class=line><span class=ln>17</span><span class=cl><span class=err>
</span></span></span><span class=line><span class=ln>18</span><span class=cl><span class=err></span><span class=k>USER</span><span class=s> hadoop:hadoop</span><span class=err>
</span></span></span></code></pre></div><p>In the default spark configuration file (<em>spark-defaults.conf</em>) shown below, I commented out the following properties that are strictly related to EMR on EKS.</p><ul><li><em>spark.master</em></li><li><em>spark.submit.deployMode</em></li><li><em>spark.kubernetes.container.image.pullPolicy</em></li><li><em>spark.kubernetes.pyspark.pythonVersion</em></li></ul><p>Then I changed the custom AWS credentials provider class from <a href=https://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/auth/WebIdentityTokenCredentialsProvider.html target=_blank rel="noopener noreferrer">WebIdentityTokenCredentialsProvider<i class="fas fa-external-link-square-alt ms-1"></i></a> to <a href=https://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/auth/EnvironmentVariableCredentialsProvider.html target=_blank rel="noopener noreferrer">EnvironmentVariableCredentialsProvider<i class="fas fa-external-link-square-alt ms-1"></i></a>. Note EMR jobs are run by a service account on EKS and authentication is managed by web identity token credentials. In a local environment, however, we don’t have an identity provider to authenticate so that access via environment variables can be an easy alternative option. We need the following environment variables to access AWS resources.</p><ul><li><em>AWS_ACCESS_KEY_ID</em></li><li><em>AWS_SECRET_ACCESS_KEY</em></li><li><em>AWS_SESSION_TOKEN</em><ul><li>note it is optional and required if authentication is made via assume role</li></ul></li><li><em>AWS_REGION</em><ul><li>note it is NOT <em>AWS_DEFAULT_REGION</em></li></ul></li></ul><p>Finally, I enabled Hive support and set <em>AWSGlueDataCatalogHiveClientFactory</em> as the Hive metastore factory class. When we start an EMR job, we can <a href=https://docs.aws.amazon.com/emr-on-eks/latest/APIReference/API_ConfigurationOverrides.html target=_blank rel="noopener noreferrer">override application configuration<i class="fas fa-external-link-square-alt ms-1"></i></a> to use AWS Glue Data Catalog as the metastore for Spark SQL and these are the relevant configuration changes for it.</p><pre tabindex=0><code class=language-conf data-lang=conf># .devcontainer/spark/spark-defaults.conf

...

#spark.master                     k8s://https://kubernetes.default.svc:443
#spark.submit.deployMode          cluster
spark.hadoop.fs.defaultFS        file:///
spark.shuffle.service.enabled    false
spark.dynamicAllocation.enabled  false
#spark.kubernetes.container.image.pullPolicy  Always
#spark.kubernetes.pyspark.pythonVersion 3
spark.hadoop.fs.s3.customAWSCredentialsProvider  com.amazonaws.auth.EnvironmentVariableCredentialsProvider
spark.hadoop.dynamodb.customAWSCredentialsProvider  com.amazonaws.auth.EnvironmentVariableCredentialsProvider
spark.authenticate               true
## for Glue catalog
spark.sql.catalogImplementation  hive
spark.hadoop.hive.metastore.client.factory.class  com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory
</code></pre><p>Even if the credentials provider class is changed, it keeps showing long warning messages while fetching EC2 metadata. The following lines are added to the Log4j properties in order to disable those messages.</p><pre tabindex=0><code class=language-conf data-lang=conf># .devcontainer/spark/log4j.properties

...

## Ignore warn messages related to EC2 metadata access failure
log4j.logger.com.amazonaws.internal.InstanceMetadataServiceResourceFetcher=FATAL
log4j.logger.com.amazonaws.util.EC2MetadataUtils=FATAL
</code></pre><h2 id=vscode-development-container data-numberify>VSCode Development Container<a class="anchor ms-1" href=#vscode-development-container></a></h2><p>We are able to run Spark Submit, pytest, PySpark shell examples as an isolated container using the custom Docker image. However it can be much more convenient if we are able to perform development inside the Docker container where our app is executed. The <a href=https://code.visualstudio.com/docs/remote/containers target=_blank rel="noopener noreferrer">Visual Studio Code Remote - Containers<i class="fas fa-external-link-square-alt ms-1"></i></a> extension allows you to open a folder inside a container and to use VSCode’s feature sets. It supports both a standalone container and <a href=https://code.visualstudio.com/docs/remote/create-dev-container#_use-docker-compose target=_blank rel="noopener noreferrer">Docker Compose<i class="fas fa-external-link-square-alt ms-1"></i></a>. In this post, we’ll use the latter as we’ll discuss an example Spark Structured Streaming application and multiple services should run and linked together for it.</p><h3 id=docker-compose data-numberify>Docker Compose<a class="anchor ms-1" href=#docker-compose></a></h3><p>The main service (container) is named <em>spark</em> and its command prevents it from being terminated. The current working directory is mapped to <em>/home/hadoop/repo</em> and it’ll be the container folder that we’ll open for development. The aws configuration folder is volume-mapped to the container user’s home directory. It is an optional configuration to access AWS services without relying on AWS credentials via environment variables. The remaining services are related to Kafka. The <em>kafka</em> and <em>zookeeper</em> services are to run a Kafka cluster and the <em>kafka-ui</em> allows us to access the cluster on a browser. The services share the same Docker network named <em>spark</em>. Note that the compose file includes other Kafka related services and their details can be found in <a href=/blog/2021-12-05-datalake-demo-part1>one of my earlier posts</a>.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=ln> 1</span><span class=cl><span class=c># .devcontainer/docker-compose.yml</span><span class=w>
</span></span></span><span class=line><span class=ln> 2</span><span class=cl><span class=w></span><span class=nt>version</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;2&#34;</span><span class=w>
</span></span></span><span class=line><span class=ln> 3</span><span class=cl><span class=w>
</span></span></span><span class=line><span class=ln> 4</span><span class=cl><span class=w></span><span class=nt>services</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=ln> 5</span><span class=cl><span class=w>  </span><span class=nt>spark</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=ln> 6</span><span class=cl><span class=w>    </span><span class=nt>image</span><span class=p>:</span><span class=w> </span><span class=l>emr-6.5.0:20211119</span><span class=w>
</span></span></span><span class=line><span class=ln> 7</span><span class=cl><span class=w>    </span><span class=nt>container_name</span><span class=p>:</span><span class=w> </span><span class=l>spark</span><span class=w>
</span></span></span><span class=line><span class=ln> 8</span><span class=cl><span class=w>    </span><span class=nt>command</span><span class=p>:</span><span class=w> </span><span class=l>/bin/bash -c &#34;while sleep 1000; do :; done&#34;</span><span class=w>
</span></span></span><span class=line><span class=ln> 9</span><span class=cl><span class=w>    </span><span class=nt>networks</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=ln>10</span><span class=cl><span class=w>      </span>- <span class=l>spark</span><span class=w>
</span></span></span><span class=line><span class=ln>11</span><span class=cl><span class=w>    </span><span class=nt>volumes</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=ln>12</span><span class=cl><span class=w>      </span>- <span class=l>${PWD}:/home/hadoop/repo</span><span class=w>
</span></span></span><span class=line><span class=ln>13</span><span class=cl><span class=w>      </span>- <span class=l>${HOME}/.aws:/home/hadoop/.aws</span><span class=w>
</span></span></span><span class=line><span class=ln>14</span><span class=cl><span class=w>  </span><span class=nt>zookeeper</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=ln>15</span><span class=cl><span class=w>    </span><span class=nt>image</span><span class=p>:</span><span class=w> </span><span class=l>bitnami/zookeeper:3.7.0</span><span class=w>
</span></span></span><span class=line><span class=ln>16</span><span class=cl><span class=w>    </span><span class=nt>container_name</span><span class=p>:</span><span class=w> </span><span class=l>zookeeper</span><span class=w>
</span></span></span><span class=line><span class=ln>17</span><span class=cl><span class=w>    </span><span class=nt>ports</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=ln>18</span><span class=cl><span class=w>      </span>- <span class=s2>&#34;2181:2181&#34;</span><span class=w>
</span></span></span><span class=line><span class=ln>19</span><span class=cl><span class=w>    </span><span class=nt>networks</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=ln>20</span><span class=cl><span class=w>      </span>- <span class=l>spark</span><span class=w>
</span></span></span><span class=line><span class=ln>21</span><span class=cl><span class=w>    </span><span class=nt>environment</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=ln>22</span><span class=cl><span class=w>      </span>- <span class=l>ALLOW_ANONYMOUS_LOGIN=yes</span><span class=w>
</span></span></span><span class=line><span class=ln>23</span><span class=cl><span class=w>  </span><span class=nt>kafka</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=ln>24</span><span class=cl><span class=w>    </span><span class=nt>image</span><span class=p>:</span><span class=w> </span><span class=l>bitnami/kafka:2.8.1</span><span class=w>
</span></span></span><span class=line><span class=ln>25</span><span class=cl><span class=w>    </span><span class=nt>container_name</span><span class=p>:</span><span class=w> </span><span class=l>kafka</span><span class=w>
</span></span></span><span class=line><span class=ln>26</span><span class=cl><span class=w>    </span><span class=nt>ports</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=ln>27</span><span class=cl><span class=w>      </span>- <span class=s2>&#34;9092:9092&#34;</span><span class=w>
</span></span></span><span class=line><span class=ln>28</span><span class=cl><span class=w>    </span><span class=nt>networks</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=ln>29</span><span class=cl><span class=w>      </span>- <span class=l>spark</span><span class=w>
</span></span></span><span class=line><span class=ln>30</span><span class=cl><span class=w>    </span><span class=nt>environment</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=ln>31</span><span class=cl><span class=w>      </span>- <span class=l>KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181</span><span class=w>
</span></span></span><span class=line><span class=ln>32</span><span class=cl><span class=w>      </span>- <span class=l>ALLOW_PLAINTEXT_LISTENER=yes</span><span class=w>
</span></span></span><span class=line><span class=ln>33</span><span class=cl><span class=w>    </span><span class=nt>depends_on</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=ln>34</span><span class=cl><span class=w>      </span>- <span class=l>zookeeper</span><span class=w>
</span></span></span><span class=line><span class=ln>35</span><span class=cl><span class=w>  </span><span class=nt>kafka-ui</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=ln>36</span><span class=cl><span class=w>    </span><span class=nt>image</span><span class=p>:</span><span class=w> </span><span class=l>provectuslabs/kafka-ui:0.3.3</span><span class=w>
</span></span></span><span class=line><span class=ln>37</span><span class=cl><span class=w>    </span><span class=nt>container_name</span><span class=p>:</span><span class=w> </span><span class=l>kafka-ui</span><span class=w>
</span></span></span><span class=line><span class=ln>38</span><span class=cl><span class=w>    </span><span class=nt>ports</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=ln>39</span><span class=cl><span class=w>      </span>- <span class=s2>&#34;8080:8080&#34;</span><span class=w>
</span></span></span><span class=line><span class=ln>40</span><span class=cl><span class=w>    </span><span class=nt>networks</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=ln>41</span><span class=cl><span class=w>      </span>- <span class=l>spark</span><span class=w>
</span></span></span><span class=line><span class=ln>42</span><span class=cl><span class=w>    </span><span class=nt>environment</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=ln>43</span><span class=cl><span class=w>      </span><span class=nt>KAFKA_CLUSTERS_0_NAME</span><span class=p>:</span><span class=w> </span><span class=l>local</span><span class=w>
</span></span></span><span class=line><span class=ln>44</span><span class=cl><span class=w>      </span><span class=nt>KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS</span><span class=p>:</span><span class=w> </span><span class=l>kafka:9092</span><span class=w>
</span></span></span><span class=line><span class=ln>45</span><span class=cl><span class=w>      </span><span class=nt>KAFKA_CLUSTERS_0_ZOOKEEPER</span><span class=p>:</span><span class=w> </span><span class=l>zookeeper:2181</span><span class=w>
</span></span></span><span class=line><span class=ln>46</span><span class=cl><span class=w>      </span><span class=l>...</span><span class=w>
</span></span></span><span class=line><span class=ln>47</span><span class=cl><span class=w>    </span><span class=nt>depends_on</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=ln>48</span><span class=cl><span class=w>      </span>- <span class=l>zookeeper</span><span class=w>
</span></span></span><span class=line><span class=ln>49</span><span class=cl><span class=w>      </span>- <span class=l>kafka</span><span class=w>
</span></span></span><span class=line><span class=ln>50</span><span class=cl><span class=w></span><span class=nn>...</span><span class=w>
</span></span></span><span class=line><span class=ln>51</span><span class=cl><span class=w>
</span></span></span><span class=line><span class=ln>52</span><span class=cl><span class=w></span><span class=nt>networks</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=ln>53</span><span class=cl><span class=w>  </span><span class=nt>spark</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=ln>54</span><span class=cl><span class=w>    </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>spark</span><span class=w>
</span></span></span></code></pre></div><h3 id=development-container data-numberify>Development Container<a class="anchor ms-1" href=#development-container></a></h3><p>The development container is configured to connect the _spark _service among the Docker Compose services. The _AWS_PROFILE _environment variable is optionally set for AWS configuration and additional folders are added to <em>PYTHONPATH</em>, which is to use the bundled pyspark and py4j packages of the Spark distribution. The port 4040 for Spark History Server is added to the forwarded ports array - I guess it’s optional as the port is made accessible in the compose file. The remaining sections are for installing VSCode extensions and adding editor configuration. Note we need the Python extension (<em>ms-python.python</em>) not only for code formatting but also for working on Jupyter Notebooks.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-json data-lang=json><span class=line><span class=ln> 1</span><span class=cl><span class=err>#</span> <span class=err>.devcontainer/devcontainer.json</span>
</span></span><span class=line><span class=ln> 2</span><span class=cl><span class=p>{</span>
</span></span><span class=line><span class=ln> 3</span><span class=cl>  <span class=nt>&#34;name&#34;</span><span class=p>:</span> <span class=s2>&#34;Spark Development&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=ln> 4</span><span class=cl>  <span class=nt>&#34;dockerComposeFile&#34;</span><span class=p>:</span> <span class=s2>&#34;docker-compose.yml&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=ln> 5</span><span class=cl>  <span class=nt>&#34;service&#34;</span><span class=p>:</span> <span class=s2>&#34;spark&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=ln> 6</span><span class=cl>  <span class=nt>&#34;runServices&#34;</span><span class=p>:</span> <span class=p>[</span>
</span></span><span class=line><span class=ln> 7</span><span class=cl>    <span class=s2>&#34;spark&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=ln> 8</span><span class=cl>    <span class=s2>&#34;zookeeper&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=ln> 9</span><span class=cl>    <span class=s2>&#34;kafka&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=ln>10</span><span class=cl>    <span class=s2>&#34;kafka-ui&#34;</span>
</span></span><span class=line><span class=ln>11</span><span class=cl>  <span class=p>],</span>
</span></span><span class=line><span class=ln>12</span><span class=cl>  <span class=nt>&#34;remoteEnv&#34;</span><span class=p>:</span> <span class=p>{</span>
</span></span><span class=line><span class=ln>13</span><span class=cl>    <span class=nt>&#34;AWS_PROFILE&#34;</span><span class=p>:</span> <span class=s2>&#34;cevo&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=ln>14</span><span class=cl>    <span class=nt>&#34;PYTHONPATH&#34;</span><span class=p>:</span> <span class=s2>&#34;/usr/lib/spark/python/lib/py4j-0.10.9-src.zip:/usr/lib/spark/python/&#34;</span>
</span></span><span class=line><span class=ln>15</span><span class=cl>  <span class=p>},</span>
</span></span><span class=line><span class=ln>16</span><span class=cl>  <span class=nt>&#34;workspaceFolder&#34;</span><span class=p>:</span> <span class=s2>&#34;/home/hadoop/repo&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=ln>17</span><span class=cl>  <span class=nt>&#34;extensions&#34;</span><span class=p>:</span> <span class=p>[</span><span class=s2>&#34;ms-python.python&#34;</span><span class=p>,</span> <span class=s2>&#34;esbenp.prettier-vscode&#34;</span><span class=p>],</span>
</span></span><span class=line><span class=ln>18</span><span class=cl>  <span class=nt>&#34;forwardPorts&#34;</span><span class=p>:</span> <span class=p>[</span><span class=mi>4040</span><span class=p>],</span>
</span></span><span class=line><span class=ln>19</span><span class=cl>  <span class=nt>&#34;settings&#34;</span><span class=p>:</span> <span class=p>{</span>
</span></span><span class=line><span class=ln>20</span><span class=cl>    <span class=nt>&#34;terminal.integrated.profiles.linux&#34;</span><span class=p>:</span> <span class=p>{</span>
</span></span><span class=line><span class=ln>21</span><span class=cl>      <span class=nt>&#34;bash&#34;</span><span class=p>:</span> <span class=p>{</span>
</span></span><span class=line><span class=ln>22</span><span class=cl>        <span class=nt>&#34;path&#34;</span><span class=p>:</span> <span class=s2>&#34;/bin/bash&#34;</span>
</span></span><span class=line><span class=ln>23</span><span class=cl>      <span class=p>}</span>
</span></span><span class=line><span class=ln>24</span><span class=cl>    <span class=p>},</span>
</span></span><span class=line><span class=ln>25</span><span class=cl>    <span class=nt>&#34;terminal.integrated.defaultProfile.linux&#34;</span><span class=p>:</span> <span class=s2>&#34;bash&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=ln>26</span><span class=cl>    <span class=nt>&#34;editor.formatOnSave&#34;</span><span class=p>:</span> <span class=kc>true</span><span class=p>,</span>
</span></span><span class=line><span class=ln>27</span><span class=cl>    <span class=nt>&#34;editor.defaultFormatter&#34;</span><span class=p>:</span> <span class=s2>&#34;esbenp.prettier-vscode&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=ln>28</span><span class=cl>    <span class=nt>&#34;editor.tabSize&#34;</span><span class=p>:</span> <span class=mi>2</span><span class=p>,</span>
</span></span><span class=line><span class=ln>29</span><span class=cl>    <span class=nt>&#34;python.defaultInterpreterPath&#34;</span><span class=p>:</span> <span class=s2>&#34;python3&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=ln>30</span><span class=cl>    <span class=nt>&#34;python.testing.pytestEnabled&#34;</span><span class=p>:</span> <span class=kc>true</span><span class=p>,</span>
</span></span><span class=line><span class=ln>31</span><span class=cl>    <span class=nt>&#34;python.linting.enabled&#34;</span><span class=p>:</span> <span class=kc>true</span><span class=p>,</span>
</span></span><span class=line><span class=ln>32</span><span class=cl>    <span class=nt>&#34;python.linting.pylintEnabled&#34;</span><span class=p>:</span> <span class=kc>false</span><span class=p>,</span>
</span></span><span class=line><span class=ln>33</span><span class=cl>    <span class=nt>&#34;python.linting.flake8Enabled&#34;</span><span class=p>:</span> <span class=kc>false</span><span class=p>,</span>
</span></span><span class=line><span class=ln>34</span><span class=cl>    <span class=nt>&#34;python.formatting.provider&#34;</span><span class=p>:</span> <span class=s2>&#34;black&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=ln>35</span><span class=cl>    <span class=nt>&#34;python.formatting.blackPath&#34;</span><span class=p>:</span> <span class=s2>&#34;black&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=ln>36</span><span class=cl>    <span class=nt>&#34;python.formatting.blackArgs&#34;</span><span class=p>:</span> <span class=p>[</span><span class=s2>&#34;--line-length&#34;</span><span class=p>,</span> <span class=s2>&#34;100&#34;</span><span class=p>],</span>
</span></span><span class=line><span class=ln>37</span><span class=cl>    <span class=nt>&#34;[python]&#34;</span><span class=p>:</span> <span class=p>{</span>
</span></span><span class=line><span class=ln>38</span><span class=cl>      <span class=nt>&#34;editor.tabSize&#34;</span><span class=p>:</span> <span class=mi>4</span><span class=p>,</span>
</span></span><span class=line><span class=ln>39</span><span class=cl>      <span class=nt>&#34;editor.defaultFormatter&#34;</span><span class=p>:</span> <span class=s2>&#34;ms-python.python&#34;</span>
</span></span><span class=line><span class=ln>40</span><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=ln>41</span><span class=cl>  <span class=p>}</span>
</span></span><span class=line><span class=ln>42</span><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>We can open the current folder in the development container after launching the Docker Compose services by executing the following command in the <a href=https://code.visualstudio.com/docs/getstarted/userinterface#_command-palette target=_blank rel="noopener noreferrer">command palette<i class="fas fa-external-link-square-alt ms-1"></i></a>.</p><ul><li><em>Remote-Containers: Open Folder in Container&mldr;</em></li></ul><p><picture><img class="img-fluid mx-auto d-block" alt src=/blog/2022-05-08-emr-local-dev/01-open-folder-in-container.png loading=lazy width=1026 height=465></picture></p><p>Once the development container is ready, the current folder will be open within the spark service container. We are able to check the container’s current folder is <code>/home/hadoop/repo</code> and the container user is <em>hadoop</em>.</p><p><picture><img class="img-fluid mx-auto d-block" alt src=/blog/2022-05-08-emr-local-dev/02-continer.png loading=lazy width=1027 height=520></picture></p><h3 id=file-permission-management data-numberify>File Permission Management<a class="anchor ms-1" href=#file-permission-management></a></h3><p>I use Ubuntu in WSL 2 for development and the user ID and group ID of my WSL user are 1000. On the other hand, the container user is <em>hadoop</em> and its user ID and group ID are 999 and 1000 respectively. When you create a file in the host, the user has the read and write permissions of the file while the group only has the read permission. Therefore, you can read the file inside the development container by the container user, but it is not possible to modify it due to lack of the write permission. This file permission issue will happen when a file is created by the container user and the WSL user tries to modify it in the host. A quick search shows this is a typical behaviour applicable only to Linux (not Mac or Windows).</p><p>In order to handle this file permission issue, we can update the file permission so that the read and write permissions are given to both the user and group. Note the host (WSL) user and container user have the same group ID and writing activities will be allowed at least by the group permission. Below shows an example. The read and write permissions for files in the project folder are given to both the user and group. Those that are created by the container user indicate the username while there are 2 files that are created by the WSL user, and it is indicated by the user ID because there is no user whose user ID is 1000 in the container.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=ln>1</span><span class=cl>bash-4.2$ ls -al <span class=p>|</span> grep <span class=s1>&#39;^-&#39;</span>
</span></span><span class=line><span class=ln>2</span><span class=cl>-rw-rw-r--  <span class=m>1</span> hadoop hadoop <span class=m>1086</span> Apr <span class=m>12</span> 22:23 .env
</span></span><span class=line><span class=ln>3</span><span class=cl>-rw-rw-r--  <span class=m>1</span>   <span class=m>1000</span> hadoop <span class=m>1855</span> Apr <span class=m>12</span> 19:45 .gitignore
</span></span><span class=line><span class=ln>4</span><span class=cl>-rw-rw-r--  <span class=m>1</span>   <span class=m>1000</span> hadoop   <span class=m>66</span> Mar <span class=m>30</span> 22:39 README.md
</span></span><span class=line><span class=ln>5</span><span class=cl>-rw-rw-r--  <span class=m>1</span> hadoop hadoop  <span class=m>874</span> Apr  <span class=m>5</span> 11:14 test_utils.py
</span></span><span class=line><span class=ln>6</span><span class=cl>-rw-rw-r--  <span class=m>1</span> hadoop hadoop <span class=m>3882</span> Apr <span class=m>12</span> 22:24 tripdata.ipynb
</span></span><span class=line><span class=ln>7</span><span class=cl>-rw-rw-r--  <span class=m>1</span> hadoop hadoop <span class=m>1653</span> Apr <span class=m>24</span> 13:09 tripdata_notify.py
</span></span><span class=line><span class=ln>8</span><span class=cl>-rw-rw-r--  <span class=m>1</span> hadoop hadoop <span class=m>1101</span> Apr <span class=m>24</span> 01:22 tripdata.py
</span></span><span class=line><span class=ln>9</span><span class=cl>-rw-rw-r--  <span class=m>1</span> hadoop hadoop  <span class=m>664</span> Apr <span class=m>12</span> 19:45 utils.py
</span></span></code></pre></div><p>Below is the same file list that is printed in the host. Note that the group name is changed into the WSL user’s group and those that are created by the container user are marked by the user ID.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=ln>1</span><span class=cl>jaehyeon@cevo:~/personal/emr-local-dev$ ls -al <span class=p>|</span> grep <span class=s1>&#39;^-&#39;</span>
</span></span><span class=line><span class=ln>2</span><span class=cl>-rw-rw-r--  <span class=m>1</span>      <span class=m>999</span> jaehyeon <span class=m>1086</span> Apr <span class=m>13</span> 08:23 .env
</span></span><span class=line><span class=ln>3</span><span class=cl>-rw-rw-r--  <span class=m>1</span> jaehyeon jaehyeon <span class=m>1855</span> Apr <span class=m>13</span> 05:45 .gitignore
</span></span><span class=line><span class=ln>4</span><span class=cl>-rw-rw-r--  <span class=m>1</span> jaehyeon jaehyeon   <span class=m>66</span> Mar <span class=m>31</span> 09:39 README.md
</span></span><span class=line><span class=ln>5</span><span class=cl>-rw-rw-r--  <span class=m>1</span>      <span class=m>999</span> jaehyeon  <span class=m>874</span> Apr  <span class=m>5</span> 21:14 test_utils.py
</span></span><span class=line><span class=ln>6</span><span class=cl>-rw-rw-r--  <span class=m>1</span>      <span class=m>999</span> jaehyeon <span class=m>3882</span> Apr <span class=m>13</span> 08:24 tripdata.ipynb
</span></span><span class=line><span class=ln>7</span><span class=cl>-rw-rw-r--  <span class=m>1</span>      <span class=m>999</span> jaehyeon <span class=m>1101</span> Apr <span class=m>24</span> 11:22 tripdata.py
</span></span><span class=line><span class=ln>8</span><span class=cl>-rw-rw-r--  <span class=m>1</span>      <span class=m>999</span> jaehyeon <span class=m>1653</span> Apr <span class=m>24</span> 23:09 tripdata_notify.py
</span></span><span class=line><span class=ln>9</span><span class=cl>-rw-rw-r--  <span class=m>1</span>      <span class=m>999</span> jaehyeon  <span class=m>664</span> Apr <span class=m>13</span> 05:45 utils.py
</span></span></code></pre></div><p>We can add the read or write permission of a single file or a folder easily as shown below - <code>g+rw</code>. Note the last example is for the AWS configuration folder and only the read access is given to the group. Note also that file permission change is not affected if the repository is cloned into a new place, and thus it only affects the local development environment.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=ln>1</span><span class=cl><span class=c1># add write access of a file to the group</span>
</span></span><span class=line><span class=ln>2</span><span class=cl>sudo chmod g+rw /home/hadoop/repo/&lt;file-name&gt;
</span></span><span class=line><span class=ln>3</span><span class=cl><span class=c1># add write access of a folder to the group</span>
</span></span><span class=line><span class=ln>4</span><span class=cl>sudo chmod -R g+rw /home/hadoop/repo/&lt;folder-name&gt;
</span></span><span class=line><span class=ln>5</span><span class=cl><span class=c1># add read access of the .aws folder to the group</span>
</span></span><span class=line><span class=ln>6</span><span class=cl>sudo chmod -R g+r /home/hadoop/.aws
</span></span></code></pre></div><h2 id=examples data-numberify>Examples<a class="anchor ms-1" href=#examples></a></h2><p>In this section, I’ll demonstrate typical Spark development examples. They’ll cover Spark Submit, Pytest, PySpark shell, Jupyter Notebook and Spark Structured Streaming. For the Spark Submit and Jupyter Notebook examples, Glue Catalog integration will be illustrated as well. And both the cases of utilising Visual Studio Code Remote - Containers extension and running as an isolated container will be covered in some key examples.</p><h3 id=spark-submit data-numberify>Spark Submit<a class="anchor ms-1" href=#spark-submit></a></h3><p>It is a simple Spark application that reads a sample NY taxi trip dataset from a public S3 bucket. Once loaded, it converts the pick-up and drop-off datetime columns from string to timestamp followed by writing the transformed data to a destination S3 bucket. The destination bucket name (<em>bucket_name</em>) can be specified by a system argument or its default value is taken. It finishes by creating a Glue table and, similar to the destination bucket name, the table name (<em>tblname</em>) can be specified as well.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=ln> 1</span><span class=cl><span class=c1># tripdata.py</span>
</span></span><span class=line><span class=ln> 2</span><span class=cl><span class=kn>import</span> <span class=nn>sys</span>
</span></span><span class=line><span class=ln> 3</span><span class=cl><span class=kn>from</span> <span class=nn>pyspark.sql</span> <span class=kn>import</span> <span class=n>SparkSession</span>
</span></span><span class=line><span class=ln> 4</span><span class=cl>
</span></span><span class=line><span class=ln> 5</span><span class=cl><span class=kn>from</span> <span class=nn>utils</span> <span class=kn>import</span> <span class=n>to_timestamp_df</span>
</span></span><span class=line><span class=ln> 6</span><span class=cl>
</span></span><span class=line><span class=ln> 7</span><span class=cl><span class=k>if</span> <span class=vm>__name__</span> <span class=o>==</span> <span class=s2>&#34;__main__&#34;</span><span class=p>:</span>
</span></span><span class=line><span class=ln> 8</span><span class=cl>    <span class=n>spark</span> <span class=o>=</span> <span class=n>SparkSession</span><span class=o>.</span><span class=n>builder</span><span class=o>.</span><span class=n>appName</span><span class=p>(</span><span class=s2>&#34;Trip Data&#34;</span><span class=p>)</span><span class=o>.</span><span class=n>getOrCreate</span><span class=p>()</span>
</span></span><span class=line><span class=ln> 9</span><span class=cl>
</span></span><span class=line><span class=ln>10</span><span class=cl>    <span class=n>dbname</span> <span class=o>=</span> <span class=s2>&#34;tripdata&#34;</span>
</span></span><span class=line><span class=ln>11</span><span class=cl>    <span class=n>tblname</span> <span class=o>=</span> <span class=s2>&#34;ny_taxi&#34;</span> <span class=k>if</span> <span class=nb>len</span><span class=p>(</span><span class=n>sys</span><span class=o>.</span><span class=n>argv</span><span class=p>)</span> <span class=o>&lt;=</span> <span class=mi>1</span> <span class=k>else</span> <span class=n>sys</span><span class=o>.</span><span class=n>argv</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=ln>12</span><span class=cl>    <span class=n>bucket_name</span> <span class=o>=</span> <span class=s2>&#34;emr-local-dev&#34;</span> <span class=k>if</span> <span class=nb>len</span><span class=p>(</span><span class=n>sys</span><span class=o>.</span><span class=n>argv</span><span class=p>)</span> <span class=o>&lt;=</span> <span class=mi>2</span> <span class=k>else</span> <span class=n>sys</span><span class=o>.</span><span class=n>argv</span><span class=p>[</span><span class=mi>2</span><span class=p>]</span>
</span></span><span class=line><span class=ln>13</span><span class=cl>    <span class=n>dest_path</span> <span class=o>=</span> <span class=sa>f</span><span class=s2>&#34;s3://</span><span class=si>{</span><span class=n>bucket_name</span><span class=si>}</span><span class=s2>/</span><span class=si>{</span><span class=n>tblname</span><span class=si>}</span><span class=s2>/&#34;</span>
</span></span><span class=line><span class=ln>14</span><span class=cl>    <span class=n>src_path</span> <span class=o>=</span> <span class=s2>&#34;s3://aws-data-analytics-workshops/shared_datasets/tripdata/&#34;</span>
</span></span><span class=line><span class=ln>15</span><span class=cl>    <span class=c1># read csv</span>
</span></span><span class=line><span class=ln>16</span><span class=cl>    <span class=n>ny_taxi</span> <span class=o>=</span> <span class=n>spark</span><span class=o>.</span><span class=n>read</span><span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>&#34;inferSchema&#34;</span><span class=p>,</span> <span class=s2>&#34;true&#34;</span><span class=p>)</span><span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>&#34;header&#34;</span><span class=p>,</span> <span class=s2>&#34;true&#34;</span><span class=p>)</span><span class=o>.</span><span class=n>csv</span><span class=p>(</span><span class=n>src_path</span><span class=p>)</span>
</span></span><span class=line><span class=ln>17</span><span class=cl>    <span class=n>ny_taxi</span> <span class=o>=</span> <span class=n>to_timestamp_df</span><span class=p>(</span><span class=n>ny_taxi</span><span class=p>,</span> <span class=p>[</span><span class=s2>&#34;lpep_pickup_datetime&#34;</span><span class=p>,</span> <span class=s2>&#34;lpep_dropoff_datetime&#34;</span><span class=p>])</span>
</span></span><span class=line><span class=ln>18</span><span class=cl>    <span class=n>ny_taxi</span><span class=o>.</span><span class=n>printSchema</span><span class=p>()</span>
</span></span><span class=line><span class=ln>19</span><span class=cl>    <span class=c1># write parquet</span>
</span></span><span class=line><span class=ln>20</span><span class=cl>    <span class=n>ny_taxi</span><span class=o>.</span><span class=n>write</span><span class=o>.</span><span class=n>mode</span><span class=p>(</span><span class=s2>&#34;overwrite&#34;</span><span class=p>)</span><span class=o>.</span><span class=n>parquet</span><span class=p>(</span><span class=n>dest_path</span><span class=p>)</span>
</span></span><span class=line><span class=ln>21</span><span class=cl>    <span class=c1># create glue table</span>
</span></span><span class=line><span class=ln>22</span><span class=cl>    <span class=n>ny_taxi</span><span class=o>.</span><span class=n>registerTempTable</span><span class=p>(</span><span class=n>tblname</span><span class=p>)</span>
</span></span><span class=line><span class=ln>23</span><span class=cl>    <span class=n>spark</span><span class=o>.</span><span class=n>sql</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;CREATE DATABASE IF NOT EXISTS </span><span class=si>{</span><span class=n>dbname</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=ln>24</span><span class=cl>    <span class=n>spark</span><span class=o>.</span><span class=n>sql</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;USE </span><span class=si>{</span><span class=n>dbname</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=ln>25</span><span class=cl>    <span class=n>spark</span><span class=o>.</span><span class=n>sql</span><span class=p>(</span>
</span></span><span class=line><span class=ln>26</span><span class=cl>        <span class=sa>f</span><span class=s2>&#34;&#34;&#34;CREATE TABLE IF NOT EXISTS </span><span class=si>{</span><span class=n>tblname</span><span class=si>}</span><span class=s2>
</span></span></span><span class=line><span class=ln>27</span><span class=cl><span class=s2>            USING PARQUET
</span></span></span><span class=line><span class=ln>28</span><span class=cl><span class=s2>            LOCATION &#39;</span><span class=si>{</span><span class=n>dest_path</span><span class=si>}</span><span class=s2>&#39;
</span></span></span><span class=line><span class=ln>29</span><span class=cl><span class=s2>            AS SELECT * FROM </span><span class=si>{</span><span class=n>tblname</span><span class=si>}</span><span class=s2>
</span></span></span><span class=line><span class=ln>30</span><span class=cl><span class=s2>        &#34;&#34;&#34;</span>
</span></span><span class=line><span class=ln>31</span><span class=cl>    <span class=p>)</span>
</span></span></code></pre></div><p>The Spark application can be submitted as shown below.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=ln> 1</span><span class=cl><span class=nb>export</span> <span class=nv>AWS_ACCESS_KEY_ID</span><span class=o>=</span>&lt;AWS-ACCESS-KEY-ID&gt;
</span></span><span class=line><span class=ln> 2</span><span class=cl><span class=nb>export</span> <span class=nv>AWS_SECRET_ACCESS_KEY</span><span class=o>=</span>&lt;AWS-SECRET-ACCESS-KEY&gt;
</span></span><span class=line><span class=ln> 3</span><span class=cl><span class=nb>export</span> <span class=nv>AWS_REGION</span><span class=o>=</span>&lt;AWS-REGION&gt;
</span></span><span class=line><span class=ln> 4</span><span class=cl><span class=c1># optional</span>
</span></span><span class=line><span class=ln> 5</span><span class=cl><span class=nb>export</span> <span class=nv>AWS_SESSION_TOKEN</span><span class=o>=</span>&lt;AWS-SESSION-TOKEN&gt;
</span></span><span class=line><span class=ln> 6</span><span class=cl>
</span></span><span class=line><span class=ln> 7</span><span class=cl><span class=nv>$SPARK_HOME</span>/bin/spark-submit <span class=se>\
</span></span></span><span class=line><span class=ln> 8</span><span class=cl><span class=se></span>  --deploy-mode client <span class=se>\
</span></span></span><span class=line><span class=ln> 9</span><span class=cl><span class=se></span>  --master local<span class=o>[</span>*<span class=o>]</span> <span class=se>\
</span></span></span><span class=line><span class=ln>10</span><span class=cl><span class=se></span>  tripdata.py
</span></span></code></pre></div><p>Once it completes, the Glue table will be created, and we can query it using Athena as shown below.</p><p><picture><img class="img-fluid mx-auto d-block" alt src=/blog/2022-05-08-emr-local-dev/04-glue-table.png loading=lazy width=1404 height=588></picture></p><p>If we want to submit the application as an isolated container, we can use the custom image directly. Below shows the equivalent Docker run command.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=ln>1</span><span class=cl>docker run --rm <span class=se>\
</span></span></span><span class=line><span class=ln>2</span><span class=cl><span class=se></span>  -e <span class=nv>AWS_ACCESS_KEY_ID</span><span class=o>=</span><span class=nv>$AWS_ACCESS_KEY_ID</span> <span class=se>\
</span></span></span><span class=line><span class=ln>3</span><span class=cl><span class=se></span>  -e <span class=nv>AWS_SECRET_ACCESS_KEY</span><span class=o>=</span><span class=nv>$AWS_SECRET_ACCESS_KEY</span> <span class=se>\
</span></span></span><span class=line><span class=ln>4</span><span class=cl><span class=se></span>  -e <span class=nv>AWS_SESSION_TOKEN</span><span class=o>=</span><span class=nv>$AWS_SESSION_TOKEN</span> <span class=se>\ </span><span class=c1># optional</span>
</span></span><span class=line><span class=ln>5</span><span class=cl>  -e <span class=nv>AWS_REGION</span><span class=o>=</span><span class=nv>$AWS_REGION</span> <span class=se>\
</span></span></span><span class=line><span class=ln>6</span><span class=cl><span class=se></span>  -v <span class=nv>$PWD</span>:/usr/hadoop <span class=se>\
</span></span></span><span class=line><span class=ln>7</span><span class=cl><span class=se></span>  emr-6.5.0:20211119 <span class=se>\
</span></span></span><span class=line><span class=ln>8</span><span class=cl><span class=se></span>  /usr/lib/spark/bin/spark-submit --deploy-mode client --master local<span class=o>[</span>*<span class=o>]</span> /usr/hadoop/tripdata.py taxi emr-local-dev
</span></span></code></pre></div><h3 id=pytest data-numberify>Pytest<a class="anchor ms-1" href=#pytest></a></h3><p>The Spark application in the earlier example uses a custom function that converts the data type of one or more columns from string to timestamp - <code>to_timestamp_df()</code>. The source of the function and the testing script of it can be found below.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=ln> 1</span><span class=cl><span class=c1># utils.py</span>
</span></span><span class=line><span class=ln> 2</span><span class=cl><span class=kn>from</span> <span class=nn>typing</span> <span class=kn>import</span> <span class=n>List</span><span class=p>,</span> <span class=n>Union</span>
</span></span><span class=line><span class=ln> 3</span><span class=cl><span class=kn>from</span> <span class=nn>pyspark.sql</span> <span class=kn>import</span> <span class=n>DataFrame</span>
</span></span><span class=line><span class=ln> 4</span><span class=cl><span class=kn>from</span> <span class=nn>pyspark.sql.functions</span> <span class=kn>import</span> <span class=n>col</span><span class=p>,</span> <span class=n>to_timestamp</span>
</span></span><span class=line><span class=ln> 5</span><span class=cl>
</span></span><span class=line><span class=ln> 6</span><span class=cl><span class=k>def</span> <span class=nf>to_timestamp_df</span><span class=p>(</span>
</span></span><span class=line><span class=ln> 7</span><span class=cl>    <span class=n>df</span><span class=p>:</span> <span class=n>DataFrame</span><span class=p>,</span> <span class=n>fields</span><span class=p>:</span> <span class=n>Union</span><span class=p>[</span><span class=n>List</span><span class=p>[</span><span class=nb>str</span><span class=p>],</span> <span class=nb>str</span><span class=p>],</span> <span class=nb>format</span><span class=p>:</span> <span class=nb>str</span> <span class=o>=</span> <span class=s2>&#34;M/d/yy H:mm&#34;</span>
</span></span><span class=line><span class=ln> 8</span><span class=cl><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>DataFrame</span><span class=p>:</span>
</span></span><span class=line><span class=ln> 9</span><span class=cl>    <span class=n>fields</span> <span class=o>=</span> <span class=p>[</span><span class=n>fields</span><span class=p>]</span> <span class=k>if</span> <span class=nb>isinstance</span><span class=p>(</span><span class=n>fields</span><span class=p>,</span> <span class=nb>str</span><span class=p>)</span> <span class=k>else</span> <span class=n>fields</span>
</span></span><span class=line><span class=ln>10</span><span class=cl>    <span class=k>for</span> <span class=n>field</span> <span class=ow>in</span> <span class=n>fields</span><span class=p>:</span>
</span></span><span class=line><span class=ln>11</span><span class=cl>        <span class=n>df</span> <span class=o>=</span> <span class=n>df</span><span class=o>.</span><span class=n>withColumn</span><span class=p>(</span><span class=n>field</span><span class=p>,</span> <span class=n>to_timestamp</span><span class=p>(</span><span class=n>col</span><span class=p>(</span><span class=n>field</span><span class=p>),</span> <span class=nb>format</span><span class=p>))</span>
</span></span><span class=line><span class=ln>12</span><span class=cl>    <span class=k>return</span> <span class=n>df</span>
</span></span><span class=line><span class=ln>13</span><span class=cl><span class=c1># test_utils.py</span>
</span></span><span class=line><span class=ln>14</span><span class=cl><span class=kn>import</span> <span class=nn>pytest</span>
</span></span><span class=line><span class=ln>15</span><span class=cl><span class=kn>import</span> <span class=nn>datetime</span>
</span></span><span class=line><span class=ln>16</span><span class=cl><span class=kn>from</span> <span class=nn>pyspark.sql</span> <span class=kn>import</span> <span class=n>SparkSession</span>
</span></span><span class=line><span class=ln>17</span><span class=cl><span class=kn>from</span> <span class=nn>py4j.protocol</span> <span class=kn>import</span> <span class=n>Py4JError</span>
</span></span><span class=line><span class=ln>18</span><span class=cl>
</span></span><span class=line><span class=ln>19</span><span class=cl><span class=kn>from</span> <span class=nn>utils</span> <span class=kn>import</span> <span class=n>to_timestamp_df</span>
</span></span><span class=line><span class=ln>20</span><span class=cl>
</span></span><span class=line><span class=ln>21</span><span class=cl><span class=nd>@pytest.fixture</span><span class=p>(</span><span class=n>scope</span><span class=o>=</span><span class=s2>&#34;session&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=ln>22</span><span class=cl><span class=k>def</span> <span class=nf>spark</span><span class=p>():</span>
</span></span><span class=line><span class=ln>23</span><span class=cl>    <span class=k>return</span> <span class=p>(</span>
</span></span><span class=line><span class=ln>24</span><span class=cl>        <span class=n>SparkSession</span><span class=o>.</span><span class=n>builder</span><span class=o>.</span><span class=n>master</span><span class=p>(</span><span class=s2>&#34;local&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=ln>25</span><span class=cl>        <span class=o>.</span><span class=n>appName</span><span class=p>(</span><span class=s2>&#34;test&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=ln>26</span><span class=cl>        <span class=o>.</span><span class=n>config</span><span class=p>(</span><span class=s2>&#34;spark.submit.deployMode&#34;</span><span class=p>,</span> <span class=s2>&#34;client&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=ln>27</span><span class=cl>        <span class=o>.</span><span class=n>getOrCreate</span><span class=p>()</span>
</span></span><span class=line><span class=ln>28</span><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=ln>29</span><span class=cl>
</span></span><span class=line><span class=ln>30</span><span class=cl>
</span></span><span class=line><span class=ln>31</span><span class=cl><span class=k>def</span> <span class=nf>test_to_timestamp_success</span><span class=p>(</span><span class=n>spark</span><span class=p>):</span>
</span></span><span class=line><span class=ln>32</span><span class=cl>    <span class=n>raw_df</span> <span class=o>=</span> <span class=n>spark</span><span class=o>.</span><span class=n>createDataFrame</span><span class=p>(</span>
</span></span><span class=line><span class=ln>33</span><span class=cl>        <span class=p>[(</span><span class=s2>&#34;1/1/17 0:01&#34;</span><span class=p>,)],</span>
</span></span><span class=line><span class=ln>34</span><span class=cl>        <span class=p>[</span><span class=s2>&#34;date&#34;</span><span class=p>],</span>
</span></span><span class=line><span class=ln>35</span><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=ln>36</span><span class=cl>
</span></span><span class=line><span class=ln>37</span><span class=cl>    <span class=n>test_df</span> <span class=o>=</span> <span class=n>to_timestamp_df</span><span class=p>(</span><span class=n>raw_df</span><span class=p>,</span> <span class=s2>&#34;date&#34;</span><span class=p>,</span> <span class=s2>&#34;M/d/yy H:mm&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=ln>38</span><span class=cl>    <span class=k>for</span> <span class=n>row</span> <span class=ow>in</span> <span class=n>test_df</span><span class=o>.</span><span class=n>collect</span><span class=p>():</span>
</span></span><span class=line><span class=ln>39</span><span class=cl>        <span class=k>assert</span> <span class=n>row</span><span class=p>[</span><span class=s2>&#34;date&#34;</span><span class=p>]</span> <span class=o>==</span> <span class=n>datetime</span><span class=o>.</span><span class=n>datetime</span><span class=p>(</span><span class=mi>2017</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=ln>40</span><span class=cl>
</span></span><span class=line><span class=ln>41</span><span class=cl>
</span></span><span class=line><span class=ln>42</span><span class=cl><span class=k>def</span> <span class=nf>test_to_timestamp_bad_format</span><span class=p>(</span><span class=n>spark</span><span class=p>):</span>
</span></span><span class=line><span class=ln>43</span><span class=cl>    <span class=n>raw_df</span> <span class=o>=</span> <span class=n>spark</span><span class=o>.</span><span class=n>createDataFrame</span><span class=p>(</span>
</span></span><span class=line><span class=ln>44</span><span class=cl>        <span class=p>[(</span><span class=s2>&#34;1/1/17 0:01&#34;</span><span class=p>,)],</span>
</span></span><span class=line><span class=ln>45</span><span class=cl>        <span class=p>[</span><span class=s2>&#34;date&#34;</span><span class=p>],</span>
</span></span><span class=line><span class=ln>46</span><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=ln>47</span><span class=cl>
</span></span><span class=line><span class=ln>48</span><span class=cl>    <span class=k>with</span> <span class=n>pytest</span><span class=o>.</span><span class=n>raises</span><span class=p>(</span><span class=n>Py4JError</span><span class=p>):</span>
</span></span><span class=line><span class=ln>49</span><span class=cl>        <span class=n>to_timestamp_df</span><span class=p>(</span><span class=n>raw_df</span><span class=p>,</span> <span class=s2>&#34;date&#34;</span><span class=p>,</span> <span class=s2>&#34;M/d/yy HH:mm&#34;</span><span class=p>)</span><span class=o>.</span><span class=n>collect</span><span class=p>()</span>
</span></span></code></pre></div><p>As the test cases don’t access AWS services, they can be executed simply by the Pytest command (e.g. <code>pytest -v</code>).</p><p><picture><img class="img-fluid mx-auto d-block" alt src=/blog/2022-05-08-emr-local-dev/05-pytest.png loading=lazy width=1261 height=206></picture></p><p>Testing can also be made in an isolated container as shown below. Note that we need to add the <em>PYTHONPATH</em> environment variable because we use the bundled Pyspark package.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=ln>1</span><span class=cl>docker run --rm <span class=se>\
</span></span></span><span class=line><span class=ln>2</span><span class=cl><span class=se></span>  -e <span class=nv>PYTHONPATH</span><span class=o>=</span><span class=s2>&#34;/usr/lib/spark/python/lib/py4j-0.10.9-src.zip:/usr/lib/spark/python/&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=ln>3</span><span class=cl><span class=se></span>  -v <span class=nv>$PWD</span>:/usr/hadoop <span class=se>\
</span></span></span><span class=line><span class=ln>4</span><span class=cl><span class=se></span>  emr-6.5.0:20211119 <span class=se>\
</span></span></span><span class=line><span class=ln>5</span><span class=cl><span class=se></span>  pytest /usr/hadoop -v
</span></span></code></pre></div><h3 id=pyspark-shell data-numberify>PySpark Shell<a class="anchor ms-1" href=#pyspark-shell></a></h3><p>The PySpark shell can be launched as shown below.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=ln>1</span><span class=cl><span class=nv>$SPARK_HOME</span>/bin/pyspark <span class=se>\
</span></span></span><span class=line><span class=ln>2</span><span class=cl><span class=se></span>  --deploy-mode client <span class=se>\
</span></span></span><span class=line><span class=ln>3</span><span class=cl><span class=se></span>  --master local<span class=o>[</span>*<span class=o>]</span>
</span></span></code></pre></div><p><picture><img class="img-fluid mx-auto d-block" alt src=/blog/2022-05-08-emr-local-dev/08-pyspark.png loading=lazy width=1126 height=402></picture></p><p>Also, below shows an example of launching it as an isolated container.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=ln>1</span><span class=cl>docker run --rm -it <span class=se>\
</span></span></span><span class=line><span class=ln>2</span><span class=cl><span class=se></span>  -e <span class=nv>AWS_ACCESS_KEY_ID</span><span class=o>=</span><span class=nv>$AWS_ACCESS_KEY_ID</span> <span class=se>\
</span></span></span><span class=line><span class=ln>3</span><span class=cl><span class=se></span>  -e <span class=nv>AWS_SECRET_ACCESS_KEY</span><span class=o>=</span><span class=nv>$AWS_SECRET_ACCESS_KEY</span> <span class=se>\
</span></span></span><span class=line><span class=ln>4</span><span class=cl><span class=se></span>  -e <span class=nv>AWS_SESSION_TOKEN</span><span class=o>=</span><span class=nv>$AWS_SESSION_TOKEN</span> <span class=se>\ </span><span class=c1># optional</span>
</span></span><span class=line><span class=ln>5</span><span class=cl>  -e <span class=nv>AWS_REGION</span><span class=o>=</span><span class=nv>$AWS_REGION</span> <span class=se>\
</span></span></span><span class=line><span class=ln>6</span><span class=cl><span class=se></span>  -v <span class=nv>$PWD</span>:/usr/hadoop <span class=se>\
</span></span></span><span class=line><span class=ln>7</span><span class=cl><span class=se></span>  emr-6.5.0:20211119 <span class=se>\
</span></span></span><span class=line><span class=ln>8</span><span class=cl><span class=se></span>  /usr/lib/spark/bin/pyspark --deploy-mode client --master local<span class=o>[</span>*<span class=o>]</span>
</span></span></code></pre></div><h3 id=jupyter-notebook data-numberify>Jupyter Notebook<a class="anchor ms-1" href=#jupyter-notebook></a></h3><p>Jupyter Notebook is a popular Spark application authoring tool, and we can create a notebook simply by <a href=https://code.visualstudio.com/docs/datascience/jupyter-notebooks#_create-or-open-a-jupyter-notebook target=_blank rel="noopener noreferrer">creating a file with the ipynb extension<i class="fas fa-external-link-square-alt ms-1"></i></a> in VSCode. Note we need the <em>ipykernel</em> package in order to run code cells, and it is already installed in the custom Docker image. For accessing AWS resources, we need the environment variables of AWS credentials mentioned earlier. We can use the <em>python-dotenv</em> package. Specifically we can create an <em>.env</em> file and add AWS credentials to it. Then we can add a <a href=https://github.com/theskumar/python-dotenv#load-env-files-in-ipython target=_blank rel="noopener noreferrer">code cell that loads the .env file<i class="fas fa-external-link-square-alt ms-1"></i></a> at the beginning of the notebook.</p><p>In the next code cell, the app reads the Glue table and adds a column of trip duration followed by showing the summary statistics of key columns. We see some puzzling records that show zero trip duration or negative total amount. Among those, we find negative total amount records should be reported immediately and a Spark Structured Streaming application turns out to be a good option.</p><p><picture><img class="img-fluid mx-auto d-block" alt src=/blog/2022-05-08-emr-local-dev/07-01-jupyter.png loading=lazy width=1129 height=883></picture></p><p><picture><img class="img-fluid mx-auto d-block" alt src=/blog/2022-05-08-emr-local-dev/07-02-jupyter.png loading=lazy width=1128 height=130></picture></p><h3 id=spark-streaming data-numberify>Spark Streaming<a class="anchor ms-1" href=#spark-streaming></a></h3><p>We need sample data that can be read by the Spark application. In order to generate it, the individual records are taken from the source CSV file and saved locally after being converted into json. Below script creates those json files in the <em>data/json</em> folder. Inside the development container, it can be executed as <code>python3 data/generate.py</code>.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=ln> 1</span><span class=cl><span class=c1># data/generate.py</span>
</span></span><span class=line><span class=ln> 2</span><span class=cl><span class=kn>import</span> <span class=nn>shutil</span>
</span></span><span class=line><span class=ln> 3</span><span class=cl><span class=kn>import</span> <span class=nn>io</span>
</span></span><span class=line><span class=ln> 4</span><span class=cl><span class=kn>import</span> <span class=nn>json</span>
</span></span><span class=line><span class=ln> 5</span><span class=cl><span class=kn>import</span> <span class=nn>csv</span>
</span></span><span class=line><span class=ln> 6</span><span class=cl><span class=kn>from</span> <span class=nn>pathlib</span> <span class=kn>import</span> <span class=n>Path</span>
</span></span><span class=line><span class=ln> 7</span><span class=cl><span class=kn>import</span> <span class=nn>boto3</span>
</span></span><span class=line><span class=ln> 8</span><span class=cl>
</span></span><span class=line><span class=ln> 9</span><span class=cl><span class=n>BUCKET_NAME</span> <span class=o>=</span> <span class=s2>&#34;aws-data-analytics-workshops&#34;</span>
</span></span><span class=line><span class=ln>10</span><span class=cl><span class=n>KEY_NAME</span> <span class=o>=</span> <span class=s2>&#34;shared_datasets/tripdata/tripdata.csv&#34;</span>
</span></span><span class=line><span class=ln>11</span><span class=cl><span class=n>DATA_PATH</span> <span class=o>=</span> <span class=n>Path</span><span class=o>.</span><span class=n>joinpath</span><span class=p>(</span><span class=n>Path</span><span class=p>(</span><span class=vm>__file__</span><span class=p>)</span><span class=o>.</span><span class=n>parent</span><span class=p>,</span> <span class=s2>&#34;json&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=ln>12</span><span class=cl>
</span></span><span class=line><span class=ln>13</span><span class=cl>
</span></span><span class=line><span class=ln>14</span><span class=cl><span class=k>def</span> <span class=nf>recreate_data_path_if</span><span class=p>(</span><span class=n>data_path</span><span class=p>:</span> <span class=n>Path</span><span class=p>,</span> <span class=n>recreate</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>True</span><span class=p>):</span>
</span></span><span class=line><span class=ln>15</span><span class=cl>    <span class=k>if</span> <span class=n>recreate</span><span class=p>:</span>
</span></span><span class=line><span class=ln>16</span><span class=cl>        <span class=n>shutil</span><span class=o>.</span><span class=n>rmtree</span><span class=p>(</span><span class=n>data_path</span><span class=p>,</span> <span class=n>ignore_errors</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=ln>17</span><span class=cl>        <span class=n>data_path</span><span class=o>.</span><span class=n>mkdir</span><span class=p>()</span>
</span></span><span class=line><span class=ln>18</span><span class=cl>
</span></span><span class=line><span class=ln>19</span><span class=cl>
</span></span><span class=line><span class=ln>20</span><span class=cl><span class=k>def</span> <span class=nf>write_to_json</span><span class=p>(</span><span class=n>bucket_name</span><span class=p>:</span> <span class=nb>str</span><span class=p>,</span> <span class=n>key_name</span><span class=p>:</span> <span class=nb>str</span><span class=p>,</span> <span class=n>data_path</span><span class=p>:</span> <span class=n>Path</span><span class=p>,</span> <span class=n>recreate</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>True</span><span class=p>):</span>
</span></span><span class=line><span class=ln>21</span><span class=cl>    <span class=n>s3</span> <span class=o>=</span> <span class=n>boto3</span><span class=o>.</span><span class=n>resource</span><span class=p>(</span><span class=s2>&#34;s3&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=ln>22</span><span class=cl>    <span class=n>data</span> <span class=o>=</span> <span class=n>io</span><span class=o>.</span><span class=n>BytesIO</span><span class=p>()</span>
</span></span><span class=line><span class=ln>23</span><span class=cl>    <span class=n>bucket</span> <span class=o>=</span> <span class=n>s3</span><span class=o>.</span><span class=n>Bucket</span><span class=p>(</span><span class=n>bucket_name</span><span class=p>)</span>
</span></span><span class=line><span class=ln>24</span><span class=cl>    <span class=n>bucket</span><span class=o>.</span><span class=n>download_fileobj</span><span class=p>(</span><span class=n>key_name</span><span class=p>,</span> <span class=n>data</span><span class=p>)</span>
</span></span><span class=line><span class=ln>25</span><span class=cl>    <span class=n>contents</span> <span class=o>=</span> <span class=n>data</span><span class=o>.</span><span class=n>getvalue</span><span class=p>()</span><span class=o>.</span><span class=n>decode</span><span class=p>(</span><span class=s2>&#34;utf-8&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=ln>26</span><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;download complete&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=ln>27</span><span class=cl>    <span class=n>reader</span> <span class=o>=</span> <span class=n>csv</span><span class=o>.</span><span class=n>DictReader</span><span class=p>(</span><span class=n>contents</span><span class=o>.</span><span class=n>split</span><span class=p>(</span><span class=s2>&#34;</span><span class=se>\n</span><span class=s2>&#34;</span><span class=p>))</span>
</span></span><span class=line><span class=ln>28</span><span class=cl>    <span class=n>recreate_data_path_if</span><span class=p>(</span><span class=n>data_path</span><span class=p>,</span> <span class=n>recreate</span><span class=p>)</span>
</span></span><span class=line><span class=ln>29</span><span class=cl>    <span class=k>for</span> <span class=n>c</span><span class=p>,</span> <span class=n>row</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>reader</span><span class=p>):</span>
</span></span><span class=line><span class=ln>30</span><span class=cl>        <span class=n>record_id</span> <span class=o>=</span> <span class=nb>str</span><span class=p>(</span><span class=n>c</span><span class=p>)</span><span class=o>.</span><span class=n>zfill</span><span class=p>(</span><span class=mi>5</span><span class=p>)</span>
</span></span><span class=line><span class=ln>31</span><span class=cl>        <span class=n>data_path</span><span class=o>.</span><span class=n>joinpath</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;</span><span class=si>{</span><span class=n>record_id</span><span class=si>}</span><span class=s2>.json&#34;</span><span class=p>)</span><span class=o>.</span><span class=n>write_text</span><span class=p>(</span>
</span></span><span class=line><span class=ln>32</span><span class=cl>            <span class=n>json</span><span class=o>.</span><span class=n>dumps</span><span class=p>({</span><span class=o>**</span><span class=p>{</span><span class=s2>&#34;record_id&#34;</span><span class=p>:</span> <span class=n>record_id</span><span class=p>},</span> <span class=o>**</span><span class=n>row</span><span class=p>})</span>
</span></span><span class=line><span class=ln>33</span><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=ln>34</span><span class=cl>
</span></span><span class=line><span class=ln>35</span><span class=cl>
</span></span><span class=line><span class=ln>36</span><span class=cl><span class=k>if</span> <span class=vm>__name__</span> <span class=o>==</span> <span class=s2>&#34;__main__&#34;</span><span class=p>:</span>
</span></span><span class=line><span class=ln>37</span><span class=cl>    <span class=n>write_to_json</span><span class=p>(</span><span class=n>BUCKET_NAME</span><span class=p>,</span> <span class=n>KEY_NAME</span><span class=p>,</span> <span class=n>DATA_PATH</span><span class=p>,</span> <span class=kc>True</span><span class=p>)</span>
</span></span></code></pre></div><p>In the Spark streaming application, the steam reader loads JSON files in the <em>data/json</em> folder and the data schema is provided by DDL statements. Then it generates the target dataframe that filters records whose total amount is negative. Note the target dataframe is structured to have the key and value columns, which is required by Kafka. Finally, it writes the records of the target dataframe to the _notifications _topics of the Kafka cluster.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=ln> 1</span><span class=cl><span class=c1># tripdata_notify.py</span>
</span></span><span class=line><span class=ln> 2</span><span class=cl><span class=kn>from</span> <span class=nn>pyspark.sql</span> <span class=kn>import</span> <span class=n>SparkSession</span>
</span></span><span class=line><span class=ln> 3</span><span class=cl><span class=kn>from</span> <span class=nn>pyspark.sql.functions</span> <span class=kn>import</span> <span class=n>col</span><span class=p>,</span> <span class=n>to_json</span><span class=p>,</span> <span class=n>struct</span>
</span></span><span class=line><span class=ln> 4</span><span class=cl><span class=kn>from</span> <span class=nn>utils</span> <span class=kn>import</span> <span class=n>remove_checkpoint</span>
</span></span><span class=line><span class=ln> 5</span><span class=cl>
</span></span><span class=line><span class=ln> 6</span><span class=cl><span class=k>if</span> <span class=vm>__name__</span> <span class=o>==</span> <span class=s2>&#34;__main__&#34;</span><span class=p>:</span>
</span></span><span class=line><span class=ln> 7</span><span class=cl>    <span class=n>remove_checkpoint</span><span class=p>()</span>
</span></span><span class=line><span class=ln> 8</span><span class=cl>
</span></span><span class=line><span class=ln> 9</span><span class=cl>    <span class=n>spark</span> <span class=o>=</span> <span class=p>(</span>
</span></span><span class=line><span class=ln>10</span><span class=cl>        <span class=n>SparkSession</span><span class=o>.</span><span class=n>builder</span><span class=o>.</span><span class=n>appName</span><span class=p>(</span><span class=s2>&#34;Trip Data Notification&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=ln>11</span><span class=cl>        <span class=o>.</span><span class=n>config</span><span class=p>(</span><span class=s2>&#34;spark.streaming.stopGracefullyOnShutdown&#34;</span><span class=p>,</span> <span class=s2>&#34;true&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=ln>12</span><span class=cl>        <span class=o>.</span><span class=n>config</span><span class=p>(</span><span class=s2>&#34;spark.sql.shuffle.partitions&#34;</span><span class=p>,</span> <span class=mi>3</span><span class=p>)</span>
</span></span><span class=line><span class=ln>13</span><span class=cl>        <span class=o>.</span><span class=n>getOrCreate</span><span class=p>()</span>
</span></span><span class=line><span class=ln>14</span><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=ln>15</span><span class=cl>
</span></span><span class=line><span class=ln>16</span><span class=cl>    <span class=n>tripdata_ddl</span> <span class=o>=</span> <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=ln>17</span><span class=cl><span class=s2>    record_id STRING,
</span></span></span><span class=line><span class=ln>18</span><span class=cl><span class=s2>    VendorID STRING,
</span></span></span><span class=line><span class=ln>19</span><span class=cl><span class=s2>    lpep_pickup_datetime STRING,
</span></span></span><span class=line><span class=ln>20</span><span class=cl><span class=s2>    lpep_dropoff_datetime STRING,
</span></span></span><span class=line><span class=ln>21</span><span class=cl><span class=s2>    store_and_fwd_flag STRING,
</span></span></span><span class=line><span class=ln>22</span><span class=cl><span class=s2>    RatecodeID STRING,
</span></span></span><span class=line><span class=ln>23</span><span class=cl><span class=s2>    PULocationID STRING,
</span></span></span><span class=line><span class=ln>24</span><span class=cl><span class=s2>    DOLocationID STRING,
</span></span></span><span class=line><span class=ln>25</span><span class=cl><span class=s2>    passenger_count STRING,
</span></span></span><span class=line><span class=ln>26</span><span class=cl><span class=s2>    trip_distance STRING,
</span></span></span><span class=line><span class=ln>27</span><span class=cl><span class=s2>    fare_amount STRING,
</span></span></span><span class=line><span class=ln>28</span><span class=cl><span class=s2>    extra STRING,
</span></span></span><span class=line><span class=ln>29</span><span class=cl><span class=s2>    mta_tax STRING,
</span></span></span><span class=line><span class=ln>30</span><span class=cl><span class=s2>    tip_amount STRING,
</span></span></span><span class=line><span class=ln>31</span><span class=cl><span class=s2>    tolls_amount STRING,
</span></span></span><span class=line><span class=ln>32</span><span class=cl><span class=s2>    ehail_fee STRING,
</span></span></span><span class=line><span class=ln>33</span><span class=cl><span class=s2>    improvement_surcharge STRING,
</span></span></span><span class=line><span class=ln>34</span><span class=cl><span class=s2>    total_amount STRING,
</span></span></span><span class=line><span class=ln>35</span><span class=cl><span class=s2>    payment_type STRING,
</span></span></span><span class=line><span class=ln>36</span><span class=cl><span class=s2>    trip_type STRING
</span></span></span><span class=line><span class=ln>37</span><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=ln>38</span><span class=cl>
</span></span><span class=line><span class=ln>39</span><span class=cl>    <span class=n>ny_taxi</span> <span class=o>=</span> <span class=p>(</span>
</span></span><span class=line><span class=ln>40</span><span class=cl>        <span class=n>spark</span><span class=o>.</span><span class=n>readStream</span><span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=s2>&#34;json&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=ln>41</span><span class=cl>        <span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>&#34;path&#34;</span><span class=p>,</span> <span class=s2>&#34;data/json&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=ln>42</span><span class=cl>        <span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>&#34;maxFilesPerTrigger&#34;</span><span class=p>,</span> <span class=s2>&#34;1000&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=ln>43</span><span class=cl>        <span class=o>.</span><span class=n>schema</span><span class=p>(</span><span class=n>tripdata_ddl</span><span class=p>)</span>
</span></span><span class=line><span class=ln>44</span><span class=cl>        <span class=o>.</span><span class=n>load</span><span class=p>()</span>
</span></span><span class=line><span class=ln>45</span><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=ln>46</span><span class=cl>
</span></span><span class=line><span class=ln>47</span><span class=cl>    <span class=n>target_df</span> <span class=o>=</span> <span class=n>ny_taxi</span><span class=o>.</span><span class=n>filter</span><span class=p>(</span><span class=n>col</span><span class=p>(</span><span class=s2>&#34;total_amount&#34;</span><span class=p>)</span> <span class=o>&lt;=</span> <span class=mi>0</span><span class=p>)</span><span class=o>.</span><span class=n>select</span><span class=p>(</span>
</span></span><span class=line><span class=ln>48</span><span class=cl>        <span class=n>col</span><span class=p>(</span><span class=s2>&#34;record_id&#34;</span><span class=p>)</span><span class=o>.</span><span class=n>alias</span><span class=p>(</span><span class=s2>&#34;key&#34;</span><span class=p>),</span> <span class=n>to_json</span><span class=p>(</span><span class=n>struct</span><span class=p>(</span><span class=s2>&#34;*&#34;</span><span class=p>))</span><span class=o>.</span><span class=n>alias</span><span class=p>(</span><span class=s2>&#34;value&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=ln>49</span><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=ln>50</span><span class=cl>
</span></span><span class=line><span class=ln>51</span><span class=cl>    <span class=n>notification_writer_query</span> <span class=o>=</span> <span class=p>(</span>
</span></span><span class=line><span class=ln>52</span><span class=cl>        <span class=n>target_df</span><span class=o>.</span><span class=n>writeStream</span><span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=s2>&#34;kafka&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=ln>53</span><span class=cl>        <span class=o>.</span><span class=n>queryName</span><span class=p>(</span><span class=s2>&#34;notifications&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=ln>54</span><span class=cl>        <span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>&#34;kafka.bootstrap.servers&#34;</span><span class=p>,</span> <span class=s2>&#34;kafka:9092&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=ln>55</span><span class=cl>        <span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>&#34;topic&#34;</span><span class=p>,</span> <span class=s2>&#34;notifications&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=ln>56</span><span class=cl>        <span class=o>.</span><span class=n>outputMode</span><span class=p>(</span><span class=s2>&#34;append&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=ln>57</span><span class=cl>        <span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>&#34;checkpointLocation&#34;</span><span class=p>,</span> <span class=s2>&#34;.checkpoint&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=ln>58</span><span class=cl>        <span class=o>.</span><span class=n>start</span><span class=p>()</span>
</span></span><span class=line><span class=ln>59</span><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=ln>60</span><span class=cl>
</span></span><span class=line><span class=ln>61</span><span class=cl>    <span class=n>notification_writer_query</span><span class=o>.</span><span class=n>awaitTermination</span><span class=p>()</span>
</span></span></code></pre></div><p>The streaming application can be submitted as shown below. Note the <a href=https://mvnrepository.com/artifact/org.apache.spark/spark-sql-kafka-0-10_2.12/3.1.2 target=_blank rel="noopener noreferrer">Kafak 0.10+ Source for Structured Streaming<i class="fas fa-external-link-square-alt ms-1"></i></a> and its dependencies are added directly to the spark submit command as indicated by the <a href=https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html#deploying target=_blank rel="noopener noreferrer">official document<i class="fas fa-external-link-square-alt ms-1"></i></a>.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=ln>1</span><span class=cl><span class=nv>$SPARK_HOME</span>/bin/spark-submit <span class=se>\
</span></span></span><span class=line><span class=ln>2</span><span class=cl><span class=se></span>  --deploy-mode client <span class=se>\
</span></span></span><span class=line><span class=ln>3</span><span class=cl><span class=se></span>  --master local<span class=o>[</span>*<span class=o>]</span> <span class=se>\
</span></span></span><span class=line><span class=ln>4</span><span class=cl><span class=se></span>  --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2 <span class=se>\
</span></span></span><span class=line><span class=ln>5</span><span class=cl><span class=se></span>  tripdata_notify.py
</span></span></code></pre></div><p>We can check the topic via Kafka UI on port 8080. We see the notifications topic has 50 messages, which matches to the number that we obtained from the notebook.</p><p><picture><img class="img-fluid mx-auto d-block" alt src=/blog/2022-05-08-emr-local-dev/06-notification-01.png loading=lazy width=1332 height=312></picture></p><p>We can check the individual messages via the UI as well.</p><p><picture><img class="img-fluid mx-auto d-block" alt src=/blog/2022-05-08-emr-local-dev/06-notification-02.png loading=lazy width=1328 height=794></picture></p><h2 id=summary data-numberify>Summary<a class="anchor ms-1" href=#summary></a></h2><p>In this post, we discussed how to create a Spark local development environment for EMR using Docker and/or VSCode. A range of Spark development examples are demonstrated, and Glue Catalog integration is illustrated in some of them. And both the cases of utilising Visual Studio Code Remote - Containers extension and running as an isolated container are covered in some key examples.</p></div></div></div><div class=card-footer><div class="post-navs d-flex justify-content-evenly"><div class="post-nav post-prev"><i class="fas fa-fw fa-chevron-down post-prev-icon me-1" data-fa-transform=rotate-90></i>
<a href=/blog/2022-04-03-schema-registry-part2/>Use External Schema Registry With MSK Connect – Part 2 MSK Deployment</a></div><div class="post-nav post-next"><a href=/blog/2022-06-26-iceberg-etl-demo/>Data Warehousing ETL Demo With Apache Iceberg on EMR Local Environment</a>
<i class="fas fa-fw fa-chevron-down post-next-icon ms-1" data-fa-transform=rotate-270></i></div></div></div></article><section class="related-posts row card component"><div class=card-header><h2 class="card-title fs-4 my-2 text-surface">Related Posts</h2></div><div class="card-body slide px-1"><div class="slide-inner row gx-0"><div class="col-12 col-md-6 col-lg-4 me-2"><div class="post-card card card-body p-0 border-0 surface"><picture><source srcset=/blog/2021-12-05-datalake-demo-part1/featured_hua25eccd3824300d3b1ed87f56797248c_164526_500x0_resize_box_3.png media="(max-width: 576px)" height=229 width=500><img class=img-fluid height=83 width=180 alt=featured.png src=/blog/2021-12-05-datalake-demo-part1/featured_hua25eccd3824300d3b1ed87f56797248c_164526_180x0_resize_box_3.png data-src=/blog/2021-12-05-datalake-demo-part1/featured.png loading=lazy></picture>
<a class=post-title href=/blog/2021-12-05-datalake-demo-part1/>Data Lake Demo Using Change Data Capture (CDC) on AWS – Part 1 Local Development</a><div class="post-meta mb-0">December 5, 2021</div></div></div><div class="col-12 col-md-6 col-lg-4 me-2"><div class="post-card card card-body p-0 border-0 surface"><picture><source srcset=/blog/2021-12-19-datalake-demo-part3/featured_hua25eccd3824300d3b1ed87f56797248c_164526_500x0_resize_box_3.png media="(max-width: 576px)" height=229 width=500><img class=img-fluid height=83 width=180 alt=featured.png src=/blog/2021-12-19-datalake-demo-part3/featured_hua25eccd3824300d3b1ed87f56797248c_164526_180x0_resize_box_3.png data-src=/blog/2021-12-19-datalake-demo-part3/featured.png loading=lazy></picture>
<a class=post-title href=/blog/2021-12-19-datalake-demo-part3/>Data Lake Demo Using Change Data Capture (CDC) on AWS – Part 3 Implement Data Lake</a><div class="post-meta mb-0">December 19, 2021</div></div></div><div class="col-12 col-md-6 col-lg-4 me-2"><div class="post-card card card-body p-0 border-0 surface"><picture><source srcset=/blog/2021-12-12-datalake-demo-part2/featured_hua25eccd3824300d3b1ed87f56797248c_164526_500x0_resize_box_3.png media="(max-width: 576px)" height=229 width=500><img class=img-fluid height=83 width=180 alt=featured.png src=/blog/2021-12-12-datalake-demo-part2/featured_hua25eccd3824300d3b1ed87f56797248c_164526_180x0_resize_box_3.png data-src=/blog/2021-12-12-datalake-demo-part2/featured.png loading=lazy></picture>
<a class=post-title href=/blog/2021-12-12-datalake-demo-part2/>Data Lake Demo Using Change Data Capture (CDC) on AWS – Part 2 Implement CDC</a><div class="post-meta mb-0">December 12, 2021</div></div></div><div class="col-12 col-md-6 col-lg-4 me-2"><div class="post-card card card-body p-0 border-0 surface"><picture><source srcset=/blog/2021-11-14-glue-3-local-development/featured_hu689859cb443a98b1c98384fedaa00395_30923_500x0_resize_box_3.png media="(max-width: 576px)" height=296 width=500><img class=img-fluid height=107 width=180 alt=featured.png src=/blog/2021-11-14-glue-3-local-development/featured_hu689859cb443a98b1c98384fedaa00395_30923_180x0_resize_box_3.png data-src=/blog/2021-11-14-glue-3-local-development/featured.png loading=lazy></picture>
<a class=post-title href=/blog/2021-11-14-glue-3-local-development/>Local Development of AWS Glue 3.0 and Later</a><div class="post-meta mb-0">November 14, 2021</div></div></div><div class="col-12 col-md-6 col-lg-4 me-2"><div class="post-card card card-body p-0 border-0 surface"><picture><source srcset=/blog/2021-08-20-glue-local-development/featured_hu248b4052e45f408d4fe80445a9d59f15_19535_500x0_resize_box_3.png media="(max-width: 576px)" height=141 width=500><img class=img-fluid height=51 width=180 alt=featured.png src=/blog/2021-08-20-glue-local-development/featured_hu248b4052e45f408d4fe80445a9d59f15_19535_180x0_resize_box_3.png data-src=/blog/2021-08-20-glue-local-development/featured.png loading=lazy></picture>
<a class=post-title href=/blog/2021-08-20-glue-local-development/>AWS Glue Local Development With Docker and Visual Studio Code</a><div class="post-meta mb-0">August 20, 2021</div></div></div></div><button class=slide-control-left>
<i class="fas fa-2x fa-chevron-circle-down" data-fa-transform=rotate-90></i>
<span class=visually-hidden>Left</span></button>
<button class=slide-control-right>
<i class="fas fa-2x fa-chevron-circle-down" data-fa-transform=rotate-270></i>
<span class=visually-hidden>Right</span></button></div></section><div class="card component row post-comments" id=post-comments><div class=card-header><h2 class="card-title my-2 fs-4 text-surface">Comments</h2></div><div class=card-body><script src=https://utteranc.es/client.js repo=jaehyeon-kim/site issue-term=pathname label=comment theme=github-light crossorigin=anonymous async></script></div></div></div></div><aside class="col-xxl-4 sidebar d-flex"><div class="container d-flex flex-column"><div class="accordion profile"><div class="accordion-item card row text-center component"><div class="accordion-header card-header border-0" id=profile-header><a class="accordion-button d-lg-none mb-2 shadow-none p-0 bg-transparent text-surface" role=button data-bs-toggle=collapse href=#profile aria-expanded=true aria-controls=profile>Profile</a></div><div class="card-body collapse accordion-collapse accordion-body d-lg-block show" id=profile aria-labelledby=profile-header><div class="col-12 d-flex align-items-center justify-content-center"><picture><img class="profile-avatar rounded-circle" alt="Jaehyeon Kim" src=https://jaehyeon.me/images/profile.png loading=lazy data-viewer-invisible width=200 height=200></picture></div><div class="col-12 profile-meta"><div class="profile-name fw-fold fs-lg">Jaehyeon Kim</div><div class=profile-bio>Consultant at Cevo 🇦🇺 ▪️ ☁ AWS Community Builder ▪️ 💭 Blogger ▪️ 🔥 Real-time Enthusiast</div><a class="profile-about text-primary" href=/about/><i class="fas fa-fw fa-user"></i>About</a><a class="profile-contact text-primary" href=/contact/><i class="fas fa-fw fa-question-circle"></i>Contact Me</a></div><nav class="social-links nav justify-content-center mt-1 justify-content-around"><a class="nav-link social-link" href=mailto:dottami@gmail.com title=Email><i class="fas fa-fw fa-2x fa-envelope" style=color:#0963ac></i></a>
<a class="nav-link social-link" target=_blank href=https://github.com/jaehyeon-kim title=GitHub rel=me><i class="fa-fw fa-2x fab fa-github"></i></a>
<a class="nav-link social-link" target=_blank href=https://linkedin.com/in/jaehyeon-kim-76b93429 title=LinkedIn rel=me><i class="fa-fw fa-2x fab fa-linkedin-in" style=color:#0a66c2></i></a></nav></div></div></div><div class="accordion taxonomies-toggle"><div class="row card component accordion-item"><div class="accordion-header card-header border-0"><a class="accordion-button d-lg-none mb-1 shadow-none p-0 bg-transparent" role=button data-bs-toggle=collapse href=#taxonomies-toggle aria-expanded=true aria-controls=taxonomies-toggle>Taxonomies</a></div><div class="card-body collapse accordion-collapse accordion-body d-lg-block show" id=taxonomies-toggle><ul class="nav nav-pills nav-fill" role=tablist><li class=nav-item role=presentation><button class="nav-link active" id=taxonomyCategoriesTab data-bs-toggle=tab data-bs-target=#taxonomyCategories type=button role=tab aria-controls=taxonomyCategories aria-selected=true>
Categories</button></li><li class=nav-item role=presentation><button class=nav-link id=taxonomyTagsTab data-bs-toggle=tab data-bs-target=#taxonomyTags type=button role=tab aria-controls=taxonomyTags aria-selected=true>
Tags</button></li><li class=nav-item role=presentation><button class=nav-link id=taxonomySeriesTab data-bs-toggle=tab data-bs-target=#taxonomySeries type=button role=tab aria-controls=taxonomySeries aria-selected=true>
Series</button></li><li class=nav-item role=presentation><button class=nav-link id=taxonomyArchivesTab data-bs-toggle=tab data-bs-target=#taxonomyArchives type=button role=tab aria-controls=taxonomyArchives aria-selected=true>
Archives</button></li></ul><div class="tab-content mt-3"><div class="tab-pane active" id=taxonomyCategories role=tabpanel aria-labelledby=taxonomyCategoriesTab tabindex=0><a href=/categories/data-engineering/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-category me-2 mb-2" title="Data Engineering">Data Engineering
<span class="badge badge-sm text-secondary bg-white ms-1">25</span></a>
<a href=/categories/engineering/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-category me-2 mb-2" title=Engineering>Engineering
<span class="badge badge-sm text-secondary bg-white ms-1">23</span></a>
<a href=/categories/machine-learning/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-category me-2 mb-2" title="Machine Learning">Machine Learning
<span class="badge badge-sm text-secondary bg-white ms-1">6</span></a>
<a href=/categories/serverless/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-category me-2 mb-2" title=Serverless>Serverless
<span class="badge badge-sm text-secondary bg-white ms-1">6</span></a>
<a href=/categories/web-development/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-category me-2 mb-2" title="Web Development">Web Development
<span class="badge badge-sm text-secondary bg-white ms-1">5</span></a></div><div class=tab-pane id=taxonomyTags role=tabpanel aria-labelledby=taxonomyTagsTab tabindex=0><a href=/tags/r/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-tag me-2 mb-2" title=R>R
<span class="badge badge-sm text-secondary bg-white ms-1">36</span></a>
<a href=/tags/aws/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-tag me-2 mb-2" title=AWS>AWS
<span class="badge badge-sm text-secondary bg-white ms-1">28</span></a>
<a href=/tags/python/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-tag me-2 mb-2" title=Python>Python
<span class="badge badge-sm text-secondary bg-white ms-1">18</span></a>
<a href=/tags/docker/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-tag me-2 mb-2" title=Docker>Docker
<span class="badge badge-sm text-secondary bg-white ms-1">17</span></a>
<a href=/tags/apache-spark/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-tag me-2 mb-2" title="Apache Spark">Apache Spark
<span class="badge badge-sm text-secondary bg-white ms-1">16</span></a>
<a href=/tags/terraform/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-tag me-2 mb-2" title=Terraform>Terraform
<span class="badge badge-sm text-secondary bg-white ms-1">13</span></a>
<a href=/tags/docker-compose/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-tag me-2 mb-2" title="Docker Compose">Docker Compose
<span class="badge badge-sm text-secondary bg-white ms-1">12</span></a>
<a href=/tags/aws-lambda/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-tag me-2 mb-2" title="AWS Lambda">AWS Lambda
<span class="badge badge-sm text-secondary bg-white ms-1">11</span></a>
<a href=/tags/amazon-emr/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-tag me-2 mb-2" title="Amazon EMR">Amazon EMR
<span class="badge badge-sm text-secondary bg-white ms-1">10</span></a>
<a href=/tags/apache-kafka/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-tag me-2 mb-2" title="Apache Kafka">Apache Kafka
<span class="badge badge-sm text-secondary bg-white ms-1">8</span></a>
<a href=https://jaehyeon.me/tags class="btn btn-sm btn-secondary post-taxonomy ps-3 post-tag me-2 mb-2" title=ALL>ALL
<span class="badge badge-sm text-secondary bg-white ms-1">66</span></a></div><div class=tab-pane id=taxonomySeries role=tabpanel aria-labelledby=taxonomySeriesTab tabindex=0><a href=/series/tree-based-methods-in-r/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-series me-2 mb-2" title="Tree based methods in R">Tree based methods in R
<span class="badge badge-sm text-secondary bg-white ms-1">6</span></a>
<a href=/series/dbt-for-effective-data-transformation-on-aws/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-series me-2 mb-2" title="DBT for Effective Data Transformation on AWS">DBT for Effective Data Transformation on AWS
<span class="badge badge-sm text-secondary bg-white ms-1">5</span></a>
<a href=/series/serverless-data-product/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-series me-2 mb-2" title="Serverless Data Product">Serverless Data Product
<span class="badge badge-sm text-secondary bg-white ms-1">4</span></a>
<a href=/series/data-lake-demo-using-change-data-capture/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-series me-2 mb-2" title="Data Lake Demo Using Change Data Capture">Data Lake Demo Using Change Data Capture
<span class="badge badge-sm text-secondary bg-white ms-1">3</span></a>
<a href=/series/parallel-processing-on-single-machine/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-series me-2 mb-2" title="Parallel processing on single machine">Parallel processing on single machine
<span class="badge badge-sm text-secondary bg-white ms-1">3</span></a>
<a href=/series/api-development-with-r/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-series me-2 mb-2" title="API development with R">API development with R
<span class="badge badge-sm text-secondary bg-white ms-1">2</span></a>
<a href=/series/integrate-schema-registry-with-msk-connect/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-series me-2 mb-2" title="Integrate Schema Registry with MSK Connect">Integrate Schema Registry with MSK Connect
<span class="badge badge-sm text-secondary bg-white ms-1">2</span></a>
<a href=/series/simplify-streaming-ingestion-on-aws/ class="btn btn-sm btn-secondary post-taxonomy ps-3 post-series me-2 mb-2" title="Simplify Streaming Ingestion on AWS">Simplify Streaming Ingestion on AWS
<span class="badge badge-sm text-secondary bg-white ms-1">2</span></a></div><div class=tab-pane id=taxonomyArchives role=tabpanel aria-labelledby=taxonomyArchivesTab tabindex=0><a href class="btn btn-sm btn-secondary post-taxonomy ps-3 me-2 mb-2" title=2023>2023 <span class="badge badge-sm text-secondary bg-white ms-1">3</span></a>
<a href=/archives/2022/ class="btn btn-sm btn-secondary post-taxonomy ps-3 me-2 mb-2" title=2022>2022 <span class="badge badge-sm text-secondary bg-white ms-1">15</span></a>
<a href class="btn btn-sm btn-secondary post-taxonomy ps-3 me-2 mb-2" title=2021>2021 <span class="badge badge-sm text-secondary bg-white ms-1">7</span></a>
<a href class="btn btn-sm btn-secondary post-taxonomy ps-3 me-2 mb-2" title=2020>2020 <span class="badge badge-sm text-secondary bg-white ms-1">1</span></a>
<a href class="btn btn-sm btn-secondary post-taxonomy ps-3 me-2 mb-2" title=2019>2019 <span class="badge badge-sm text-secondary bg-white ms-1">5</span></a>
<a href class="btn btn-sm btn-secondary post-taxonomy ps-3 me-2 mb-2" title=2018>2018 <span class="badge badge-sm text-secondary bg-white ms-1">2</span></a>
<a href class="btn btn-sm btn-secondary post-taxonomy ps-3 me-2 mb-2" title=2017>2017 <span class="badge badge-sm text-secondary bg-white ms-1">6</span></a>
<a href class="btn btn-sm btn-secondary post-taxonomy ps-3 me-2 mb-2" title=2016>2016 <span class="badge badge-sm text-secondary bg-white ms-1">6</span></a>
<a href class="btn btn-sm btn-secondary post-taxonomy ps-3 me-2 mb-2" title=2015>2015 <span class="badge badge-sm text-secondary bg-white ms-1">15</span></a>
<a href class="btn btn-sm btn-secondary post-taxonomy ps-3 me-2 mb-2" title=2014>2014 <span class="badge badge-sm text-secondary bg-white ms-1">5</span></a></div></div></div></div></div><div class="accordion posts-toggle"><div class="row card component accordion-item"><div class="accordion-header card-header border-0"><a class="accordion-button d-lg-none mb-1 shadow-none p-0 bg-transparent" role=button data-bs-toggle=collapse href=#posts-toggle aria-expanded=true aria-controls=posts-toggle>Posts</a></div><div class="card-body collapse accordion-collapse accordion-body d-lg-block show" id=posts-toggle><ul class="nav nav-pills nav-fill" role=tablist><li class=nav-item role=presentation><button class="nav-link active" id=recent-posts-tab data-bs-toggle=tab data-bs-target=#recent-posts type=button role=tab aria-controls=recent-posts aria-selected=true>
Recent Posts</button></li></ul><div class="tab-content mt-3"><div class="tab-pane active" id=recent-posts role=tabpanel aria-labelledby=recent-posts-tab tabindex=0><ul class="post-list list-unstyled ms-1"><li class=mb-2><div class=d-flex><div class="flex-shrink-0 d-flex justify-content-center align-items-center" style=max-width:100px><picture><source srcset=/blog/2023-03-14-simplify-streaming-ingestion-athena/featured_huef6334952f8505bdd5ee5a48de258194_43403_500x0_resize_box_3.png media="(max-width: 576px)" height=258 width=500><img class=img-fluid height=93 width=180 alt=featured.png src=/blog/2023-03-14-simplify-streaming-ingestion-athena/featured_huef6334952f8505bdd5ee5a48de258194_43403_180x0_resize_box_3.png data-src=/blog/2023-03-14-simplify-streaming-ingestion-athena/featured.png loading=lazy></picture></div><div class="flex-grow-1 d-flex flex-column h-auto justify-content-center ms-3"><a class=post-title href=/blog/2023-03-14-simplify-streaming-ingestion-athena/>Simplify Streaming Ingestion on AWS – Part 2 MSK and Athena</a><div class="post-meta mt-2"><span class=post-date>March 14, 2023</span></div></div></div></li><li class=mb-2><div class=d-flex><div class="flex-shrink-0 d-flex justify-content-center align-items-center" style=max-width:100px><picture><source srcset=/blog/2023-02-08-simplify-streaming-ingestion-redshift/featured_hu77abd5c8ed84cebc51bde5ecaefc7320_32864_500x0_resize_box_3.png media="(max-width: 576px)" height=239 width=500><img class=img-fluid height=86 width=180 alt=featured.png src=/blog/2023-02-08-simplify-streaming-ingestion-redshift/featured_hu77abd5c8ed84cebc51bde5ecaefc7320_32864_180x0_resize_box_3.png data-src=/blog/2023-02-08-simplify-streaming-ingestion-redshift/featured.png loading=lazy></picture></div><div class="flex-grow-1 d-flex flex-column h-auto justify-content-center ms-3"><a class=post-title href=/blog/2023-02-08-simplify-streaming-ingestion-redshift/>Simplify Streaming Ingestion on AWS – Part 1 MSK and Redshift</a><div class="post-meta mt-2"><span class=post-date>February 8, 2023</span></div></div></div></li><li class=mb-2><div class=d-flex><div class="flex-shrink-0 d-flex justify-content-center align-items-center" style=max-width:100px><picture><source srcset=/blog/2023-01-10-kafka-consumer-seek-offsets/featured_hua222fe3eeaae92dfa98b24cdf7061694_47217_500x0_resize_box_3.png media="(max-width: 576px)" height=368 width=500><img class=img-fluid height=132 width=180 alt=featured.png src=/blog/2023-01-10-kafka-consumer-seek-offsets/featured_hua222fe3eeaae92dfa98b24cdf7061694_47217_180x0_resize_box_3.png data-src=/blog/2023-01-10-kafka-consumer-seek-offsets/featured.png loading=lazy></picture></div><div class="flex-grow-1 d-flex flex-column h-auto justify-content-center ms-3"><a class=post-title href=/blog/2023-01-10-kafka-consumer-seek-offsets/>How to Configure Kafka Consumers to Seek Offsets by Timestamp</a><div class="post-meta mt-2"><span class=post-date>January 10, 2023</span></div></div></div></li><li class=mb-2><div class=d-flex><div class="flex-shrink-0 d-flex justify-content-center align-items-center" style=max-width:100px><picture><source srcset=/blog/2022-12-06-dbt-on-aws-part-5-athena/featured_hu2cb992d854f23cc962e70e6949802760_91796_500x0_resize_box_3.png media="(max-width: 576px)" height=243 width=500><img class=img-fluid height=87 width=180 alt=featured.png src=/blog/2022-12-06-dbt-on-aws-part-5-athena/featured_hu2cb992d854f23cc962e70e6949802760_91796_180x0_resize_box_3.png data-src=/blog/2022-12-06-dbt-on-aws-part-5-athena/featured.png loading=lazy></picture></div><div class="flex-grow-1 d-flex flex-column h-auto justify-content-center ms-3"><a class=post-title href=/blog/2022-12-06-dbt-on-aws-part-5-athena/>Data Build Tool (Dbt) for Effective Data Transformation on AWS – Part 5 Athena</a><div class="post-meta mt-2"><span class=post-date>December 6, 2022</span></div></div></div></li><li class=mb-2><div class=d-flex><div class="flex-shrink-0 d-flex justify-content-center align-items-center" style=max-width:100px><picture><source srcset=/blog/2022-11-01-dbt-on-aws-part-4-emr-eks/featured_huad7bc56c3140ba61f8a863cb7496d5ad_91067_500x0_resize_box_3.png media="(max-width: 576px)" height=244 width=500><img class=img-fluid height=88 width=180 alt=featured.png src=/blog/2022-11-01-dbt-on-aws-part-4-emr-eks/featured_huad7bc56c3140ba61f8a863cb7496d5ad_91067_180x0_resize_box_3.png data-src=/blog/2022-11-01-dbt-on-aws-part-4-emr-eks/featured.png loading=lazy></picture></div><div class="flex-grow-1 d-flex flex-column h-auto justify-content-center ms-3"><a class=post-title href=/blog/2022-11-01-dbt-on-aws-part-4-emr-eks/>Data Build Tool (Dbt) for Effective Data Transformation on AWS – Part 4 EMR on EKS</a><div class="post-meta mt-2"><span class=post-date>November 1, 2022</span></div></div></div></li></ul></div></div></div></div></div></div></aside></div></main><footer class="footer mt-auto py-3 text-center container"><div class="offcanvas offcanvas-bottom h-auto" tabindex=-1 id=offcanvasActionsPanel aria-labelledby=offcanvasActionsPanelLabel><div class=offcanvas-header><div class="offcanvas-title h5" id=offcanvasActionsPanelLabel><i class="fas fa-fw fa-th-large me-1"></i>
Actions</div><button type=button class="btn-close ms-auto" data-bs-dismiss=offcanvas data-bs-target=offcanvasActionsPanel aria-label=Close></button></div><div class="offcanvas-body mt-2"><div class="actions d-flex overflow-auto align-items-center"><a role=button class="action action-go-back d-flex flex-column align-items-center me-3" href="javascript: window.history.back();"><span class="action-icon mb-2"><i class="fas fa-2x fa-chevron-circle-down" data-fa-transform=rotate-90></i></span> Go back</a>
<a role=button class="action action-reload-page d-flex flex-column align-items-center me-3"><span class="action-icon mb-2"><i class="fas fa-2x fa-redo-alt"></i></span> Reload</a>
<a role=button class="action action-copy-url d-flex flex-column align-items-center me-3"><span class="action-icon mb-2"><i class="fas fa-2x fa-link"></i></span> Copy URL</a></div></div></div><div class="row text-center"><div class="col-12 mt-2"><p class=mb-2>Jaehyeon's Personal Site</p><p class="text-secondary mb-2"><small></small></p><div class="copyright mb-2 text-secondary"><small>Copyright © 2023-2023 Jaehyeon Kim. All Rights Reserved.</small></div><nav class="social-links nav justify-content-center mb-2 mt-3"><a class="nav-link social-link p-0 me-1 mb-2" target=_blank href=/index.xml title=RSS rel=me><i class="fas fa-fw fa-2x fa-rss" style=color:#ea6221></i></a></nav></div><div class="col-12 col-lg-8 offset-0 offset-lg-1"></div></div></footer><script data-precache src=/assets/main/bundle.min.50b1bf287e3f8fc2563b999a6f2163b28902caaba1fc9e59763fb6c83fe51011.js integrity="sha256-ULG/KH4/j8JWO5mabyFjsokCyquh/J5Zdj+2yD/lEBE=" crossorigin=anonymous async></script><script data-precache src=/assets/icons/bundle.min.4f30d5267a9f2f9d45ed93969d45ec626100a969a8b91f71f753315a261e9034.js integrity="sha256-TzDVJnqfL51F7ZOWnUXsYmEAqWmouR9x91MxWiYekDQ=" crossorigin=anonymous defer></script>
<script data-precache src=/assets/viewer/bundle.min.7f7e123a50432aec1a2b911aa055ea464134e1c29cbdd67ff0ec18fab6ab3335.js integrity="sha256-f34SOlBDKuwaK5EaoFXqRkE04cKcvdZ/8OwY+rarMzU=" crossorigin=anonymous defer></script>
<script data-precache defer src=/assets/katex/bundle.min.f2bed17e252bf9064f3762211c0c01ca20721553c300c0870497712bd9ae4b9d.js integrity="sha256-8r7RfiUr+QZPN2IhHAwByiByFVPDAMCHBJdxK9muS50=" crossorigin=anonymous></script>
<script data-precache defer src=/assets/mermaid/bundle.min.6ac89481082878c4db0f641e78a7c950995fe4d7978008551531a080754919ca.js integrity="sha256-asiUgQgoeMTbD2QeeKfJUJlf5NeXgAhVFTGggHVJGco=" crossorigin=anonymous></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-076N8WCGWZ"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-076N8WCGWZ",{anonymize_ip:!1})}</script><script>"serviceWorker"in navigator&&window.addEventListener("load",()=>{navigator.serviceWorker.register("/service-worker.min.js").then(function(e){console.log("Successfully registered service worker",e)}).catch(function(e){console.warn("Error whilst registering service worker",e)})})</script></body></html>