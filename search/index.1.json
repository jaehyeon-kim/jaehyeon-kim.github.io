[{"categories":[],"content":"Jaehyeon works as a consultant focused on working with customers to design and build their data and analtyics solutions on AWS.\n","date":"April 10, 2023","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/about/","series":[],"smallImg":"","tags":[],"timestamp":1681084800,"title":"About"},{"categories":[{"title":"Data Engineering","url":"/categories/data-engineering/"}],"content":"In Part 1, we discussed a streaming ingestion solution using EventBridge, Lambda, MSK and Redshift Serverless. Athena provides the MSK connector to enable SQL queries on Apache Kafka topics directly, and it can also facilitate the extraction of insights without setting up an additional pipeline to store data into S3. In this post, we discuss how to update the streaming ingestion solution so that data in the Kafka topic can be queried by Athena instead of Redshift.\nPart 1 MSK and Redshift Part 2 MSK and Athena (this post) Architecture As Part 1, fake online order data is generated by multiple Lambda functions that are invoked by an EventBridge schedule rule. The schedule is set to run every minute and the associating rule has a configurable number (e.g. 5) of targets. Each target points to the same Kafka producer Lambda function. In this way we are able to generate test data using multiple Lambda functions according to the desired volume of messages. Once messages are sent to a Kafka topic, they can be consumed by the Athena MSK Connector, which is a Lambda function that can be installed from the AWS Serverless Application Repository. A new Athena data source needs to be created in order to deploy the connector and the schema of the topic should be registered with AWS Glue Schema Registry. The infrastructure is built by Terraform and the AWS SAM CLI is used to develop the producer Lambda function locally before deploying to AWS.\nInfrastructure The ingestion solution shares a large portion of infrastructure and only new resources are covered in this post. The source can be found in the GitHub repository of this post.\nGlue Schema The order data is JSON format, and it has 4 attributes - order_id, ordered_at, _user_id _and items. Although the items attribute keeps an array of objects that includes _product_id _and quantity, it is specified as VARCHAR because the MSK connector doesn\u0026rsquo;t support complex types.\n1{ 2 \u0026#34;topicName\u0026#34;: \u0026#34;orders\u0026#34;, 3 \u0026#34;message\u0026#34;: { 4 \u0026#34;dataFormat\u0026#34;: \u0026#34;json\u0026#34;, 5 \u0026#34;fields\u0026#34;: [ 6 { 7 \u0026#34;name\u0026#34;: \u0026#34;order_id\u0026#34;, 8 \u0026#34;mapping\u0026#34;: \u0026#34;order_id\u0026#34;, 9 \u0026#34;type\u0026#34;: \u0026#34;VARCHAR\u0026#34; 10 }, 11 { 12 \u0026#34;name\u0026#34;: \u0026#34;ordered_at\u0026#34;, 13 \u0026#34;mapping\u0026#34;: \u0026#34;ordered_at\u0026#34;, 14 \u0026#34;type\u0026#34;: \u0026#34;TIMESTAMP\u0026#34;, 15 \u0026#34;formatHint\u0026#34;: \u0026#34;yyyy-MM-dd HH:mm:ss.SSS\u0026#34; 16 }, 17 { 18 \u0026#34;name\u0026#34;: \u0026#34;user_id\u0026#34;, 19 \u0026#34;mapping\u0026#34;: \u0026#34;user_id\u0026#34;, 20 \u0026#34;type\u0026#34;: \u0026#34;VARCHAR\u0026#34; 21 }, 22 { 23 \u0026#34;name\u0026#34;: \u0026#34;items\u0026#34;, 24 \u0026#34;mapping\u0026#34;: \u0026#34;items\u0026#34;, 25 \u0026#34;type\u0026#34;: \u0026#34;VARCHAR\u0026#34; 26 } 27 ] 28 } 29} The registry and schema can be created as shown below. Note the description should include the string {AthenaFederationMSK} as the marker string is required for AWS Glue Registries that you use with the Amazon Athena MSK connector.\n1# integration-athena/infra/athena.tf 2resource \u0026#34;aws_glue_registry\u0026#34; \u0026#34;msk_registry\u0026#34; { 3 registry_name = \u0026#34;customer\u0026#34; 4 description = \u0026#34;{AthenaFederationMSK}\u0026#34; 5 6 tags = local.tags 7} 8 9resource \u0026#34;aws_glue_schema\u0026#34; \u0026#34;msk_schema\u0026#34; { 10 schema_name = \u0026#34;orders\u0026#34; 11 registry_arn = aws_glue_registry.msk_registry.arn 12 data_format = \u0026#34;JSON\u0026#34; 13 compatibility = \u0026#34;NONE\u0026#34; 14 schema_definition = jsonencode({ \u0026#34;topicName\u0026#34; : \u0026#34;orders\u0026#34;, \u0026#34;message\u0026#34; : { \u0026#34;dataFormat\u0026#34; : \u0026#34;json\u0026#34;, \u0026#34;fields\u0026#34; : [{ \u0026#34;name\u0026#34; : \u0026#34;order_id\u0026#34;, \u0026#34;mapping\u0026#34; : \u0026#34;order_id\u0026#34;, \u0026#34;type\u0026#34; : \u0026#34;VARCHAR\u0026#34; }, { \u0026#34;name\u0026#34; : \u0026#34;ordered_at\u0026#34;, \u0026#34;mapping\u0026#34; : \u0026#34;ordered_at\u0026#34;, \u0026#34;type\u0026#34; : \u0026#34;TIMESTAMP\u0026#34;, \u0026#34;formatHint\u0026#34; : \u0026#34;yyyy-MM-dd HH:mm:ss.SSS\u0026#34; }, { \u0026#34;name\u0026#34; : \u0026#34;user_id\u0026#34;, \u0026#34;mapping\u0026#34; : \u0026#34;user_id\u0026#34;, \u0026#34;type\u0026#34; : \u0026#34;VARCHAR\u0026#34; }, { \u0026#34;name\u0026#34; : \u0026#34;items\u0026#34;, \u0026#34;mapping\u0026#34; : \u0026#34;items\u0026#34;, \u0026#34;type\u0026#34; : \u0026#34;VARCHAR\u0026#34; }] } }) 15 16 tags = local.tags 17} Athena MSK Connector In Terraform, the MSK Connector Lambda function can be created by deploying the associated CloudFormation stack from the AWS Serverless Application Repository. The stack parameters are passed into environment variables of the function, and they are mostly used to establish connection to Kafka topics.\n1# integration-athena/infra/athena.tf 2resource \u0026#34;aws_serverlessapplicationrepository_cloudformation_stack\u0026#34; \u0026#34;athena_msk_connector\u0026#34; { 3 name = \u0026#34;${local.name}-athena-msk-connector\u0026#34; 4 application_id = \u0026#34;arn:aws:serverlessrepo:us-east-1:292517598671:applications/AthenaMSKConnector\u0026#34; 5 semantic_version = \u0026#34;2023.8.3\u0026#34; 6 capabilities = [ 7 \u0026#34;CAPABILITY_IAM\u0026#34;, 8 \u0026#34;CAPABILITY_RESOURCE_POLICY\u0026#34;, 9 ] 10 parameters = { 11 AuthType = \u0026#34;SASL_SSL_AWS_MSK_IAM\u0026#34; 12 KafkaEndpoint = aws_msk_cluster.msk_data_cluster.bootstrap_brokers_sasl_iam 13 LambdaFunctionName = \u0026#34;${local.name}-ingest-orders\u0026#34; 14 SpillBucket = aws_s3_bucket.default_bucket.id 15 SpillPrefix = \u0026#34;athena-spill\u0026#34; 16 SecurityGroupIds = aws_security_group.athena_connector.id 17 SubnetIds = join(\u0026#34;,\u0026#34;, module.vpc.private_subnets) 18 LambdaRoleARN = aws_iam_role.athena_connector_role.arn 19 } 20} Lambda Execution Role The AWS document doesn\u0026rsquo;t include the specific IAM permissions that are necessary for the connector function, and they are updated by making trials and errors. Therefore, some of them are too generous, and it should be refined later.\nFirst it needs permission to access an MSK cluster and topics, and they are copied from Part 1. Next access to the Glue registry and schema is required. I consider the required permission would have been more specific if a specific registry or schema could be specified to the connector Lambda function. Rather it searches applicable registries using a string marker and that requires an additional set of permissions. Then permission to the spill S3 bucket is added. I initially included a typical read/write permission on a specific bucket and objects, but the Lambda function complained by throwing 403 authorized errors. Therefore, I escalated the level of permissions, which is by no means acceptable in a strict environment. Further investigation is necessary for it. Finally, permission to get Athena query executions is added. 1# integration-athena/infra/athena.tf 2resource \u0026#34;aws_iam_role\u0026#34; \u0026#34;athena_connector_role\u0026#34; { 3 name = \u0026#34;${local.name}-athena-connector-role\u0026#34; 4 5 assume_role_policy = data.aws_iam_policy_document.athena_connector_assume_role_policy.json 6 managed_policy_arns = [ 7 \u0026#34;arn:aws:iam::aws:policy/service-role/AWSLambdaVPCAccessExecutionRole\u0026#34;, 8 aws_iam_policy.athena_connector_permission.arn 9 ] 10} 11 12data \u0026#34;aws_iam_policy_document\u0026#34; \u0026#34;athena_connector_assume_role_policy\u0026#34; { 13 statement { 14 actions = [\u0026#34;sts:AssumeRole\u0026#34;] 15 16 principals { 17 type = \u0026#34;Service\u0026#34; 18 identifiers = [ 19 \u0026#34;lambda.amazonaws.com\u0026#34; 20 ] 21 } 22 } 23} 24 25resource \u0026#34;aws_iam_policy\u0026#34; \u0026#34;athena_connector_permission\u0026#34; { 26 name = \u0026#34;${local.name}-athena-connector-permission\u0026#34; 27 28 policy = jsonencode({ 29 Version = \u0026#34;2012-10-17\u0026#34; 30 Statement = [ 31 { 32 Sid = \u0026#34;PermissionOnCluster\u0026#34; 33 Action = [ 34 \u0026#34;kafka-cluster:ReadData\u0026#34;, 35 \u0026#34;kafka-cluster:DescribeTopic\u0026#34;, 36 \u0026#34;kafka-cluster:Connect\u0026#34;, 37 ] 38 Effect = \u0026#34;Allow\u0026#34; 39 Resource = [ 40 \u0026#34;arn:aws:kafka:*:${data.aws_caller_identity.current.account_id}:cluster/*/*\u0026#34;, 41 \u0026#34;arn:aws:kafka:*:${data.aws_caller_identity.current.account_id}:topic/*/*\u0026#34; 42 ] 43 }, 44 { 45 Sid = \u0026#34;PermissionOnGroups\u0026#34; 46 Action = [ 47 \u0026#34;kafka:GetBootstrapBrokers\u0026#34; 48 ] 49 Effect = \u0026#34;Allow\u0026#34; 50 Resource = \u0026#34;*\u0026#34; 51 }, 52 { 53 Sid = \u0026#34;PermissionOnGlueSchema\u0026#34; 54 Action = [ 55 \u0026#34;glue:*Schema*\u0026#34;, 56 \u0026#34;glue:ListRegistries\u0026#34; 57 ] 58 Effect = \u0026#34;Allow\u0026#34; 59 Resource = \u0026#34;*\u0026#34; 60 }, 61 { 62 Sid = \u0026#34;PermissionOnS3\u0026#34; 63 Action = [\u0026#34;s3:*\u0026#34;] 64 Effect = \u0026#34;Allow\u0026#34; 65 Resource = \u0026#34;arn:aws:s3:::*\u0026#34; 66 }, 67 { 68 Sid = \u0026#34;PermissionOnAthenaQuery\u0026#34; 69 Action = [ 70 \u0026#34;athena:GetQueryExecution\u0026#34; 71 ] 72 Effect = \u0026#34;Allow\u0026#34; 73 Resource = \u0026#34;*\u0026#34; 74 } 75 ] 76 }) 77} Security Group The security group and rules are shown below. Although the outbound rule is set to allow all protocol and port ranges, only port 443 and 9098 with the TCP protocol would be sufficient. The former is to access the Glue schema registry while the latter is for an MSK cluster with IAM authentication.\n1# integration-athena/infra/athena.tf 2resource \u0026#34;aws_security_group\u0026#34; \u0026#34;athena_connector\u0026#34; { 3 name = \u0026#34;${local.name}-athena-connector\u0026#34; 4 vpc_id = module.vpc.vpc_id 5 6 lifecycle { 7 create_before_destroy = true 8 } 9 10 tags = local.tags 11} 12 13resource \u0026#34;aws_security_group_rule\u0026#34; \u0026#34;athena_connector_msk_egress\u0026#34; { 14 type = \u0026#34;egress\u0026#34; 15 description = \u0026#34;allow outbound all\u0026#34; 16 security_group_id = aws_security_group.athena_connector.id 17 protocol = \u0026#34;-1\u0026#34; 18 from_port = 0 19 to_port = 0 20 cidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;] 21} Athena Data Source Unfortunately connecting to MSK from Athena is yet to be supported by CloudFormation or Terraform, and it is performed on AWS console as shown below. First we begin by clicking on the Create data source button.\nThen we can search the Amazon MSK data source and proceed by clicking on the _Next _button.\nWe can update data source details followed by selecting the connector Lambda function ARN in connection details.\nOnce the data source connection is established, we are able to see the customer database we created earlier - the Glue registry name becomes the database name.\nAlso, we can check the table details from the Athena editor as shown below.\nKafka Producer As in Part 1, the resources related to the Kafka producer Lambda function are managed in a separate Terraform stack. This is because it is easier to build the relevant resources iteratively. Note the SAM CLI builds the whole Terraform stack even for a small change of code, and it wouldn\u0026rsquo;t be convenient if the entire resources are managed in the same stack. The terraform stack of the producer is the same as Part 1, and it won\u0026rsquo;t be covered here. Only the producer Lambda function source is covered here as it is modified in order to comply with the MSK connector.\nProducer Source The Kafka producer is created to send messages to a topic named orders where fake order data is generated using the Faker package. The Order class generates one or more fake order records by the _create _method and an order record includes order ID, order timestamp, user ID and order items. Note order items are converted into string. It is because the MSK connector fails to parse them correctly. Actually the AWS document indicates the MSK connector interprets complex types as strings and I thought it would be converted into strings internally. However, it turned out the list items (or array of objects) cannot be queried by Athena. Therefore, it is converted into string in the first place. The Lambda function sends 100 records at a time followed by sleeping for 1 second. It repeats until it reaches MAX_RUN_SEC (e.g. 60) environment variable value. A Kafka message is made up of an order ID as the key and an order record as the value. Both the key and value are serialised as JSON. Note that the stable version of the kafka-python package does not support the IAM authentication method. Therefore, we need to install the package from a forked repository as discussed in this GitHub issue.\n1# integration-athena/kafka_producer/src/app.py 2import os 3import re 4import datetime 5import string 6import json 7import time 8from kafka import KafkaProducer 9from faker import Faker 10 11 12class Order: 13 def __init__(self, fake: Faker = None): 14 self.fake = fake or Faker() 15 16 def order(self): 17 rand_int = self.fake.random_int(1, 1000) 18 user_id = \u0026#34;\u0026#34;.join( 19 [string.ascii_lowercase[int(s)] if s.isdigit() else s for s in hex(rand_int)] 20 )[::-1] 21 return { 22 \u0026#34;order_id\u0026#34;: self.fake.uuid4(), 23 \u0026#34;ordered_at\u0026#34;: datetime.datetime.utcnow(), 24 \u0026#34;user_id\u0026#34;: user_id, 25 } 26 27 def items(self): 28 return [ 29 { 30 \u0026#34;product_id\u0026#34;: self.fake.random_int(1, 9999), 31 \u0026#34;quantity\u0026#34;: self.fake.random_int(1, 10), 32 } 33 for _ in range(self.fake.random_int(1, 4)) 34 ] 35 36 def create(self, num: int): 37 return [{**self.order(), **{\u0026#34;items\u0026#34;: json.dumps(self.items())}} for _ in range(num)] 38 39 40class Producer: 41 def __init__(self, bootstrap_servers: list, topic: str): 42 self.bootstrap_servers = bootstrap_servers 43 self.topic = topic 44 self.producer = self.create() 45 46 def create(self): 47 return KafkaProducer( 48 security_protocol=\u0026#34;SASL_SSL\u0026#34;, 49 sasl_mechanism=\u0026#34;AWS_MSK_IAM\u0026#34;, 50 bootstrap_servers=self.bootstrap_servers, 51 value_serializer=lambda v: json.dumps(v, default=self.serialize).encode(\u0026#34;utf-8\u0026#34;), 52 key_serializer=lambda v: json.dumps(v, default=self.serialize).encode(\u0026#34;utf-8\u0026#34;), 53 ) 54 55 def send(self, orders: list): 56 for order in orders: 57 self.producer.send(self.topic, key={\u0026#34;order_id\u0026#34;: order[\u0026#34;order_id\u0026#34;]}, value=order) 58 self.producer.flush() 59 60 def serialize(self, obj): 61 if isinstance(obj, datetime.datetime): 62 return re.sub(\u0026#34;T\u0026#34;, \u0026#34; \u0026#34;, obj.isoformat(timespec=\u0026#34;milliseconds\u0026#34;)) 63 if isinstance(obj, datetime.date): 64 return str(obj) 65 return obj 66 67 68def lambda_function(event, context): 69 if os.getenv(\u0026#34;BOOTSTRAP_SERVERS\u0026#34;, \u0026#34;\u0026#34;) == \u0026#34;\u0026#34;: 70 return 71 fake = Faker() 72 producer = Producer( 73 bootstrap_servers=os.getenv(\u0026#34;BOOTSTRAP_SERVERS\u0026#34;).split(\u0026#34;,\u0026#34;), topic=os.getenv(\u0026#34;TOPIC_NAME\u0026#34;) 74 ) 75 s = datetime.datetime.now() 76 ttl_rec = 0 77 while True: 78 orders = Order(fake).create(100) 79 producer.send(orders) 80 ttl_rec += len(orders) 81 print(f\u0026#34;sent {len(orders)} messages\u0026#34;) 82 elapsed_sec = (datetime.datetime.now() - s).seconds 83 if elapsed_sec \u0026gt; int(os.getenv(\u0026#34;MAX_RUN_SEC\u0026#34;, \u0026#34;60\u0026#34;)): 84 print(f\u0026#34;{ttl_rec} records are sent in {elapsed_sec} seconds ...\u0026#34;) 85 break 86 time.sleep(1) A sample order record is shown below.\n1{ 2 \u0026#34;order_id\u0026#34;: \u0026#34;6049dc71-063b-49bd-8b68-f2326d1c8544\u0026#34;, 3 \u0026#34;ordered_at\u0026#34;: \u0026#34;2023-03-09 21:05:00.073\u0026#34;, 4 \u0026#34;user_id\u0026#34;: \u0026#34;febxa\u0026#34;, 5 \u0026#34;items\u0026#34;: \u0026#34;[{\\\u0026#34;product_id\\\u0026#34;: 4793, \\\u0026#34;quantity\\\u0026#34;: 8}]\u0026#34; 6} Deployment In this section, we skip shared steps except for local development with SAM and analytics query building. See Part 1 for other steps.\nLocal Testing with SAM To simplify development, the Eventbridge permission is disabled by setting to_enable_trigger to false. Also, it is shortened to loop before it gets stopped by reducing msx_run_sec to 10.\n1# integration-athena/kafka_producer/variables.tf 2locals { 3 producer = { 4 ... 5 to_enable_trigger = false 6 environment = { 7 topic_name = \u0026#34;orders\u0026#34; 8 max_run_sec = 10 9 } 10 } 11 ... 12} The Lambda function can be built with the SAM build command while specifying the hook name as terraform and enabling beta features. Once completed, it stores the build artifacts and template in the .aws-sam folder.\n1$ sam build --hook-name terraform --beta-features 2 3# Apply complete! Resources: 3 added, 0 changed, 0 destroyed. 4 5 6# Build Succeeded 7 8# Built Artifacts : .aws-sam/build 9# Built Template : .aws-sam/build/template.yaml 10 11# Commands you can use next 12# ========================= 13# [*] Invoke Function: sam local invoke --hook-name terraform 14# [*] Emulate local Lambda functions: sam local start-lambda --hook-name terraform We can invoke the Lambda function locally using the SAM local invoke command. The Lambda function is invoked in a Docker container and the invocation logs are printed in the terminal as shown below.\n1$ sam local invoke --hook-name terraform module.kafka_producer_lambda.aws_lambda_function.this[0] --beta-features 2 3# Experimental features are enabled for this session. 4# Visit the docs page to learn more about the AWS Beta terms https://aws.amazon.com/service-terms/. 5 6# Skipped prepare hook. Current application is already prepared. 7# Invoking app.lambda_function (python3.8) 8# Skip pulling image and use local one: public.ecr.aws/sam/emulation-python3.8:rapid-1.70.0-x86_64. 9 10# Mounting .../kafka-pocs/integration-athena/kafka_producer/.aws-sam/build/ModuleKafkaProducerLambdaAwsLambdaFunctionThis069E06354 as /var/task:ro,delegated inside runtime container 11# START RequestId: d800173a-ceb5-4002-be0e-6f0d9628b639 Version: $LATEST 12# sent 100 messages 13# sent 100 messages 14# sent 100 messages 15# sent 100 messages 16# sent 100 messages 17# sent 100 messages 18# sent 100 messages 19# sent 100 messages 20# sent 100 messages 21# sent 100 messages 22# sent 100 messages 23# sent 100 messages 24# 1200 records are sent in 11 seconds ... 25# END RequestId: d800173a-ceb5-4002-be0e-6f0d9628b639 26# REPORT RequestId: d800173a-ceb5-4002-be0e-6f0d9628b639 Init Duration: 0.16 ms Duration: 12117.64 ms Billed Duration: 12118 ms Memory Size: 128 MB Max Memory Used: 128 MB 27# null We can also check the messages using kafka-ui.\nOrder Items Query Below shows the query result of the orders table. The _items _column is a JSON array but it is stored as string. In order to build analytics queries, we need to flatten the array elements into rows and it is discussed below.\nWe can flatten the order items using the _UNNEST _function and CROSS JOIN. We first need to convert it into an array type, and it is implemented by parsing the column into JSON followed by type-casting it into an array in a CTE.\n1WITH parsed AS ( 2 SELECT 3 order_id, 4 ordered_at, 5 user_id, 6 CAST(json_parse(items) as ARRAY(ROW(product_id INT, quantity INT))) AS items 7 FROM msk.customer.orders 8) 9SELECT 10 order_id, 11 ordered_at, 12 user_id, 13 items_unnested.product_id, 14 items_unnested.quantity 15FROM parsed 16CROSS JOIN unnest(parsed.items) AS t(items_unnested) We can see the flattened order items as shown below.\nThe remaining sections cover deploying the Kafka producer Lambda, producing messages and executing an analytics query. They are skipped in this post as they are exactly and/or almost the same. See Part 1 if you would like to check it.\nSummary Streaming ingestion to Redshift and Athena becomes much simpler thanks to new features. In this series of posts, we discussed those features by building a solution using EventBridge, Lambda, MSK, Redshift and Athena. We also covered AWS SAM integrated with Terraform for developing a Lambda function locally.\n","date":"March 14, 2023","img":"/blog/2023-03-14-simplify-streaming-ingestion-athena/featured.png","lang":"en","langName":"English","largeImg":"/blog/2023-03-14-simplify-streaming-ingestion-athena/featured_huef6334952f8505bdd5ee5a48de258194_43403_500x0_resize_box_3.png","permalink":"/blog/2023-03-14-simplify-streaming-ingestion-athena/","series":[{"title":"Simplify Streaming Ingestion on AWS","url":"/series/simplify-streaming-ingestion-on-aws/"}],"smallImg":"/blog/2023-03-14-simplify-streaming-ingestion-athena/featured_huef6334952f8505bdd5ee5a48de258194_43403_180x0_resize_box_3.png","tags":[{"title":"AWS","url":"/tags/aws/"},{"title":"Amazon MSK","url":"/tags/amazon-msk/"},{"title":"Amazon Athena","url":"/tags/amazon-athena/"},{"title":"Amazon EventBridge","url":"/tags/amazon-eventbridge/"},{"title":"AWS Lambda","url":"/tags/aws-lambda/"},{"title":"AWS SAM","url":"/tags/aws-sam/"},{"title":"Apache Kafka","url":"/tags/apache-kafka/"},{"title":"Python","url":"/tags/python/"},{"title":"Terraform","url":"/tags/terraform/"}],"timestamp":1678752000,"title":"Simplify Streaming Ingestion on AWS – Part 2 MSK and Athena"},{"categories":[{"title":"Data Engineering","url":"/categories/data-engineering/"}],"content":"Apache Kafka is a popular distributed event store and stream processing platform. Previously loading data from Kafka into Redshift and Athena usually required Kafka connectors (e.g. Amazon Redshift Sink Connector and Amazon S3 Sink Connector). Recently these AWS services provide features to ingest data from Kafka directly, which facilitates a simpler architecture that achieves low-latency and high-speed ingestion of streaming data. In part 1 of the simplify streaming ingestion on AWS series, we discuss how to develop an end-to-end streaming ingestion solution using EventBridge, Lambda, MSK and Redshift Serverless on AWS.\nPart 1 MSK and Redshift (this post) Part 2 MSK and Athena Architecture Fake online order data is generated by multiple Lambda functions that are invoked by an EventBridge schedule rule. The schedule is set to run every minute and the associating rule has a configurable number (e.g. 5) of targets. Each target points to the same Kafka producer Lambda function. In this way we are able to generate test data using multiple Lambda functions according to the desired volume of messages. Once the messages are sent to a Kafka topic, they can be consumed by a materialized view in an external schema that sources data from the MKS cluster. The infrastructure is built by Terraform and the AWS SAM CLI is used to develop the producer Lambda function locally before deploying to AWS.\nInfrastructure A VPC with 3 public and private subnets is created using the AWS VPC Terraform module (vpc.tf). Also, a SoftEther VPN server is deployed in order to access the resources in the private subnets from the developer machine (vpn.tf). It is particularly useful to monitor and manage the MSK cluster and Kafka topic as well as developing the Kafka producer Lambda function locally. The details about how to configure the VPN server can be found in an earlier post. The source can be found in the GitHub repository of this post.\nMSK An MSK cluster with 3 brokers is created. The broker nodes are deployed with the kafka.m5.large instance type in the private subnets. IAM authentication is used for the client authentication method. Note this method is the only secured authentication method supported by Redshift because the external schema supports either the no authentication or IAM authentication method only.\n1# integration-redshift/infra/variable.tf 2locals { 3 ... 4 msk = { 5 version = \u0026#34;3.3.1\u0026#34; 6 instance_size = \u0026#34;kafka.m5.large\u0026#34; 7 ebs_volume_size = 20 8 log_retention_ms = 604800000 # 7 days 9 } 10 ... 11} 12 13# integration-redshift/infra/msk.tf 14resource \u0026#34;aws_msk_cluster\u0026#34; \u0026#34;msk_data_cluster\u0026#34; { 15 cluster_name = \u0026#34;${local.name}-msk-cluster\u0026#34; 16 kafka_version = local.msk.version 17 number_of_broker_nodes = length(module.vpc.private_subnets) 18 configuration_info { 19 arn = aws_msk_configuration.msk_config.arn 20 revision = aws_msk_configuration.msk_config.latest_revision 21 } 22 23 broker_node_group_info { 24 instance_type = local.msk.instance_size 25 client_subnets = module.vpc.private_subnets 26 security_groups = [aws_security_group.msk.id] 27 storage_info { 28 ebs_storage_info { 29 volume_size = local.msk.ebs_volume_size 30 } 31 } 32 } 33 34 client_authentication { 35 sasl { 36 iam = true 37 } 38 } 39 40 logging_info { 41 broker_logs { 42 cloudwatch_logs { 43 enabled = true 44 log_group = aws_cloudwatch_log_group.msk_cluster_lg.name 45 } 46 s3 { 47 enabled = true 48 bucket = aws_s3_bucket.default_bucket.id 49 prefix = \u0026#34;logs/msk/cluster-\u0026#34; 50 } 51 } 52 } 53 54 tags = local.tags 55 56 depends_on = [aws_msk_configuration.msk_config] 57} 58 59resource \u0026#34;aws_msk_configuration\u0026#34; \u0026#34;msk_config\u0026#34; { 60 name = \u0026#34;${local.name}-msk-configuration\u0026#34; 61 62 kafka_versions = [local.msk.version] 63 64 server_properties = \u0026lt;\u0026lt;PROPERTIES 65 auto.create.topics.enable = true 66 delete.topic.enable = true 67 log.retention.ms = ${local.msk.log_retention_ms} 68 PROPERTIES 69} Inbound Rules for MSK Cluster We need to allow access to the MSK cluster from multiple AWS resources. Specifically the VPN server needs access for monitoring/managing the cluster and topic as well as developing the producer Lambda function locally. Also, the Lambda function and Redshift cluster need access for producing and consuming messages respectively. Only the port 9098 is added to the inbound/outbound rules because client access is enabled by the IAM authentication method exclusively. Note that the security group and outbound rule of the Lambda function are created here while the Lambda function is created in a different Terraform stack. This is for ease of adding it to the inbound rule of the MSK’s security group, and later we will discuss how to make use of it with the Lambda function.\n1# integration-redshift/infra/msk.tf 2resource \u0026#34;aws_security_group\u0026#34; \u0026#34;msk\u0026#34; { 3 name = \u0026#34;${local.name}-msk-sg\u0026#34; 4 vpc_id = module.vpc.vpc_id 5 6 lifecycle { 7 create_before_destroy = true 8 } 9 10 tags = local.tags 11} 12 13resource \u0026#34;aws_security_group_rule\u0026#34; \u0026#34;msk_vpn_inbound\u0026#34; { 14 count = local.vpn.to_create ? 1 : 0 15 type = \u0026#34;ingress\u0026#34; 16 description = \u0026#34;VPN access\u0026#34; 17 security_group_id = aws_security_group.msk.id 18 protocol = \u0026#34;tcp\u0026#34; 19 from_port = 9098 20 to_port = 9098 21 source_security_group_id = aws_security_group.vpn[0].id 22} 23 24resource \u0026#34;aws_security_group_rule\u0026#34; \u0026#34;msk_lambda_inbound\u0026#34; { 25 type = \u0026#34;ingress\u0026#34; 26 description = \u0026#34;lambda access\u0026#34; 27 security_group_id = aws_security_group.msk.id 28 protocol = \u0026#34;tcp\u0026#34; 29 from_port = 9098 30 to_port = 9098 31 source_security_group_id = aws_security_group.kafka_producer_lambda.id 32} 33 34resource \u0026#34;aws_security_group_rule\u0026#34; \u0026#34;msk_redshift_inbound\u0026#34; { 35 type = \u0026#34;ingress\u0026#34; 36 description = \u0026#34;redshift access\u0026#34; 37 security_group_id = aws_security_group.msk.id 38 protocol = \u0026#34;tcp\u0026#34; 39 from_port = 9098 40 to_port = 9098 41 source_security_group_id = aws_security_group.redshift_serverless.id 42} 43 44... 45 46resource \u0026#34;aws_security_group\u0026#34; \u0026#34;kafka_producer_lambda\u0026#34; { 47 name = \u0026#34;${local.name}-lambda-msk-access\u0026#34; 48 vpc_id = module.vpc.vpc_id 49 50 lifecycle { 51 create_before_destroy = true 52 } 53 54 tags = local.tags 55} 56 57resource \u0026#34;aws_security_group_rule\u0026#34; \u0026#34;kafka_producer_lambda_msk_egress\u0026#34; { 58 type = \u0026#34;egress\u0026#34; 59 description = \u0026#34;lambda msk access\u0026#34; 60 security_group_id = aws_security_group.kafka_producer_lambda.id 61 protocol = \u0026#34;tcp\u0026#34; 62 from_port = 9098 63 to_port = 9098 64 cidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;] 65} 66 67# integration-redshift/infra/redshift.tf 68resource \u0026#34;aws_security_group\u0026#34; \u0026#34;redshift_serverless\u0026#34; { 69 name = \u0026#34;${local.name}-redshift-serverless\u0026#34; 70 vpc_id = module.vpc.vpc_id 71 72 lifecycle { 73 create_before_destroy = true 74 } 75 76 tags = local.tags 77} 78 79... 80 81resource \u0026#34;aws_security_group_rule\u0026#34; \u0026#34;redshift_msk_egress\u0026#34; { 82 type = \u0026#34;egress\u0026#34; 83 description = \u0026#34;lambda msk access\u0026#34; 84 security_group_id = aws_security_group.redshift_serverless.id 85 protocol = \u0026#34;tcp\u0026#34; 86 from_port = 9098 87 to_port = 9098 88 cidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;] 89} Redshift Serverless A namespace and workgroup are created to deploy a Redshift serverless cluster. As explained in the Redshift user guide, a namespace is a collection of database objects and users and a workgroup is a collection of compute resources.\n1# integration-redshift/infra/redshift.tf 2resource \u0026#34;aws_redshiftserverless_namespace\u0026#34; \u0026#34;namespace\u0026#34; { 3 namespace_name = \u0026#34;${local.name}-namespace\u0026#34; 4 5 admin_username = local.redshift.admin_username 6 admin_user_password = random_password.redshift_admin_pw.result 7 db_name = local.redshift.db_name 8 default_iam_role_arn = aws_iam_role.redshift_serverless_role.arn 9 iam_roles = [aws_iam_role.redshift_serverless_role.arn] 10 11 tags = local.tags 12} 13 14resource \u0026#34;aws_redshiftserverless_workgroup\u0026#34; \u0026#34;workgroup\u0026#34; { 15 namespace_name = aws_redshiftserverless_namespace.namespace.id 16 workgroup_name = \u0026#34;${local.name}-workgroup\u0026#34; 17 18 base_capacity = local.redshift.base_capacity 19 subnet_ids = module.vpc.private_subnets 20 security_group_ids = [aws_security_group.redshift_serverless.id] 21 22 tags = local.tags 23} IAM Permission for MSK Access As illustrated in the AWS documentation, we need an IAM policy that provides permission for communication with the Amazon MSK cluster. The applicable policy is added to the default IAM role of the cluster.\n1# integration-redshift/infra/redshift.tf 2resource \u0026#34;aws_iam_role\u0026#34; \u0026#34;redshift_serverless_role\u0026#34; { 3 name = \u0026#34;${local.name}-redshift-serverless-role\u0026#34; 4 5 assume_role_policy = data.aws_iam_policy_document.redshift_serverless_assume_role_policy.json 6 managed_policy_arns = [ 7 \u0026#34;arn:aws:iam::aws:policy/AmazonS3FullAccess\u0026#34;, 8 \u0026#34;arn:aws:iam::aws:policy/AWSGlueConsoleFullAccess\u0026#34;, 9 \u0026#34;arn:aws:iam::aws:policy/AmazonRedshiftFullAccess\u0026#34;, 10 \u0026#34;arn:aws:iam::aws:policy/AmazonSageMakerFullAccess\u0026#34;, 11 aws_iam_policy.msk_redshift_permission.arn 12 ] 13} 14 15data \u0026#34;aws_iam_policy_document\u0026#34; \u0026#34;redshift_serverless_assume_role_policy\u0026#34; { 16 statement { 17 actions = [\u0026#34;sts:AssumeRole\u0026#34;] 18 19 principals { 20 type = \u0026#34;Service\u0026#34; 21 identifiers = [ 22 \u0026#34;redshift.amazonaws.com\u0026#34;, 23 \u0026#34;sagemaker.amazonaws.com\u0026#34;, 24 \u0026#34;events.amazonaws.com\u0026#34;, 25 \u0026#34;scheduler.redshift.amazonaws.com\u0026#34; 26 ] 27 } 28 29 principals { 30 type = \u0026#34;AWS\u0026#34; 31 identifiers = [\u0026#34;arn:aws:iam::${data.aws_caller_identity.current.account_id}:root\u0026#34;] 32 } 33 } 34} 35 36resource \u0026#34;aws_iam_policy\u0026#34; \u0026#34;msk_redshift_permission\u0026#34; { 37 name = \u0026#34;${local.name}-msk-redshift-permission\u0026#34; 38 39 policy = jsonencode({ 40 Version = \u0026#34;2012-10-17\u0026#34; 41 Statement = [ 42 { 43 Sid = \u0026#34;PermissionOnCluster\u0026#34; 44 Action = [ 45 \u0026#34;kafka-cluster:ReadData\u0026#34;, 46 \u0026#34;kafka-cluster:DescribeTopic\u0026#34;, 47 \u0026#34;kafka-cluster:Connect\u0026#34;, 48 ] 49 Effect = \u0026#34;Allow\u0026#34; 50 Resource = [ 51 \u0026#34;arn:aws:kafka:*:${data.aws_caller_identity.current.account_id}:cluster/*/*\u0026#34;, 52 \u0026#34;arn:aws:kafka:*:${data.aws_caller_identity.current.account_id}:topic/*/*\u0026#34; 53 ] 54 }, 55 { 56 Sid = \u0026#34;PermissionOnGroups\u0026#34; 57 Action = [ 58 \u0026#34;kafka:GetBootstrapBrokers\u0026#34; 59 ] 60 Effect = \u0026#34;Allow\u0026#34; 61 Resource = \u0026#34;*\u0026#34; 62 } 63 ] 64 }) 65} Kafka Producer The resources related to the Kafka producer Lambda function are managed in a separate Terraform stack. This is because it is easier to build the relevant resources iteratively. Note the SAM CLI builds the whole Terraform stack even for a small change of code, and it wouldn’t be convenient if the entire resources are managed in the same stack.\nProducer Source The Kafka producer is created to send messages to a topic named orders where fake order data is generated using the Faker package. The Order class generates one or more fake order records by the _create _method and an order record includes order ID, order timestamp, user ID and order items. The Lambda function sends 100 records at a time followed by sleeping for 1 second. It repeats until it reaches MAX_RUN_SEC (e.g. 60) environment variable value. A Kafka message is made up of an order ID as the key and an order record as the value. Both the key and value are serialised as JSON. Note that the stable version of the kafka-python package does not support the IAM authentication method. Therefore we need to install the package from a forked repository as discussed in this GitHub issue.\n1# integration-redshift/kafka_producer/src/app.py 2import os 3import datetime 4import string 5import json 6import time 7from kafka import KafkaProducer 8from faker import Faker 9 10 11class Order: 12 def __init__(self, fake: Faker = None): 13 self.fake = fake or Faker() 14 15 def order(self): 16 rand_int = self.fake.random_int(1, 1000) 17 user_id = \u0026#34;\u0026#34;.join( 18 [string.ascii_lowercase[int(s)] if s.isdigit() else s for s in hex(rand_int)] 19 )[::-1] 20 return { 21 \u0026#34;order_id\u0026#34;: self.fake.uuid4(), 22 \u0026#34;ordered_at\u0026#34;: datetime.datetime.utcnow(), 23 \u0026#34;user_id\u0026#34;: user_id, 24 } 25 26 def items(self): 27 return [ 28 { 29 \u0026#34;product_id\u0026#34;: self.fake.random_int(1, 9999), 30 \u0026#34;quantity\u0026#34;: self.fake.random_int(1, 10), 31 } 32 for _ in range(self.fake.random_int(1, 4)) 33 ] 34 35 def create(self, num: int): 36 return [{**self.order(), **{\u0026#34;items\u0026#34;: self.items()}} for _ in range(num)] 37 38 39class Producer: 40 def __init__(self, bootstrap_servers: list, topic: str): 41 self.bootstrap_servers = bootstrap_servers 42 self.topic = topic 43 self.producer = self.create() 44 45 def create(self): 46 return KafkaProducer( 47 security_protocol=\u0026#34;SASL_SSL\u0026#34;, 48 sasl_mechanism=\u0026#34;AWS_MSK_IAM\u0026#34;, 49 bootstrap_servers=self.bootstrap_servers, 50 value_serializer=lambda v: json.dumps(v, default=self.serialize).encode(\u0026#34;utf-8\u0026#34;), 51 key_serializer=lambda v: json.dumps(v, default=self.serialize).encode(\u0026#34;utf-8\u0026#34;), 52 ) 53 54 def send(self, orders: list): 55 for order in orders: 56 self.producer.send(self.topic, key={\u0026#34;order_id\u0026#34;: order[\u0026#34;order_id\u0026#34;]}, value=order) 57 self.producer.flush() 58 59 def serialize(self, obj): 60 if isinstance(obj, datetime.datetime): 61 return obj.isoformat() 62 if isinstance(obj, datetime.date): 63 return str(obj) 64 return obj 65 66 67def lambda_function(event, context): 68 if os.getenv(\u0026#34;BOOTSTRAP_SERVERS\u0026#34;, \u0026#34;\u0026#34;) == \u0026#34;\u0026#34;: 69 return 70 fake = Faker() 71 producer = Producer( 72 bootstrap_servers=os.getenv(\u0026#34;BOOTSTRAP_SERVERS\u0026#34;).split(\u0026#34;,\u0026#34;), topic=os.getenv(\u0026#34;TOPIC_NAME\u0026#34;) 73 ) 74 s = datetime.datetime.now() 75 ttl_rec = 0 76 while True: 77 orders = Order(fake).create(100) 78 producer.send(orders) 79 ttl_rec += len(orders) 80 print(f\u0026#34;sent {len(orders)} messages\u0026#34;) 81 elapsed_sec = (datetime.datetime.now() - s).seconds 82 if elapsed_sec \u0026gt; int(os.getenv(\u0026#34;MAX_RUN_SEC\u0026#34;, \u0026#34;60\u0026#34;)): 83 print(f\u0026#34;{ttl_rec} records are sent in {elapsed_sec} seconds ...\u0026#34;) 84 break 85 time.sleep(1) A sample order record is shown below.\n1{ 2 \u0026#34;order_id\u0026#34;: \u0026#34;fc72ccf4-8e98-42b1-9a48-4a6222996be4\u0026#34;, 3 \u0026#34;ordered_at\u0026#34;: \u0026#34;2023-02-05T04:30:58.722158\u0026#34;, 4 \u0026#34;user_id\u0026#34;: \u0026#34;hfbxa\u0026#34;, 5 \u0026#34;items\u0026#34;: [ 6 { 7 \u0026#34;product_id\u0026#34;: 8576, 8 \u0026#34;quantity\u0026#34;: 5 9 }, 10 { 11 \u0026#34;product_id\u0026#34;: 3101, 12 \u0026#34;quantity\u0026#34;: 8 13 } 14 ] 15} Lambda Function As the VPC, subnets, Lambda security group and MSK cluster are created in the infra Terraform stack, they need to be obtained from the producer Lambda stack. It can be achieved using the Terraform data sources as shown below. Note that the private subnets can be filtered by a specific tag (Tier: Private), which is added while creating them.\n1# integration-redshift/infra/vpc.tf 2module \u0026#34;vpc\u0026#34; { 3 source = \u0026#34;terraform-aws-modules/vpc/aws\u0026#34; 4 version = \u0026#34;~\u0026gt; 3.14\u0026#34; 5 6 name = \u0026#34;${local.name}-vpc\u0026#34; 7 cidr = local.vpc.cidr 8 9 azs = local.vpc.azs 10 public_subnets = [for k, v in local.vpc.azs : cidrsubnet(local.vpc.cidr, 3, k)] 11 private_subnets = [for k, v in local.vpc.azs : cidrsubnet(local.vpc.cidr, 3, k + 3)] 12 ... 13 14 private_subnet_tags = { 15 \u0026#34;Tier\u0026#34; = \u0026#34;Private\u0026#34; 16 } 17 18 tags = local.tags 19} 20 21# integration-redshift/kafka_producer/variables.tf 22data \u0026#34;aws_caller_identity\u0026#34; \u0026#34;current\u0026#34; {} 23 24data \u0026#34;aws_region\u0026#34; \u0026#34;current\u0026#34; {} 25 26data \u0026#34;aws_vpc\u0026#34; \u0026#34;selected\u0026#34; { 27 filter { 28 name = \u0026#34;tag:Name\u0026#34; 29 values = [\u0026#34;${local.infra_prefix}\u0026#34;] 30 } 31} 32 33data \u0026#34;aws_subnets\u0026#34; \u0026#34;private\u0026#34; { 34 filter { 35 name = \u0026#34;vpc-id\u0026#34; 36 values = [data.aws_vpc.selected.id] 37 } 38 39 tags = { 40 Tier = \u0026#34;Private\u0026#34; 41 } 42} 43 44data \u0026#34;aws_msk_cluster\u0026#34; \u0026#34;msk_data_cluster\u0026#34; { 45 cluster_name = \u0026#34;${local.infra_prefix}-msk-cluster\u0026#34; 46} 47 48data \u0026#34;aws_security_group\u0026#34; \u0026#34;kafka_producer_lambda\u0026#34; { 49 name = \u0026#34;${local.infra_prefix}-lambda-msk-access\u0026#34; 50} 51locals { 52 ... 53 infra_prefix = \u0026#34;integration-redshift\u0026#34; 54 ... 55} The AWS Lambda Terraform module is used to create the producer Lambda function. Note that, in order to develop a Lambda function using AWS SAM, we need to create sam metadata resource, which provides the AWS SAM CLI with the information it needs to locate Lambda functions and layers, along with their source code, build dependencies, and build logic from within your Terraform project. It is created by default by the Terraform module, which is convenient. Also, we need to give permission to the EventBridge rule to invoke the Lambda function, and it is given by the aws_lambda_permission resource.\n1# integration-redshift/kafka_producer/variables.tf 2locals { 3 name = local.infra_prefix 4 region = data.aws_region.current.name 5 environment = \u0026#34;dev\u0026#34; 6 7 infra_prefix = \u0026#34;integration-redshift\u0026#34; 8 9 producer = { 10 src_path = \u0026#34;src\u0026#34; 11 function_name = \u0026#34;kafka_producer\u0026#34; 12 handler = \u0026#34;app.lambda_function\u0026#34; 13 concurrency = 5 14 timeout = 90 15 memory_size = 128 16 runtime = \u0026#34;python3.8\u0026#34; 17 schedule_rate = \u0026#34;rate(1 minute)\u0026#34; 18 to_enable_trigger = false 19 environment = { 20 topic_name = \u0026#34;orders\u0026#34; 21 max_run_sec = 60 22 } 23 } 24 ... 25} 26 27# integration-redshift/kafka_producer/main.tf 28module \u0026#34;kafka_producer_lambda\u0026#34; { 29 source = \u0026#34;terraform-aws-modules/lambda/aws\u0026#34; 30 31 function_name = local.producer.function_name 32 handler = local.producer.handler 33 runtime = local.producer.runtime 34 timeout = local.producer.timeout 35 memory_size = local.producer.memory_size 36 source_path = local.producer.src_path 37 vpc_subnet_ids = data.aws_subnets.private.ids 38 vpc_security_group_ids = [data.aws_security_group.kafka_producer_lambda.id] 39 attach_network_policy = true 40 attach_policies = true 41 policies = [aws_iam_policy.msk_lambda_permission.arn] 42 number_of_policies = 1 43 environment_variables = { 44 BOOTSTRAP_SERVERS = data.aws_msk_cluster.msk_data_cluster.bootstrap_brokers_sasl_iam 45 TOPIC_NAME = local.producer.environment.topic_name 46 MAX_RUN_SEC = local.producer.environment.max_run_sec 47 } 48 49 tags = local.tags 50} 51 52resource \u0026#34;aws_lambda_function_event_invoke_config\u0026#34; \u0026#34;kafka_producer_lambda\u0026#34; { 53 function_name = module.kafka_producer_lambda.lambda_function_name 54 maximum_retry_attempts = 0 55} 56 57resource \u0026#34;aws_lambda_permission\u0026#34; \u0026#34;allow_eventbridge\u0026#34; { 58 count = local.producer.to_enable_trigger ? 1 : 0 59 statement_id = \u0026#34;InvokeLambdaFunction\u0026#34; 60 action = \u0026#34;lambda:InvokeFunction\u0026#34; 61 function_name = local.producer.function_name 62 principal = \u0026#34;events.amazonaws.com\u0026#34; 63 source_arn = module.eventbridge.eventbridge_rule_arns[\u0026#34;crons\u0026#34;] 64 65 depends_on = [ 66 module.eventbridge 67 ] 68} IAM Permission for MSK The producer Lambda function needs permission to send messages to the orders topic of the MSK cluster. The following IAM policy is added to the Lambda function according to the AWS documentation.\n1# integration-redshift/kafka_producer/main.tf 2resource \u0026#34;aws_iam_policy\u0026#34; \u0026#34;msk_lambda_permission\u0026#34; { 3 name = \u0026#34;${local.name}-msk-lambda-permission\u0026#34; 4 5 policy = jsonencode({ 6 Version = \u0026#34;2012-10-17\u0026#34; 7 Statement = [ 8 { 9 Sid = \u0026#34;PermissionOnCluster\u0026#34; 10 Action = [ 11 \u0026#34;kafka-cluster:Connect\u0026#34;, 12 \u0026#34;kafka-cluster:AlterCluster\u0026#34;, 13 \u0026#34;kafka-cluster:DescribeCluster\u0026#34; 14 ] 15 Effect = \u0026#34;Allow\u0026#34; 16 Resource = \u0026#34;arn:aws:kafka:${local.region}:${data.aws_caller_identity.current.account_id}:cluster/${local.infra_prefix}-msk-cluster/*\u0026#34; 17 }, 18 { 19 Sid = \u0026#34;PermissionOnTopics\u0026#34; 20 Action = [ 21 \u0026#34;kafka-cluster:*Topic*\u0026#34;, 22 \u0026#34;kafka-cluster:WriteData\u0026#34;, 23 \u0026#34;kafka-cluster:ReadData\u0026#34; 24 ] 25 Effect = \u0026#34;Allow\u0026#34; 26 Resource = \u0026#34;arn:aws:kafka:${local.region}:${data.aws_caller_identity.current.account_id}:topic/${local.infra_prefix}-msk-cluster/*\u0026#34; 27 }, 28 { 29 Sid = \u0026#34;PermissionOnGroups\u0026#34; 30 Action = [ 31 \u0026#34;kafka-cluster:AlterGroup\u0026#34;, 32 \u0026#34;kafka-cluster:DescribeGroup\u0026#34; 33 ] 34 Effect = \u0026#34;Allow\u0026#34; 35 Resource = \u0026#34;arn:aws:kafka:${local.region}:${data.aws_caller_identity.current.account_id}:group/${local.infra_prefix}-msk-cluster/*\u0026#34; 36 } 37 ] 38 }) 39} EventBridge Rule The AWS EventBridge Terraform module is used to create the EventBridge schedule rule and targets. Note that 5 targets that point to the Kafka producer Lambda function are created so that it is invoked concurrently every minute.\n1# integration-redshift/kafka_producer/main.tf 2module \u0026#34;eventbridge\u0026#34; { 3 source = \u0026#34;terraform-aws-modules/eventbridge/aws\u0026#34; 4 5 create_bus = false 6 7 rules = { 8 crons = { 9 description = \u0026#34;Kafka producer lambda schedule\u0026#34; 10 schedule_expression = local.producer.schedule_rate 11 } 12 } 13 14 targets = { 15 crons = [for i in range(local.producer.concurrency) : { 16 name = \u0026#34;lambda-target-${i}\u0026#34; 17 arn = module.kafka_producer_lambda.lambda_function_arn 18 }] 19 } 20 21 depends_on = [ 22 module.kafka_producer_lambda 23 ] 24 25 tags = local.tags 26} Deployment Topic Creation We first need to create the Kafka topic and it is done using kafka-ui. The UI can be started using docker-compose with the following compose file. Note the VPN connection has to be established in order to access the cluster from the developer machine.\n1# integration-redshift/docker-compose.yml 2version: \u0026#34;3\u0026#34; 3 4services: 5 kafka-ui: 6 image: provectuslabs/kafka-ui:master 7 container_name: kafka-ui 8 ports: 9 - \u0026#34;8080:8080\u0026#34; 10 networks: 11 - kafkanet 12 environment: 13 AWS_ACCESS_KEY_ID: $AWS_ACCESS_KEY_ID 14 AWS_SECRET_ACCESS_KEY: $AWS_SECRET_ACCESS_KEY 15 AWS_SESSION_TOKEN: $AWS_SESSION_TOKEN 16 KAFKA_CLUSTERS_0_NAME: msk 17 KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: $BOOTSTRAP_SERVERS 18 KAFKA_CLUSTERS_0_PROPERTIES_SECURITY_PROTOCOL: SASL_SSL 19 KAFKA_CLUSTERS_0_PROPERTIES_SASL_MECHANISM: AWS_MSK_IAM 20 KAFKA_CLUSTERS_0_PROPERTIES_SASL_CLIENT_CALLBACK_HANDLER_CLASS: software.amazon.msk.auth.iam.IAMClientCallbackHandler 21 KAFKA_CLUSTERS_0_PROPERTIES_SASL_JAAS_CONFIG: software.amazon.msk.auth.iam.IAMLoginModule required; 22 23networks: 24 kafkanet: 25 name: kafka-network A topic named orders is created that has 3 partitions and replication factors. Also, it is set to retain data for 4 weeks.\nOnce created, it redirects to the overview section of the topic.\nLocal Testing with SAM To simplify development, the Eventbridge permission is disabled by setting to_enable_trigger to false. Also, it is shortened to loop before it gets stopped by reducing msx_run_sec to 10.\n1# integration-redshift/kafka_producer/variables.tf 2locals { 3 producer = { 4 ... 5 to_enable_trigger = false 6 environment = { 7 topic_name = \u0026#34;orders\u0026#34; 8 max_run_sec = 10 9 } 10 } 11 ... 12} The Lambda function can be built with the SAM build command while specifying the hook name as terraform and enabling beta features. Once completed, it stores the build artifacts and template in the .aws-sam folder.\n1$ sam build --hook-name terraform --beta-features 2# ... 3# 4# Apply complete! ... 5 6# Build Succeeded 7 8# Built Artifacts : .aws-sam/build 9# Built Template : .aws-sam/build/template.yaml 10 11# Commands you can use next 12# ========================= 13# [*] Invoke Function: sam local invoke --hook-name terraform 14# [*] Emulate local Lambda functions: sam local start-lambda --hook-name terraform We can invoke the Lambda function locally using the SAM local invoke command. The Lambda function is invoked in a Docker container and the invocation logs are printed in the terminal as shown below.\n1$ sam local invoke --hook-name terraform module.kafka_producer_lambda.aws_lambda_function.this[0] --beta-features 2# Experimental features are enabled for this session. 3# Visit the docs page to learn more about the AWS Beta terms https://aws.amazon.com/service-terms/. 4 5# Skipped prepare hook. Current application is already prepared. 6# Invoking app.lambda_function (python3.8) 7# Skip pulling image and use local one: public.ecr.aws/sam/emulation-python3.8:rapid-1.70.0-x86_64. 8 9# Mounting .../kafka-pocs/integration-redshift/kafka_producer/.aws-sam/build/ModuleKafkaProducerLambdaAwsLambdaFunctionThis069E06354 as /var/task:ro,delegated inside runtime container 10# START RequestId: fbfc11be-362a-48df-b894-232cc88234ee Version: $LATEST 11# sent 100 messages 12# sent 100 messages 13# sent 100 messages 14# sent 100 messages 15# sent 100 messages 16# sent 100 messages 17# sent 100 messages 18# sent 100 messages 19# sent 100 messages 20# sent 100 messages 21# sent 100 messages 22# 1100 records are sent in 11 seconds ... 23# END RequestId: fbfc11be-362a-48df-b894-232cc88234ee 24# REPORT RequestId: fbfc11be-362a-48df-b894-232cc88234ee Init Duration: 0.18 ms Duration: 12494.86 ms Billed Duration: 12495 ms Memory Size: 128 MB Max Memory Used: 128 MB 25# null We can also check the messages using kafka-ui.\nExternal Schema and Materialized View Creation As we have messages in the orders topic, we can create a materialized view to consume data from it. First we need to create an external schema that sources data from the MSK cluster. We can use the default IAM role as we have already added the necessary IAM permission to it. Also, we should specify the IAM authentication method as it is the only allowed method for the MSK cluster. The materialized view selects key Kafka configuration variables and parses the Kafka value as data. The _JSON_PARSE _function converts the JSON string into the SUPER type, which makes it easy to select individual attributes. Also, it is configured to refresh automatically so that it ingests up-to-date data without manual refresh.\n1CREATE EXTERNAL SCHEMA msk_orders 2FROM MSK 3IAM_ROLE default 4AUTHENTICATION iam 5CLUSTER_ARN \u0026#39;\u0026lt;MSK Cluster ARN\u0026gt;\u0026#39;; 6 7CREATE MATERIALIZED VIEW orders AUTO REFRESH YES AS 8SELECT 9 \u0026#34;kafka_partition\u0026#34;, 10 \u0026#34;kafka_offset\u0026#34;, 11 \u0026#34;kafka_timestamp_type\u0026#34;, 12 \u0026#34;kafka_timestamp\u0026#34;, 13 \u0026#34;kafka_key\u0026#34;, 14 JSON_PARSE(\u0026#34;kafka_value\u0026#34;) as data, 15 \u0026#34;kafka_headers\u0026#34; 16FROM msk_orders.orders; We can see the ingested Kafka messages as shown below.\nOrder Items View Creation The materialized view keeps the entire order data in a single column, and it is not easy to build queries for analytics. As mentioned earlier, we can easily select individual attributes from the data column, but the issue is each record has an array of order items that has a variable length. Redshift doesn’t have a function to explode an array into rows, but we can achieve it using a recursive CTE. Below shows a view that converts order items array into rows recursively.\n1CREATE OR REPLACE VIEW order_items AS 2 WITH RECURSIVE exploded_items (order_id, ordered_at, user_id, idx, product_id, quantity) AS ( 3 WITH cte AS ( 4 SELECT 5 data.order_id::character(36) AS order_id, 6 data.ordered_at::timestamp AS ordered_at, 7 data.user_id::varchar(10) AS user_id, 8 data.items AS items, 9 get_array_length(data.items) AS num_items 10 FROM orders 11 ) 12 SELECT 13 order_id, 14 ordered_at, 15 user_id, 16 0 AS idx, 17 items[0].product_id::int AS product_id, 18 items[0].quantity::int AS quantity 19 FROM cte 20 UNION ALL 21 SELECT 22 cte.order_id, 23 cte.ordered_at, 24 cte.user_id, 25 idx + 1, 26 cte.items[idx + 1].product_id::int, 27 cte.items[idx + 1].quantity::int 28 FROM cte 29 JOIN exploded_items ON cte.order_id = exploded_items.order_id 30 WHERE idx \u0026lt; cte.num_items - 1 31 ) 32 SELECT * 33 FROM exploded_items 34 ORDER BY order_id; We can see the exploded order items as shown below.\nKafka Producer Deployment Now we can deploy the Kafka producer Lambda function and EventBridge scheduler using Terraform as usual after resetting the configuration variables. Once deployed, we can see that the scheduler rule has 5 targets of the same Lambda function.\nWe can check if the Kafka producer sends messages correctly using kafka-ui. After about 30 minutes, we see about 840,000 messages are created in the orders topic.\nQuery Order Items As the materialized view is set to refresh automatically, we don’t have to refresh it manually. Using the order items view, we can query the top 10 popular products as shown below.\nSummary Streaming ingestion from Kafka (MSK) into Redshift and Athena can be much simpler as they now support direct integration. In part 1 of this series, we discussed an end-to-end streaming ingestion solution using EventBridge, Lambda, MSK and Redshift. We also used AWS SAM integrated with Terraform for developing a Lambda function locally.\n","date":"February 8, 2023","img":"/blog/2023-02-08-simplify-streaming-ingestion-redshift/featured.png","lang":"en","langName":"English","largeImg":"/blog/2023-02-08-simplify-streaming-ingestion-redshift/featured_hu77abd5c8ed84cebc51bde5ecaefc7320_32864_500x0_resize_box_3.png","permalink":"/blog/2023-02-08-simplify-streaming-ingestion-redshift/","series":[{"title":"Simplify Streaming Ingestion on AWS","url":"/series/simplify-streaming-ingestion-on-aws/"}],"smallImg":"/blog/2023-02-08-simplify-streaming-ingestion-redshift/featured_hu77abd5c8ed84cebc51bde5ecaefc7320_32864_180x0_resize_box_3.png","tags":[{"title":"AWS","url":"/tags/aws/"},{"title":"Amazon MSK","url":"/tags/amazon-msk/"},{"title":"Amazon Redshift","url":"/tags/amazon-redshift/"},{"title":"Amazon EventBridge","url":"/tags/amazon-eventbridge/"},{"title":"AWS Lambda","url":"/tags/aws-lambda/"},{"title":"AWS SAM","url":"/tags/aws-sam/"},{"title":"Apache Kafka","url":"/tags/apache-kafka/"},{"title":"Python","url":"/tags/python/"},{"title":"Terraform","url":"/tags/terraform/"}],"timestamp":1675814400,"title":"Simplify Streaming Ingestion on AWS – Part 1 MSK and Redshift"},{"categories":[{"title":"Data Engineering","url":"/categories/data-engineering/"}],"content":"Normally we consume Kafka messages from the beginning/end of a topic or last committed offsets. For backfilling or troubleshooting, however, we need to consume messages from a certain timestamp occasionally. The Kafka consumer class of the kafka-python package has a method to seek a particular offset for a topic partition. Therefore, if we know which topic partition to choose e.g. by assigning a topic partition, we can easily override the fetch offset. When we deploy multiple consumer instances together, however, we make them subscribe to a topic and topic partitions are dynamically assigned, which means we do not know which topic partition will be assigned to a consumer instance in advance. In this post, we will discuss how to configure the Kafka consumer to seek offsets by timestamp where topic partitions are dynamically assigned by subscription.\nKafka Docker Environment A single node Kafka cluster is created as a docker-compose service with Zookeeper, which is used to store the cluster metadata. Note that the Kafka and Zookeeper data directories are mapped to host directories so that Kafka topics and messages are preserved when the services are restarted. As discussed below, fake messages are published into a Kafka topic by a producer application, and it runs outside the docker network (kafkanet). In order for the producer to access the Kafka cluster, we need to add an external listener, and it is configured on port 9093. Finally, the Kafka UI is added for monitoring the Kafka broker and related resources. The source can be found in the GitHub repository for this post.\n1# offset-seeking/compose-kafka.yml 2version: \u0026#34;3\u0026#34; 3 4services: 5 zookeeper: 6 image: bitnami/zookeeper:3.7.0 7 container_name: zookeeper 8 ports: 9 - \u0026#34;2181:2181\u0026#34; 10 networks: 11 - kafkanet 12 environment: 13 - ALLOW_ANONYMOUS_LOGIN=yes 14 volumes: 15 - ./.bitnami/zookeeper/data:/bitnami/zookeeper/data 16 kafka: 17 image: bitnami/kafka:2.8.1 18 container_name: kafka 19 expose: 20 - 9092 21 ports: 22 - \u0026#34;9093:9093\u0026#34; 23 networks: 24 - kafkanet 25 environment: 26 - ALLOW_PLAINTEXT_LISTENER=yes 27 - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181 28 - KAFKA_CFG_BROKER_ID=0 29 - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=CLIENT:PLAINTEXT,EXTERNAL:PLAINTEXT 30 - KAFKA_CFG_LISTENERS=CLIENT://:9092,EXTERNAL://:9093 31 - KAFKA_CFG_ADVERTISED_LISTENERS=CLIENT://kafka:9092,EXTERNAL://localhost:9093 32 - KAFKA_INTER_BROKER_LISTENER_NAME=CLIENT 33 volumes: 34 - ./.bitnami/kafka/data:/bitnami/kafka/data 35 - ./.bitnami/kafka/logs:/opt/bitnami/kafka/logs 36 depends_on: 37 - zookeeper 38 kafka-ui: 39 image: provectuslabs/kafka-ui:master 40 container_name: kafka-ui 41 ports: 42 - \u0026#34;8080:8080\u0026#34; 43 networks: 44 - kafkanet 45 environment: 46 KAFKA_CLUSTERS_0_NAME: local 47 KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:9092 48 KAFKA_CLUSTERS_0_ZOOKEEPER: zookeeper:2181 49 depends_on: 50 - zookeeper 51 - kafka 52 53networks: 54 kafkanet: 55 name: kafka-network Before we start the services, we need to create the directories that are used for volume-mapping and to update their permission. Then the services can be started as usual. A Kafka topic having two partitions is used in this post, and it is created manually as it is different from the default configuration.\n1# create folders that will be volume-mapped and update permission 2$ mkdir -p .bitnami/zookeeper/data .bitnami/kafka/data .bitnami/kafka/logs \\ 3 \u0026amp;\u0026amp; chmod 777 -R .bitnami 4 5# start docker services - zookeeper, kafka and kafka-ui 6$ docker-compose -f compose-kafka.yml up -d 7 8# create a topic named orders with 2 partitions 9$ docker exec -it kafka \\ 10 bash -c \u0026#34;/opt/bitnami/kafka/bin/kafka-topics.sh \\ 11 --create --topic orders --partitions 2 --bootstrap-server kafka:9092\u0026#34; The topic can be checked in the Kafka UI as shown below.\nKafka Producer A Kafka producer is created to send messages to the orders topic and fake messages are generated using the Faker package.\nOrder Data The Order class generates one or more fake order records by the create method. An order record includes order ID, order timestamp, customer and order items.\n1# offset-seeking/producer.py 2class Order: 3 def __init__(self, fake: Faker = None): 4 self.fake = fake or Faker() 5 6 def order(self): 7 return {\u0026#34;order_id\u0026#34;: self.fake.uuid4(), \u0026#34;ordered_at\u0026#34;: self.fake.date_time_this_decade()} 8 9 def items(self): 10 return [ 11 {\u0026#34;product_id\u0026#34;: self.fake.uuid4(), \u0026#34;quantity\u0026#34;: self.fake.random_int(1, 10)} 12 for _ in range(self.fake.random_int(1, 4)) 13 ] 14 15 def customer(self): 16 name = self.fake.name() 17 email = f\u0026#39;{re.sub(\u0026#34; \u0026#34;, \u0026#34;_\u0026#34;, name.lower())}@{re.sub(r\u0026#34;^.*?@\u0026#34;, \u0026#34;\u0026#34;, self.fake.email())}\u0026#39; 18 return { 19 \u0026#34;user_id\u0026#34;: self.fake.uuid4(), 20 \u0026#34;name\u0026#34;: name, 21 \u0026#34;dob\u0026#34;: self.fake.date_of_birth(), 22 \u0026#34;address\u0026#34;: self.fake.address(), 23 \u0026#34;phone\u0026#34;: self.fake.phone_number(), 24 \u0026#34;email\u0026#34;: email, 25 } 26 27 def create(self, num: int): 28 return [ 29 {**self.order(), **{\u0026#34;items\u0026#34;: self.items(), \u0026#34;customer\u0026#34;: self.customer()}} 30 for _ in range(num) 31 ] A sample order record is shown below.\n1{ 2 \u0026#34;order_id\u0026#34;: \u0026#34;567b3036-9ac4-440c-8849-ba4d263796db\u0026#34;, 3 \u0026#34;ordered_at\u0026#34;: \u0026#34;2022-11-09T21:24:55\u0026#34;, 4 \u0026#34;items\u0026#34;: [ 5 { 6 \u0026#34;product_id\u0026#34;: \u0026#34;7289ca92-eabf-4ebc-883c-530e16ecf9a3\u0026#34;, 7 \u0026#34;quantity\u0026#34;: 7 8 }, 9 { 10 \u0026#34;product_id\u0026#34;: \u0026#34;2ab8a155-bb15-4550-9ade-44d0bf2c730a\u0026#34;, 11 \u0026#34;quantity\u0026#34;: 5 12 }, 13 { 14 \u0026#34;product_id\u0026#34;: \u0026#34;81538fa2-6bc0-4903-a40f-a9303e5d3583\u0026#34;, 15 \u0026#34;quantity\u0026#34;: 3 16 } 17 ], 18 \u0026#34;customer\u0026#34;: { 19 \u0026#34;user_id\u0026#34;: \u0026#34;9a18e5f0-62eb-4b50-ae12-9f6f1bd1a80b\u0026#34;, 20 \u0026#34;name\u0026#34;: \u0026#34;David Boyle\u0026#34;, 21 \u0026#34;dob\u0026#34;: \u0026#34;1965-11-25\u0026#34;, 22 \u0026#34;address\u0026#34;: \u0026#34;8128 Whitney Branch\\nNorth Brianmouth, MD 24870\u0026#34;, 23 \u0026#34;phone\u0026#34;: \u0026#34;843-345-1004\u0026#34;, 24 \u0026#34;email\u0026#34;: \u0026#34;david_boyle@example.org\u0026#34; 25 } 26} Kafka Producer The Kafka producer sends one or more order records. A message is made up of an order ID as the key and an order record as the value. Both the key and value are serialised as JSON. Once started, it sends order messages to the topic indefinitely and ten messages are sent in a loop. Note that the external listener (localhost:9093) is specified as the bootstrap server because it runs outside the docker network. We can run the producer app simply by python producer.py.\n1# offset-seeking/producer.py 2class Producer: 3 def __init__(self, bootstrap_servers: list, topic: str): 4 self.bootstrap_servers = bootstrap_servers 5 self.topic = topic 6 self.producer = self.create() 7 8 def create(self): 9 return KafkaProducer( 10 bootstrap_servers=self.bootstrap_servers, 11 value_serializer=lambda v: json.dumps(v, default=self.serialize).encode(\u0026#34;utf-8\u0026#34;), 12 key_serializer=lambda v: json.dumps(v, default=self.serialize).encode(\u0026#34;utf-8\u0026#34;), 13 ) 14 15 def send(self, orders: list): 16 for order in orders: 17 self.producer.send(self.topic, key={\u0026#34;order_id\u0026#34;: order[\u0026#34;order_id\u0026#34;]}, value=order) 18 self.producer.flush() 19 20 def serialize(self, obj): 21 if isinstance(obj, datetime.datetime): 22 return obj.isoformat() 23 if isinstance(obj, datetime.date): 24 return str(obj) 25 return obj 26 27 28if __name__ == \u0026#34;__main__\u0026#34;: 29 fake = Faker() 30 # Faker.seed(1237) 31 producer = Producer(bootstrap_servers=[\u0026#34;localhost:9093\u0026#34;], topic=\u0026#34;orders\u0026#34;) 32 33 while True: 34 orders = Order(fake).create(10) 35 producer.send(orders) 36 print(\u0026#34;messages sent...\u0026#34;) 37 time.sleep(5) After a while, we can see that messages are sent to the orders topic. Out of 2390 messages, 1179 and 1211 messages are sent to the partition 0 and 1 respectively.\nKafka Consumer Two consumer instances are deployed in the same consumer group. As the topic has two partitions, it is expected each instance is assigned to a single topic partition. A custom consumer rebalance listener is registered so that the fetch offset is overridden with an offset timestamp environment variable (offset_str) when a topic partition is assigned.\nCustom Consumer Rebalance Listener The consumer rebalancer listener is a callback interface that custom actions can be implemented when topic partitions are assigned or revoked. For each topic partition assigned, it obtains the earliest offset whose timestamp is greater than or equal to the given timestamp in the corresponding partition using the offsets_for_times method. Then it overrides the fetch offset using the seek method. Note that, as consumer instances can be rebalanced multiple times over time, the OFFSET_STR value is better to be stored in an external configuration store. In this way we can control whether to override fetch offsets by changing configuration externally.\n1# offset-seeking/consumer.py 2class RebalanceListener(ConsumerRebalanceListener): 3 def __init__(self, consumer: KafkaConsumer, offset_str: str = None): 4 self.consumer = consumer 5 self.offset_str = offset_str 6 7 def on_partitions_revoked(self, revoked): 8 pass 9 10 def on_partitions_assigned(self, assigned): 11 ts = self.convert_to_ts(self.offset_str) 12 logging.info(f\u0026#34;offset_str - {self.offset_str}, timestamp - {ts}\u0026#34;) 13 if ts is not None: 14 for tp in assigned: 15 logging.info(f\u0026#34;topic partition - {tp}\u0026#34;) 16 self.seek_by_timestamp(tp.topic, tp.partition, ts) 17 18 def convert_to_ts(self, offset_str: str): 19 try: 20 dt = datetime.datetime.fromisoformat(offset_str) 21 return int(dt.timestamp() * 1000) 22 except Exception: 23 return None 24 25 def seek_by_timestamp(self, topic_name: str, partition: int, ts: int): 26 tp = TopicPartition(topic_name, partition) 27 offset_n_ts = self.consumer.offsets_for_times({tp: ts}) 28 logging.info(f\u0026#34;offset and ts - {offset_n_ts}\u0026#34;) 29 if offset_n_ts[tp] is not None: 30 offset = offset_n_ts[tp].offset 31 try: 32 self.consumer.seek(tp, offset) 33 except KafkaError: 34 logging.error(\u0026#34;fails to seek offset\u0026#34;) 35 else: 36 logging.warning(\u0026#34;offset is not looked up\u0026#34;) Kafka Consumer While it is a common practice to specify one or more Kafka topics in the Kafka consumer class when it is instantiated, the consumer omits them in the create method. It is in order to register the custom rebalance listener. In the process method, the consumer subscribes to the orders topic while registering the custom listener. After subscribing to the topic, it polls a single message at a time for ease of tracking.\n1# offset-seeking/consumer.py 2class Consumer: 3 def __init__( 4 self, topics: list, group_id: str, bootstrap_servers: list, offset_str: str = None 5 ): 6 self.topics = topics 7 self.group_id = group_id 8 self.bootstrap_servers = bootstrap_servers 9 self.offset_str = offset_str 10 self.consumer = self.create() 11 12 def create(self): 13 return KafkaConsumer( 14 bootstrap_servers=self.bootstrap_servers, 15 auto_offset_reset=\u0026#34;earliest\u0026#34;, 16 enable_auto_commit=True, 17 group_id=self.group_id, 18 key_deserializer=lambda v: json.loads(v.decode(\u0026#34;utf-8\u0026#34;)), 19 value_deserializer=lambda v: json.loads(v.decode(\u0026#34;utf-8\u0026#34;)), 20 ) 21 22 def process(self): 23 self.consumer.subscribe( 24 self.topics, listener=RebalanceListener(self.consumer, self.offset_str) 25 ) 26 try: 27 while True: 28 msg = self.consumer.poll(timeout_ms=1000, max_records=1) 29 if msg is None: 30 continue 31 self.print_info(msg) 32 time.sleep(5) 33 except KafkaError as error: 34 logging.error(error) 35 finally: 36 self.consumer.close() 37 38 def print_info(self, msg: dict): 39 for _, v in msg.items(): 40 for r in v: 41 ts = r.timestamp 42 dt = datetime.datetime.fromtimestamp(ts / 1000).isoformat() 43 logging.info( 44 f\u0026#34;topic - {r.topic}, partition - {r.partition}, offset - {r.offset}, ts - {ts}, dt - {dt})\u0026#34; 45 ) 46 47 48if __name__ == \u0026#34;__main__\u0026#34;: 49 consumer = Consumer( 50 topics=os.getenv(\u0026#34;TOPICS\u0026#34;, \u0026#34;orders\u0026#34;).split(\u0026#34;,\u0026#34;), 51 group_id=os.getenv(\u0026#34;GROUP_ID\u0026#34;, \u0026#34;orders-group\u0026#34;), 52 bootstrap_servers=os.getenv(\u0026#34;BOOTSTRAP_SERVERS\u0026#34;, \u0026#34;localhost:9093\u0026#34;).split(\u0026#34;,\u0026#34;), 53 offset_str=os.getenv(\u0026#34;OFFSET_STR\u0026#34;, None), 54 ) 55 consumer.process() Docker-compose is used to deploy multiple instances of the producer. Note that the compose service uses the same docker network (kafkanet) so that it can use kafka:9092 as the bootstrap server address. The OFFSET_STR environment variable is used to override the fetch offset.\n1# offset-seeking/compose-consumer.yml 2version: \u0026#34;3\u0026#34; 3 4services: 5 consumer: 6 image: bitnami/python:3.9 7 command: \u0026#34;sh -c \u0026#39;pip install -r requirements.txt \u0026amp;\u0026amp; python consumer.py\u0026#39;\u0026#34; 8 networks: 9 - kafkanet 10 environment: 11 TOPICS: orders 12 GROUP_ID: orders-group 13 BOOTSTRAP_SERVERS: kafka:9092 14 OFFSET_STR: \u0026#34;2023-01-06T19:00:00\u0026#34; 15 TZ: Australia/Sydney 16 volumes: 17 - .:/app 18networks: 19 kafkanet: 20 external: true 21 name: kafka-network We can start two consumer instances by scaling the consumer service number to 2.\n1# start 2 instances of kafka consumer 2$ docker-compose -f compose-consumer.yml up -d --scale consumer=2 Soon after the instances start to poll messages, we can see that their fetch offsets are updated as the current offset values are much higher than 0.\nWe can check logs of the consumer instances in order to check their behaviour further. Below shows the logs of one of the instances.\n1# check logs of consumer instance 1 2$ docker logs offset-seeking-consumer-1 We see that the partition 1 is assigned to this instance. The offset 901 is taken to override and the message timestamp of that message is 2023-01-06T19:20:16.107000, which is later than the OFFSET_STR environment value.\nWe can also check that the correct offset is obtained as the message timestamp of offset 900 is earlier than the OFFSET_STR value.\nSummary In this post, we discussed how to configure Kafka consumers to seek offsets by timestamp. A single node Kafka cluster was created using docker compose and a Kafka producer was used to send fake order messages. While subscribing to the orders topic, the consumer registered a custom consumer rebalance listener that overrides the fetch offsets by timestamp. Two consumer instances were deployed using docker compose and their behaviour was analysed in detail.\n","date":"January 10, 2023","img":"/blog/2023-01-10-kafka-consumer-seek-offsets/featured.png","lang":"en","langName":"English","largeImg":"/blog/2023-01-10-kafka-consumer-seek-offsets/featured_hua222fe3eeaae92dfa98b24cdf7061694_47217_500x0_resize_box_3.png","permalink":"/blog/2023-01-10-kafka-consumer-seek-offsets/","series":[],"smallImg":"/blog/2023-01-10-kafka-consumer-seek-offsets/featured_hua222fe3eeaae92dfa98b24cdf7061694_47217_180x0_resize_box_3.png","tags":[{"title":"Apache Kafka","url":"/tags/apache-kafka/"},{"title":"Python","url":"/tags/python/"},{"title":"Docker","url":"/tags/docker/"},{"title":"Docker Compose","url":"/tags/docker-compose/"}],"timestamp":1673308800,"title":"How to Configure Kafka Consumers to Seek Offsets by Timestamp"},{"categories":[{"title":"Data Engineering","url":"/categories/data-engineering/"}],"content":"The data build tool (dbt) is an effective data transformation tool and it supports key AWS analytics services - Redshift, Glue, EMR and Athena. In the previous posts, we discussed benefits of a common data transformation tool and the potential of dbt to cover a wide range of data projects from data warehousing to data lake to data lakehouse. Demo data projects that target Redshift Serverless, Glue, EMR on EC2 and EMR on EKS are illustrated as well. In the last part of the dbt on AWS series, we discuss data transformation pipelines using dbt on Amazon Athena. Subsets of IMDb data are used as source and data models are developed in multiple layers according to the dbt best practices. A list of posts of this series can be found below.\nPart 1 Redshift Part 2 Glue Part 3 EMR on EC2 Part 4 EMR on EKS Part 5 Athena (this post) Below shows an overview diagram of the scope of this dbt on AWS series. Athena is highlighted as it is discussed in this post.\nInfrastructure The infrastructure hosting this solution leverages Athena Workgroup, AWS Glue Data Catalog, AWS Glue Crawlers and a S3 bucket. They are deployed using Terraform and the source can be found in the GitHub repository of this post.\nAthena Workgroup The dbt athena adapter requires an Athena workgroup and it only supports the Athena engine version 2. The workgroup used in the dbt project is created as shown below.\n1resource \u0026#34;aws_athena_workgroup\u0026#34; \u0026#34;imdb\u0026#34; { 2 name = \u0026#34;${local.name}-imdb\u0026#34; 3 4 configuration { 5 enforce_workgroup_configuration = true 6 publish_cloudwatch_metrics_enabled = false 7 8 engine_version { 9 selected_engine_version = \u0026#34;Athena engine version 2\u0026#34; 10 } 11 12 result_configuration { 13 output_location = \u0026#34;s3://${local.default_bucket.name}/athena/\u0026#34; 14 15 encryption_configuration { 16 encryption_option = \u0026#34;SSE_S3\u0026#34; 17 } 18 } 19 } 20 21 force_destroy = true 22 23 tags = local.tags 24} Glue Databases We have two Glue databases. The source tables and the tables of the staging and intermediate layers are kept in the imdb database. The tables of the marts layer are stored in the _imdb_analytics _database.\n1# athena/infra/main.tf 2resource \u0026#34;aws_glue_catalog_database\u0026#34; \u0026#34;imdb_db\u0026#34; { 3 name = \u0026#34;imdb\u0026#34; 4 description = \u0026#34;Database that contains IMDb staging/intermediate model datasets\u0026#34; 5} 6 7resource \u0026#34;aws_glue_catalog_database\u0026#34; \u0026#34;imdb_db_marts\u0026#34; { 8 name = \u0026#34;imdb_analytics\u0026#34; 9 description = \u0026#34;Database that contains IMDb marts model datasets\u0026#34; 10} Glue Crawlers We use Glue crawlers to create source tables in the imdb database. We can create a single crawler for the seven source tables but it was not satisfactory, especially header detection. Instead a dedicated crawler is created for each of the tables with its own custom classifier where it includes header columns specifically. The Terraform count meta-argument is used to create the crawlers and classifiers recursively.\n1# athena/infra/main.tf 2resource \u0026#34;aws_glue_crawler\u0026#34; \u0026#34;imdb_crawler\u0026#34; { 3 count = length(local.glue.tables) 4 5 name = local.glue.tables[count.index].name 6 database_name = aws_glue_catalog_database.imdb_db.name 7 role = aws_iam_role.imdb_crawler.arn 8 classifiers = [aws_glue_classifier.imdb_crawler[count.index].id] 9 10 s3_target { 11 path = \u0026#34;s3://${local.default_bucket.name}/${local.glue.tables[count.index].name}\u0026#34; 12 } 13 14 tags = local.tags 15} 16 17resource \u0026#34;aws_glue_classifier\u0026#34; \u0026#34;imdb_crawler\u0026#34; { 18 count = length(local.glue.tables) 19 20 name = local.glue.tables[count.index].name 21 22 csv_classifier { 23 contains_header = \u0026#34;PRESENT\u0026#34; 24 delimiter = \u0026#34;\\t\u0026#34; 25 header = local.glue.tables[count.index].header 26 } 27} 28 29# athena/infra/variables.tf 30locals { 31 name = basename(path.cwd) == \u0026#34;infra\u0026#34; ? basename(dirname(path.cwd)) : basename(path.cwd) 32 region = data.aws_region.current.name 33 environment = \u0026#34;dev\u0026#34; 34 35 default_bucket = { 36 name = \u0026#34;${local.name}-${data.aws_caller_identity.current.account_id}-${local.region}\u0026#34; 37 } 38 39 glue = { 40 tables = [ 41 { name = \u0026#34;name_basics\u0026#34;, header = [\u0026#34;nconst\u0026#34;, \u0026#34;primaryName\u0026#34;, \u0026#34;birthYear\u0026#34;, \u0026#34;deathYear\u0026#34;, \u0026#34;primaryProfession\u0026#34;, \u0026#34;knownForTitles\u0026#34;] }, 42 { name = \u0026#34;title_akas\u0026#34;, header = [\u0026#34;titleId\u0026#34;, \u0026#34;ordering\u0026#34;, \u0026#34;title\u0026#34;, \u0026#34;region\u0026#34;, \u0026#34;language\u0026#34;, \u0026#34;types\u0026#34;, \u0026#34;attributes\u0026#34;, \u0026#34;isOriginalTitle\u0026#34;] }, 43 { name = \u0026#34;title_basics\u0026#34;, header = [\u0026#34;tconst\u0026#34;, \u0026#34;titleType\u0026#34;, \u0026#34;primaryTitle\u0026#34;, \u0026#34;originalTitle\u0026#34;, \u0026#34;isAdult\u0026#34;, \u0026#34;startYear\u0026#34;, \u0026#34;endYear\u0026#34;, \u0026#34;runtimeMinutes\u0026#34;, \u0026#34;genres\u0026#34;] }, 44 { name = \u0026#34;title_crew\u0026#34;, header = [\u0026#34;tconst\u0026#34;, \u0026#34;directors\u0026#34;, \u0026#34;writers\u0026#34;] }, 45 { name = \u0026#34;title_episode\u0026#34;, header = [\u0026#34;tconst\u0026#34;, \u0026#34;parentTconst\u0026#34;, \u0026#34;seasonNumber\u0026#34;, \u0026#34;episodeNumber\u0026#34;] }, 46 { name = \u0026#34;title_principals\u0026#34;, header = [\u0026#34;tconst\u0026#34;, \u0026#34;ordering\u0026#34;, \u0026#34;nconst\u0026#34;, \u0026#34;category\u0026#34;, \u0026#34;job\u0026#34;, \u0026#34;characters\u0026#34;] }, 47 { name = \u0026#34;title_ratings\u0026#34;, header = [\u0026#34;tconst\u0026#34;, \u0026#34;averageRating\u0026#34;, \u0026#34;numVotes\u0026#34;] } 48 ] 49 } 50 51 tags = { 52 Name = local.name 53 Environment = local.environment 54 } 55} Project We build a data transformation pipeline using subsets of IMDb data - seven titles and names related datasets are provided as gzipped, tab-separated-values (TSV) formatted files. This results in three tables that can be used for reporting and analysis.\nSave Data to S3 The Axel download accelerator is used to download the data files locally followed by decompressing with the gzip utility. Note that simple retry logic is added as I see download failure from time to time. Finally, the decompressed files are saved into the project S3 bucket using the S3 sync command.\n1# athena/upload-data.sh 2#!/usr/bin/env bash 3 4s3_bucket=$(terraform -chdir=./infra output --raw default_bucket_name) 5hostname=\u0026#34;datasets.imdbws.com\u0026#34; 6declare -a file_names=( 7 \u0026#34;name.basics.tsv.gz\u0026#34; \\ 8 \u0026#34;title.akas.tsv.gz\u0026#34; \\ 9 \u0026#34;title.basics.tsv.gz\u0026#34; \\ 10 \u0026#34;title.crew.tsv.gz\u0026#34; \\ 11 \u0026#34;title.episode.tsv.gz\u0026#34; \\ 12 \u0026#34;title.principals.tsv.gz\u0026#34; \\ 13 \u0026#34;title.ratings.tsv.gz\u0026#34; 14 ) 15 16rm -rf imdb-data 17 18for fn in \u0026#34;${file_names[@]}\u0026#34; 19do 20 download_url=\u0026#34;https://$hostname/$fn\u0026#34; 21 prefix=$(echo ${fn::-7} | tr \u0026#39;.\u0026#39; \u0026#39;_\u0026#39;) 22 echo \u0026#34;download imdb-data/$prefix/$fn from $download_url\u0026#34; 23 while true; 24 do 25 mkdir -p imdb-data/$prefix 26 axel -n 32 -a -o imdb-data/$prefix/$fn $download_url 27 gzip -d imdb-data/$prefix/$fn 28 num_files=$(ls imdb-data/$prefix | wc -l) 29 if [ $num_files == 1 ]; then 30 break 31 fi 32 rm -rf imdb-data/$prefix 33 done 34done 35 36aws s3 sync ./imdb-data s3://$s3_bucket Run Glue Crawlers The Glue crawlers for the seven source tables are executed as shown below.\n1# athena/start-crawlers.sh 2#!/usr/bin/env bash 3 4declare -a crawler_names=( 5 \u0026#34;name_basics\u0026#34; \\ 6 \u0026#34;title_akas\u0026#34; \\ 7 \u0026#34;title_basics\u0026#34; \\ 8 \u0026#34;title_crew\u0026#34; \\ 9 \u0026#34;title_episode\u0026#34; \\ 10 \u0026#34;title_principals\u0026#34; \\ 11 \u0026#34;title_ratings\u0026#34; 12 ) 13 14for cn in \u0026#34;${crawler_names[@]}\u0026#34; 15do 16 echo \u0026#34;start crawler $cn ...\u0026#34; 17 aws glue start-crawler --name $cn 18done After the crawlers run successfully, we are able to check the seven source tables. Below shows a query example of one of the source tables in Athena.\nSetup dbt Project We need the dbt-core and dbt-athena-adapter packages for the main data transformation project - the former can be installed as dependency of the latter. The dbt project is initialised while skipping the connection profile as it does not allow to select key connection details such as aws_profile_name and threads. Instead the profile is created manually as shown below. Note that, after this post was complete, it announced a new project repository called dbt-athena-community and new features are planned to be supported in the new project. Those new features cover the Athena engine version 3 and Apache Iceberg support and a new dbt targeting Athena is encouraged to use it.\n1$ pip install dbt-athena-adapter 2$ dbt init --skip-profile-setup 3# 09:32:09 Running with dbt=1.3.1 4# Enter a name for your project (letters, digits, underscore): athena_proj The attributes are self-explanatory, and their details can be checked further in the GitHub repository of the dbt-athena adapter.\n1# athena/set-profile.sh 2#!/usr/bin/env bash 3 4aws_region=$(aws ec2 describe-availability-zones --output text --query \u0026#39;AvailabilityZones[0].[RegionName]\u0026#39;) 5dbt_s3_location=$(terraform -chdir=./infra output --raw default_bucket_name) 6dbt_work_group=$(terraform -chdir=./infra output --raw aws_athena_workgroup_name) 7 8cat \u0026lt;\u0026lt; EOF \u0026gt; ~/.dbt/profiles.yml 9athena_proj: 10 outputs: 11 dev: 12 type: athena 13 region_name: ${aws_region} 14 s3_staging_dir: \u0026#34;s3://${dbt_s3_location}/dbt/\u0026#34; 15 schema: imdb 16 database: awsdatacatalog 17 work_group: ${dbt_work_group} 18 threads: 3 19 aws_profile_name: \u0026lt;aws-profile\u0026gt; 20 target: dev 21EOF dbt initialises a project in a folder that matches to the project name and generates project boilerplate as shown below. Some of the main objects are dbt_project.yml, and the model folder. The former is required because dbt doesn\u0026rsquo;t know if a folder is a dbt project without it. Also it contains information that tells dbt how to operate on the project. The latter is for including dbt models, which is basically a set of SQL select statements. See dbt documentation for more details.\n1$ tree athena/athena_proj/ -L 1 2athena/athena_proj/ 3├── README.md 4├── analyses 5├── dbt_packages 6├── dbt_project.yml 7├── logs 8├── macros 9├── models 10├── packages.yml 11├── seeds 12├── snapshots 13├── target 14└── tests We can check Athena connection with the dbt debug command as shown below.\n1$ dbt debug 209:33:53 Running with dbt=1.3.1 3dbt version: 1.3.1 4python version: 3.8.10 5python path: \u0026lt;path-to-python-path\u0026gt; 6os info: Linux-5.4.72-microsoft-standard-WSL2-x86_64-with-glibc2.29 7Using profiles.yml file at /home/\u0026lt;username\u0026gt;/.dbt/profiles.yml 8Using dbt_project.yml file at \u0026lt;path-to-dbt-project\u0026gt;/dbt_project.yml 9 10Configuration: 11 profiles.yml file [OK found and valid] 12 dbt_project.yml file [OK found and valid] 13 14Required dependencies: 15 - git [OK found] 16 17Connection: 18 s3_staging_dir: s3://\u0026lt;s3-bucket-name\u0026gt;/dbt/ 19 work_group: athena-imdb 20 region_name: ap-southeast-2 21 database: imdb 22 schema: imdb 23 poll_interval: 1.0 24 aws_profile_name: \u0026lt;aws-profile\u0026gt; 25 Connection test: [OK connection ok] 26 27All checks passed! After initialisation, the model configuration is updated. The project materialisation is specified as view, although it is the default materialisation. Also tags are added to the entire model folder as well as folders of specific layers - staging, intermediate and marts. As shown below, tags can simplify model execution.\n1# athena/athena_proj/dbt_project.yml 2name: \u0026#34;dbt_glue_proj\u0026#34; 3 4... 5 6models: 7 dbt_glue_proj: 8 +materialized: view 9 +tags: 10 - \u0026#34;imdb\u0026#34; 11 staging: 12 +tags: 13 - \u0026#34;staging\u0026#34; 14 intermediate: 15 +tags: 16 - \u0026#34;intermediate\u0026#34; 17 marts: 18 +tags: 19 - \u0026#34;marts\u0026#34; The dbt_utils package is installed for adding tests to the final marts models. The packages can be installed by the dbt deps command.\n1# athena/athena_proj/packages.yml 2packages: 3 - package: dbt-labs/dbt_utils 4 version: 0.9.5 Create dbt Models The models for this post are organised into three layers according to the dbt best practices - staging, intermediate and marts.\nStaging The seven tables that are loaded from S3 are dbt source tables and their details are declared in a YAML file (_imdb_sources.yml). By doing so, we are able to refer to the source tables with the {{ source() }} function. Also we can add tests to source tables. For example below two tests (unique, not_null) are added to the tconst column of the title_basics table below and these tests can be executed by the dbt test command.\n1# athena/athena_proj/models/staging/imdb/_imdb__sources.yml 2version: 2 3 4sources: 5 - name: imdb 6 description: Subsets of IMDb data, which are available for access to customers for personal and non-commercial use 7 tables: 8 - name: title_basics 9 description: Table that contains basic information of titles 10 columns: 11 - name: tconst 12 description: alphanumeric unique identifier of the title 13 tests: 14 - unique 15 - not_null 16 - name: titletype 17 description: the type/format of the title (e.g. movie, short, tvseries, tvepisode, video, etc) 18 - name: primarytitle 19 description: the more popular title / the title used by the filmmakers on promotional materials at the point of release 20 - name: originaltitle 21 description: original title, in the original language 22 - name: isadult 23 description: flag that indicates whether it is an adult title or not 24 - name: startyear 25 description: represents the release year of a title. In the case of TV Series, it is the series start year 26 - name: endyear 27 description: TV Series end year. NULL for all other title types 28 - name: runtime minutes 29 description: primary runtime of the title, in minutes 30 - name: genres 31 description: includes up to three genres associated with the title 32 ... Based on the source tables, staging models are created. They are created as views, which is the project’s default materialisation. In the SQL statements, column names and data types are modified mainly.\n1# athena/athena_proj/models/staging/imdb/stg_imdb__title_basics.sql 2with source as ( 3 4 select * from {{ source(\u0026#39;imdb\u0026#39;, \u0026#39;title_basics\u0026#39;) }} 5 6), 7 8renamed as ( 9 10 select 11 tconst as title_id, 12 titletype as title_type, 13 primarytitle as primary_title, 14 originaltitle as original_title, 15 cast(isadult as boolean) as is_adult, 16 cast(startyear as int) as start_year, 17 cast(endyear as int) as end_year, 18 cast(runtimeminutes as int) as runtime_minutes, 19 genres 20 from source 21 22) 23 24select * from renamed Below shows the file tree of the staging models. The staging models can be executed using the dbt run command. As we’ve added tags to the staging layer models, we can limit to execute only this layer by dbt run --select staging.\n1$ tree athena/athena_proj/models/staging/ 2athena/athena_proj/models/staging/ 3└── imdb 4 ├── _imdb__models.yml 5 ├── _imdb__sources.yml 6 ├── stg_imdb__name_basics.sql 7 ├── stg_imdb__title_akas.sql 8 ├── stg_imdb__title_basics.sql 9 ├── stg_imdb__title_crews.sql 10 ├── stg_imdb__title_episodes.sql 11 ├── stg_imdb__title_principals.sql 12 └── stg_imdb__title_ratings.sql The views in the staging layer can be queried in Athena as shown below.\nIntermediate We can keep intermediate results in this layer so that the models of the final marts layer can be simplified. The source data includes columns where array values are kept as comma separated strings. For example, the genres column of the stg_imdb__title_basics model includes up to three genre values as shown in the previous screenshot. A total of seven columns in three models are columns of comma-separated strings and it is better to flatten them in the intermediate layer. Also, in order to avoid repetition, a dbt macro (f_latten_fields_) is created to share the column-flattening logic.\n1# athena/athena_proj/macros/flatten_fields.sql 2{% macro flatten_fields(model, field_name, id_field_name) %} 3 select 4 {{ id_field_name }} as id, 5 field 6 from {{ model }} 7 cross join unnest(split({{ field_name }}, \u0026#39;,\u0026#39;)) as x(field) 8{% endmacro %} The macro function can be added inside a common table expression (CTE) by specifying the relevant model, field name to flatten and ID field name.\n1-- athena/athena_proj/models/intermediate/title/int_genres_flattened_from_title_basics.sql 2with flattened as ( 3 {{ flatten_fields(ref(\u0026#39;stg_imdb__title_basics\u0026#39;), \u0026#39;genres\u0026#39;, \u0026#39;title_id\u0026#39;) }} 4) 5 6select 7 id as title_id, 8 field as genre 9from flattened 10order by id The intermediate models are also materialised as views, and we can check the array columns are flattened as expected.\nBelow shows the file tree of the intermediate models. Similar to the staging models, the intermediate models can be executed by dbt run --select intermediate.\n1$ tree athena/athena_proj/models/intermediate/ athena/athena_proj/macros/ 2athena/athena_proj/models/intermediate/ 3├── name 4│ ├── _int_name__models.yml 5│ ├── int_known_for_titles_flattened_from_name_basics.sql 6│ └── int_primary_profession_flattened_from_name_basics.sql 7└── title 8 ├── _int_title__models.yml 9 ├── int_directors_flattened_from_title_crews.sql 10 ├── int_genres_flattened_from_title_basics.sql 11 └── int_writers_flattened_from_title_crews.sql 12 13athena/athena_proj/macros/ 14└── flatten_fields.sql Marts The models in the marts layer are configured to be materialised as tables in a custom schema. Their materialisation is set to table and the custom schema is specified as analytics while taking _parquet _as the file format. Note that the custom schema name becomes imdb_analytics according to the naming convention of dbt custom schemas. Models of both the staging and intermediate layers are used to create final models to be used for reporting and analytics.\n1-- athena/athena_proj/models/marts/analytics/titles.sql 2{{ 3 config( 4 schema=\u0026#39;analytics\u0026#39;, 5 materialized=\u0026#39;table\u0026#39;, 6 file_format=\u0026#39;parquet\u0026#39; 7 ) 8}} 9 10with titles as ( 11 12 select * from {{ ref(\u0026#39;stg_imdb__title_basics\u0026#39;) }} 13 14), 15 16principals as ( 17 18 select 19 title_id, 20 count(name_id) as num_principals 21 from {{ ref(\u0026#39;stg_imdb__title_principals\u0026#39;) }} 22 group by title_id 23 24), 25 26names as ( 27 28 select 29 title_id, 30 count(name_id) as num_names 31 from {{ ref(\u0026#39;int_known_for_titles_flattened_from_name_basics\u0026#39;) }} 32 group by title_id 33 34), 35 36ratings as ( 37 38 select 39 title_id, 40 average_rating, 41 num_votes 42 from {{ ref(\u0026#39;stg_imdb__title_ratings\u0026#39;) }} 43 44), 45 46episodes as ( 47 48 select 49 parent_title_id, 50 count(title_id) as num_episodes 51 from {{ ref(\u0026#39;stg_imdb__title_episodes\u0026#39;) }} 52 group by parent_title_id 53 54), 55 56distributions as ( 57 58 select 59 title_id, 60 count(title) as num_distributions 61 from {{ ref(\u0026#39;stg_imdb__title_akas\u0026#39;) }} 62 group by title_id 63 64), 65 66final as ( 67 68 select 69 t.title_id, 70 t.title_type, 71 t.primary_title, 72 t.original_title, 73 t.is_adult, 74 t.start_year, 75 t.end_year, 76 t.runtime_minutes, 77 t.genres, 78 p.num_principals, 79 n.num_names, 80 r.average_rating, 81 r.num_votes, 82 e.num_episodes, 83 d.num_distributions 84 from titles as t 85 left join principals as p on t.title_id = p.title_id 86 left join names as n on t.title_id = n.title_id 87 left join ratings as r on t.title_id = r.title_id 88 left join episodes as e on t.title_id = e.parent_title_id 89 left join distributions as d on t.title_id = d.title_id 90 91) 92 93select * from final The details of the three models can be found in a YAML file (_analytics__models.yml). We can add tests to models and below we see tests of row count matching to their corresponding staging models.\n1# athena/athena_proj/models/marts/analytics/_analytics__models.yml 2version: 2 3 4models: 5 - name: names 6 description: Table that contains all names with additional details 7 tests: 8 - dbt_utils.equal_rowcount: 9 compare_model: ref(\u0026#39;stg_imdb__name_basics\u0026#39;) 10 - name: titles 11 description: Table that contains all titles with additional details 12 tests: 13 - dbt_utils.equal_rowcount: 14 compare_model: ref(\u0026#39;stg_imdb__title_basics\u0026#39;) 15 - name: genre_titles 16 description: Table that contains basic title details after flattening genres The models of the marts layer can be tested using the dbt test command as shown below.\n1$ dbt test --select marts 207:18:42 Running with dbt=1.3.1 307:18:43 Found 15 models, 17 tests, 0 snapshots, 0 analyses, 473 macros, 0 operations, 0 seed files, 7 sources, 0 exposures, 0 metrics 407:18:43 507:18:48 Concurrency: 3 threads (target=\u0026#39;dev\u0026#39;) 607:18:48 707:18:48 1 of 2 START test dbt_utils_equal_rowcount_names_ref_stg_imdb__name_basics_ .... [RUN] 807:18:48 2 of 2 START test dbt_utils_equal_rowcount_titles_ref_stg_imdb__title_basics_ .. [RUN] 907:18:51 2 of 2 PASS dbt_utils_equal_rowcount_titles_ref_stg_imdb__title_basics_ ........ [PASS in 2.76s] 1007:18:52 1 of 2 PASS dbt_utils_equal_rowcount_names_ref_stg_imdb__name_basics_ .......... [PASS in 3.80s] 1107:18:52 1207:18:52 Finished running 2 tests in 0 hours 0 minutes and 9.17 seconds (9.17s). 1307:18:52 1407:18:52 Completed successfully 1507:18:52 1607:18:52 Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2 Below shows the file tree of the marts models. As with the other layers, the marts models can be executed by dbt run --select marts.\n1$ tree athena/athena_proj/models/marts/ 2athena/athena_proj/models/marts/ 3└── analytics 4 ├── _analytics__models.yml 5 ├── genre_titles.sql 6 ├── names.sql 7 └── titles.sql Build Dashboard The models of the marts layer can be consumed by external tools such as Amazon QuickSight. Below shows an example dashboard. The pie chart on the left shows the proportion of titles by genre while the box plot on the right shows the dispersion of average rating by start year.\nGenerate dbt Documentation A nice feature of dbt is documentation. It provides information about the project and the data warehouse, and it facilitates consumers as well as other developers to discover and understand the datasets better. We can generate the project documents and start a document server as shown below.\n1$ dbt docs generate 2$ dbt docs serve A very useful element of dbt documentation is data lineage, which provides an overall view about how data is transformed and consumed. Below we can see that the final titles model consumes all title-related stating models and an intermediate model from the name basics staging model.\nSummary In this post, we discussed how to build data transformation pipelines using dbt on AWS Athena. Subsets of IMDb data are used as source and data models are developed in multiple layers according to the dbt best practices. dbt can be used as an effective tool for data transformation in a wide range of data projects from data warehousing to data lake to data lakehouse and it supports key AWS analytics services - Redshift, Glue, EMR and Athena. In this series, we mainly focus on how to develop a data project with dbt targeting variable AWS analytics services. It is quite an effective framework for data transformation and advanced features will be covered in a new series of posts.\n","date":"December 6, 2022","img":"/blog/2022-12-06-dbt-on-aws-part-5-athena/featured.png","lang":"en","langName":"English","largeImg":"/blog/2022-12-06-dbt-on-aws-part-5-athena/featured_hu2cb992d854f23cc962e70e6949802760_91796_500x0_resize_box_3.png","permalink":"/blog/2022-12-06-dbt-on-aws-part-5-athena/","series":[{"title":"DBT for Effective Data Transformation on AWS","url":"/series/dbt-for-effective-data-transformation-on-aws/"}],"smallImg":"/blog/2022-12-06-dbt-on-aws-part-5-athena/featured_hu2cb992d854f23cc962e70e6949802760_91796_180x0_resize_box_3.png","tags":[{"title":"AWS","url":"/tags/aws/"},{"title":"Amazon Athena","url":"/tags/amazon-athena/"},{"title":"Amazon QuickSight","url":"/tags/amazon-quicksight/"},{"title":"Data Build Tool (DBT)","url":"/tags/data-build-tool-dbt/"},{"title":"Terraform","url":"/tags/terraform/"}],"timestamp":1670284800,"title":"Data Build Tool (Dbt) for Effective Data Transformation on AWS – Part 5 Athena"},{"categories":[{"title":"Data Engineering","url":"/categories/data-engineering/"}],"content":"The data build tool (dbt) is an effective data transformation tool and it supports key AWS analytics services - Redshift, Glue, EMR and Athena. In the previous posts, we discussed benefits of a common data transformation tool and the potential of dbt to cover a wide range of data projects from data warehousing to data lake to data lakehouse. Demo data projects that target Redshift Serverless, Glue and EMR on EC2 are illustrated as well. In part 4 of the dbt on AWS series, we discuss data transformation pipelines using dbt on Amazon EMR on EKS. As Spark Submit does not allow the spark thrift server to run in cluster mode on Kubernetes, a simple wrapper class is created to overcome the limitation and it makes the thrift server run indefinitely. Subsets of IMDb data are used as source and data models are developed in multiple layers according to the dbt best practices. A list of posts of this series can be found below.\nPart 1 Redshift Part 2 Glue Part 3 EMR on EC2 Part 4 EMR on EKS (this post) Part 5 Athena Below shows an overview diagram of the scope of this dbt on AWS series. EMR is highlighted as it is discussed in this post.\nInfrastructure The main infrastructure hosting this solution leverages an Amazon EKS cluster and EMR virtual cluster. As discussed in one of the earlier posts, EMR job pods (controller, driver and executors) can be configured to be managed by Karpenter, which simplifies autoscaling by provisioning just-in-time capacity as well as reduces scheduling latency. While the infrastructure elements are discussed in depth in the earlier post and part 3, this section focuses on how to set up a long-running Thrift JDBC/ODBC server on EMR on EKS, which is a critical part of using the dbt-spark adapter. The source can be found in the GitHub repository of this post.\nThrift JDBC/ODBC Server The Spark Submit does not allow the spark thrift server to run in cluster mode on Kubernetes. I have found a number of implementations that handle this issue. The first one is executing the thrift server start script in the container command, but it is not allowed in pod templates of EMR on EKS. Besides, it creates the driver and executors in a single pod, which is not scalable. The second example relies on Apache Kyuubi that manages Spark applications while providing JDBC connectivity. However, there is no dbt adapter that supports Kyuubi as well as I am concerned it could make dbt transformations more complicated. The last one is creating a wrapper class that makes the thrift server run indefinitely. It is an interesting approach to deploy the thrift server on EMR on EKS (and the spark kubernetes operator in general) with minimal effort. Following that example, a wrapper class is created in this post.\nSpark Thrift Server Runner The wrapper class (SparkThriftServerRunner) is created as shown below, and it makes the HiveThriftServer2 class run indefinitely. In this way, we are able to use the runner class as the entry point for a spark application.\n1// emr-eks/hive-on-spark-in-kubernetes/examples/spark-thrift-server/src/main/java/io/jaehyeon/hive/SparkThriftServerRunner.java 2package io.jaehyeon.hive; 3 4public class SparkThriftServerRunner { 5 6 public static void main(String[] args) { 7 org.apache.spark.sql.hive.thriftserver.HiveThriftServer2.main(args); 8 9 while (true) { 10 try { 11 Thread.sleep(Long.MAX_VALUE); 12 } catch (Exception e) { 13 e.printStackTrace(); 14 } 15 } 16 } 17} While the reference implementation includes all of its dependent libraries when building the class, only the runner class itself is built for this post. It is because we do not have to include those as they are available in the EMR on EKS container image. To do so, the Project Object Model (POM) file is updated so that all the provided dependency scopes are changed into runtime except for spark-hive-thriftserver_2.12 - see pom.xml for details. The runner class can be built as shown below.\n1cd emr-eks/hive-on-spark-in-kubernetes/examples/spark-thrift-server 2mvn -e -DskipTests=true clean install; Once completed, the JAR file of the runner class can be found in the target folder - spark-thrift-server-1.0.0-SNAPSHOT.jar.\n1$ tree target/ -L 1 2target/ 3├── classes 4├── generated-sources 5├── maven-archiver 6├── maven-status 7├── spark-thrift-server-1.0.0-SNAPSHOT-sources.jar 8├── spark-thrift-server-1.0.0-SNAPSHOT.jar 9└── test-classes 10 115 directories, 2 files Driver Template We can expose the spark driver pod by a service. A label (app) is added to the driver pod template so that it can be selected by the service.\n1# emr-eks/resources/driver-template.yaml 2apiVersion: v1 3kind: Pod 4metadata: 5 labels: 6 app: spark-thrift-server-driver 7spec: 8 nodeSelector: 9 type: karpenter 10 provisioner: spark-driver 11 tolerations: 12 - key: spark-driver 13 operator: Exists 14 effect: NoSchedule 15 containers: 16 - name: spark-kubernetes-driver Spark Job Run The wrapper class and (driver and executor) pod templates are referred from the default S3 bucket of this post. The runner class is specified as the entry point of the spark application and three executor instances with 2G of memory are configured to run. In the application configuration, the dynamic resource allocation is disabled and the AWS Glue Data Catalog is set to be used as the metastore for Spark SQL.\n1# emr-eks/job-run.sh 2export VIRTUAL_CLUSTER_ID=$(terraform -chdir=./infra output --raw emrcontainers_virtual_cluster_id) 3export EMR_ROLE_ARN=$(terraform -chdir=./infra output --json emr_on_eks_role_arn | jq \u0026#39;.[0]\u0026#39; -r) 4export DEFAULT_BUCKET_NAME=$(terraform -chdir=./infra output --raw default_bucket_name) 5export AWS_REGION=$(aws ec2 describe-availability-zones --query \u0026#39;AvailabilityZones[0].[RegionName]\u0026#39; --output text) 6 7aws emr-containers start-job-run \\ 8--virtual-cluster-id $VIRTUAL_CLUSTER_ID \\ 9--name thrift-server \\ 10--execution-role-arn $EMR_ROLE_ARN \\ 11--release-label emr-6.8.0-latest \\ 12--region $AWS_REGION \\ 13--job-driver \u0026#39;{ 14 \u0026#34;sparkSubmitJobDriver\u0026#34;: { 15 \u0026#34;entryPoint\u0026#34;: \u0026#34;s3://\u0026#39;${DEFAULT_BUCKET_NAME}\u0026#39;/resources/jars/spark-thrift-server-1.0.0-SNAPSHOT.jar\u0026#34;, 16 \u0026#34;sparkSubmitParameters\u0026#34;: \u0026#34;--class io.jaehyeon.hive.SparkThriftServerRunner --jars s3://\u0026#39;${DEFAULT_BUCKET_NAME}\u0026#39;/resources/jars/spark-thrift-server-1.0.0-SNAPSHOT.jar --conf spark.executor.instances=3 --conf spark.executor.memory=2G --conf spark.executor.cores=1 --conf spark.driver.cores=1 --conf spark.driver.memory=2G\u0026#34; 17 } 18 }\u0026#39; \\ 19--configuration-overrides \u0026#39;{ 20 \u0026#34;applicationConfiguration\u0026#34;: [ 21 { 22 \u0026#34;classification\u0026#34;: \u0026#34;spark-defaults\u0026#34;, 23 \u0026#34;properties\u0026#34;: { 24 \u0026#34;spark.dynamicAllocation.enabled\u0026#34;:\u0026#34;false\u0026#34;, 25 \u0026#34;spark.kubernetes.executor.deleteOnTermination\u0026#34;: \u0026#34;true\u0026#34;, 26 \u0026#34;spark.kubernetes.driver.podTemplateFile\u0026#34;:\u0026#34;s3://\u0026#39;${DEFAULT_BUCKET_NAME}\u0026#39;/resources/templates/driver-template.yaml\u0026#34;, 27 \u0026#34;spark.kubernetes.executor.podTemplateFile\u0026#34;:\u0026#34;s3://\u0026#39;${DEFAULT_BUCKET_NAME}\u0026#39;/resources/templates/executor-template.yaml\u0026#34;, 28 \u0026#34;spark.hadoop.hive.metastore.client.factory.class\u0026#34;:\u0026#34;com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory\u0026#34; 29 } 30 } 31 ] 32}\u0026#39; Once it is started, we can check the spark job pods as shown below. A single driver and three executor pods are deployed as expected.\n1$ kubectl get pod -n analytics 2NAME READY STATUS RESTARTS AGE 30000000310t0tkbcftg-nmzfh 2/2 Running 0 5m39s 4spark-0000000310t0tkbcftg-6385c0841d09d389-exec-1 1/1 Running 0 3m2s 5spark-0000000310t0tkbcftg-6385c0841d09d389-exec-2 1/1 Running 0 3m1s 6spark-0000000310t0tkbcftg-6385c0841d09d389-exec-3 1/1 Running 0 3m1s 7spark-0000000310t0tkbcftg-driver 2/2 Running 0 5m26s Spark Thrift Server Service As mentioned earlier, the driver pod is exposed by a service. The service manifest file uses a label selector to identify the spark driver pod. As EMR on EC2, the thrift server is mapped to port 10001 by default. Note that the container port is not allowed in the pod templates of EMR on EKS but the service can still access it.\n1# emr-eks/resources/spark-thrift-server-service.yaml 2kind: Service 3apiVersion: v1 4metadata: 5 name: spark-thrift-server-service 6 namespace: analytics 7spec: 8 type: LoadBalancer 9 selector: 10 app: spark-thrift-server-driver 11 ports: 12 - name: jdbc-port 13 protocol: TCP 14 port: 10001 15 targetPort: 10001 The service can be deployed by kubectl apply -f resources/spark-thrift-server-service.yaml, and we can check the service details as shown below - we will use the hostname of the service (EXTERNAL-IP) later.\nSimilar to beeline, we can check the connection using the pyhive package.\n1# emr-eks/resources/test_conn.py 2from pyhive import hive 3import pandas as pd 4 5conn = hive.connect( 6 host=\u0026#34;\u0026lt;hostname-of-spark-thrift-server-service\u0026gt;\u0026#34;, 7 port=10001, 8 username=\u0026#34;hadoop\u0026#34;, 9 auth=None 10 ) 11print(pd.read_sql(con=conn, sql=\u0026#34;show databases\u0026#34;)) 12conn.close() 1$ python resources/test_conn.py 2 print(pd.read_sql(con=conn, sql=\u0026#34;show databases\u0026#34;)) 3 namespace 40 default 51 imdb 62 imdb_analytics Project We build a data transformation pipeline using subsets of IMDb data - seven titles and names related datasets are provided as gzipped, tab-separated-values (TSV) formatted files. The project ends up creating three tables that can be used for reporting and analysis.\nSave Data to S3 The Axel download accelerator is used to download the data files locally followed by decompressing with the gzip utility. Note that simple retry logic is added as I see download failure from time to time. Finally, the decompressed files are saved into the project S3 bucket using the S3 sync command.\n1# dbt-on-aws/emr-eks/upload-data.sh 2#!/usr/bin/env bash 3 4s3_bucket=$(terraform -chdir=./infra output --raw default_bucket_name) 5hostname=\u0026#34;datasets.imdbws.com\u0026#34; 6declare -a file_names=( 7 \u0026#34;name.basics.tsv.gz\u0026#34; \\ 8 \u0026#34;title.akas.tsv.gz\u0026#34; \\ 9 \u0026#34;title.basics.tsv.gz\u0026#34; \\ 10 \u0026#34;title.crew.tsv.gz\u0026#34; \\ 11 \u0026#34;title.episode.tsv.gz\u0026#34; \\ 12 \u0026#34;title.principals.tsv.gz\u0026#34; \\ 13 \u0026#34;title.ratings.tsv.gz\u0026#34; 14 ) 15 16rm -rf imdb-data 17 18for fn in \u0026#34;${file_names[@]}\u0026#34; 19do 20 download_url=\u0026#34;https://$hostname/$fn\u0026#34; 21 prefix=$(echo ${fn::-7} | tr \u0026#39;.\u0026#39; \u0026#39;_\u0026#39;) 22 echo \u0026#34;download imdb-data/$prefix/$fn from $download_url\u0026#34; 23 while true; 24 do 25 mkdir -p imdb-data/$prefix 26 axel -n 32 -a -o imdb-data/$prefix/$fn $download_url 27 gzip -d imdb-data/$prefix/$fn 28 num_files=$(ls imdb-data/$prefix | wc -l) 29 if [ $num_files == 1 ]; then 30 break 31 fi 32 rm -rf imdb-data/$prefix 33 done 34done 35 36aws s3 sync ./imdb-data s3://$s3_bucket Setup dbt Project We use the dbt-spark adapter to work with the EMR cluster. As connection is made by the Thrift JDBC/ODBC server, it is necessary to install the adapter with the PyHive package. I use Ubuntu 20.04 in WSL 2 and it needs to install the libsasl2-dev apt package, which is required for one of the dependent packages of PyHive (pure-sasl). After installing it, we can install the dbt packages as usual.\n1$ sudo apt-get install libsasl2-dev 2$ python3 -m venv venv 3$ source venv/bin/activate 4$ pip install --upgrade pip 5$ pip install dbt-core \u0026#34;dbt-spark[PyHive]\u0026#34; We can initialise a dbt project with the dbt init command. We are required to specify project details - project name, host, connection method, port, schema and the number of threads. Note dbt creates the project profile to .dbt/profile.yml of the user home directory by default.\n1$ dbt init 205:29:39 Running with dbt=1.3.0 3Enter a name for your project (letters, digits, underscore): emr_eks 4Which database would you like to use? 5[1] spark 6 7(Don\u0026#39;t see the one you want? https://docs.getdbt.com/docs/available-adapters) 8 9Enter a number: 1 10host (yourorg.sparkhost.com): \u0026lt;hostname-of-spark-thrift-server-service\u0026gt; 11[1] odbc 12[2] http 13[3] thrift 14Desired authentication method option (enter a number): 3 15port [443]: 10001 16schema (default schema that dbt will build objects in): imdb 17threads (1 or more) [1]: 3 1805:30:13 Profile emr_eks written to /home/\u0026lt;username\u0026gt;/.dbt/profiles.yml using target\u0026#39;s profile_template.yml and your supplied values. Run \u0026#39;dbt debug\u0026#39; to validate the connection. 1905:30:13 20Your new dbt project \u0026#34;emr_eks\u0026#34; was created! 21 22For more information on how to configure the profiles.yml file, 23please consult the dbt documentation here: 24 25 https://docs.getdbt.com/docs/configure-your-profile 26 27One more thing: 28 29Need help? Don\u0026#39;t hesitate to reach out to us via GitHub issues or on Slack: 30 31 https://community.getdbt.com/ 32 33Happy modeling! dbt initialises a project in a folder that matches to the project name and generates project boilerplate as shown below. Some of the main objects are dbt_project.yml, and the model folder. The former is required because dbt doesn\u0026rsquo;t know if a folder is a dbt project without it. Also it contains information that tells dbt how to operate on the project. The latter is for including dbt models, which is basically a set of SQL select statements. See dbt documentation for more details.\n1$ tree emr-eks/emr_eks/ -L 1 2emr-eks/emr_eks/ 3├── README.md 4├── analyses 5├── dbt_packages 6├── dbt_project.yml 7├── logs 8├── macros 9├── models 10├── packages.yml 11├── seeds 12├── snapshots 13├── target 14└── tests We can check connection to the EMR cluster with the dbt debug command as shown below.\n1$ dbt debug 205:31:22 Running with dbt=1.3.0 3dbt version: 1.3.0 4python version: 3.8.10 5python path: \u0026lt;path-to-python-path\u0026gt; 6os info: Linux-5.4.72-microsoft-standard-WSL2-x86_64-with-glibc2.29 7Using profiles.yml file at /home/\u0026lt;username\u0026gt;/.dbt/profiles.yml 8Using dbt_project.yml file at \u0026lt;path-to-dbt-project\u0026gt;/dbt_project.yml 9 10Configuration: 11 profiles.yml file [OK found and valid] 12 dbt_project.yml file [OK found and valid] 13 14Required dependencies: 15 - git [OK found] 16 17Connection: 18 host: \u0026lt;hostname-of-spark-thrift-server-service\u0026gt; 19 port: 10001 20 cluster: None 21 endpoint: None 22 schema: imdb 23 organization: 0 24 Connection test: [OK connection ok] 25 26All checks passed! After initialisation, the model configuration is updated. The project materialisation is specified as view although it is the default materialisation. Also tags are added to the entire model folder as well as folders of specific layers - staging, intermediate and marts. As shown below, tags can simplify model execution.\n1# emr-eks/emr_eks/dbt_project.yml 2name: \u0026#34;emr_eks\u0026#34; 3 4... 5 6models: 7 dbt_glue_proj: 8 +materialized: view 9 +tags: 10 - \u0026#34;imdb\u0026#34; 11 staging: 12 +tags: 13 - \u0026#34;staging\u0026#34; 14 intermediate: 15 +tags: 16 - \u0026#34;intermediate\u0026#34; 17 marts: 18 +tags: 19 - \u0026#34;marts\u0026#34; While we created source tables using Glue crawlers in part 2, they are created directly from S3 by the dbt_external_tables package in this post. Also the dbt_utils package is installed for adding tests to the final marts models. They can be installed by the dbt deps command.\n1# emr-eks/emr_eks/packages.yml 2packages: 3 - package: dbt-labs/dbt_external_tables 4 version: 0.8.2 5 - package: dbt-labs/dbt_utils 6 version: 0.9.2 Create dbt Models The models for this post are organised into three layers according to the dbt best practices - staging, intermediate and marts.\nExternal Source The seven tables that are loaded from S3 are dbt source tables and their details are declared in a YAML file (_imdb_sources.yml). Macros of the dbt_external_tables package parse properties of each table and execute SQL to create each of them. By doing so, we are able to refer to the source tables with the {{ source() }} function. Also, we can add tests to source tables. For example two tests (unique, not_null) are added to the tconst column of the title_basics table below and these tests can be executed by the dbt test command.\n1# emr-eks/emr_eks/models/staging/imdb/_imdb__sources.yml 2version: 2 3 4sources: 5 - name: imdb 6 description: Subsets of IMDb data, which are available for access to customers for personal and non-commercial use 7 tables: 8 - name: title_basics 9 description: Table that contains basic information of titles 10 external: 11 location: \u0026#34;s3://\u0026lt;s3-bucket-name\u0026gt;/title_basics/\u0026#34; 12 row_format: \u0026gt; 13 serde \u0026#39;org.apache.hadoop.hive.serde2.OpenCSVSerde\u0026#39; 14 with serdeproperties ( 15 \u0026#39;separatorChar\u0026#39;=\u0026#39;\\t\u0026#39; 16 ) 17 table_properties: \u0026#34;(\u0026#39;skip.header.line.count\u0026#39;=\u0026#39;1\u0026#39;)\u0026#34; 18 columns: 19 - name: tconst 20 data_type: string 21 description: alphanumeric unique identifier of the title 22 tests: 23 - unique 24 - not_null 25 - name: titletype 26 data_type: string 27 description: the type/format of the title (e.g. movie, short, tvseries, tvepisode, video, etc) 28 - name: primarytitle 29 data_type: string 30 description: the more popular title / the title used by the filmmakers on promotional materials at the point of release 31 - name: originaltitle 32 data_type: string 33 description: original title, in the original language 34 - name: isadult 35 data_type: string 36 description: flag that indicates whether it is an adult title or not 37 - name: startyear 38 data_type: string 39 description: represents the release year of a title. In the case of TV Series, it is the series start year 40 - name: endyear 41 data_type: string 42 description: TV Series end year. NULL for all other title types 43 - name: runtimeminutes 44 data_type: string 45 description: primary runtime of the title, in minutes 46 - name: genres 47 data_type: string 48 description: includes up to three genres associated with the title The source tables can be created by dbt run-operation stage_external_sources. Note that the following SQL is executed for the _title_basics _table under the hood.\n1create table imdb.title_basics ( 2 tconst string, 3 titletype string, 4 primarytitle string, 5 originaltitle string, 6 isadult string, 7 startyear string, 8 endyear string, 9 runtimeminutes string, 10 genres string 11) 12row format serde \u0026#39;org.apache.hadoop.hive.serde2.OpenCSVSerde\u0026#39; with serdeproperties ( 13\u0026#39;separatorChar\u0026#39;=\u0026#39;\\t\u0026#39; 14) 15location \u0026#39;s3://\u0026lt;s3-bucket-name\u0026gt;/title_basics/\u0026#39; 16tblproperties (\u0026#39;skip.header.line.count\u0026#39;=\u0026#39;1\u0026#39;) Interestingly the header rows of the source tables are not skipped when they are queried by spark while they are skipped by Athena. They have to be filtered out in the stage models of the dbt project as spark is the query engine.\nStaging Based on the source tables, staging models are created. They are created as views, which is the project’s default materialisation. In the SQL statements, column names and data types are modified mainly.\n1# emr-eks/emr_eks/models/staging/imdb/stg_imdb__title_basics.sql 2with source as ( 3 4 select * from {{ source(\u0026#39;imdb\u0026#39;, \u0026#39;title_basics\u0026#39;) }} 5 6), 7 8renamed as ( 9 10 select 11 tconst as title_id, 12 titletype as title_type, 13 primarytitle as primary_title, 14 originaltitle as original_title, 15 cast(isadult as boolean) as is_adult, 16 cast(startyear as int) as start_year, 17 cast(endyear as int) as end_year, 18 cast(runtimeminutes as int) as runtime_minutes, 19 case when genres = \u0026#39;N\u0026#39; then null else genres end as genres 20 from source 21 where tconst \u0026lt;\u0026gt; \u0026#39;tconst\u0026#39; 22 23) 24 25select * from renamed Below shows the file tree of the staging models. The staging models can be executed using the dbt run command. As we’ve added tags to the staging layer models, we can limit to execute only this layer by dbt run --select staging.\n1$ tree emr-eks/emr_eks/models/staging/ 2emr-eks/emr_eks/models/staging/ 3└── imdb 4 ├── _imdb__models.yml 5 ├── _imdb__sources.yml 6 ├── stg_imdb__name_basics.sql 7 ├── stg_imdb__title_akas.sql 8 ├── stg_imdb__title_basics.sql 9 ├── stg_imdb__title_crews.sql 10 ├── stg_imdb__title_episodes.sql 11 ├── stg_imdb__title_principals.sql 12 └── stg_imdb__title_ratings.sql Note that the model materialisation of the staging and intermediate models is view and the dbt project creates VIRTUAL_VIEW tables. Although we are able to reference those tables in other models, they cannot be queried by Athena.\n1$ aws glue get-tables --database imdb \\ 2 --query \u0026#34;TableList[?Name==\u0026#39;stg_imdb__title_basics\u0026#39;].[Name, TableType, StorageDescriptor.Columns]\u0026#34; --output yaml 3- - stg_imdb__title_basics 4 - VIRTUAL_VIEW 5 - - Name: title_id 6 Type: string 7 - Name: title_type 8 Type: string 9 - Name: primary_title 10 Type: string 11 - Name: original_title 12 Type: string 13 - Name: is_adult 14 Type: boolean 15 - Name: start_year 16 Type: int 17 - Name: end_year 18 Type: int 19 - Name: runtime_minutes 20 Type: int 21 - Name: genres 22 Type: string Instead we can use spark sql to query the tables. Below shows a query result of the title basics staging table in Glue Studio notebook.\nIntermediate We can keep intermediate results in this layer so that the models of the final marts layer can be simplified. The source data includes columns where array values are kept as comma separated strings. For example, the genres column of the stg_imdb__title_basics model includes up to three genre values as shown in the previous screenshot. A total of seven columns in three models are columns of comma-separated strings, and it is better to flatten them in the intermediate layer. Also, in order to avoid repetition, a dbt macro (f_latten_fields_) is created to share the column-flattening logic.\n1# emr-eks/emr_eks/macros/flatten_fields.sql 2{% macro flatten_fields(model, field_name, id_field_name) %} 3 select 4 {{ id_field_name }} as id, 5 explode(split({{ field_name }}, \u0026#39;,\u0026#39;)) as field 6 from {{ model }} 7{% endmacro %} The macro function can be added inside a common table expression (CTE) by specifying the relevant model, field name to flatten and ID field name.\n1-- emr-eks/emr_eks/models/intermediate/title/int_genres_flattened_from_title_basics.sql 2with flattened as ( 3 {{ flatten_fields(ref(\u0026#39;stg_imdb__title_basics\u0026#39;), \u0026#39;genres\u0026#39;, \u0026#39;title_id\u0026#39;) }} 4) 5 6select 7 id as title_id, 8 field as genre 9from flattened 10order by id The intermediate models are also materialised as views and we can check the array columns are flattened as expected.\nBelow shows the file tree of the intermediate models. Similar to the staging models, the intermediate models can be executed by dbt run --select intermediate.\n1$ tree emr-eks/emr_eks/models/intermediate/ emr-eks/emr_eks/macros/ 2emr-eks/emr_eks/models/intermediate/ 3├── name 4│ ├── _int_name__models.yml 5│ ├── int_known_for_titles_flattened_from_name_basics.sql 6│ └── int_primary_profession_flattened_from_name_basics.sql 7└── title 8 ├── _int_title__models.yml 9 ├── int_directors_flattened_from_title_crews.sql 10 ├── int_genres_flattened_from_title_basics.sql 11 └── int_writers_flattened_from_title_crews.sql 12 13emr-eks/emr_eks/macros/ 14└── flatten_fields.sql Marts The models in the marts layer are configured to be materialised as tables in a custom schema. Their materialisation is set to table and the custom schema is specified as analytics while taking _parquet _as the file format. Note that the custom schema name becomes imdb_analytics according to the naming convention of dbt custom schemas. Models of both the staging and intermediate layers are used to create final models to be used for reporting and analytics.\n1-- emr-eks/emr_eks/models/marts/analytics/titles.sql 2{{ 3 config( 4 schema=\u0026#39;analytics\u0026#39;, 5 materialized=\u0026#39;table\u0026#39;, 6 file_format=\u0026#39;parquet\u0026#39; 7 ) 8}} 9 10with titles as ( 11 12 select * from {{ ref(\u0026#39;stg_imdb__title_basics\u0026#39;) }} 13 14), 15 16principals as ( 17 18 select 19 title_id, 20 count(name_id) as num_principals 21 from {{ ref(\u0026#39;stg_imdb__title_principals\u0026#39;) }} 22 group by title_id 23 24), 25 26names as ( 27 28 select 29 title_id, 30 count(name_id) as num_names 31 from {{ ref(\u0026#39;int_known_for_titles_flattened_from_name_basics\u0026#39;) }} 32 group by title_id 33 34), 35 36ratings as ( 37 38 select 39 title_id, 40 average_rating, 41 num_votes 42 from {{ ref(\u0026#39;stg_imdb__title_ratings\u0026#39;) }} 43 44), 45 46episodes as ( 47 48 select 49 parent_title_id, 50 count(title_id) as num_episodes 51 from {{ ref(\u0026#39;stg_imdb__title_episodes\u0026#39;) }} 52 group by parent_title_id 53 54), 55 56distributions as ( 57 58 select 59 title_id, 60 count(title) as num_distributions 61 from {{ ref(\u0026#39;stg_imdb__title_akas\u0026#39;) }} 62 group by title_id 63 64), 65 66final as ( 67 68 select 69 t.title_id, 70 t.title_type, 71 t.primary_title, 72 t.original_title, 73 t.is_adult, 74 t.start_year, 75 t.end_year, 76 t.runtime_minutes, 77 t.genres, 78 p.num_principals, 79 n.num_names, 80 r.average_rating, 81 r.num_votes, 82 e.num_episodes, 83 d.num_distributions 84 from titles as t 85 left join principals as p on t.title_id = p.title_id 86 left join names as n on t.title_id = n.title_id 87 left join ratings as r on t.title_id = r.title_id 88 left join episodes as e on t.title_id = e.parent_title_id 89 left join distributions as d on t.title_id = d.title_id 90 91) 92 93select * from final The details of the three models can be found in a YAML file (_analytics__models.yml). We can add tests to models and below we see tests of row count matching to their corresponding staging models.\n1# emr-eks/emr_eks/models/marts/analytics/_analytics__models.yml 2version: 2 3 4models: 5 - name: names 6 description: Table that contains all names with additional details 7 tests: 8 - dbt_utils.equal_rowcount: 9 compare_model: ref(\u0026#39;stg_imdb__name_basics\u0026#39;) 10 - name: titles 11 description: Table that contains all titles with additional details 12 tests: 13 - dbt_utils.equal_rowcount: 14 compare_model: ref(\u0026#39;stg_imdb__title_basics\u0026#39;) 15 - name: genre_titles 16 description: Table that contains basic title details after flattening genres The models of the marts layer can be tested using the dbt test command as shown below.\n1$ dbt test --select marts 206:05:30 Running with dbt=1.3.0 306:05:30 Found 15 models, 17 tests, 0 snapshots, 0 analyses, 569 macros, 0 operations, 0 seed files, 7 sources, 0 exposures, 0 metrics 406:05:30 506:06:03 Concurrency: 3 threads (target=\u0026#39;dev\u0026#39;) 606:06:03 706:06:03 1 of 2 START test dbt_utils_equal_rowcount_names_ref_stg_imdb__name_basics_ .... [RUN] 806:06:03 2 of 2 START test dbt_utils_equal_rowcount_titles_ref_stg_imdb__title_basics_ .. [RUN] 906:06:57 1 of 2 PASS dbt_utils_equal_rowcount_names_ref_stg_imdb__name_basics_ .......... [PASS in 53.54s] 1006:06:59 2 of 2 PASS dbt_utils_equal_rowcount_titles_ref_stg_imdb__title_basics_ ........ [PASS in 56.40s] 1106:07:02 1206:07:02 Finished running 2 tests in 0 hours 1 minutes and 31.75 seconds (91.75s). 1306:07:02 1406:07:02 Completed successfully 1506:07:02 1606:07:02 Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2 As with the other layers, the marts models can be executed by dbt run --select marts. While the transformation is performed, we can check the details from the spark history server. The SQL tab shows the three transformations in the marts layer.\nThe file tree of the marts models can be found below.\n1$ tree emr-eks/emr_eks/models/marts/ 2emr-eks/emr_eks/models/marts/ 3└── analytics 4 ├── _analytics__models.yml 5 ├── genre_titles.sql 6 ├── names.sql 7 └── titles.sql Build Dashboard The models of the marts layer can be consumed by external tools such as Amazon QuickSight. Below shows an example dashboard. The pie chart on the left shows the proportion of titles by genre while the box plot on the right shows the dispersion of average rating by start year.\nGenerate dbt Documentation A nice feature of dbt is documentation. It provides information about the project and the data warehouse, and it facilitates consumers as well as other developers to discover and understand the datasets better. We can generate the project documents and start a document server as shown below.\n1$ dbt docs generate 2$ dbt docs serve A very useful element of dbt documentation is data lineage, which provides an overall view about how data is transformed and consumed. Below we can see that the final titles model consumes all title-related stating models and an intermediate model from the name basics staging model.\nSummary In this post, we discussed how to build data transformation pipelines using dbt on Amazon EMR on EKS. As Spark Submit does not allow the spark thrift server to run in cluster mode on Kubernetes, a simple wrapper class was created that makes the thrift server run indefinitely. Subsets of IMDb data are used as source and data models are developed in multiple layers according to the dbt best practices. dbt can be used as an effective tool for data transformation in a wide range of data projects from data warehousing to data lake to data lakehouse, and it supports key AWS analytics services - Redshift, Glue, EMR and Athena. More examples of using dbt will be discussed in subsequent posts.\n","date":"November 1, 2022","img":"/blog/2022-11-01-dbt-on-aws-part-4-emr-eks/featured.png","lang":"en","langName":"English","largeImg":"/blog/2022-11-01-dbt-on-aws-part-4-emr-eks/featured_huad7bc56c3140ba61f8a863cb7496d5ad_91067_500x0_resize_box_3.png","permalink":"/blog/2022-11-01-dbt-on-aws-part-4-emr-eks/","series":[{"title":"DBT for Effective Data Transformation on AWS","url":"/series/dbt-for-effective-data-transformation-on-aws/"}],"smallImg":"/blog/2022-11-01-dbt-on-aws-part-4-emr-eks/featured_huad7bc56c3140ba61f8a863cb7496d5ad_91067_180x0_resize_box_3.png","tags":[{"title":"AWS","url":"/tags/aws/"},{"title":"Amazon EMR","url":"/tags/amazon-emr/"},{"title":"Amazon EKS","url":"/tags/amazon-eks/"},{"title":"Amazon QuickSight","url":"/tags/amazon-quicksight/"},{"title":"Data Build Tool (DBT)","url":"/tags/data-build-tool-dbt/"},{"title":"Apache Spark","url":"/tags/apache-spark/"},{"title":"Terraform","url":"/tags/terraform/"}],"timestamp":1667260800,"title":"Data Build Tool (Dbt) for Effective Data Transformation on AWS – Part 4 EMR on EKS"},{"categories":[{"title":"Data Engineering","url":"/categories/data-engineering/"}],"content":"The data build tool (dbt) is an effective data transformation tool and it supports key AWS analytics services - Redshift, Glue, EMR and Athena. In the previous posts, we discussed benefits of a common data transformation tool and the potential of dbt to cover a wide range of data projects from data warehousing to data lake to data lakehouse. Demo data projects that target Redshift Serverless and Glue are illustrated as well. In part 3 of the dbt on AWS series, we discuss data transformation pipelines using dbt on Amazon EMR. Subsets of IMDb data are used as source and data models are developed in multiple layers according to the dbt best practices. A list of posts of this series can be found below.\nPart 1 Redshift Part 2 Glue Part 3 EMR on EC2 (this post) Part 4 EMR on EKS Part 5 Athena Below shows an overview diagram of the scope of this dbt on AWS series. EMR is highlighted as it is discussed in this post.\nInfrastructure The infrastructure hosting this solution leverages an Amazon EMR cluster and a S3 bucket. We also need a VPN server so that a developer can connect to the EMR cluster in a private subnet. It is extended from a previous post and the resources covered there (VPC, subnets, auto scaling group for VPN etc) are not repeated. All resources are deployed using Terraform and the source can be found in the GitHub repository of this post.\nEMR Cluster The EMR 6.7.0 release is deployed with single master and core node instances. It is configured to use the AWS Glue Data Catalog as the metastore for Hive and Spark SQL and it is done by adding the corresponding configuration classification. Also a managed scaling policy is created so that up to 4 additional task instances are added to the cluster. Note an additional security group is attached to the master and core groups for VPN access - the details of that security group is shown below.\n1# dbt-on-aws/emr-ec2/infra/emr.tf 2resource \u0026#34;aws_emr_cluster\u0026#34; \u0026#34;emr_cluster\u0026#34; { 3 name = \u0026#34;${local.name}-emr-cluster\u0026#34; 4 release_label = local.emr.release_label # emr-6.7.0 5 service_role = aws_iam_role.emr_service_role.arn 6 autoscaling_role = aws_iam_role.emr_autoscaling_role.arn 7 applications = local.emr.applications # [\u0026#34;Spark\u0026#34;, \u0026#34;Livy\u0026#34;, \u0026#34;JupyterEnterpriseGateway\u0026#34;, \u0026#34;Hive\u0026#34;] 8 ebs_root_volume_size = local.emr.ebs_root_volume_size 9 log_uri = \u0026#34;s3n://${aws_s3_bucket.default_bucket[0].id}/elasticmapreduce/\u0026#34; 10 step_concurrency_level = 256 11 keep_job_flow_alive_when_no_steps = true 12 termination_protection = false 13 14 ec2_attributes { 15 key_name = aws_key_pair.emr_key_pair.key_name 16 instance_profile = aws_iam_instance_profile.emr_ec2_instance_profile.arn 17 subnet_id = element(tolist(module.vpc.private_subnets), 0) 18 emr_managed_master_security_group = aws_security_group.emr_master.id 19 emr_managed_slave_security_group = aws_security_group.emr_slave.id 20 service_access_security_group = aws_security_group.emr_service_access.id 21 additional_master_security_groups = aws_security_group.emr_vpn_access.id # grant access to VPN server 22 additional_slave_security_groups = aws_security_group.emr_vpn_access.id # grant access to VPN server 23 } 24 25 master_instance_group { 26 instance_type = local.emr.instance_type # m5.xlarge 27 instance_count = local.emr.instance_count # 1 28 } 29 core_instance_group { 30 instance_type = local.emr.instance_type # m5.xlarge 31 instance_count = local.emr.instance_count # 1 32 } 33 34 configurations_json = \u0026lt;\u0026lt;EOF 35 [ 36 { 37 \u0026#34;Classification\u0026#34;: \u0026#34;hive-site\u0026#34;, 38 \u0026#34;Properties\u0026#34;: { 39 \u0026#34;hive.metastore.client.factory.class\u0026#34;: \u0026#34;com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory\u0026#34; 40 } 41 }, 42 { 43 \u0026#34;Classification\u0026#34;: \u0026#34;spark-hive-site\u0026#34;, 44 \u0026#34;Properties\u0026#34;: { 45 \u0026#34;hive.metastore.client.factory.class\u0026#34;: \u0026#34;com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory\u0026#34; 46 } 47 } 48 ] 49 EOF 50 51 tags = local.tags 52 53 depends_on = [module.vpc] 54} 55 56resource \u0026#34;aws_emr_managed_scaling_policy\u0026#34; \u0026#34;emr_scaling_policy\u0026#34; { 57 cluster_id = aws_emr_cluster.emr_cluster.id 58 59 compute_limits { 60 unit_type = \u0026#34;Instances\u0026#34; 61 minimum_capacity_units = 1 62 maximum_capacity_units = 5 63 } 64} The following security group is created to enable access from the VPN server to the EMR instances. Note that the inbound rule is created only when the local.vpn.to_create variable value is true while the security group is created always - if the value is false, the security group has no inbound rule.\n1# dbt-on-aws/emr-ec2/infra/emr.tf 2resource \u0026#34;aws_security_group\u0026#34; \u0026#34;emr_vpn_access\u0026#34; { 3 name = \u0026#34;${local.name}-emr-vpn-access\u0026#34; 4 vpc_id = module.vpc.vpc_id 5 6 lifecycle { 7 create_before_destroy = true 8 } 9 10 tags = local.tags 11} 12 13resource \u0026#34;aws_security_group_rule\u0026#34; \u0026#34;emr_vpn_inbound\u0026#34; { 14 count = local.vpn.to_create ? 1 : 0 15 type = \u0026#34;ingress\u0026#34; 16 description = \u0026#34;VPN access\u0026#34; 17 security_group_id = aws_security_group.emr_vpn_access.id 18 protocol = \u0026#34;tcp\u0026#34; 19 from_port = 0 20 to_port = 65535 21 source_security_group_id = aws_security_group.vpn[0].id 22} As in the previous post, we connect to the EMR cluster via SoftEther VPN. Instead of providing VPN related secrets as Terraform variables, they are created internally and stored to AWS Secrets Manager. The details can be found in dbt-on-aws/emr-ec2/infra/secrets.tf and the secret string can be retrieved as shown below.\n1$ aws secretsmanager get-secret-value --secret-id emr-ec2-all-secrets --query \u0026#34;SecretString\u0026#34; --output text 2 { 3 \u0026#34;vpn_pre_shared_key\u0026#34;: \u0026#34;\u0026lt;vpn-pre-shared-key\u0026gt;\u0026#34;, 4 \u0026#34;vpn_admin_password\u0026#34;: \u0026#34;\u0026lt;vpn-admin-password\u0026gt;\u0026#34; 5 } The previous post demonstrates how to create a VPN user and to establish connection in detail. An example of a successful connection is shown below.\nGlue Databases We have two Glue databases. The source tables and the tables of the staging and intermediate layers are kept in the imdb database. The tables of the marts layer are stored in the _imdb_analytics _database.\n1# glue databases 2resource \u0026#34;aws_glue_catalog_database\u0026#34; \u0026#34;imdb_db\u0026#34; { 3 name = \u0026#34;imdb\u0026#34; 4 location_uri = \u0026#34;s3://${local.default_bucket.name}/imdb\u0026#34; 5 description = \u0026#34;Database that contains IMDb staging/intermediate model datasets\u0026#34; 6} 7 8resource \u0026#34;aws_glue_catalog_database\u0026#34; \u0026#34;imdb_db_marts\u0026#34; { 9 name = \u0026#34;imdb_analytics\u0026#34; 10 location_uri = \u0026#34;s3://${local.default_bucket.name}/imdb_analytics\u0026#34; 11 description = \u0026#34;Database that contains IMDb marts model datasets\u0026#34; 12} Project We build a data transformation pipeline using subsets of IMDb data - seven titles and names related datasets are provided as gzipped, tab-separated-values (TSV) formatted files. The project ends up creating three tables that can be used for reporting and analysis.\nSave Data to S3 The Axel download accelerator is used to download the data files locally followed by decompressing with the gzip utility. Note that simple retry logic is added as I see download failure from time to time. Finally, the decompressed files are saved into the project S3 bucket using the S3 sync command.\n1# dbt-on-aws/emr-ec2/upload-data.sh 2#!/usr/bin/env bash 3 4s3_bucket=$(terraform -chdir=./infra output --raw default_bucket_name) 5hostname=\u0026#34;datasets.imdbws.com\u0026#34; 6declare -a file_names=( 7 \u0026#34;name.basics.tsv.gz\u0026#34; \\ 8 \u0026#34;title.akas.tsv.gz\u0026#34; \\ 9 \u0026#34;title.basics.tsv.gz\u0026#34; \\ 10 \u0026#34;title.crew.tsv.gz\u0026#34; \\ 11 \u0026#34;title.episode.tsv.gz\u0026#34; \\ 12 \u0026#34;title.principals.tsv.gz\u0026#34; \\ 13 \u0026#34;title.ratings.tsv.gz\u0026#34; 14 ) 15 16rm -rf imdb-data 17 18for fn in \u0026#34;${file_names[@]}\u0026#34; 19do 20 download_url=\u0026#34;https://$hostname/$fn\u0026#34; 21 prefix=$(echo ${fn::-7} | tr \u0026#39;.\u0026#39; \u0026#39;_\u0026#39;) 22 echo \u0026#34;download imdb-data/$prefix/$fn from $download_url\u0026#34; 23 while true; 24 do 25 mkdir -p imdb-data/$prefix 26 axel -n 32 -a -o imdb-data/$prefix/$fn $download_url 27 gzip -d imdb-data/$prefix/$fn 28 num_files=$(ls imdb-data/$prefix | wc -l) 29 if [ $num_files == 1 ]; then 30 break 31 fi 32 rm -rf imdb-data/$prefix 33 done 34done 35 36aws s3 sync ./imdb-data s3://$s3_bucket Start Thrift JDBC/ODBC Server The connection from dbt to the EMR cluster is made by the Thrift JDBC/ODBC server and it can be started by adding an EMR step as shown below.\n1$ cd emr-ec2 2$ CLUSTER_ID=$(terraform -chdir=./infra output --raw emr_cluster_id) 3$ aws emr add-steps \\ 4 --cluster-id $CLUSTER_ID \\ 5 --steps Type=CUSTOM_JAR,Name=\u0026#34;spark thrift server\u0026#34;,ActionOnFailure=CONTINUE,Jar=command-runner.jar,Args=[sudo,/usr/lib/spark/sbin/start-thriftserver.sh] We can quickly check if the thrift server is started using the beeline JDBC client. The port is 10001 and, as connection is made in non-secure mode, we can simply enter the default username and a blank password. When we query databases, we see the Glue databases that are created earlier.\n1$ STACK_NAME=emr-ec2 2$ EMR_CLUSTER_MASTER_DNS=$(terraform -chdir=./infra output --raw emr_cluster_master_dns) 3$ ssh -i infra/key-pair/$STACK_NAME-emr-key.pem hadoop@$EMR_CLUSTER_MASTER_DNS 4Last login: Thu Oct 13 08:59:51 2022 from ip-10-0-32-240.ap-southeast-2.compute.internal 5 6 7... 8 9 10[hadoop@ip-10-0-113-195 ~]$ beeline 11SLF4J: Class path contains multiple SLF4J bindings. 12SLF4J: Found binding in [jar:file:/usr/lib/hive/lib/log4j-slf4j-impl-2.17.1.jar!/org/slf4j/impl/StaticLoggerBinder.class] 13SLF4J: Found binding in [jar:file:/usr/lib/tez/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class] 14SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation. 15SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory] 16SLF4J: Class path contains multiple SLF4J bindings. 17SLF4J: Found binding in [jar:file:/usr/lib/hive/lib/log4j-slf4j-impl-2.17.1.jar!/org/slf4j/impl/StaticLoggerBinder.class] 18SLF4J: Found binding in [jar:file:/usr/lib/tez/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class] 19SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation. 20SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory] 21Beeline version 3.1.3-amzn-0 by Apache Hive 22beeline\u0026gt; !connect jdbc:hive2://localhost:10001 23Connecting to jdbc:hive2://localhost:10001 24Enter username for jdbc:hive2://localhost:10001: hadoop 25Enter password for jdbc:hive2://localhost:10001: 26Connected to: Spark SQL (version 3.2.1-amzn-0) 27Driver: Hive JDBC (version 3.1.3-amzn-0) 28Transaction isolation: TRANSACTION_REPEATABLE_READ 290: jdbc:hive2://localhost:10001\u0026gt; show databases; 30+-------------------------------+ 31| namespace | 32+-------------------------------+ 33| default | 34| imdb | 35| imdb_analytics | 36+-------------------------------+ 373 rows selected (0.311 seconds) Setup dbt Project We use the dbt-spark adapter to work with the EMR cluster. As connection is made by the Thrift JDBC/ODBC server, it is necessary to install the adapter with the PyHive package. I use Ubuntu 20.04 in WSL 2 and it needs to install the libsasl2-dev apt package, which is required for one of the dependent packages of PyHive (pure-sasl). After installing it, we can install the dbt packages as usual.\n1$ sudo apt-get install libsasl2-dev 2$ python3 -m venv venv 3$ source venv/bin/activate 4$ pip install --upgrade pip 5$ pip install dbt-core \u0026#34;dbt-spark[PyHive]\u0026#34; We can initialise a dbt project with the dbt init command. We are required to specify project details - project name, host, connection method, port, schema and the number of threads. Note dbt creates the project profile to .dbt/profile.yml of the user home directory by default.\n1$ dbt init 221:00:16 Running with dbt=1.2.2 3Enter a name for your project (letters, digits, underscore): emr_ec2 4Which database would you like to use? 5[1] spark 6 7(Don\u0026#39;t see the one you want? https://docs.getdbt.com/docs/available-adapters) 8 9Enter a number: 1 10host (yourorg.sparkhost.com): \u0026lt;hostname-or-ip-address-of-master-instance\u0026gt; 11[1] odbc 12[2] http 13[3] thrift 14Desired authentication method option (enter a number): 3 15port [443]: 10001 16schema (default schema that dbt will build objects in): imdb 17threads (1 or more) [1]: 3 1821:50:28 Profile emr_ec2 written to /home/\u0026lt;username\u0026gt;/.dbt/profiles.yml using target\u0026#39;s profile_template.yml and your supplied values. Run \u0026#39;dbt debug\u0026#39; to validate the connection. 1921:50:28 20Your new dbt project \u0026#34;emr_ec2\u0026#34; was created! 21 22For more information on how to configure the profiles.yml file, 23please consult the dbt documentation here: 24 25 https://docs.getdbt.com/docs/configure-your-profile 26 27One more thing: 28 29Need help? Don\u0026#39;t hesitate to reach out to us via GitHub issues or on Slack: 30 31 https://community.getdbt.com/ 32 33Happy modeling! dbt initialises a project in a folder that matches to the project name and generates project boilerplate as shown below. Some of the main objects are dbt_project.yml, and the model folder. The former is required because dbt doesn\u0026rsquo;t know if a folder is a dbt project without it. Also it contains information that tells dbt how to operate on the project. The latter is for including dbt models, which is basically a set of SQL select statements. See dbt documentation for more details.\n1$ tree emr-ec2/emr_ec2/ -L 1 2emr-ec2/emr_ec2/ 3├── README.md 4├── analyses 5├── dbt_packages 6├── dbt_project.yml 7├── logs 8├── macros 9├── models 10├── packages.yml 11├── seeds 12├── snapshots 13├── target 14└── tests We can check connection to the EMR cluster with the dbt debug command as shown below.\n1$ dbt debug 221:51:38 Running with dbt=1.2.2 3dbt version: 1.2.2 4python version: 3.8.10 5python path: \u0026lt;path-to-python-path\u0026gt; 6os info: Linux-5.4.72-microsoft-standard-WSL2-x86_64-with-glibc2.29 7Using profiles.yml file at /home/\u0026lt;username\u0026gt;/.dbt/profiles.yml 8Using dbt_project.yml file at \u0026lt;path-to-dbt-project\u0026gt;/dbt_project.yml 9 10Configuration: 11 profiles.yml file [OK found and valid] 12 dbt_project.yml file [OK found and valid] 13 14Required dependencies: 15 - git [OK found] 16 17Connection: 18 host: 10.0.113.195 19 port: 10001 20 cluster: None 21 endpoint: None 22 schema: imdb 23 organization: 0 24 Connection test: [OK connection ok] 25 26All checks passed! After initialisation, the model configuration is updated. The project materialisation is specified as view, although it is the default materialisation. Also, tags are added to the entire model folder as well as folders of specific layers - staging, intermediate and marts. As shown below, tags can simplify model execution.\n1# emr-ec2/emr_ec2/dbt_project.yml 2name: \u0026#34;emr_ec2\u0026#34; 3 4... 5 6models: 7 dbt_glue_proj: 8 +materialized: view 9 +tags: 10 - \u0026#34;imdb\u0026#34; 11 staging: 12 +tags: 13 - \u0026#34;staging\u0026#34; 14 intermediate: 15 +tags: 16 - \u0026#34;intermediate\u0026#34; 17 marts: 18 +tags: 19 - \u0026#34;marts\u0026#34; While we created source tables using Glue crawlers in part 2, they are created directly from S3 by the dbt_external_tables package in this post. Also, the dbt_utils package is installed for adding tests to the final marts models. They can be installed by the dbt deps command.\n1# emr-ec2/emr_ec2/packages.yml 2packages: 3 - package: dbt-labs/dbt_external_tables 4 version: 0.8.2 5 - package: dbt-labs/dbt_utils 6 version: 0.9.2 Create dbt Models The models for this post are organised into three layers according to the dbt best practices - staging, intermediate and marts.\nExternal Source The seven tables that are loaded from S3 are dbt source tables and their details are declared in a YAML file (_imdb_sources.yml). Macros of the dbt_external_tables package parse properties of each table and execute SQL to create each of them. By doing so, we are able to refer to the source tables with the {{ source() }} function. Also we can add tests to source tables. For example two tests (unique, not_null) are added to the tconst column of the title_basics table below and these tests can be executed by the dbt test command.\n1# emr-ec2/emr_ec2/models/staging/imdb/_imdb__sources.yml 2version: 2 3 4sources: 5 - name: imdb 6 description: Subsets of IMDb data, which are available for access to customers for personal and non-commercial use 7 tables: 8 - name: title_basics 9 description: Table that contains basic information of titles 10 external: 11 location: \u0026#34;s3://\u0026lt;s3-bucket-name\u0026gt;/title_basics/\u0026#34; 12 row_format: \u0026gt; 13 serde \u0026#39;org.apache.hadoop.hive.serde2.OpenCSVSerde\u0026#39; 14 with serdeproperties ( 15 \u0026#39;separatorChar\u0026#39;=\u0026#39;\\t\u0026#39; 16 ) 17 table_properties: \u0026#34;(\u0026#39;skip.header.line.count\u0026#39;=\u0026#39;1\u0026#39;)\u0026#34; 18 columns: 19 - name: tconst 20 data_type: string 21 description: alphanumeric unique identifier of the title 22 tests: 23 - unique 24 - not_null 25 - name: titletype 26 data_type: string 27 description: the type/format of the title (e.g. movie, short, tvseries, tvepisode, video, etc) 28 - name: primarytitle 29 data_type: string 30 description: the more popular title / the title used by the filmmakers on promotional materials at the point of release 31 - name: originaltitle 32 data_type: string 33 description: original title, in the original language 34 - name: isadult 35 data_type: string 36 description: flag that indicates whether it is an adult title or not 37 - name: startyear 38 data_type: string 39 description: represents the release year of a title. In the case of TV Series, it is the series start year 40 - name: endyear 41 data_type: string 42 description: TV Series end year. NULL for all other title types 43 - name: runtimeminutes 44 data_type: string 45 description: primary runtime of the title, in minutes 46 - name: genres 47 data_type: string 48 description: includes up to three genres associated with the title The source tables can be created by dbt run-operation stage_external_sources. Note that the following SQL is executed for the _title_basics _table under the hood.\n1create table imdb.title_basics ( 2 tconst string, 3 titletype string, 4 primarytitle string, 5 originaltitle string, 6 isadult string, 7 startyear string, 8 endyear string, 9 runtimeminutes string, 10 genres string 11) 12row format serde \u0026#39;org.apache.hadoop.hive.serde2.OpenCSVSerde\u0026#39; with serdeproperties ( 13\u0026#39;separatorChar\u0026#39;=\u0026#39;\\t\u0026#39; 14) 15location \u0026#39;s3://\u0026lt;s3-bucket-name\u0026gt;/title_basics/\u0026#39; 16tblproperties (\u0026#39;skip.header.line.count\u0026#39;=\u0026#39;1\u0026#39;) Interestingly the header rows of the source tables are not skipped when they are queried by spark while they are skipped by Athena. They have to be filtered out in the stage models of the dbt project as spark is the query engine.\nStaging Based on the source tables, staging models are created. They are created as views, which is the project’s default materialisation. In the SQL statements, column names and data types are modified mainly.\n1# emr-ec2/emr_ec2/models/staging/imdb/stg_imdb__title_basics.sql 2with source as ( 3 4 select * from {{ source(\u0026#39;imdb\u0026#39;, \u0026#39;title_basics\u0026#39;) }} 5 6), 7 8renamed as ( 9 10 select 11 tconst as title_id, 12 titletype as title_type, 13 primarytitle as primary_title, 14 originaltitle as original_title, 15 cast(isadult as boolean) as is_adult, 16 cast(startyear as int) as start_year, 17 cast(endyear as int) as end_year, 18 cast(runtimeminutes as int) as runtime_minutes, 19 case when genres = \u0026#39;N\u0026#39; then null else genres end as genres 20 from source 21 where tconst \u0026lt;\u0026gt; \u0026#39;tconst\u0026#39; 22 23) 24 25select * from renamed Below shows the file tree of the staging models. The staging models can be executed using the dbt run command. As we’ve added tags to the staging layer models, we can limit to execute only this layer by dbt run \u0026ndash;select staging.\n1$ tree emr-ec2/emr_ec2/models/staging/ 2emr-ec2/emr_ec2/models/staging/ 3└── imdb 4 ├── _imdb__models.yml 5 ├── _imdb__sources.yml 6 ├── stg_imdb__name_basics.sql 7 ├── stg_imdb__title_akas.sql 8 ├── stg_imdb__title_basics.sql 9 ├── stg_imdb__title_crews.sql 10 ├── stg_imdb__title_episodes.sql 11 ├── stg_imdb__title_principals.sql 12 └── stg_imdb__title_ratings.sql Note that the model materialisation of the staging and intermediate models is view and the dbt project creates VIRTUAL_VIEW tables. Although we are able to reference those tables in other models, they cannot be queried by Athena.\n1$ aws glue get-tables --database imdb \\ 2 --query \u0026#34;TableList[?Name==\u0026#39;stg_imdb__title_basics\u0026#39;].[Name, TableType, StorageDescriptor.Columns]\u0026#34; --output yaml 3- - stg_imdb__title_basics 4 - VIRTUAL_VIEW 5 - - Name: title_id 6 Type: string 7 - Name: title_type 8 Type: string 9 - Name: primary_title 10 Type: string 11 - Name: original_title 12 Type: string 13 - Name: is_adult 14 Type: boolean 15 - Name: start_year 16 Type: int 17 - Name: end_year 18 Type: int 19 - Name: runtime_minutes 20 Type: int 21 - Name: genres 22 Type: string Instead we can use spark sql to query the tables as shown below.\nIntermediate We can keep intermediate results in this layer so that the models of the final marts layer can be simplified. The source data includes columns where array values are kept as comma separated strings. For example, the genres column of the stg_imdb__title_basics model includes up to three genre values as shown in the previous screenshot. A total of seven columns in three models are columns of comma-separated strings and it is better to flatten them in the intermediate layer. Also, in order to avoid repetition, a dbt macro (f_latten_fields_) is created to share the column-flattening logic.\n1# emr-ec2/emr_ec2/macros/flatten_fields.sql 2{% macro flatten_fields(model, field_name, id_field_name) %} 3 select 4 {{ id_field_name }} as id, 5 explode(split({{ field_name }}, \u0026#39;,\u0026#39;)) as field 6 from {{ model }} 7{% endmacro %} The macro function can be added inside a common table expression (CTE) by specifying the relevant model, field name to flatten and id field name.\n1-- emr-ec2/emr_ec2/models/intermediate/title/int_genres_flattened_from_title_basics.sql 2with flattened as ( 3 {{ flatten_fields(ref(\u0026#39;stg_imdb__title_basics\u0026#39;), \u0026#39;genres\u0026#39;, \u0026#39;title_id\u0026#39;) }} 4) 5 6select 7 id as title_id, 8 field as genre 9from flattened 10order by id The intermediate models are also materialised as views and we can check the array columns are flattened as expected.\nBelow shows the file tree of the intermediate models. Similar to the staging models, the intermediate models can be executed by dbt run --select intermediate.\n1$ tree emr-ec2/emr_ec2/models/intermediate/ emr-ec2/emr_ec2/macros/ 2emr-ec2/emr_ec2/models/intermediate/ 3├── name 4│ ├── _int_name__models.yml 5│ ├── int_known_for_titles_flattened_from_name_basics.sql 6│ └── int_primary_profession_flattened_from_name_basics.sql 7└── title 8 ├── _int_title__models.yml 9 ├── int_directors_flattened_from_title_crews.sql 10 ├── int_genres_flattened_from_title_basics.sql 11 └── int_writers_flattened_from_title_crews.sql 12 13emr-ec2/emr_ec2/macros/ 14└── flatten_fields.sql Marts The models in the marts layer are configured to be materialised as tables in a custom schema. Their materialisation is set to table and the custom schema is specified as analytics while taking _parquet _as the file format. Note that the custom schema name becomes imdb_analytics according to the naming convention of dbt custom schemas. Models of both the staging and intermediate layers are used to create final models to be used for reporting and analytics.\n1-- emr-ec2/emr_ec2/models/marts/analytics/titles.sql 2{{ 3 config( 4 schema=\u0026#39;analytics\u0026#39;, 5 materialized=\u0026#39;table\u0026#39;, 6 file_format=\u0026#39;parquet\u0026#39; 7 ) 8}} 9 10with titles as ( 11 12 select * from {{ ref(\u0026#39;stg_imdb__title_basics\u0026#39;) }} 13 14), 15 16principals as ( 17 18 select 19 title_id, 20 count(name_id) as num_principals 21 from {{ ref(\u0026#39;stg_imdb__title_principals\u0026#39;) }} 22 group by title_id 23 24), 25 26names as ( 27 28 select 29 title_id, 30 count(name_id) as num_names 31 from {{ ref(\u0026#39;int_known_for_titles_flattened_from_name_basics\u0026#39;) }} 32 group by title_id 33 34), 35 36ratings as ( 37 38 select 39 title_id, 40 average_rating, 41 num_votes 42 from {{ ref(\u0026#39;stg_imdb__title_ratings\u0026#39;) }} 43 44), 45 46episodes as ( 47 48 select 49 parent_title_id, 50 count(title_id) as num_episodes 51 from {{ ref(\u0026#39;stg_imdb__title_episodes\u0026#39;) }} 52 group by parent_title_id 53 54), 55 56distributions as ( 57 58 select 59 title_id, 60 count(title) as num_distributions 61 from {{ ref(\u0026#39;stg_imdb__title_akas\u0026#39;) }} 62 group by title_id 63 64), 65 66final as ( 67 68 select 69 t.title_id, 70 t.title_type, 71 t.primary_title, 72 t.original_title, 73 t.is_adult, 74 t.start_year, 75 t.end_year, 76 t.runtime_minutes, 77 t.genres, 78 p.num_principals, 79 n.num_names, 80 r.average_rating, 81 r.num_votes, 82 e.num_episodes, 83 d.num_distributions 84 from titles as t 85 left join principals as p on t.title_id = p.title_id 86 left join names as n on t.title_id = n.title_id 87 left join ratings as r on t.title_id = r.title_id 88 left join episodes as e on t.title_id = e.parent_title_id 89 left join distributions as d on t.title_id = d.title_id 90 91) 92 93select * from final The details of the three models can be found in a YAML file (_analytics__models.yml). We can add tests to models and below we see tests of row count matching to their corresponding staging models.\n1# emr-ec2/emr_ec2/models/marts/analytics/_analytics__models.yml 2version: 2 3 4models: 5 - name: names 6 description: Table that contains all names with additional details 7 tests: 8 - dbt_utils.equal_rowcount: 9 compare_model: ref(\u0026#39;stg_imdb__name_basics\u0026#39;) 10 - name: titles 11 description: Table that contains all titles with additional details 12 tests: 13 - dbt_utils.equal_rowcount: 14 compare_model: ref(\u0026#39;stg_imdb__title_basics\u0026#39;) 15 - name: genre_titles 16 description: Table that contains basic title details after flattening genres The models of the marts layer can be tested using the dbt test command as shown below.\n1$ dbt test --select marts 219:29:31 Running with dbt=1.2.2 319:29:31 Found 15 models, 17 tests, 0 snapshots, 0 analyses, 533 macros, 0 operations, 0 seed files, 7 sources, 0 exposures, 0 metrics 419:29:31 519:29:41 Concurrency: 3 threads (target=\u0026#39;dev\u0026#39;) 619:29:41 719:29:41 1 of 2 START test dbt_utils_equal_rowcount_names_ref_stg_imdb__name_basics_ .... [RUN] 819:29:41 2 of 2 START test dbt_utils_equal_rowcount_titles_ref_stg_imdb__title_basics_ .. [RUN] 919:29:54 1 of 2 PASS dbt_utils_equal_rowcount_names_ref_stg_imdb__name_basics_ .......... [PASS in 13.11s] 1019:29:56 2 of 2 PASS dbt_utils_equal_rowcount_titles_ref_stg_imdb__title_basics_ ........ [PASS in 15.14s] 1119:29:56 1219:29:56 Finished running 2 tests in 0 hours 0 minutes and 25.54 seconds (25.54s). 1319:29:56 1419:29:56 Completed successfully 1519:29:56 1619:29:56 Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2 Below shows the file tree of the marts models. As with the other layers, the marts models can be executed by dbt run --select marts.\n1$ tree emr-ec2/emr_ec2/models/marts/ 2emr-ec2/emr_ec2/models/marts/ 3└── analytics 4 ├── _analytics__models.yml 5 ├── genre_titles.sql 6 ├── names.sql 7 └── titles.sql Build Dashboard The models of the marts layer can be consumed by external tools such as Amazon QuickSight. Below shows an example dashboard. The pie chart on the left shows the proportion of titles by genre while the box plot on the right shows the dispersion of average rating by title type.\nGenerate dbt Documentation A nice feature of dbt is documentation. It provides information about the project and the data warehouse and it facilitates consumers as well as other developers to discover and understand the datasets better. We can generate the project documents and start a document server as shown below.\n1$ dbt docs generate 2$ dbt docs serve A very useful element of dbt documentation is data lineage, which provides an overall view about how data is transformed and consumed. Below we can see that the final titles model consumes all title-related stating models and an intermediate model from the name basics staging model.\nSummary In this post, we discussed how to build data transformation pipelines using dbt on Amazon EMR. Subsets of IMDb data are used as source and data models are developed in multiple layers according to the dbt best practices. dbt can be used as an effective tool for data transformation in a wide range of data projects from data warehousing to data lake to data lakehouse and it supports key AWS analytics services - Redshift, Glue, EMR and Athena. More examples of using dbt will be discussed in subsequent posts.\n","date":"October 19, 2022","img":"/blog/2022-10-19-dbt-on-aws-part-3-emr-ec2/featured.png","lang":"en","langName":"English","largeImg":"/blog/2022-10-19-dbt-on-aws-part-3-emr-ec2/featured_huad7bc56c3140ba61f8a863cb7496d5ad_91067_500x0_resize_box_3.png","permalink":"/blog/2022-10-19-dbt-on-aws-part-3-emr-ec2/","series":[{"title":"DBT for Effective Data Transformation on AWS","url":"/series/dbt-for-effective-data-transformation-on-aws/"}],"smallImg":"/blog/2022-10-19-dbt-on-aws-part-3-emr-ec2/featured_huad7bc56c3140ba61f8a863cb7496d5ad_91067_180x0_resize_box_3.png","tags":[{"title":"AWS","url":"/tags/aws/"},{"title":"Amazon EMR","url":"/tags/amazon-emr/"},{"title":"Amazon QuickSight","url":"/tags/amazon-quicksight/"},{"title":"Data Build Tool (DBT)","url":"/tags/data-build-tool-dbt/"},{"title":"Apache Spark","url":"/tags/apache-spark/"},{"title":"Terraform","url":"/tags/terraform/"}],"timestamp":1666137600,"title":"Data Build Tool (Dbt) for Effective Data Transformation on AWS – Part 3 EMR on EC2"},{"categories":[{"title":"Data Engineering","url":"/categories/data-engineering/"}],"content":"The data build tool (dbt) is an effective data transformation tool and it supports key AWS analytics services - Redshift, Glue, EMR and Athena. In part 1, we discussed benefits of a common data transformation tool and the potential of dbt to cover a wide range of data projects from data warehousing to data lake to data lakehouse. A demo data project that targets Redshift Serverless is illustrated as well. In part 2 of the dbt on AWS series, we discuss data transformation pipelines using dbt on AWS Glue. Subsets of IMDb data are used as source and data models are developed in multiple layers according to the dbt best practices. A list of posts of this series can be found below.\nPart 1 Redshift Part 2 Glue (this post) Part 3 EMR on EC2 Part 4 EMR on EKS Part 5 Athena Below shows an overview diagram of the scope of this dbt on AWS series. Glue is highlighted as it is discussed in this post.\nInfrastructure The infrastructure hosting this solution leverages AWS Glue Data Catalog, AWS Glue Crawlers and a S3 bucket. We also need a runtime IAM role for AWS Glue interactive sessions for data transformation. They are deployed using Terraform and the source can be found in the GitHub repository of this post.\nGlue Databases We have two Glue databases. The source tables and the tables of the staging and intermediate layers are kept in the imdb database. The tables of the marts layer are stored in the _imdb_analytics _database.\n1# dbt-on-aws/glue/infra/main.tf 2resource \u0026#34;aws_glue_catalog_database\u0026#34; \u0026#34;imdb_db\u0026#34; { 3 name = \u0026#34;imdb\u0026#34; 4 description = \u0026#34;Database that contains IMDb staging/intermediate model datasets\u0026#34; 5} 6 7resource \u0026#34;aws_glue_catalog_database\u0026#34; \u0026#34;imdb_db_marts\u0026#34; { 8 name = \u0026#34;imdb_analytics\u0026#34; 9 description = \u0026#34;Database that contains IMDb marts model datasets\u0026#34; 10} Glue Crawlers We use Glue crawlers to create source tables in the imdb database. We can create a single crawler for the seven source tables but it was not satisfactory, especially header detection. Instead, a dedicated crawler is created for each of the tables with its own custom classifier where it includes header columns specifically. The Terraform count meta-argument is used to create the crawlers and classifiers recursively.\n1# dbt-on-aws/glue/infra/main.tf 2resource \u0026#34;aws_glue_crawler\u0026#34; \u0026#34;imdb_crawler\u0026#34; { 3 count = length(local.glue.tables) 4 5 name = local.glue.tables[count.index].name 6 database_name = aws_glue_catalog_database.imdb_db.name 7 role = aws_iam_role.imdb_crawler.arn 8 classifiers = [aws_glue_classifier.imdb_crawler[count.index].id] 9 10 s3_target { 11 path = \u0026#34;s3://${local.default_bucket.name}/${local.glue.tables[count.index].name}\u0026#34; 12 } 13 14 tags = local.tags 15} 16 17resource \u0026#34;aws_glue_classifier\u0026#34; \u0026#34;imdb_crawler\u0026#34; { 18 count = length(local.glue.tables) 19 20 name = local.glue.tables[count.index].name 21 22 csv_classifier { 23 contains_header = \u0026#34;PRESENT\u0026#34; 24 delimiter = \u0026#34;\\t\u0026#34; 25 header = local.glue.tables[count.index].header 26 } 27} 28 29# dbt-on-aws/glue/infra/variables.tf 30locals { 31 name = basename(path.cwd) == \u0026#34;infra\u0026#34; ? basename(dirname(path.cwd)) : basename(path.cwd) 32 region = data.aws_region.current.name 33 environment = \u0026#34;dev\u0026#34; 34 35 default_bucket = { 36 name = \u0026#34;${local.name}-${data.aws_caller_identity.current.account_id}-${local.region}\u0026#34; 37 } 38 39 glue = { 40 tables = [ 41 { name = \u0026#34;name_basics\u0026#34;, header = [\u0026#34;nconst\u0026#34;, \u0026#34;primaryName\u0026#34;, \u0026#34;birthYear\u0026#34;, \u0026#34;deathYear\u0026#34;, \u0026#34;primaryProfession\u0026#34;, \u0026#34;knownForTitles\u0026#34;] }, 42 { name = \u0026#34;title_akas\u0026#34;, header = [\u0026#34;titleId\u0026#34;, \u0026#34;ordering\u0026#34;, \u0026#34;title\u0026#34;, \u0026#34;region\u0026#34;, \u0026#34;language\u0026#34;, \u0026#34;types\u0026#34;, \u0026#34;attributes\u0026#34;, \u0026#34;isOriginalTitle\u0026#34;] }, 43 { name = \u0026#34;title_basics\u0026#34;, header = [\u0026#34;tconst\u0026#34;, \u0026#34;titleType\u0026#34;, \u0026#34;primaryTitle\u0026#34;, \u0026#34;originalTitle\u0026#34;, \u0026#34;isAdult\u0026#34;, \u0026#34;startYear\u0026#34;, \u0026#34;endYear\u0026#34;, \u0026#34;runtimeMinutes\u0026#34;, \u0026#34;genres\u0026#34;] }, 44 { name = \u0026#34;title_crew\u0026#34;, header = [\u0026#34;tconst\u0026#34;, \u0026#34;directors\u0026#34;, \u0026#34;writers\u0026#34;] }, 45 { name = \u0026#34;title_episode\u0026#34;, header = [\u0026#34;tconst\u0026#34;, \u0026#34;parentTconst\u0026#34;, \u0026#34;seasonNumber\u0026#34;, \u0026#34;episodeNumber\u0026#34;] }, 46 { name = \u0026#34;title_principals\u0026#34;, header = [\u0026#34;tconst\u0026#34;, \u0026#34;ordering\u0026#34;, \u0026#34;nconst\u0026#34;, \u0026#34;category\u0026#34;, \u0026#34;job\u0026#34;, \u0026#34;characters\u0026#34;] }, 47 { name = \u0026#34;title_ratings\u0026#34;, header = [\u0026#34;tconst\u0026#34;, \u0026#34;averageRating\u0026#34;, \u0026#34;numVotes\u0026#34;] } 48 ] 49 } 50 51 tags = { 52 Name = local.name 53 Environment = local.environment 54 } 55} Glue Runtime Role for Interactive Sessions We need a runtime (or service) role for Glue interaction sessions. AWS Glue uses this role to run statements in a session, and it is required to generate a profile by the dbt-glue adapter. Two policies are attached to the runtime role - the former is related to managing Glue interactive sessions while the latter is for actual data transformation by Glue.\n1# dbt-on-aws/glue/infra/main.tf 2resource \u0026#34;aws_iam_role\u0026#34; \u0026#34;glue_interactive_session\u0026#34; { 3 name = \u0026#34;${local.name}-glue-interactive-session\u0026#34; 4 5 assume_role_policy = data.aws_iam_policy_document.glue_interactive_session_assume_role_policy.json 6 managed_policy_arns = [ 7 aws_iam_policy.glue_interactive_session.arn, 8 aws_iam_policy.glue_dbt.arn 9 ] 10 11 tags = local.tags 12} 13 14data \u0026#34;aws_iam_policy_document\u0026#34; \u0026#34;glue_interactive_session_assume_role_policy\u0026#34; { 15 statement { 16 actions = [\u0026#34;sts:AssumeRole\u0026#34;] 17 18 principals { 19 type = \u0026#34;Service\u0026#34; 20 identifiers = [ 21 \u0026#34;lakeformation.amazonaws.com\u0026#34;, 22 \u0026#34;glue.amazonaws.com\u0026#34; 23 ] 24 } 25 } 26} 27 28resource \u0026#34;aws_iam_policy\u0026#34; \u0026#34;glue_interactive_session\u0026#34; { 29 name = \u0026#34;${local.name}-glue-interactive-session\u0026#34; 30 path = \u0026#34;/\u0026#34; 31 policy = data.aws_iam_policy_document.glue_interactive_session.json 32 tags = local.tags 33} 34 35resource \u0026#34;aws_iam_policy\u0026#34; \u0026#34;glue_dbt\u0026#34; { 36 name = \u0026#34;${local.name}-glue-dbt\u0026#34; 37 path = \u0026#34;/\u0026#34; 38 policy = data.aws_iam_policy_document.glue_dbt.json 39 tags = local.tags 40} 41 42data \u0026#34;aws_iam_policy_document\u0026#34; \u0026#34;glue_interactive_session\u0026#34; { 43 statement { 44 sid = \u0026#34;AllowStatementInASessionToAUser\u0026#34; 45 46 actions = [ 47 \u0026#34;glue:ListSessions\u0026#34;, 48 \u0026#34;glue:GetSession\u0026#34;, 49 \u0026#34;glue:ListStatements\u0026#34;, 50 \u0026#34;glue:GetStatement\u0026#34;, 51 \u0026#34;glue:RunStatement\u0026#34;, 52 \u0026#34;glue:CancelStatement\u0026#34;, 53 \u0026#34;glue:DeleteSession\u0026#34; 54 ] 55 56 resources = [ 57 \u0026#34;arn:aws:glue:${local.region}:${data.aws_caller_identity.current.account_id}:session/*\u0026#34;, 58 ] 59 } 60 61 statement { 62 actions = [\u0026#34;glue:CreateSession\u0026#34;] 63 64 resources = [\u0026#34;*\u0026#34;] 65 } 66 67 statement { 68 actions = [\u0026#34;iam:PassRole\u0026#34;] 69 70 resources = [\u0026#34;arn:aws:iam::*:role/${local.name}-glue-interactive-session*\u0026#34;] 71 72 condition { 73 test = \u0026#34;StringLike\u0026#34; 74 variable = \u0026#34;iam:PassedToService\u0026#34; 75 76 values = [\u0026#34;glue.amazonaws.com\u0026#34;] 77 } 78 } 79 80 statement { 81 actions = [\u0026#34;iam:PassRole\u0026#34;] 82 83 resources = [\u0026#34;arn:aws:iam::*:role/service-role/${local.name}-glue-interactive-session*\u0026#34;] 84 85 condition { 86 test = \u0026#34;StringLike\u0026#34; 87 variable = \u0026#34;iam:PassedToService\u0026#34; 88 89 values = [\u0026#34;glue.amazonaws.com\u0026#34;] 90 } 91 } 92} 93 94data \u0026#34;aws_iam_policy_document\u0026#34; \u0026#34;glue_dbt\u0026#34; { 95 statement { 96 actions = [ 97 \u0026#34;glue:SearchTables\u0026#34;, 98 \u0026#34;glue:BatchCreatePartition\u0026#34;, 99 \u0026#34;glue:CreatePartitionIndex\u0026#34;, 100 \u0026#34;glue:DeleteDatabase\u0026#34;, 101 \u0026#34;glue:GetTableVersions\u0026#34;, 102 \u0026#34;glue:GetPartitions\u0026#34;, 103 \u0026#34;glue:DeleteTableVersion\u0026#34;, 104 \u0026#34;glue:UpdateTable\u0026#34;, 105 \u0026#34;glue:DeleteTable\u0026#34;, 106 \u0026#34;glue:DeletePartitionIndex\u0026#34;, 107 \u0026#34;glue:GetTableVersion\u0026#34;, 108 \u0026#34;glue:UpdateColumnStatisticsForTable\u0026#34;, 109 \u0026#34;glue:CreatePartition\u0026#34;, 110 \u0026#34;glue:UpdateDatabase\u0026#34;, 111 \u0026#34;glue:CreateTable\u0026#34;, 112 \u0026#34;glue:GetTables\u0026#34;, 113 \u0026#34;glue:GetDatabases\u0026#34;, 114 \u0026#34;glue:GetTable\u0026#34;, 115 \u0026#34;glue:GetDatabase\u0026#34;, 116 \u0026#34;glue:GetPartition\u0026#34;, 117 \u0026#34;glue:UpdateColumnStatisticsForPartition\u0026#34;, 118 \u0026#34;glue:CreateDatabase\u0026#34;, 119 \u0026#34;glue:BatchDeleteTableVersion\u0026#34;, 120 \u0026#34;glue:BatchDeleteTable\u0026#34;, 121 \u0026#34;glue:DeletePartition\u0026#34;, 122 \u0026#34;glue:GetUserDefinedFunctions\u0026#34; 123 ] 124 125 resources = [ 126 \u0026#34;arn:aws:glue:${local.region}:${data.aws_caller_identity.current.account_id}:catalog\u0026#34;, 127 \u0026#34;arn:aws:glue:${local.region}:${data.aws_caller_identity.current.account_id}:table/*/*\u0026#34;, 128 \u0026#34;arn:aws:glue:${local.region}:${data.aws_caller_identity.current.account_id}:database/*\u0026#34;, 129 ] 130 } 131 132 statement { 133 actions = [ 134 \u0026#34;lakeformation:UpdateResource\u0026#34;, 135 \u0026#34;lakeformation:ListResources\u0026#34;, 136 \u0026#34;lakeformation:BatchGrantPermissions\u0026#34;, 137 \u0026#34;lakeformation:GrantPermissions\u0026#34;, 138 \u0026#34;lakeformation:GetDataAccess\u0026#34;, 139 \u0026#34;lakeformation:GetTableObjects\u0026#34;, 140 \u0026#34;lakeformation:PutDataLakeSettings\u0026#34;, 141 \u0026#34;lakeformation:RevokePermissions\u0026#34;, 142 \u0026#34;lakeformation:ListPermissions\u0026#34;, 143 \u0026#34;lakeformation:BatchRevokePermissions\u0026#34;, 144 \u0026#34;lakeformation:UpdateTableObjects\u0026#34; 145 ] 146 147 resources = [\u0026#34;*\u0026#34;] 148 } 149 150 statement { 151 actions = [ 152 \u0026#34;s3:GetBucketLocation\u0026#34;, 153 \u0026#34;s3:ListBucket\u0026#34; 154 ] 155 156 resources = [\u0026#34;arn:aws:s3:::${local.default_bucket.name}\u0026#34;] 157 } 158 159 statement { 160 actions = [ 161 \u0026#34;s3:PutObject\u0026#34;, 162 \u0026#34;s3:PutObjectAcl\u0026#34;, 163 \u0026#34;s3:GetObject\u0026#34;, 164 \u0026#34;s3:DeleteObject\u0026#34; 165 ] 166 167 resources = [\u0026#34;arn:aws:s3:::${local.default_bucket.name}/*\u0026#34;] 168 } 169} Project We build a data transformation pipeline using subsets of IMDb data - seven titles and names related datasets are provided as gzipped, tab-separated-values (TSV) formatted files. This results in three tables that can be used for reporting and analysis.\nSave Data to S3 The Axel download accelerator is used to download the data files locally followed by decompressing with the gzip utility. Note that simple retry logic is added as I see download failure from time to time. Finally, the decompressed files are saved into the project S3 bucket using the S3 sync command.\n1# dbt-on-aws/glue/upload-data.sh 2#!/usr/bin/env bash 3 4s3_bucket=$(terraform -chdir=./infra output --raw default_bucket_name) 5hostname=\u0026#34;datasets.imdbws.com\u0026#34; 6declare -a file_names=( 7 \u0026#34;name.basics.tsv.gz\u0026#34; \\ 8 \u0026#34;title.akas.tsv.gz\u0026#34; \\ 9 \u0026#34;title.basics.tsv.gz\u0026#34; \\ 10 \u0026#34;title.crew.tsv.gz\u0026#34; \\ 11 \u0026#34;title.episode.tsv.gz\u0026#34; \\ 12 \u0026#34;title.principals.tsv.gz\u0026#34; \\ 13 \u0026#34;title.ratings.tsv.gz\u0026#34; 14 ) 15 16rm -rf imdb-data 17 18for fn in \u0026#34;${file_names[@]}\u0026#34; 19do 20 download_url=\u0026#34;https://$hostname/$fn\u0026#34; 21 prefix=$(echo ${fn::-7} | tr \u0026#39;.\u0026#39; \u0026#39;_\u0026#39;) 22 echo \u0026#34;download imdb-data/$prefix/$fn from $download_url\u0026#34; 23 while true; 24 do 25 mkdir -p imdb-data/$prefix 26 axel -n 32 -a -o imdb-data/$prefix/$fn $download_url 27 gzip -d imdb-data/$prefix/$fn 28 num_files=$(ls imdb-data/$prefix | wc -l) 29 if [ $num_files == 1 ]; then 30 break 31 fi 32 rm -rf imdb-data/$prefix 33 done 34done 35 36aws s3 sync ./imdb-data s3://$s3_bucket Run Glue Crawlers The Glue crawlers for the seven source tables are executed as shown below.\n1# dbt-on-aws/glue/start-crawlers.sh 2#!/usr/bin/env bash 3 4declare -a crawler_names=( 5 \u0026#34;name_basics\u0026#34; \\ 6 \u0026#34;title_akas\u0026#34; \\ 7 \u0026#34;title_basics\u0026#34; \\ 8 \u0026#34;title_crew\u0026#34; \\ 9 \u0026#34;title_episode\u0026#34; \\ 10 \u0026#34;title_principals\u0026#34; \\ 11 \u0026#34;title_ratings\u0026#34; 12 ) 13 14for cn in \u0026#34;${crawler_names[@]}\u0026#34; 15do 16 echo \u0026#34;start crawler $cn ...\u0026#34; 17 aws glue start-crawler --name $cn 18done Note that the header rows of the source tables are not detected properly by the Glue crawlers, and they have to be filtered out in the stage models of the dbt project.\nSetup dbt Project We need the dbt-core and dbt-glue packages for the main data transformation project. Also the boto3 and aws-glue-sessions packages are necessary for setting up interactive sessions locally. The dbt Glue adapter doesn’t support creating a profile with the dbt init command so that profile creation is skipped when initialising the project.\n1$ pip install --no-cache-dir --upgrade boto3 aws-glue-sessions dbt-core dbt-glue 2$ dbt init --skip-profile-setup 310:04:00 Running with dbt=1.2.1 4Enter a name for your project (letters, digits, underscore): dbt_glue_proj The project profile is manually created as shown below. The attributes are self-explanatory and their details can be checked further in the GitHub repository of the dbt-glue adapter.\n1# dbt-on-aws/glue/set-profile.sh 2#!/usr/bin/env bash 3 4dbt_role_arn=$(terraform -chdir=./infra output --raw glue_interactive_session_role_arn) 5dbt_s3_location=$(terraform -chdir=./infra output --raw default_bucket_name) 6 7cat \u0026lt;\u0026lt; EOF \u0026gt; ~/.dbt/profiles.yml 8dbt_glue_proj: 9 outputs: 10 dev: 11 type: glue 12 role_arn: \u0026#34;${dbt_role_arn}\u0026#34; 13 region: ap-southeast-2 14 workers: 3 15 worker_type: G.1X 16 schema: imdb 17 database: imdb 18 session_provisioning_timeout_in_seconds: 240 19 location: \u0026#34;s3://${dbt_s3_location}\u0026#34; 20 query_timeout_in_seconds: 300 21 idle_timeout: 60 22 glue_version: \u0026#34;3.0\u0026#34; 23 target: dev 24EOF dbt initialises a project in a folder that matches to the project name and generates project boilerplate as shown below. Some of the main objects are dbt_project.yml, and the model folder. The former is required because dbt doesn\u0026rsquo;t know if a folder is a dbt project without it. Also, it contains information that tells dbt how to operate on the project. The latter is for including dbt models, which is basically a set of SQL select statements. See dbt documentation for more details.\n1$ tree glue/dbt_glue_proj/ -L 1 2glue/dbt_glue_proj/ 3├── README.md 4├── analyses 5├── dbt_packages 6├── dbt_project.yml 7├── logs 8├── macros 9├── models 10├── packages.yml 11├── seeds 12├── snapshots 13├── target 14└── tests We can check Glue interactive session connection with the dbt debug command as shown below.\n1$ dbt debug 208:50:58 Running with dbt=1.2.1 3dbt version: 1.2.1 4python version: 3.8.10 5python path: \u0026lt;path-to-python-path\u0026gt; 6os info: Linux-5.4.72-microsoft-standard-WSL2-x86_64-with-glibc2.29 7Using profiles.yml file at /home/\u0026lt;username\u0026gt;/.dbt/profiles.yml 8Using dbt_project.yml file at \u0026lt;path-to-dbt-project\u0026gt;/dbt_project.yml 9 10Configuration: 11 profiles.yml file [OK found and valid] 12 dbt_project.yml file [OK found and valid] 13 14Required dependencies: 15 - git [OK found] 16 17Connection: 18 role_arn: \u0026lt;glue-interactive-session-role-arn\u0026gt; 19 region: ap-southeast-2 20 session_id: None 21 workers: 3 22 worker_type: G.1X 23 session_provisioning_timeout_in_seconds: 240 24 database: imdb 25 schema: imdb 26 location: s3://\u0026lt;s3-bucket-name\u0026gt; 27 extra_jars: None 28 idle_timeout: 60 29 query_timeout_in_seconds: 300 30 glue_version: 3.0 31 security_configuration: None 32 connections: None 33 conf: None 34 extra_py_files: None 35 delta_athena_prefix: None 36 tags: None 37 Connection test: [OK connection ok] 38 39All checks passed! After initialisation, the model configuration is updated. The project materialisation is specified as view although it is the default materialisation. Also tags are added to the entire model folder as well as folders of specific layers - staging, intermediate and marts. As shown below, tags can simplify model execution.\n1# glue/dbt_glue_proj/dbt_project.yml 2name: \u0026#34;dbt_glue_proj\u0026#34; 3 4... 5 6models: 7 dbt_glue_proj: 8 +materialized: view 9 +tags: 10 - \u0026#34;imdb\u0026#34; 11 staging: 12 +tags: 13 - \u0026#34;staging\u0026#34; 14 intermediate: 15 +tags: 16 - \u0026#34;intermediate\u0026#34; 17 marts: 18 +tags: 19 - \u0026#34;marts\u0026#34; The dbt_utils package is installed for adding tests to the final marts models. The packages can be installed by the dbt deps command.\n1# glue/dbt_glue_proj/packages.yml 2packages: 3 - package: dbt-labs/dbt_utils 4 version: 0.9.2 Create dbt Models The models for this post are organised into three layers according to the dbt best practices - staging, intermediate and marts.\nStaging The seven tables that are loaded from S3 are dbt source tables and their details are declared in a YAML file (_imdb_sources.yml). By doing so, we are able to refer to the source tables with the {{ source() }} function. Also we can add tests to source tables. For example below two tests (unique, not_null) are added to the tconst column of the title_basics table below and these tests can be executed by the dbt test command.\n1# glue/dbt_glue_proj/models/staging/imdb/_imdb__sources.yml 2version: 2 3 4sources: 5 - name: imdb 6 description: Subsets of IMDb data, which are available for access to customers for personal and non-commercial use 7 tables: 8 - name: title_basics 9 description: Table that contains basic information of titles 10 columns: 11 - name: tconst 12 description: alphanumeric unique identifier of the title 13 tests: 14 - unique 15 - not_null 16 - name: titletype 17 description: the type/format of the title (e.g. movie, short, tvseries, tvepisode, video, etc) 18 - name: primarytitle 19 description: the more popular title / the title used by the filmmakers on promotional materials at the point of release 20 - name: originaltitle 21 description: original title, in the original language 22 - name: isadult 23 description: flag that indicates whether it is an adult title or not 24 - name: startyear 25 description: represents the release year of a title. In the case of TV Series, it is the series start year 26 - name: endyear 27 description: TV Series end year. NULL for all other title types 28 - name: runtime minutes 29 description: primary runtime of the title, in minutes 30 - name: genres 31 description: includes up to three genres associated with the title 32 ... Based on the source tables, staging models are created. They are created as views, which is the project’s default materialisation. In the SQL statements, column names and data types are modified mainly.\n1# glue/dbt_glue_proj/models/staging/imdb/stg_imdb__title_basics.sql 2with source as ( 3 4 select * from {{ source(\u0026#39;imdb\u0026#39;, \u0026#39;title_basics\u0026#39;) }} 5 6), 7 8renamed as ( 9 10 select 11 tconst as title_id, 12 titletype as title_type, 13 primarytitle as primary_title, 14 originaltitle as original_title, 15 cast(isadult as boolean) as is_adult, 16 cast(startyear as int) as start_year, 17 cast(endyear as int) as end_year, 18 cast(runtimeminutes as int) as runtime_minutes, 19 genres 20 from source 21 where tconst \u0026lt;\u0026gt; \u0026#39;tconst\u0026#39; 22 23) 24 25select * from renamed Below shows the file tree of the staging models. The staging models can be executed using the dbt run command. As we’ve added tags to the staging layer models, we can limit to execute only this layer by dbt run --select staging.\n1$ tree glue/dbt_glue_proj/models/staging/ 2glue/dbt_glue_proj/models/staging/ 3└── imdb 4 ├── _imdb__models.yml 5 ├── _imdb__sources.yml 6 ├── stg_imdb__name_basics.sql 7 ├── stg_imdb__title_akas.sql 8 ├── stg_imdb__title_basics.sql 9 ├── stg_imdb__title_crews.sql 10 ├── stg_imdb__title_episodes.sql 11 ├── stg_imdb__title_principals.sql 12 └── stg_imdb__title_ratings.sql Note that the model materialisation of the staging and intermediate models is view and the dbt project creates VIRTUAL_VIEW tables. Although we are able to reference those tables in other models, they cannot be queried by Athena.\n1$ aws glue get-tables --database imdb \\ 2 --query \u0026#34;TableList[?Name==\u0026#39;stg_imdb__title_basics\u0026#39;].[Name, TableType, StorageDescriptor.Columns]\u0026#34; --output yaml 3- - stg_imdb__title_basics 4 - VIRTUAL_VIEW 5 - - Name: title_id 6 Type: string 7 - Name: title_type 8 Type: string 9 - Name: primary_title 10 Type: string 11 - Name: original_title 12 Type: string 13 - Name: is_adult 14 Type: boolean 15 - Name: start_year 16 Type: int 17 - Name: end_year 18 Type: int 19 - Name: runtime_minutes 20 Type: int 21 - Name: genres 22 Type: string Instead we can use Glue Studio notebooks to query the tables, which is a bit inconvenient.\nIntermediate We can keep intermediate results in this layer so that the models of the final marts layer can be simplified. The source data includes columns where array values are kept as comma separated strings. For example, the genres column of the stg_imdb__title_basics model includes up to three genre values as shown in the previous screenshot. A total of seven columns in three models are columns of comma-separated strings and it is better to flatten them in the intermediate layer. Also, in order to avoid repetition, a dbt macro (f_latten_fields_) is created to share the column-flattening logic.\n1# glue/dbt_glue_proj/macros/flatten_fields.sql 2{% macro flatten_fields(model, field_name, id_field_name) %} 3 select 4 {{ id_field_name }} as id, 5 explode(split({{ field_name }}, \u0026#39;,\u0026#39;)) as field 6 from {{ model }} 7{% endmacro %} The macro function can be added inside a common table expression (CTE) by specifying the relevant model, field name to flatten and id field name.\n1-- glue/dbt_glue_proj/models/intermediate/title/int_genres_flattened_from_title_basics.sql 2with flattened as ( 3 {{ flatten_fields(ref(\u0026#39;stg_imdb__title_basics\u0026#39;), \u0026#39;genres\u0026#39;, \u0026#39;title_id\u0026#39;) }} 4) 5 6select 7 id as title_id, 8 field as genre 9from flattened 10order by id The intermediate models are also materialised as views and we can check the array columns are flattened as expected.\nBelow shows the file tree of the intermediate models. Similar to the staging models, the intermediate models can be executed by dbt run --select intermediate.\n1$ tree glue/dbt_glue_proj/models/intermediate/ glue/dbt_glue_proj/macros/ 2glue/dbt_glue_proj/models/intermediate/ 3├── name 4│ ├── _int_name__models.yml 5│ ├── int_known_for_titles_flattened_from_name_basics.sql 6│ └── int_primary_profession_flattened_from_name_basics.sql 7└── title 8 ├── _int_title__models.yml 9 ├── int_directors_flattened_from_title_crews.sql 10 ├── int_genres_flattened_from_title_basics.sql 11 └── int_writers_flattened_from_title_crews.sql 12 13glue/dbt_glue_proj/macros/ 14└── flatten_fields.sql Marts The models in the marts layer are configured to be materialised as tables in a custom schema. Their materialisation is set to table and the custom schema is specified as analytics while taking _parquet _as the file format. Note that the custom schema name becomes imdb_analytics according to the naming convention of dbt custom schemas. Models of both the staging and intermediate layers are used to create final models to be used for reporting and analytics.\n1-- glue/dbt_glue_proj/models/marts/analytics/titles.sql 2{{ 3 config( 4 schema=\u0026#39;analytics\u0026#39;, 5 materialized=\u0026#39;table\u0026#39;, 6 file_format=\u0026#39;parquet\u0026#39; 7 ) 8}} 9 10with titles as ( 11 12 select * from {{ ref(\u0026#39;stg_imdb__title_basics\u0026#39;) }} 13 14), 15 16principals as ( 17 18 select 19 title_id, 20 count(name_id) as num_principals 21 from {{ ref(\u0026#39;stg_imdb__title_principals\u0026#39;) }} 22 group by title_id 23 24), 25 26names as ( 27 28 select 29 title_id, 30 count(name_id) as num_names 31 from {{ ref(\u0026#39;int_known_for_titles_flattened_from_name_basics\u0026#39;) }} 32 group by title_id 33 34), 35 36ratings as ( 37 38 select 39 title_id, 40 average_rating, 41 num_votes 42 from {{ ref(\u0026#39;stg_imdb__title_ratings\u0026#39;) }} 43 44), 45 46episodes as ( 47 48 select 49 parent_title_id, 50 count(title_id) as num_episodes 51 from {{ ref(\u0026#39;stg_imdb__title_episodes\u0026#39;) }} 52 group by parent_title_id 53 54), 55 56distributions as ( 57 58 select 59 title_id, 60 count(title) as num_distributions 61 from {{ ref(\u0026#39;stg_imdb__title_akas\u0026#39;) }} 62 group by title_id 63 64), 65 66final as ( 67 68 select 69 t.title_id, 70 t.title_type, 71 t.primary_title, 72 t.original_title, 73 t.is_adult, 74 t.start_year, 75 t.end_year, 76 t.runtime_minutes, 77 t.genres, 78 p.num_principals, 79 n.num_names, 80 r.average_rating, 81 r.num_votes, 82 e.num_episodes, 83 d.num_distributions 84 from titles as t 85 left join principals as p on t.title_id = p.title_id 86 left join names as n on t.title_id = n.title_id 87 left join ratings as r on t.title_id = r.title_id 88 left join episodes as e on t.title_id = e.parent_title_id 89 left join distributions as d on t.title_id = d.title_id 90 91) 92 93select * from final The details of the three models can be found in a YAML file (_analytics__models.yml). We can add tests to models and below we see tests of row count matching to their corresponding staging models.\n1# glue/dbt_glue_proj/models/marts/analytics/_analytics__models.yml 2version: 2 3 4models: 5 - name: names 6 description: Table that contains all names with additional details 7 tests: 8 - dbt_utils.equal_rowcount: 9 compare_model: ref(\u0026#39;stg_imdb__name_basics\u0026#39;) 10 - name: titles 11 description: Table that contains all titles with additional details 12 tests: 13 - dbt_utils.equal_rowcount: 14 compare_model: ref(\u0026#39;stg_imdb__title_basics\u0026#39;) 15 - name: genre_titles 16 description: Table that contains basic title details after flattening genres The models of the marts layer can be tested using the dbt test command as shown below.\n1$ dbt test --select marts 209:11:51 Running with dbt=1.2.1 309:11:51 Found 15 models, 17 tests, 0 snapshots, 0 analyses, 521 macros, 0 operations, 0 seed files, 7 sources, 0 exposures, 0 metrics 409:11:51 509:12:28 Concurrency: 1 threads (target=\u0026#39;dev\u0026#39;) 609:12:28 709:12:28 1 of 2 START test dbt_utils_equal_rowcount_names_ref_stg_imdb__name_basics_ .... [RUN] 809:12:53 1 of 2 PASS dbt_utils_equal_rowcount_names_ref_stg_imdb__name_basics_ .......... [PASS in 24.94s] 909:12:53 2 of 2 START test dbt_utils_equal_rowcount_titles_ref_stg_imdb__title_basics_ .. [RUN] 1009:13:04 2 of 2 PASS dbt_utils_equal_rowcount_titles_ref_stg_imdb__title_basics_ ........ [PASS in 11.63s] 1109:13:07 1209:13:07 Finished running 2 tests in 0 hours 1 minutes and 15.74 seconds (75.74s). 1309:13:07 1409:13:07 Completed successfully 1509:13:07 1609:13:07 Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2 Below shows the file tree of the marts models. As with the other layers, the marts models can be executed by dbt run \u0026ndash;select marts.\n$ tree glue/dbt_glue_proj/models/marts/\rglue/dbt_glue_proj/models/marts/\r└── analytics\r├── _analytics__models.yml\r├── genre_titles.sql\r├── names.sql\r└── titles.sql Build Dashboard The models of the marts layer can be consumed by external tools such as Amazon QuickSight. Below shows an example dashboard. The two pie charts on top show proportions of genre and title type. The box plots at the bottom show dispersion of the number of votes and average rating by title type.\nGenerate dbt Documentation A nice feature of dbt is documentation. It provides information about the project and the data warehouse, and it facilitates consumers as well as other developers to discover and understand the datasets better. We can generate the project documents and start a document server as shown below.\n1$ dbt docs generate 2$ dbt docs serve A very useful element of dbt documentation is data lineage, which provides an overall view about how data is transformed and consumed. Below we can see that the final titles model consumes all title-related stating models and an intermediate model from the name basics staging model.\nSummary In this post, we discussed how to build data transformation pipelines using dbt on AWS Glue. Subsets of IMDb data are used as source and data models are developed in multiple layers according to the dbt best practices. dbt can be used as an effective tool for data transformation in a wide range of data projects from data warehousing to data lake to data lakehouse and it supports key AWS analytics services - Redshift, Glue, EMR and Athena. More examples of using dbt will be discussed in subsequent posts.\n","date":"October 9, 2022","img":"/blog/2022-10-09-dbt-on-aws-part-2-glue/featured.png","lang":"en","langName":"English","largeImg":"/blog/2022-10-09-dbt-on-aws-part-2-glue/featured_hu1d701231f4bedda4f795cc28c9478fc5_90647_500x0_resize_box_3.png","permalink":"/blog/2022-10-09-dbt-on-aws-part-2-glue/","series":[{"title":"DBT for Effective Data Transformation on AWS","url":"/series/dbt-for-effective-data-transformation-on-aws/"}],"smallImg":"/blog/2022-10-09-dbt-on-aws-part-2-glue/featured_hu1d701231f4bedda4f795cc28c9478fc5_90647_180x0_resize_box_3.png","tags":[{"title":"AWS","url":"/tags/aws/"},{"title":"AWS Glue","url":"/tags/aws-glue/"},{"title":"Amazon QuickSight","url":"/tags/amazon-quicksight/"},{"title":"Data Build Tool (DBT)","url":"/tags/data-build-tool-dbt/"},{"title":"Apache Spark","url":"/tags/apache-spark/"},{"title":"Terraform","url":"/tags/terraform/"}],"timestamp":1665273600,"title":"Data Build Tool (Dbt) for Effective Data Transformation on AWS – Part 2 Glue"},{"categories":[{"title":"Data Engineering","url":"/categories/data-engineering/"}],"content":"The data build tool (dbt) is an effective data transformation tool and it supports key AWS analytics services - Redshift, Glue, EMR and Athena. In part 1 of the dbt on AWS series, we discuss data transformation pipelines using dbt on Redshift Serverless. Subsets of IMDb data are used as source and data models are developed in multiple layers according to the dbt best practices.\nPart 1 Redshift (this post) Part 2 Glue Part 3 EMR on EC2 Part 4 EMR on EKS Part 5 Athena Motivation In our experience delivering data solutions for our customers, we have observed a desire to move away from a centralised team function, responsible for the data collection, analysis and reporting, towards shifting this responsibility to an organisation\u0026rsquo;s lines of business (LOB) teams. The key driver for this comes from the recognition that LOBs retain the deep data knowledge and business understanding for their respective data domain; which improves the speed with which these teams can develop data solutions and gain customer insights. This shift away from centralised data engineering to LOBs exposed a skills and tooling gap.\nLet\u0026rsquo;s assume as a starting point that the central data engineering team has chosen a project that migrates an on-premise data warehouse into a data lake (spark + iceberg + redshift) on AWS, to provide a cost-effective way to serve data consumers thanks to iceberg\u0026rsquo;s ACID transaction features. The LOB data engineers are new to spark, and they have a little bit of experience in python while the majority of their work is based on SQL. Thanks to their expertise in SQL, however, they are able to get started building data transformation logic on jupyter notebooks using Pyspark. However, they soon find the codebase gets quite bigger even during the minimum valuable product (MVP) phase, which would only amplify the issue as they extend it to cover the entire data warehouse. Additionally the use of notebooks makes development challenging mainly due to lack of modularity and fai,ling to incorporate testing. Upon contacting the central data engineering team for assistance they are advised that the team uses scala and many other tools (e.g. Metorikku) that are successful for them, however cannot be used directly by the engineers of the LOB. Moreover, the engineering team don\u0026rsquo;t even have a suitable data transformation framework that supports iceberg. The LOB data engineering team understand that the data democratisation plan of the enterprise can be more effective if there is a tool or framework that:\ncan be shared across LOBs although they can have different technology stack and practices, fits into various project types from traditional data warehousing to data lakehouse projects, and supports more than a notebook environment by facilitating code modularity and incorporating testing. The data build tool (dbt) is an open-source command line tool, and it does the T in ELT (Extract, Load, Transform) processes well. It supports a wide range of data platforms and the following key AWS analytics services are covered - Redshift, Glue, EMR and Athena. It is one of the most popular tools in the modern data stack that originally covers data warehousing projects. Its scope is extended to data lake projects by the addition of the dbt-spark and dbt-glue adapter where we can develop data lakes with spark SQL. Recently the spark adapter added open source table formats (hudi, iceberg and delta lake) as the supported file formats, and it allows you to work on data lake house projects with it. As discussed in this blog post, dbt has clear advantages compared to spark in terms of\nlow learning curve as SQL is easier than spark better code organisation as there is no correct way of organising transformation pipeline with spark On the other hand, its weaknesses are\nlack of expressiveness as Jinja is quite heavy and verbose, not very readable, and unit-testing is rather tedious limitation of SQL as some logic is much easier to implement with user defined functions rather than SQL Those weaknesses can be overcome by Python models as it allows you to apply transformations as DataFrame operations. Unfortunately the beta feature is not available on any of the AWS services, however it is available on Snowflake, Databricks and BigQuery. Hopefully we can use this feature on Redshift, Glue and EMR in the near future.\nFinally, the following areas are supported by spark, however not supported by DBT:\nE and L of ELT processes real time data processing Overall dbt can be used as an effective tool for data transformation in a wide range of data projects from data warehousing to data lake to data lakehouse. Also it can be more powerful with spark by its Python models feature. Below shows an overview diagram of the scope of this dbt on AWS series. Redshift is highlighted as it is discussed in this post.\nInfrastructure A VPC with 3 public and private subnets is created using the AWS VPC Terraform module. The following Redshift serverless resources are deployed to work with the dbt project. As explained in the Redshift user guide, a namespace is a collection of database objects and users and a workgroup is a collection of compute resources. We also need a Redshift-managed VPC endpoint for a private connection from a client tool. The source can be found in the GitHub repository of this post.\n1# redshift-sls/infra/redshift-sls.tf 2resource \u0026#34;aws_redshiftserverless_namespace\u0026#34; \u0026#34;namespace\u0026#34; { 3 namespace_name = \u0026#34;${local.name}-namespace\u0026#34; 4 5 admin_username = local.redshift.admin_username 6 admin_user_password = local.secrets.redshift_admin_password 7 db_name = local.redshift.db_name 8 default_iam_role_arn = aws_iam_role.redshift_serverless_role.arn 9 iam_roles = [aws_iam_role.redshift_serverless_role.arn] 10 11 tags = local.tags 12} 13 14resource \u0026#34;aws_redshiftserverless_workgroup\u0026#34; \u0026#34;workgroup\u0026#34; { 15 namespace_name = aws_redshiftserverless_namespace.namespace.id 16 workgroup_name = \u0026#34;${local.name}-workgroup\u0026#34; 17 18 base_capacity = local.redshift.base_capacity # 128 19 subnet_ids = module.vpc.private_subnets 20 security_group_ids = [aws_security_group.vpn_redshift_serverless_access.id] 21 22 tags = local.tags 23} 24 25resource \u0026#34;aws_redshiftserverless_endpoint_access\u0026#34; \u0026#34;endpoint_access\u0026#34; { 26 endpoint_name = \u0026#34;${local.name}-endpoint\u0026#34; 27 28 workgroup_name = aws_redshiftserverless_workgroup.workgroup.id 29 subnet_ids = module.vpc.private_subnets 30} As in the previous post, we connect to Redshift via SoftEther VPN to improve developer experience significantly by accessing the database directly from the developer machine. Instead of providing VPN related secrets as Terraform variables in the earlier post, they are created internally and stored to AWS Secrets Manager. Also, the Redshift admin username and password are included so that the secrets can be accessed securely. The details can be found in redshift-sls/infra/secrets.tf and the secret string can be retrieved as shown below.\n1$ aws secretsmanager get-secret-value --secret-id redshift-sls-all-secrets --query \u0026#34;SecretString\u0026#34; --output text 2 { 3 \u0026#34;vpn_pre_shared_key\u0026#34;: \u0026#34;\u0026lt;vpn-pre-shared-key\u0026gt;\u0026#34;, 4 \u0026#34;vpn_admin_password\u0026#34;: \u0026#34;\u0026lt;vpn-admin-password\u0026gt;\u0026#34;, 5 \u0026#34;redshift_admin_username\u0026#34;: \u0026#34;master\u0026#34;, 6 \u0026#34;redshift_admin_password\u0026#34;: \u0026#34;\u0026lt;redshift-admin-password\u0026gt;\u0026#34; 7 } The previous post demonstrates how to create a VPN user and to establish connection in detail. An example of a successful connection is shown below.\nProject We build a data transformation pipeline using subsets of IMDb data - seven titles and names related datasets are provided as gzipped, tab-separated-values (TSV) formatted files. This results in three tables that can be used for reporting and analysis.\nCreate Database Objects The majority of data transformation is performed in the imdb schema, which is configured as the dbt target schema. We create the final three tables in a custom schema named imdb_analytics. Note that its name is according to the naming convention of the dbt custom schema, which is \u0026lt;target_schema\u0026gt;\u0026lt;custom_schema\u0026gt;_. After creating the database schemas, we create a development user (dbt) and a group that the user belongs to, followed by granting necessary permissions of the new schemas to the new group and reassigning schema ownership to the new user.\n1-- redshift-sls/setup-redshift.sql 2-- // create db schemas 3create schema if not exists imdb; 4create schema if not exists imdb_analytics; 5 6-- // create db user and group 7create user dbt with password \u0026#39;\u0026lt;password\u0026gt;\u0026#39;; 8create group dbt with user dbt; 9 10-- // grant permissions to new schemas 11grant usage on schema imdb to group dbt; 12grant create on schema imdb to group dbt; 13grant all on all tables in schema imdb to group dbt; 14 15grant usage on schema imdb_analytics to group dbt; 16grant create on schema imdb_analytics to group dbt; 17grant all on all tables in schema imdb_analytics to group dbt; 18 19-- reassign schema ownership to dbt 20alter schema imdb owner to dbt; 21alter schema imdb_analytics owner to dbt; Save Data to S3 The Axel download accelerator is used to download the data files locally followed by decompressing with the gzip utility. Note that simple retry logic is added as I see download failure from time to time. Finally the decompressed files are saved into the project S3 bucket using the S3 sync command.,\n1# redshift-sls/upload-data.sh 2#!/usr/bin/env bash 3 4s3_bucket=\u0026#34;\u0026lt;s3-bucket-name\u0026gt;\u0026#34; 5hostname=\u0026#34;datasets.imdbws.com\u0026#34; 6declare -a file_names=( 7 \u0026#34;name.basics.tsv.gz\u0026#34; \\ 8 \u0026#34;title.akas.tsv.gz\u0026#34; \\ 9 \u0026#34;title.basics.tsv.gz\u0026#34; \\ 10 \u0026#34;title.crew.tsv.gz\u0026#34; \\ 11 \u0026#34;title.episode.tsv.gz\u0026#34; \\ 12 \u0026#34;title.principals.tsv.gz\u0026#34; \\ 13 \u0026#34;title.ratings.tsv.gz\u0026#34; 14 ) 15 16rm -rf imdb-data 17 18for fn in \u0026#34;${file_names[@]}\u0026#34; 19do 20 download_url=\u0026#34;https://$hostname/$fn\u0026#34; 21 prefix=$(echo ${fn::-7} | tr \u0026#39;.\u0026#39; \u0026#39;_\u0026#39;) 22 echo \u0026#34;download imdb-data/$prefix/$fn from $download_url\u0026#34; 23 # download can fail, retry after removing temporary files if failed 24 while true; 25 do 26 mkdir -p imdb-data/$prefix 27 axel -n 32 -a -o imdb-data/$prefix/$fn $download_url 28 gzip -d imdb-data/$prefix/$fn 29 num_files=$(ls imdb-data/$prefix | wc -l) 30 if [ $num_files == 1 ]; then 31 break 32 fi 33 rm -rf imdb-data/$prefix 34 done 35done 36 37aws s3 sync ./imdb-data s3://$s3_bucket Copy Data The data files in S3 are loaded into Redshift using the COPY command as shown below.\n1-- redshift-sls/setup-redshift.sql 2-- // copy data to tables 3-- name_basics 4drop table if exists imdb.name_basics; 5create table imdb.name_basics ( 6 nconst text, 7 primary_name text, 8 birth_year text, 9 death_year text, 10 primary_profession text, 11 known_for_titles text 12); 13 14copy imdb.name_basics 15from \u0026#39;s3://\u0026lt;s3-bucket-name\u0026gt;/name_basics\u0026#39; 16iam_role default 17delimiter \u0026#39;\\t\u0026#39; 18region \u0026#39;ap-southeast-2\u0026#39; 19ignoreheader 1; 20 21-- title_akas 22drop table if exists imdb.title_akas; 23create table imdb.title_akas ( 24 title_id text, 25 ordering int, 26 title varchar(max), 27 region text, 28 language text, 29 types text, 30 attributes text, 31 is_original_title boolean 32); 33 34copy imdb.title_akas 35from \u0026#39;s3://\u0026lt;s3-bucket-name\u0026gt;/title_akas\u0026#39; 36iam_role default 37delimiter \u0026#39;\\t\u0026#39; 38region \u0026#39;ap-southeast-2\u0026#39; 39ignoreheader 1; 40 41-- title_basics 42drop table if exists imdb.title_basics; 43create table imdb.title_basics ( 44 tconst text, 45 title_type text, 46 primary_title varchar(max), 47 original_title varchar(max), 48 is_adult boolean, 49 start_year text, 50 end_year text, 51 runtime_minutes text, 52 genres text 53); 54 55copy imdb.title_basics 56from \u0026#39;s3://\u0026lt;s3-bucket-name\u0026gt;/title_basics\u0026#39; 57iam_role default 58delimiter \u0026#39;\\t\u0026#39; 59region \u0026#39;ap-southeast-2\u0026#39; 60ignoreheader 1; 61 62-- title_crews 63drop table if exists imdb.title_crews; 64create table imdb.title_crews ( 65 tconst text, 66 directors varchar(max), 67 writers varchar(max) 68); 69 70copy imdb.title_crews 71from \u0026#39;s3://\u0026lt;s3-bucket-name\u0026gt;/title_crew\u0026#39; 72iam_role default 73delimiter \u0026#39;\\t\u0026#39; 74region \u0026#39;ap-southeast-2\u0026#39; 75ignoreheader 1; 76 77-- title_episodes 78drop table if exists imdb.title_episodes; 79create table imdb.title_episodes ( 80 tconst text, 81 parent_tconst text, 82 season_number int, 83 episode_number int 84); 85 86copy imdb.title_episodes 87from \u0026#39;s3://\u0026lt;s3-bucket-name\u0026gt;/title_episode\u0026#39; 88iam_role default 89delimiter \u0026#39;\\t\u0026#39; 90region \u0026#39;ap-southeast-2\u0026#39; 91ignoreheader 1; 92 93-- title_principals 94drop table if exists imdb.title_principals; 95create table imdb.title_principals ( 96 tconst text, 97 ordering int, 98 nconst text, 99 category text, 100 job varchar(max), 101 characters varchar(max) 102); 103 104copy imdb.title_principals 105from \u0026#39;s3://\u0026lt;s3-bucket-name\u0026gt;/title_principals\u0026#39; 106iam_role default 107delimiter \u0026#39;\\t\u0026#39; 108region \u0026#39;ap-southeast-2\u0026#39; 109ignoreheader 1; 110 111-- title_ratings 112drop table if exists imdb.title_ratings; 113create table imdb.title_ratings ( 114 tconst text, 115 average_rating float, 116 num_votes int 117); 118 119copy imdb.title_ratings 120from \u0026#39;s3://\u0026lt;s3-bucket-name\u0026gt;/title_ratings\u0026#39; 121iam_role default 122delimiter \u0026#39;\\t\u0026#39; 123region \u0026#39;ap-southeast-2\u0026#39; 124ignoreheader 1; Initialise dbt Project We need the dbt-core and dbt-redshift. Once installed, we can initialise a dbt project with the dbt init command. We are required to specify project details such as project name, database adapter and database connection info. Note dbt creates the project profile to .dbt/profile.yml of the user home directory by default.\n1$ dbt init 207:07:16 Running with dbt=1.2.1 3Enter a name for your project (letters, digits, underscore): dbt_redshift_sls 4Which database would you like to use? 5[1] postgres 6[2] redshift 7 8(Don\u0026#39;t see the one you want? https://docs.getdbt.com/docs/available-adapters) 9 10Enter a number: 2 11host (hostname.region.redshift.amazonaws.com): \u0026lt;redshift-endpoint-url\u0026gt; 12port [5439]: 13user (dev username): dbt 14[1] password 15[2] iam 16Desired authentication method option (enter a number): 1 17password (dev password): 18dbname (default database that dbt will build objects in): main 19schema (default schema that dbt will build objects in): gdelt 20threads (1 or more) [1]: 4 2107:08:13 Profile dbt_redshift_sls written to /home/\u0026lt;username\u0026gt;/.dbt/profiles.yml using target\u0026#39;s profile_template.yml and your supplied values. Run \u0026#39;dbt debug\u0026#39; to validate the connection. 2207:08:13 23Your new dbt project \u0026#34;dbt_redshift_sls\u0026#34; was created! 24 25For more information on how to configure the profiles.yml file, 26please consult the dbt documentation here: 27 28 https://docs.getdbt.com/docs/configure-your-profile 29 30One more thing: 31 32Need help? Don\u0026#39;t hesitate to reach out to us via GitHub issues or on Slack: 33 34 https://community.getdbt.com/ 35 36Happy modeling! dbt initialises a project in a folder that matches to the project name and generates project boilerplate as shown below. Some of the main objects are dbt_project.yml, and the model folder. The former is required because dbt doesn\u0026rsquo;t know if a folder is a dbt project without it. Also it contains information that tells dbt how to operate on the project. The latter is for including dbt models, which is basically a set of SQL select statements. See dbt documentation for more details.\n1$ tree dbt_redshift_sls/ -L 1 2dbt_redshift_sls/ 3├── README.md 4├── analyses 5├── dbt_project.yml 6├── macros 7├── models 8├── seeds 9├── snapshots 10└── tests We can check the database connection with the dbt debug command. Do not forget to connect to VPN as mentioned earlier.\n1$ dbt debug 203:50:58 Running with dbt=1.2.1 3dbt version: 1.2.1 4python version: 3.8.10 5python path: \u0026lt;path-to-python-path\u0026gt; 6os info: Linux-5.4.72-microsoft-standard-WSL2-x86_64-with-glibc2.29 7Using profiles.yml file at /home/\u0026lt;username\u0026gt;/.dbt/profiles.yml 8Using dbt_project.yml file at \u0026lt;path-to-dbt-project\u0026gt;/dbt_project.yml 9 10Configuration: 11 profiles.yml file [OK found and valid] 12 dbt_project.yml file [OK found and valid] 13 14Required dependencies: 15 - git [OK found] 16 17Connection: 18 host: \u0026lt;redshift-endpoint-url\u0026gt; 19 port: 5439 20 user: dbt 21 database: main 22 schema: imdb 23 search_path: None 24 keepalives_idle: 240 25 sslmode: None 26 method: database 27 cluster_id: None 28 iam_profile: None 29 iam_duration_seconds: 900 30 Connection test: [OK connection ok] 31 32All checks passed! After initialisation, the model configuration is updated. The project materialisation is specified as view although it is the default materialisation. Also tags are added to the entire model folder as well as folders of specific layers - staging, intermediate and marts. As shown below, tags can simplify model execution.\n1# redshift-sls/dbt_redshift_sls/dbt_project.yml 2name: \u0026#34;dbt_redshift_sls\u0026#34; 3... 4 5models: 6 dbt_redshift_sls: 7 +materialized: view 8 +tags: 9 - \u0026#34;imdb\u0026#34; 10 staging: 11 +tags: 12 - \u0026#34;staging\u0026#34; 13 intermediate: 14 +tags: 15 - \u0026#34;intermediate\u0026#34; 16 marts: 17 +tags: 18 - \u0026#34;marts\u0026#34; Two dbt packages are used in this project. The dbt-labs/codegen is used to save typing when generating the source and base models while dbt-labda/dbt_utils for adding tests to the final marts models. The packages can be installed by the dbt deps command.\n1# redshift-sls/dbt_redshift_sls/packages.yml 2packages: 3 - package: dbt-labs/codegen 4 version: 0.8.0 5 - package: dbt-labs/dbt_utils 6 version: 0.9.2 Create dbt Models The models for this post are organised into three layers according to the dbt best practices - staging, intermediate and marts.\nStaging The seven tables that are loaded from S3 are dbt source tables and their details are declared in a YAML file (_imdb_sources.yml). By doing so, we are able to refer to the source tables with the {{ source() }} function. Also we can add tests to source tables. For example below two tests (unique, not_null) are added to the tconst column of the title_basics table below and these tests can be executed by the dbt test command.\n1# redshift-sls/dbt_redshift_sls/models/staging/imdb/_imdb__sources.yml 2version: 2 3 4sources: 5 - name: imdb 6 description: Subsets of IMDb data, which are available for access to customers for personal and non-commercial use 7 tables: 8 ... 9 - name: title_basics 10 description: Table that contains basic information of titles 11 columns: 12 - name: tconst 13 description: alphanumeric unique identifier of the title 14 tests: 15 - unique 16 - not_null 17 - name: title_type 18 description: the type/format of the title (e.g. movie, short, tvseries, tvepisode, video, etc) 19 - name: primary_title 20 description: the more popular title / the title used by the filmmakers on promotional materials at the point of release 21 - name: original_title 22 description: original title, in the original language 23 - name: is_adult 24 description: flag that indicates whether it is an adult title or not 25 - name: start_year 26 description: represents the release year of a title. In the case of TV Series, it is the series start year 27 - name: end_year 28 description: TV Series end year. NULL for all other title types 29 - name: runtime_minutes 30 description: primary runtime of the title, in minutes 31 - name: genres 32 description: includes up to three genres associated with the title 33 ... Based on the source tables, staging models are created. They are created as views, which is the project\u0026rsquo;s default materialisation. In the SQL statements, column names and data types are modified mainly.\n1-- // redshift-sls/dbt_redshift_sls/models/staging/imdb/stg_imdb__title_basics.sql 2with source as ( 3 4 select * from {{ source(\u0026#39;imdb\u0026#39;, \u0026#39;title_basics\u0026#39;) }} 5 6), 7 8renamed as ( 9 10 select 11 tconst as title_id, 12 title_type, 13 primary_title, 14 original_title, 15 is_adult, 16 start_year::int as start_year, 17 end_year::int as end_year, 18 runtime_minutes::int as runtime_minutes, 19 genres 20 from source 21 22) 23 24select * from renamed Below shows the file tree of the staging models. The staging models can be executed using the dbt run command. As we\u0026rsquo;ve added tags to the staging layer models, we can limit to execute only this layer by dbt run --select staging.\n1redshift-sls/dbt_redshift_sls/models/staging/ 2└── imdb 3 ├── _imdb__models.yml 4 ├── _imdb__sources.yml 5 ├── stg_imdb__name_basics.sql 6 ├── stg_imdb__title_akas.sql 7 ├── stg_imdb__title_basics.sql 8 ├── stg_imdb__title_crews.sql 9 ├── stg_imdb__title_episodes.sql 10 ├── stg_imdb__title_principals.sql 11 └── stg_imdb__title_ratings.sql Intermediate We can keep intermediate results in this layer so that the models of the final marts layer can be simplified. The source data includes columns where array values are kept as comma separated strings. For example, the genres column of the stg_imdb__title_basics model includes up to 3 genre values as shown below.\nA total of seven columns in three models are columns of comma-separated strings and it is better to flatten them in the intermediate layer. Also, in order to avoid repetition, a dbt macro (f_latten_fields_) is created to share the column-flattening logic.\n1# redshift-sls/dbt_redshift_sls/macros/flatten_fields.sql 2{% macro flatten_fields(model, field_name, id_field_name) %} 3 with subset as ( 4 select 5 {{ id_field_name }} as id, 6 regexp_count({{ field_name }}, \u0026#39;,\u0026#39;) + 1 AS num_fields, 7 {{ field_name }} as fields 8 from {{ model }} 9 ) 10 select 11 id, 12 1 as idx, 13 split_part(fields, \u0026#39;,\u0026#39;, 1) as field 14 from subset 15 union all 16 select 17 s.id, 18 idx + 1 as idx, 19 split_part(s.fields, \u0026#39;,\u0026#39;, idx + 1) 20 from subset s 21 join cte on s.id = cte.id 22 where idx \u0026lt; num_fields 23{% endmacro %} The macro function can be added inside a recursive cte by specifying the relevant model, field name to flatten and ID field name.\n1-- dbt_redshift_sls/models/intermediate/title/int_genres_flattened_from_title_basics.sql 2with recursive cte (id, idx, field) as ( 3 {{ flatten_fields(ref(\u0026#39;stg_imdb__title_basics\u0026#39;), \u0026#39;genres\u0026#39;, \u0026#39;title_id\u0026#39;) }} 4) 5 6select 7 id as title_id, 8 field as genre 9from cte 10order by id The intermediate models are also materialised as views, and we can check the array columns are flattened as expected.\nBelow shows the file tree of the intermediate models. Similar to the staging models, the intermediate models can be executed by dbt run --select intermediate.\n1redshift-sls/dbt_redshift_sls/models/intermediate/ 2├── name 3│ ├── _int_name__models.yml 4│ ├── int_known_for_titles_flattened_from_name_basics.sql 5│ └── int_primary_profession_flattened_from_name_basics.sql 6└── title 7 ├── _int_title__models.yml 8 ├── int_directors_flattened_from_title_crews.sql 9 ├── int_genres_flattened_from_title_basics.sql 10 └── int_writers_flattened_from_title_crews.sql 11 12redshift-sls/dbt_redshift_sls/macros/ 13└── flatten_fields.sql Marts The models in the marts layer are configured to be materialised as tables in a custom schema. Their materialisation is set to table and the custom schema is specified as analytics. Note that the custom schema name becomes imdb_analytics according to the naming convention of dbt custom schemas. Models of both the staging and intermediate layers are used to create final models to be used for reporting and analytics.\n1-- redshift-sls/dbt_redshift_sls/models/marts/analytics/titles.sql 2{{ 3 config( 4 schema=\u0026#39;analytics\u0026#39;, 5 materialized=\u0026#39;table\u0026#39;, 6 sort=\u0026#39;title_id\u0026#39;, 7 dist=\u0026#39;title_id\u0026#39; 8 ) 9}} 10 11with titles as ( 12 13 select * from {{ ref(\u0026#39;stg_imdb__title_basics\u0026#39;) }} 14 15), 16 17principals as ( 18 19 select 20 title_id, 21 count(name_id) as num_principals 22 from {{ ref(\u0026#39;stg_imdb__title_principals\u0026#39;) }} 23 group by title_id 24 25), 26 27names as ( 28 29 select 30 title_id, 31 count(name_id) as num_names 32 from {{ ref(\u0026#39;int_known_for_titles_flattened_from_name_basics\u0026#39;) }} 33 group by title_id 34 35), 36 37ratings as ( 38 39 select 40 title_id, 41 average_rating, 42 num_votes 43 from {{ ref(\u0026#39;stg_imdb__title_ratings\u0026#39;) }} 44 45), 46 47episodes as ( 48 49 select 50 parent_title_id, 51 count(title_id) as num_episodes 52 from {{ ref(\u0026#39;stg_imdb__title_episodes\u0026#39;) }} 53 group by parent_title_id 54 55), 56 57distributions as ( 58 59 select 60 title_id, 61 count(title) as num_distributions 62 from {{ ref(\u0026#39;stg_imdb__title_akas\u0026#39;) }} 63 group by title_id 64 65), 66 67final as ( 68 69 select 70 t.title_id, 71 t.title_type, 72 t.primary_title, 73 t.original_title, 74 t.is_adult, 75 t.start_year, 76 t.end_year, 77 t.runtime_minutes, 78 t.genres, 79 p.num_principals, 80 n.num_names, 81 r.average_rating, 82 r.num_votes, 83 e.num_episodes, 84 d.num_distributions 85 from titles as t 86 left join principals as p on t.title_id = p.title_id 87 left join names as n on t.title_id = n.title_id 88 left join ratings as r on t.title_id = r.title_id 89 left join episodes as e on t.title_id = e.parent_title_id 90 left join distributions as d on t.title_id = d.title_id 91 92) 93 94select * from final The details of the three models can be found in a YAML file (_analytics__models.yml). We can add tests to models and below we see tests of row count matching to their corresponding staging models.\n1# redshift-sls/dbt_redshift_sls/models/marts/analytics/_analytics__models.yml 2version: 2 3 4models: 5 - name: names 6 description: Table that contains all names with additional details 7 tests: 8 - dbt_utils.equal_rowcount: 9 compare_model: ref(\u0026#39;stg_imdb__name_basics\u0026#39;) 10 - name: titles 11 description: Table that contains all titles with additional details 12 tests: 13 - dbt_utils.equal_rowcount: 14 compare_model: ref(\u0026#39;stg_imdb__title_basics\u0026#39;) 15 - name: genre_titles 16 description: Table that contains basic title details after flattening genres Below shows the file tree of the marts models. As with the other layers, the marts models can be executed by dbt run \u0026ndash;select marts.\n1redshift-sls/dbt_redshift_sls/models/marts/ 2└── analytics 3 ├── _analytics__models.yml 4 ├── genre_titles.sql 5 ├── names.sql 6 └── titles.sql Using the Redshift query editor v2, we can quickly create charts with the final models. The example below shows a pie chart and we see about 50% of titles are from the top 5 genres.\nGenerate dbt Documentation A nice feature of dbt is documentation. It provides information about the project and the data warehouse, and it facilitates consumers as well as other developers to discover and understand the datasets better. We can generate the project documents and start a document server as shown below.\n1$ dbt docs generate 2$ dbt docs serve A very useful element of dbt documentation is data lineage, which provides an overall view about how data is transformed and consumed. Below we can see that the final titles model consumes all title-related stating models and an intermediate model from the name basics staging model.\nSummary In this post, we discussed how to build data transformation pipelines using dbt on Redshift Serverless. Subsets of IMDb data are used as source and data models are developed in multiple layers according to the dbt best practices. dbt can be used as an effective tool for data transformation in a wide range of data projects from data warehousing to data lake to data lakehouse, and it supports key AWS analytics services - Redshift, Glue, EMR and Athena. More examples of using dbt will be discussed in subsequent posts.\n","date":"September 28, 2022","img":"/blog/2022-09-28-dbt-on-aws-part-1-redshift/featured.png","lang":"en","langName":"English","largeImg":"/blog/2022-09-28-dbt-on-aws-part-1-redshift/featured_hu35d67be5daab492ffba00e48de3bbd29_97234_500x0_resize_box_3.png","permalink":"/blog/2022-09-28-dbt-on-aws-part-1-redshift/","series":[{"title":"DBT for Effective Data Transformation on AWS","url":"/series/dbt-for-effective-data-transformation-on-aws/"}],"smallImg":"/blog/2022-09-28-dbt-on-aws-part-1-redshift/featured_hu35d67be5daab492ffba00e48de3bbd29_97234_180x0_resize_box_3.png","tags":[{"title":"AWS","url":"/tags/aws/"},{"title":"Amazon Redshift","url":"/tags/amazon-redshift/"},{"title":"Data Build Tool (DBT)","url":"/tags/data-build-tool-dbt/"},{"title":"Terraform","url":"/tags/terraform/"}],"timestamp":1664323200,"title":"Data Build Tool (Dbt) for Effective Data Transformation on AWS – Part 1 Redshift"},{"categories":[{"title":"Engineering","url":"/categories/engineering/"}],"content":"When we develop a Spark application on EMR, we can use docker for local development or notebooks via EMR Studio (or EMR Notebooks). However, the local development option is not viable if the size of data is large. Also, I am not a fan of notebooks as it is not possible to utilise the features my editor supports such as syntax highlighting, autocomplete and code formatting. Moreover, it is not possible to organise code into modules and to perform unit testing properly with that option. In this post, We will discuss how to set up a remote development environment on an EMR cluster deployed in a private subnet with VPN and the VS Code remote SSH extension. Typical Spark development examples will be illustrated while sharing the cluster with multiple users. Overall it brings another effective way of developing Spark apps on EMR, which improves developer experience significantly.\nArchitecture An EMR cluster is deployed in a private subnet and, by default, it is not possible to access it from the developer machine. We can construct a PC-to-PC VPN with SoftEther VPN to establish connection to the master node of the cluster. The VPN server runs in a public subnet, and it is managed by an autoscaling group where only a single instance is maintained. An elastic IP address is associated with the instance so that its public IP doesn\u0026rsquo;t change even if the EC2 instance is recreated. Access from the VPN server to the master node is allowed by an additional security group where the VPN\u0026rsquo;s security group is granted access to the master node. The infrastructure is built using Terraform and the source can be found in the post\u0026rsquo;s GitHub repository.\nSoftEther VPN provides the server and client manager programs and they can be downloaded from the download centre page. We can create a VPN user using the server manager and the user can establish connection using the client manager. In this way a developer can access an EMR cluster deployed in a private subnet from the developer machine. Check one of my earlier posts titled Simplify Your Development on AWS with Terraform for a step-by-step illustration of creating a user and making a connection. The VS Code Remote - SSH extension is used to open a folder in the master node of an EMR cluster. In this way, developer experience can be improved significantly while making use of the full feature set of VS Code. The architecture of the remote development environment is shown below.\nInfrastructure The infrastructure of this post is an extension that I illustrated in the previous post. The resources covered there (VPC, subnets, auto scaling group for VPN etc.) won\u0026rsquo;t be repeated. The main resource in this post is an EMR cluster and the latest EMR 6.7.0 release is deployed with single master and core node instances. It is set up to use the AWS Glue Data Catalog as the metastore for Hive and Spark SQL by updating the corresponding configuration classification. Additionally, a managed scaling policy is created so that up to 5 instances are added to the core node. Note the additional security group of the master and slave by which the VPN server is granted access to the master and core node instances - the details of that security group is shown below.\n1# infra/emr.tf 2resource \u0026#34;aws_emr_cluster\u0026#34; \u0026#34;emr_cluster\u0026#34; { 3 name = \u0026#34;${local.name}-emr-cluster\u0026#34; 4 release_label = local.emr.release_label # emr-6.7.0 5 service_role = aws_iam_role.emr_service_role.arn 6 autoscaling_role = aws_iam_role.emr_autoscaling_role.arn 7 applications = local.emr.applications # [\u0026#34;Spark\u0026#34;, \u0026#34;Livy\u0026#34;, \u0026#34;JupyterEnterpriseGateway\u0026#34;, \u0026#34;Hive\u0026#34;] 8 ebs_root_volume_size = local.emr.ebs_root_volume_size 9 log_uri = \u0026#34;s3n://${aws_s3_bucket.default_bucket[0].id}/elasticmapreduce/\u0026#34; 10 step_concurrency_level = 256 11 keep_job_flow_alive_when_no_steps = true 12 termination_protection = false 13 14 ec2_attributes { 15 key_name = aws_key_pair.emr_key_pair.key_name 16 instance_profile = aws_iam_instance_profile.emr_ec2_instance_profile.arn 17 subnet_id = element(tolist(module.vpc.private_subnets), 0) 18 emr_managed_master_security_group = aws_security_group.emr_master.id 19 emr_managed_slave_security_group = aws_security_group.emr_slave.id 20 service_access_security_group = aws_security_group.emr_service_access.id 21 additional_master_security_groups = aws_security_group.emr_vpn_access.id # grant access to VPN server 22 additional_slave_security_groups = aws_security_group.emr_vpn_access.id # grant access to VPN server 23 } 24 25 master_instance_group { 26 instance_type = local.emr.instance_type # m5.xlarge 27 instance_count = local.emr.instance_count # 1 28 } 29 core_instance_group { 30 instance_type = local.emr.instance_type # m5.xlarge 31 instance_count = local.emr.instance_count # 1 32 } 33 34 configurations_json = \u0026lt;\u0026lt;EOF 35 [ 36 { 37 \u0026#34;Classification\u0026#34;: \u0026#34;hive-site\u0026#34;, 38 \u0026#34;Properties\u0026#34;: { 39 \u0026#34;hive.metastore.client.factory.class\u0026#34;: \u0026#34;com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory\u0026#34; 40 } 41 }, 42 { 43 \u0026#34;Classification\u0026#34;: \u0026#34;spark-hive-site\u0026#34;, 44 \u0026#34;Properties\u0026#34;: { 45 \u0026#34;hive.metastore.client.factory.class\u0026#34;: \u0026#34;com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory\u0026#34; 46 } 47 } 48 ] 49 EOF 50 51 tags = local.tags 52 53 depends_on = [module.vpc] 54} 55 56resource \u0026#34;aws_emr_managed_scaling_policy\u0026#34; \u0026#34;emr_scaling_policy\u0026#34; { 57 cluster_id = aws_emr_cluster.emr_cluster.id 58 59 compute_limits { 60 unit_type = \u0026#34;Instances\u0026#34; 61 minimum_capacity_units = 1 62 maximum_capacity_units = 5 63 } 64} The following security group is created to enable access from the VPN server to the EMR instances. Note that the inbound rule is created only when the local.vpn.to_create variable value is true while the security group is created always - if the value is false, the security group has no inbound rule.\n1# infra/emr.tf 2resource \u0026#34;aws_security_group\u0026#34; \u0026#34;emr_vpn_access\u0026#34; { 3 name = \u0026#34;${local.name}-emr-vpn-access\u0026#34; 4 vpc_id = module.vpc.vpc_id 5 6 lifecycle { 7 create_before_destroy = true 8 } 9 10 tags = local.tags 11} 12 13resource \u0026#34;aws_security_group_rule\u0026#34; \u0026#34;emr_vpn_inbound\u0026#34; { 14 count = local.vpn.to_create ? 1 : 0 15 type = \u0026#34;ingress\u0026#34; 16 description = \u0026#34;VPN access\u0026#34; 17 security_group_id = aws_security_group.emr_vpn_access.id 18 protocol = \u0026#34;tcp\u0026#34; 19 from_port = 0 20 to_port = 65535 21 source_security_group_id = aws_security_group.vpn[0].id 22} Change to Secret Generation For configuring the VPN server, we need a IPsec pre-shared key and admin password. While those are specified as variables earlier, they are generated internally in this post for simplicity. The Terraform shell resource module generates and concatenates them with double dashes (\u0026ndash;). The corresponding values are parsed into the user data of the VPN instance and the string is saved into a file to be used for configuring the VPN server manager.\n1## create VPN secrets - IPsec Pre-Shared Key and admin password for VPN 2## see https://cloud.google.com/network-connectivity/docs/vpn/how-to/generating-pre-shared-key 3module \u0026#34;vpn_secrets\u0026#34; { 4 source = \u0026#34;Invicton-Labs/shell-resource/external\u0026#34; 5 6 # generate \u0026lt;IPsec Pre-Shared Key\u0026gt;--\u0026lt;admin password\u0026gt; and parsed in vpn module 7 command_unix = \u0026#34;echo $(openssl rand -base64 24)--$(openssl rand -base64 24)\u0026#34; 8} 9 10resource \u0026#34;local_file\u0026#34; \u0026#34;vpn_secrets\u0026#34; { 11 content = module.vpn_secrets.stdout 12 filename = \u0026#34;${path.module}/secrets/vpn_secrets\u0026#34; 13} 14 15module \u0026#34;vpn\u0026#34; { 16 source = \u0026#34;terraform-aws-modules/autoscaling/aws\u0026#34; 17 version = \u0026#34;~\u0026gt; 6.5\u0026#34; 18 count = local.vpn.to_create ? 1 : 0 19 20 name = \u0026#34;${local.name}-vpn-asg\u0026#34; 21 22 key_name = local.vpn.to_create ? aws_key_pair.key_pair[0].key_name : null 23 vpc_zone_identifier = module.vpc.public_subnets 24 min_size = 1 25 max_size = 1 26 desired_capacity = 1 27 28 image_id = data.aws_ami.amazon_linux_2.id 29 instance_type = element([for s in local.vpn.spot_override : s.instance_type], 0) 30 security_groups = [aws_security_group.vpn[0].id] 31 iam_instance_profile_arn = aws_iam_instance_profile.vpn[0].arn 32 33 # Launch template 34 create_launch_template = true 35 update_default_version = true 36 37 user_data = base64encode(join(\u0026#34;\\n\u0026#34;, [ 38 \u0026#34;#cloud-config\u0026#34;, 39 yamlencode({ 40 # https://cloudinit.readthedocs.io/en/latest/topics/modules.html 41 write_files : [ 42 { 43 path : \u0026#34;/opt/vpn/bootstrap.sh\u0026#34;, 44 content : templatefile(\u0026#34;${path.module}/scripts/bootstrap.sh\u0026#34;, { 45 aws_region = local.region, 46 allocation_id = aws_eip.vpn[0].allocation_id, 47 vpn_psk = split(\u0026#34;--\u0026#34;, replace(module.vpn_secrets.stdout, \u0026#34;\\n\u0026#34;, \u0026#34;\u0026#34;))[0], # specify IPsec pre-shared key 48 admin_password = split(\u0026#34;--\u0026#34;, replace(module.vpn_secrets.stdout, \u0026#34;\\n\u0026#34;, \u0026#34;\u0026#34;))[1] # specify admin password 49 }), 50 permissions : \u0026#34;0755\u0026#34;, 51 } 52 ], 53 runcmd : [ 54 [\u0026#34;/opt/vpn/bootstrap.sh\u0026#34;], 55 ], 56 }) 57 ])) 58 59 ... 60 61 tags = local.tags 62} After deploying all the resources, it is good to go to the next section if we\u0026rsquo;re able to connect to the VPN server as shown below.\nPreparation User Creation While we are able to use the default hadoop user for development, we can add additional users to share the cluster as well. First let\u0026rsquo;s access the master node via ssh as shown below. Note the access key is stored in the infra/key-pair folder and the master private DNS name can be obtained from the _emr_cluster_master_dns _output value.\n1# access to the master node via ssh 2jaehyeon@cevo$ export EMR_MASTER_DNS=$(terraform -chdir=./infra output --raw emr_cluster_master_dns) 3jaehyeon@cevo$ ssh -i infra/key-pair/emr-remote-dev-emr-key.pem hadoop@$EMR_MASTER_DNS 4The authenticity of host \u0026#39;ip-10-0-113-113.ap-southeast-2.compute.internal (10.0.113.113)\u0026#39; can\u0026#39;t be established. 5ECDSA key fingerprint is SHA256:mNdgPWnDkCG/6IsUDdHAETe/InciOatb8jwELnwfWR4. 6Are you sure you want to continue connecting (yes/no/[fingerprint])? yes 7Warning: Permanently added \u0026#39;ip-10-0-113-113.ap-southeast-2.compute.internal,10.0.113.113\u0026#39; (ECDSA) to the list of known hosts. 8 9 __| __|_ ) 10 _| ( / Amazon Linux 2 AMI 11 ___|\\___|___| 12 13https://aws.amazon.com/amazon-linux-2/ 1417 package(s) needed for security, out of 38 available 15Run \u0026#34;sudo yum update\u0026#34; to apply all updates. 16 17EEEEEEEEEEEEEEEEEEEE MMMMMMMM MMMMMMMM RRRRRRRRRRRRRRR 18E::::::::::::::::::E M:::::::M M:::::::M R::::::::::::::R 19EE:::::EEEEEEEEE:::E M::::::::M M::::::::M R:::::RRRRRR:::::R 20 E::::E EEEEE M:::::::::M M:::::::::M RR::::R R::::R 21 E::::E M::::::M:::M M:::M::::::M R:::R R::::R 22 E:::::EEEEEEEEEE M:::::M M:::M M:::M M:::::M R:::RRRRRR:::::R 23 E::::::::::::::E M:::::M M:::M:::M M:::::M R:::::::::::RR 24 E:::::EEEEEEEEEE M:::::M M:::::M M:::::M R:::RRRRRR::::R 25 E::::E M:::::M M:::M M:::::M R:::R R::::R 26 E::::E EEEEE M:::::M MMM M:::::M R:::R R::::R 27EE:::::EEEEEEEE::::E M:::::M M:::::M R:::R R::::R 28E::::::::::::::::::E M:::::M M:::::M RR::::R R::::R 29EEEEEEEEEEEEEEEEEEEE MMMMMMM MMMMMMM RRRRRR RRRRRR 30 31[hadoop@ip-10-0-113-113 ~]$ A user can be created as shown below. Optionally the user is added to the sudoers file so that the user is allowed to run a command as the root user without specifying the password. Note this is a shortcut only, and please check this page for proper usage of editing the sudoers file.\n1# create a user and add to sudoers 2[hadoop@ip-10-0-113-113 ~]$ sudo adduser jaehyeon 3[hadoop@ip-10-0-113-113 ~]$ ls /home/ 4ec2-user emr-notebook hadoop jaehyeon 5[hadoop@ip-10-0-113-113 ~]$ sudo su 6[root@ip-10-0-113-113 hadoop]# chmod +w /etc/sudoers 7[root@ip-10-0-113-113 hadoop]# echo \u0026#34;jaehyeon ALL=(ALL) NOPASSWD:ALL\u0026#34; \u0026gt;\u0026gt; /etc/sudoers Also, as described in the EMR documentation, we must add the HDFS user directory for the user account and grant ownership of the directory so that the user is allowed to log in to the cluster to run Hadoop jobs.\n1[root@ip-10-0-113-113 hadoop]# sudo su - hdfs 2# create user directory 3-bash-4.2$ hdfs dfs -mkdir /user/jaehyeon 4SLF4J: Class path contains multiple SLF4J bindings. 5SLF4J: Found binding in [jar:file:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class] 6SLF4J: Found binding in [jar:file:/usr/lib/tez/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class] 7SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation. 8SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory] 9# update directory ownership 10-bash-4.2$ hdfs dfs -chown jaehyeon:jaehyeon /user/jaehyeon 11SLF4J: Class path contains multiple SLF4J bindings. 12SLF4J: Found binding in [jar:file:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class] 13SLF4J: Found binding in [jar:file:/usr/lib/tez/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class] 14SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation. 15SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory] 16# check directories in /user 17-bash-4.2$ hdfs dfs -ls /user 18SLF4J: Class path contains multiple SLF4J bindings. 19SLF4J: Found binding in [jar:file:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class] 20SLF4J: Found binding in [jar:file:/usr/lib/tez/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class] 21SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation. 22SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory] 23Found 7 items 24drwxrwxrwx - hadoop hdfsadmingroup 0 2022-09-03 10:19 /user/hadoop 25drwxr-xr-x - mapred mapred 0 2022-09-03 10:19 /user/history 26drwxrwxrwx - hdfs hdfsadmingroup 0 2022-09-03 10:19 /user/hive 27drwxr-xr-x - jaehyeon jaehyeon 0 2022-09-03 10:39 /user/jaehyeon 28drwxrwxrwx - livy livy 0 2022-09-03 10:19 /user/livy 29drwxrwxrwx - root hdfsadmingroup 0 2022-09-03 10:19 /user/root 30drwxrwxrwx - spark spark 0 2022-09-03 10:19 /user/spark Finally, we need to add the public key to the .ssh/authorized_keys file in order to set up public key authentication for SSH access.\n1# add public key 2[hadoop@ip-10-0-113-113 ~]$ sudo su - jaehyeon 3[jaehyeon@ip-10-0-113-113 ~]$ mkdir .ssh 4[jaehyeon@ip-10-0-113-113 ~]$ chmod 700 .ssh 5[jaehyeon@ip-10-0-113-113 ~]$ touch .ssh/authorized_keys 6[jaehyeon@ip-10-0-113-113 ~]$ chmod 600 .ssh/authorized_keys 7[jaehyeon@ip-10-0-113-113 ~]$ PUBLIC_KEY=\u0026#34;\u0026lt;SSH-PUBLIC-KEY\u0026gt;\u0026#34; 8[jaehyeon@ip-10-0-113-113 ~]$ echo $PUBLIC_KEY \u0026gt; .ssh/authorized_keys Clone Repository As we can open a folder in the master node, the GitHub repository is cloned to each user\u0026rsquo;s home folder to open later.\n1[hadoop@ip-10-0-113-113 ~]$ sudo yum install git 2[hadoop@ip-10-0-113-113 ~]$ git clone https://github.com/jaehyeon-kim/emr-remote-dev.git 3[hadoop@ip-10-0-113-113 ~]$ ls 4emr-remote-dev 5[hadoop@ip-10-0-113-113 ~]$ sudo su - jaehyeon 6[jaehyeon@ip-10-0-113-113 ~]$ git clone https://github.com/jaehyeon-kim/emr-remote-dev.git 7[jaehyeon@ip-10-0-113-113 ~]$ ls 8emr-remote-dev Access to EMR Cluster Now we have two users that have access to the EMR cluster and their connection details are saved into an SSH configuration file as shown below.\nHost emr-hadoop\rHostName ip-10-0-113-113.ap-southeast-2.compute.internal\rUser hadoop\rForwardAgent yes\rIdentityFile C:\\Users\\\u0026lt;username\u0026gt;\\.ssh\\emr-remote-dev-emr-key.pem\rHost emr-jaehyeon\rHostName ip-10-0-113-113.ap-southeast-2.compute.internal\rUser jaehyeon\rForwardAgent yes\rIdentityFile C:\\Users\\\u0026lt;username\u0026gt;\\.ssh\\id_rsa Then we can see the connection details in the remote explorer menu of VS Code. Note the remote SSH extension should be installed for it. On right-clicking the mouse on the emr-hadoop connection, we can select the option to connect to the host in a new window.\nIn a new window, a menu pops up to select the platform of the remote host.\nIf it\u0026rsquo;s the first time connecting to the server, it requests to confirm whether you trust and want to continue connecting to the host. We can hit Continue.\nOnce we are connected, we can open a folder in the server. On selecting File \u0026gt; Open Folder… menu, we can see a list of folders that we can open. Let\u0026rsquo;s open the repository folder we cloned earlier.\nVS Code asks whether we trust the authors of the files in this folder and we can hit Yes.\nNow access to the server with the remote SSH extension is complete and we can check it by opening a terminal where it shows the typical EMR shell.\nPython Configuration We can install the Python extension at minimum and it indicates the extension will be installed in the remote server (emr-hadoop).\nWe\u0026rsquo;ll use the Pyspark and py4j packages that are included in the existing spark distribution. It can be done simply by creating an .env file that adds the relevant paths to the PYTHONPATH variable. In the following screenshot, you see that there is no warning to import SparkSession.\nRemote Development Transform Data It is a simple Spark application that reads a sample NY taxi trip dataset from a public S3 bucket. Once loaded, it converts the pick-up and drop-off datetime columns from string to timestamp followed by writing the transformed data to a destination S3 bucket. It finishes by creating a Glue table with the transformed data.\n1# tripdata_write.py 2from pyspark.sql import SparkSession 3 4from utils import to_timestamp_df 5 6if __name__ == \u0026#34;__main__\u0026#34;: 7 spark = SparkSession.builder.appName(\u0026#34;Trip Data\u0026#34;).enableHiveSupport().getOrCreate() 8 9 dbname = \u0026#34;tripdata\u0026#34; 10 tblname = \u0026#34;ny_taxi\u0026#34; 11 bucket_name = \u0026#34;emr-remote-dev-590312749310-ap-southeast-2\u0026#34; 12 dest_path = f\u0026#34;s3://{bucket_name}/{tblname}/\u0026#34; 13 src_path = \u0026#34;s3://aws-data-analytics-workshops/shared_datasets/tripdata/\u0026#34; 14 # read csv 15 ny_taxi = spark.read.option(\u0026#34;inferSchema\u0026#34;, \u0026#34;true\u0026#34;).option(\u0026#34;header\u0026#34;, \u0026#34;true\u0026#34;).csv(src_path) 16 ny_taxi = to_timestamp_df(ny_taxi, [\u0026#34;lpep_pickup_datetime\u0026#34;, \u0026#34;lpep_dropoff_datetime\u0026#34;]) 17 ny_taxi.printSchema() 18 # write parquet 19 ny_taxi.write.mode(\u0026#34;overwrite\u0026#34;).parquet(dest_path) 20 # create glue table 21 ny_taxi.createOrReplaceTempView(tblname) 22 spark.sql(f\u0026#34;CREATE DATABASE IF NOT EXISTS {dbname}\u0026#34;) 23 spark.sql(f\u0026#34;USE {dbname}\u0026#34;) 24 spark.sql( 25 f\u0026#34;\u0026#34;\u0026#34;CREATE TABLE IF NOT EXISTS {tblname} 26 USING PARQUET 27 LOCATION \u0026#39;{dest_path}\u0026#39; 28 AS SELECT * FROM {tblname} 29 \u0026#34;\u0026#34;\u0026#34; 30 ) As the spark application should run in a cluster, we need to copy it into HDFS. For simplicity, I copied the current folder into the /user/hadoop/emr-remote-dev directory.\n1# copy current folder into /user/hadoop/emr-remote-dev 2[hadoop@ip-10-0-113-113 emr-remote-dev]$ hdfs dfs -put . /user/hadoop/emr-remote-dev 3SLF4J: Class path contains multiple SLF4J bindings. 4SLF4J: Found binding in [jar:file:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class] 5SLF4J: Found binding in [jar:file:/usr/lib/tez/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class] 6SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation. 7SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory] 8# check contents in /user/hadoop/emr-remote-dev 9[hadoop@ip-10-0-113-113 emr-remote-dev]$ hdfs dfs -ls /user/hadoop/emr-remote-dev 10SLF4J: Class path contains multiple SLF4J bindings. 11SLF4J: Found binding in [jar:file:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class] 12SLF4J: Found binding in [jar:file:/usr/lib/tez/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class] 13SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation. 14SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory] 15Found 10 items 16-rw-r--r-- 1 hadoop hdfsadmingroup 93 2022-09-03 11:11 /user/hadoop/emr-remote-dev/.env 17drwxr-xr-x - hadoop hdfsadmingroup 0 2022-09-03 11:11 /user/hadoop/emr-remote-dev/.git 18-rw-r--r-- 1 hadoop hdfsadmingroup 741 2022-09-03 11:11 /user/hadoop/emr-remote-dev/.gitignore 19drwxr-xr-x - hadoop hdfsadmingroup 0 2022-09-03 11:11 /user/hadoop/emr-remote-dev/.vscode 20-rw-r--r-- 1 hadoop hdfsadmingroup 25 2022-09-03 11:11 /user/hadoop/emr-remote-dev/README.md 21drwxr-xr-x - hadoop hdfsadmingroup 0 2022-09-03 11:11 /user/hadoop/emr-remote-dev/infra 22-rw-r--r-- 1 hadoop hdfsadmingroup 873 2022-09-03 11:11 /user/hadoop/emr-remote-dev/test_utils.py 23-rw-r--r-- 1 hadoop hdfsadmingroup 421 2022-09-03 11:11 /user/hadoop/emr-remote-dev/tripdata_read.sh 24-rw-r--r-- 1 hadoop hdfsadmingroup 1066 2022-09-03 11:11 /user/hadoop/emr-remote-dev/tripdata_write.py 25-rw-r--r-- 1 hadoop hdfsadmingroup 441 2022-09-03 11:11 /user/hadoop/emr-remote-dev/utils.py The app can be submitted by specifying the HDFS locations of the app and source file. It is deployed to the YARN cluster with the client deployment mode. In this way, data processing can be performed by executors in the core node and we are able to check execution details in the same terminal.\n1[hadoop@ip-10-0-113-113 emr-remote-dev]$ spark-submit \\ 2 --master yarn \\ 3 --deploy-mode client \\ 4 --py-files hdfs:/user/hadoop/emr-remote-dev/utils.py \\ 5 hdfs:/user/hadoop/emr-remote-dev/tripdata_write.py Once the app completes, we can see that a Glue database named tripdata is created, and it includes a table named ny_taxi.\nRead Data We can connect to the cluster with the other user account as well. Below shows an example of the PySpark shell that reads data from the table created earlier. It just reads the Glue table and adds a column of trip duration followed by showing the summary statistics of key columns.\nUnit Test The Spark application uses a custom function that converts the data type of one or more columns from string to timestamp - to_timestamp_df(). The source of the function and the testing script can be found below.\n1# utils.py 2from typing import List, Union 3from pyspark.sql import DataFrame 4from pyspark.sql.functions import col, to_timestamp 5 6def to_timestamp_df( 7 df: DataFrame, fields: Union[List[str], str], format: str = \u0026#34;M/d/yy H:mm\u0026#34; 8) -\u0026gt; DataFrame: 9 fields = [fields] if isinstance(fields, str) else fields 10 for field in fields: 11 df = df.withColumn(field, to_timestamp(col(field), format)) 12 return df 13 14# test_utils.py 15import pytest 16import datetime 17from pyspark.sql import SparkSession 18from py4j.protocol import Py4JError 19 20from utils import to_timestamp_df 21 22 23@pytest.fixture(scope=\u0026#34;session\u0026#34;) 24def spark(): 25 return ( 26 SparkSession.builder.master(\u0026#34;local\u0026#34;) 27 .appName(\u0026#34;test\u0026#34;) 28 .config(\u0026#34;spark.submit.deployMode\u0026#34;, \u0026#34;client\u0026#34;) 29 .getOrCreate() 30 ) 31 32 33def test_to_timestamp_success(spark): 34 raw_df = spark.createDataFrame( 35 [(\u0026#34;1/1/17 0:01\u0026#34;,)], 36 [\u0026#34;date\u0026#34;], 37 ) 38 39 test_df = to_timestamp_df(raw_df, \u0026#34;date\u0026#34;, \u0026#34;M/d/yy H:mm\u0026#34;) 40 for row in test_df.collect(): 41 assert row[\u0026#34;date\u0026#34;] == datetime.datetime(2017, 1, 1, 0, 1) 42 43 44def test_to_timestamp_bad_format(spark): 45 raw_df = spark.createDataFrame( 46 [(\u0026#34;1/1/17 0:01\u0026#34;,)], 47 [\u0026#34;date\u0026#34;], 48 ) 49 50 with pytest.raises(Py4JError): 51 to_timestamp_df(raw_df, \u0026#34;date\u0026#34;, \u0026#34;M/d/yy HH:mm\u0026#34;).collect() For unit testing, we need to install the Pytest package and export the PYTHONPATH variable that can be found in the .env file. Note, as testing can be run with a local Spark session, the testing package can only be installed in the master node. Below shows an example test run output.\nSummary In this post, we discussed how to set up a remote development environment on an EMR cluster. A cluster is deployed in a private subnet, access from a developer machine is established via PC-to-PC VPN and the VS Code Remote - SSH extension is used to perform remote development. Aside from the default hadoop user, an additional user account is created to show how to share the cluster with multiple users and spark development examples are illustrated with those user accounts. Overall the remote development brings another effective option to develop spark applications on EMR, which improves developer experience significantly.\n","date":"September 7, 2022","img":"/blog/2022-09-07-emr-remote-dev/featured.png","lang":"en","langName":"English","largeImg":"/blog/2022-09-07-emr-remote-dev/featured_huab2404d3988c555f66e1beb603c1cd6b_72448_500x0_resize_box_3.png","permalink":"/blog/2022-09-07-emr-remote-dev/","series":[],"smallImg":"/blog/2022-09-07-emr-remote-dev/featured_huab2404d3988c555f66e1beb603c1cd6b_72448_180x0_resize_box_3.png","tags":[{"title":"AWS","url":"/tags/aws/"},{"title":"Amazon EMR","url":"/tags/amazon-emr/"},{"title":"Apache Spark","url":"/tags/apache-spark/"},{"title":"PySpark","url":"/tags/pyspark/"},{"title":"Visual Studio Code","url":"/tags/visual-studio-code/"},{"title":"Terraform","url":"/tags/terraform/"}],"timestamp":1662508800,"title":"Develop and Test Apache Spark Apps for EMR Remotely Using Visual Studio Code"},{"categories":[],"content":"This guide show you how to install on Arch Linux.\n","date":"September 6, 2022","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/docs/installation/linux/archlinux/","series":[],"smallImg":"","tags":[],"timestamp":1662475343,"title":"Install on Arch Linux"},{"categories":[],"content":"A fast, responsive and feature-rich Hugo theme for blog and documentations site.\n","date":"September 6, 2022","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/docs/introduction/","series":[],"smallImg":"","tags":[],"timestamp":1662475343,"title":"Introduction"},{"categories":[],"content":"This guide show you how to install on Ubuntu.\n","date":"September 6, 2022","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/docs/installation/linux/ubuntu/","series":[],"smallImg":"","tags":[],"timestamp":1662475343,"title":"Install on Ubuntu"},{"categories":[],"content":"This guide show you how to install on Windows.\n","date":"September 6, 2022","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/docs/installation/windows/","series":[],"smallImg":"","tags":[],"timestamp":1662475343,"title":"Install on Windows"},{"categories":[{"title":"Data Engineering","url":"/categories/data-engineering/"}],"content":"Amazon EMR on EKS is a deployment option for Amazon EMR that allows you to automate the provisioning and management of open-source big data frameworks on EKS. While eksctl is popular for working with Amazon EKS clusters, it has limitations when it comes to building infrastructure that integrates multiple AWS services. Also, it is not straightforward to update EKS cluster resources incrementally with it. On the other hand Terraform can be an effective tool for managing infrastructure that includes not only EKS and EMR virtual clusters but also other AWS resources. Moreover, Terraform has a wide range of modules, and it can even be simpler to build and manage infrastructure using those compared to the CLI tool. In this post, we’ll discuss how to provision and manage Spark jobs on EMR on EKS with Terraform. Amazon EKS Blueprints for Terraform will be used for provisioning EKS, EMR virtual cluster and related resources. Also, Spark job autoscaling will be managed by Karpenter where two Spark jobs with and without Dynamic Resource Allocation (DRA) will be compared.\nInfrastructure When a user submits a Spark job, multiple Pods (controller, driver and executors) will be deployed to the EKS cluster that is registered with EMR. In general, Karpenter provides just-in-time capacity for unschedulable Pods by creating (and terminating afterwards) additional nodes. We can configure the pod templates of a Spark job so that all the Pods are managed by Karpenter. In this way, we are able to run it only in transient nodes. Karpenter simplifies autoscaling by provisioning just-in-time capacity, and it also reduces scheduling latency. The source can be found in the post’s GitHub repository.\nVPC Both private and public subnets are created in three availability zones using the AWS VPC module. The first two subnet tags are in relation to the subnet requirements and considerations of Amazon EKS. The last one of the private subnet tags (karpenter.sh/discovery) is added so that Karpenter can discover the relevant subnets when provisioning a node for Spark jobs.\n1# infra/main.tf 2module \u0026#34;vpc\u0026#34; { 3 source = \u0026#34;terraform-aws-modules/vpc/aws\u0026#34; 4 version = \u0026#34;~\u0026gt; 3.14\u0026#34; 5 6 name = \u0026#34;${local.name}-vpc\u0026#34; 7 cidr = local.vpc.cidr 8 9 azs = local.vpc.azs 10 public_subnets = [for k, v in local.vpc.azs : cidrsubnet(local.vpc.cidr, 3, k)] 11 private_subnets = [for k, v in local.vpc.azs : cidrsubnet(local.vpc.cidr, 3, k + 3)] 12 13 enable_nat_gateway = true 14 single_nat_gateway = true 15 enable_dns_hostnames = true 16 create_igw = true 17 18 public_subnet_tags = { 19 \u0026#34;kubernetes.io/cluster/${local.name}\u0026#34; = \u0026#34;shared\u0026#34; 20 \u0026#34;kubernetes.io/role/elb\u0026#34; = 1 21 } 22 23 private_subnet_tags = { 24 \u0026#34;kubernetes.io/cluster/${local.name}\u0026#34; = \u0026#34;shared\u0026#34; 25 \u0026#34;kubernetes.io/role/internal-elb\u0026#34; = 1 26 \u0026#34;karpenter.sh/discovery\u0026#34; = local.name 27 } 28 29 tags = local.tags 30} EKS Cluster Amazon EKS Blueprints for Terraform extends the AWS EKS module, and it simplifies to create EKS clusters and Kubenetes add-ons. When it comes to EMR on EKS, it deploys the necessary resources to run EMR Spark jobs. Specifically it automates steps 4 to 7 of the setup documentation and it is possible to configure multiple teams (namespaces) as well. In the module configuration, only one managed node group (managed-ondemand) is created, and it’ll be used to deploy all the critical add-ons. Note that Spark jobs will run in transient nodes, which are managed by Karpenter. Therefore, we don’t need to create node groups for them.\n1# infra/main.tf 2module \u0026#34;eks_blueprints\u0026#34; { 3 source = \u0026#34;github.com/aws-ia/terraform-aws-eks-blueprints?ref=v4.7.0\u0026#34; 4 5 cluster_name = local.name 6 cluster_version = local.eks.cluster_version 7 8 # EKS network config 9 vpc_id = module.vpc.vpc_id 10 private_subnet_ids = module.vpc.private_subnets 11 12 cluster_endpoint_private_access = true 13 cluster_endpoint_public_access = true 14 15 node_security_group_additional_rules = { 16 ingress_self_all = { 17 description = \u0026#34;Node to node all ports/protocols, recommended and required for Add-ons\u0026#34; 18 protocol = \u0026#34;-1\u0026#34; 19 from_port = 0 20 to_port = 0 21 type = \u0026#34;ingress\u0026#34; 22 self = true 23 } 24 egress_all = { 25 description = \u0026#34;Node all egress, recommended outbound traffic for Node groups\u0026#34; 26 protocol = \u0026#34;-1\u0026#34; 27 from_port = 0 28 to_port = 0 29 type = \u0026#34;egress\u0026#34; 30 cidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;] 31 ipv6_cidr_blocks = [\u0026#34;::/0\u0026#34;] 32 } 33 ingress_cluster_to_node_all_traffic = { 34 description = \u0026#34;Cluster API to Nodegroup all traffic, can be restricted further eg, spark-operator 8080...\u0026#34; 35 protocol = \u0026#34;-1\u0026#34; 36 from_port = 0 37 to_port = 0 38 type = \u0026#34;ingress\u0026#34; 39 source_cluster_security_group = true 40 } 41 } 42 43 # EKS manage node groups 44 managed_node_groups = { 45 ondemand = { 46 node_group_name = \u0026#34;managed-ondemand\u0026#34; 47 instance_types = [\u0026#34;m5.xlarge\u0026#34;] 48 subnet_ids = module.vpc.private_subnets 49 max_size = 5 50 min_size = 1 51 desired_size = 1 52 create_launch_template = true 53 launch_template_os = \u0026#34;amazonlinux2eks\u0026#34; 54 update_config = [{ 55 max_unavailable_percentage = 30 56 }] 57 } 58 } 59 60 # EMR on EKS 61 enable_emr_on_eks = true 62 emr_on_eks_teams = { 63 analytics = { 64 namespace = \u0026#34;analytics\u0026#34; 65 job_execution_role = \u0026#34;analytics-job-execution-role\u0026#34; 66 additional_iam_policies = [aws_iam_policy.emr_on_eks.arn] 67 } 68 } 69 70 tags = local.tags 71} EMR Virtual Cluster Terraform has the EMR virtual cluster resource and the EKS cluster can be registered with the associating namespace (analytics). It’ll complete the last step of the setup documentation.\n1# infra/main.tf 2resource \u0026#34;aws_emrcontainers_virtual_cluster\u0026#34; \u0026#34;analytics\u0026#34; { 3 name = \u0026#34;${module.eks_blueprints.eks_cluster_id}-analytics\u0026#34; 4 5 container_provider { 6 id = module.eks_blueprints.eks_cluster_id 7 type = \u0026#34;EKS\u0026#34; 8 9 info { 10 eks_info { 11 namespace = \u0026#34;analytics\u0026#34; 12 } 13 } 14 } 15} Kubernetes Add-ons The Blueprints include the kubernetes-addons module that simplifies deployment of Amazon EKS add-ons as well as Kubernetes add-ons. For scaling Spark jobs in transient nodes, Karpenter and AWS Node Termination Handler add-ons will be used mainly.\n1# infra/main.tf 2module \u0026#34;eks_blueprints_kubernetes_addons\u0026#34; { 3 source = \u0026#34;github.com/aws-ia/terraform-aws-eks-blueprints//modules/kubernetes-addons?ref=v4.7.0\u0026#34; 4 5 eks_cluster_id = module.eks_blueprints.eks_cluster_id 6 eks_cluster_endpoint = module.eks_blueprints.eks_cluster_endpoint 7 eks_oidc_provider = module.eks_blueprints.oidc_provider 8 eks_cluster_version = module.eks_blueprints.eks_cluster_version 9 10 # EKS add-ons 11 enable_amazon_eks_vpc_cni = true 12 enable_amazon_eks_coredns = true 13 enable_amazon_eks_kube_proxy = true 14 15 # K8s add-ons 16 enable_coredns_autoscaler = true 17 enable_metrics_server = true 18 enable_cluster_autoscaler = true 19 enable_karpenter = true 20 enable_aws_node_termination_handler = true 21 22 tags = local.tags 23} Karpenter According to the AWS News Blog,\nKarpenter is an open-source, flexible, high-performance Kubernetes cluster autoscaler built with AWS. It helps improve your application availability and cluster efficiency by rapidly launching right-sized compute resources in response to changing application load. Karpenter also provides just-in-time compute resources to meet your application’s needs and will soon automatically optimize a cluster’s compute resource footprint to reduce costs and improve performance.\nSimply put, Karpeter adds nodes to handle unschedulable pods, schedules pods on those nodes, and removes the nodes when they are not needed. To configure Karpenter, we need to create provisioners that define how Karpenter manages unschedulable pods and expired nodes. For Spark jobs, we can deploy separate provisioners for the driver and executor programs.\nSpark Driver Provisioner The labels contain arbitrary key-value pairs. As shown later, we can add it to the nodeSelector field of the Spark pod template. Then Karpenter provisions a node (if not existing) as defined by this Provisioner object. The requirements define which nodes to provision. Here 3 well-known labels are specified - availability zone, instance family and capacity type. The provider section is specific to cloud providers and, for AWS, we need to indicate InstanceProfile, LaunchTemplate, SubnetSelector or SecurityGroupSelector. Here we’ll use a launch template that keeps the instance group and security group ids. SubnetSelector is added separately as it is not covered by the launch template. Recall that we added a tag to private subnets (\u0026ldquo;karpenter.sh/discovery\u0026rdquo; = local.name) and we can use it here so that Karpenter discovers the relevant subnets when provisioning a node.\n1# infra/provisioners/spark-driver.yaml 2apiVersion: karpenter.sh/v1alpha5 3kind: Provisioner 4metadata: 5 name: spark-driver 6spec: 7 labels: 8 type: karpenter 9 provisioner: spark-driver 10 ttlSecondsAfterEmpty: 30 11 requirements: 12 - key: \u0026#34;topology.kubernetes.io/zone\u0026#34; 13 operator: In 14 values: [${az}] 15 - key: karpenter.k8s.aws/instance-family 16 operator: In 17 values: [m4, m5] 18 - key: \u0026#34;karpenter.sh/capacity-type\u0026#34; 19 operator: In 20 values: [\u0026#34;on-demand\u0026#34;] 21 limits: 22 resources: 23 cpu: \u0026#34;1000\u0026#34; 24 memory: 1000Gi 25 provider: 26 launchTemplate: \u0026#34;karpenter-${cluster_name}\u0026#34; 27 subnetSelector: 28 karpenter.sh/discovery: ${cluster_name} Spark Executor Provisioner The executor provisioner configuration is similar except that it allows more instance family values and the capacity type value is changed into spot.\n1# infra/provisioners/spark-executor.yaml 2apiVersion: karpenter.sh/v1alpha5 3kind: Provisioner 4metadata: 5 name: spark-executor 6spec: 7 labels: 8 type: karpenter 9 provisioner: spark-executor 10 ttlSecondsAfterEmpty: 30 11 requirements: 12 - key: \u0026#34;topology.kubernetes.io/zone\u0026#34; 13 operator: In 14 values: [${az}] 15 - key: karpenter.k8s.aws/instance-family 16 operator: In 17 values: [m4, m5, r4, r5] 18 - key: \u0026#34;karpenter.sh/capacity-type\u0026#34; 19 operator: In 20 values: [\u0026#34;spot\u0026#34;] 21 limits: 22 resources: 23 cpu: \u0026#34;1000\u0026#34; 24 memory: 1000Gi 25 provider: 26 launchTemplate: \u0026#34;karpenter-${cluster_name}\u0026#34; 27 subnetSelector: 28 karpenter.sh/discovery: ${cluster_name} Terraform Resources As mentioned earlier, a launch template is created for the provisioners, and it includes the instance profile, security group ID and additional configuration. The provisioner resources are created from the YAML manifests. Note we only select a single available zone in order to save cost and improve performance of Spark jobs.\n1# infra/main.tf 2module \u0026#34;karpenter_launch_templates\u0026#34; { 3 source = \u0026#34;github.com/aws-ia/terraform-aws-eks-blueprints//modules/launch-templates?ref=v4.7.0\u0026#34; 4 5 eks_cluster_id = module.eks_blueprints.eks_cluster_id 6 7 launch_template_config = { 8 linux = { 9 ami = data.aws_ami.eks.id 10 launch_template_prefix = \u0026#34;karpenter\u0026#34; 11 iam_instance_profile = module.eks_blueprints.managed_node_group_iam_instance_profile_id[0] 12 vpc_security_group_ids = [module.eks_blueprints.worker_node_security_group_id] 13 block_device_mappings = [ 14 { 15 device_name = \u0026#34;/dev/xvda\u0026#34; 16 volume_type = \u0026#34;gp3\u0026#34; 17 volume_size = 100 18 } 19 ] 20 } 21 } 22 23 tags = merge(local.tags, { Name = \u0026#34;karpenter\u0026#34; }) 24} 25 26# deploy spark provisioners for Karpenter autoscaler 27data \u0026#34;kubectl_path_documents\u0026#34; \u0026#34;karpenter_provisioners\u0026#34; { 28 pattern = \u0026#34;${path.module}/provisioners/spark*.yaml\u0026#34; 29 vars = { 30 az = join(\u0026#34;,\u0026#34;, slice(local.vpc.azs, 0, 1)) 31 cluster_name = local.name 32 } 33} 34 35resource \u0026#34;kubectl_manifest\u0026#34; \u0026#34;karpenter_provisioner\u0026#34; { 36 for_each = toset(data.kubectl_path_documents.karpenter_provisioners.documents) 37 yaml_body = each.value 38 39 depends_on = [module.eks_blueprints_kubernetes_addons] 40} Now we can deploy the infrastructure. Be patient until it completes.\nSpark Job A test spark app and pod templates are uploaded to a S3 bucket. The spark app is for testing autoscaling, and it creates multiple parallel threads and waits for a few seconds - it is obtained from EKS Workshop. The pod templates basically select the relevant provisioners for the driver and executor programs. Two Spark jobs will run with and without Dynamic Resource Allocation (DRA). DRA is a Spark feature where the initial number of executors are spawned, and then it is increased until the maximum number of executors is met to process the pending tasks. Idle executors are terminated when there are no pending tasks. This feature is particularly useful if we are not sure how many executors are necessary.\n1## upload.sh 2#!/usr/bin/env bash 3 4# write test script 5mkdir -p scripts/src 6cat \u0026lt;\u0026lt; EOF \u0026gt; scripts/src/threadsleep.py 7import sys 8from time import sleep 9from pyspark.sql import SparkSession 10spark = SparkSession.builder.appName(\u0026#34;threadsleep\u0026#34;).getOrCreate() 11def sleep_for_x_seconds(x):sleep(x*20) 12sc=spark.sparkContext 13sc.parallelize(range(1,6), 5).foreach(sleep_for_x_seconds) 14spark.stop() 15EOF 16 17# write pod templates 18mkdir -p scripts/config 19cat \u0026lt;\u0026lt; EOF \u0026gt; scripts/config/driver-template.yaml 20apiVersion: v1 21kind: Pod 22spec: 23 nodeSelector: 24 type: \u0026#39;karpenter\u0026#39; 25 provisioner: \u0026#39;spark-driver\u0026#39; 26 tolerations: 27 - key: \u0026#39;spark-driver\u0026#39; 28 operator: \u0026#39;Exists\u0026#39; 29 effect: \u0026#39;NoSchedule\u0026#39; 30 containers: 31 - name: spark-kubernetes-driver 32EOF 33 34cat \u0026lt;\u0026lt; EOF \u0026gt; scripts/config/executor-template.yaml 35apiVersion: v1 36kind: Pod 37spec: 38 nodeSelector: 39 type: \u0026#39;karpenter\u0026#39; 40 provisioner: \u0026#39;spark-executor\u0026#39; 41 tolerations: 42 - key: \u0026#39;spark-executor\u0026#39; 43 operator: \u0026#39;Exists\u0026#39; 44 effect: \u0026#39;NoSchedule\u0026#39; 45 containers: 46 - name: spark-kubernetes-executor 47EOF 48 49# sync to S3 50DEFAULT_BUCKET_NAME=$(terraform -chdir=./infra output --raw default_bucket_name) 51aws s3 sync . s3://$DEFAULT_BUCKET_NAME --exclude \u0026#34;*\u0026#34; --include \u0026#34;scripts/*\u0026#34; Without Dynamic Resource Allocation (DRA) 15 executors are configured to run for the Spark job without DRA. The application configuration is overridden to disable DRA and maps pod templates for the diver and executor programs.\n1export VIRTUAL_CLUSTER_ID=$(terraform -chdir=./infra output --raw emrcontainers_virtual_cluster_id) 2export EMR_ROLE_ARN=$(terraform -chdir=./infra output --json emr_on_eks_role_arn | jq \u0026#39;.[0]\u0026#39; -r) 3export DEFAULT_BUCKET_NAME=$(terraform -chdir=./infra output --raw default_bucket_name) 4export AWS_REGION=$(aws ec2 describe-availability-zones --query \u0026#39;AvailabilityZones[0].[RegionName]\u0026#39; --output text) 5 6## without DRA 7aws emr-containers start-job-run \\ 8--virtual-cluster-id $VIRTUAL_CLUSTER_ID \\ 9--name threadsleep-karpenter-wo-dra \\ 10--execution-role-arn $EMR_ROLE_ARN \\ 11--release-label emr-6.7.0-latest \\ 12--region $AWS_REGION \\ 13--job-driver \u0026#39;{ 14 \u0026#34;sparkSubmitJobDriver\u0026#34;: { 15 \u0026#34;entryPoint\u0026#34;: \u0026#34;s3://\u0026#39;${DEFAULT_BUCKET_NAME}\u0026#39;/scripts/src/threadsleep.py\u0026#34;, 16 \u0026#34;sparkSubmitParameters\u0026#34;: \u0026#34;--conf spark.executor.instances=15 --conf spark.executor.memory=1G --conf spark.executor.cores=1 --conf spark.driver.cores=1\u0026#34; 17 } 18 }\u0026#39; \\ 19--configuration-overrides \u0026#39;{ 20 \u0026#34;applicationConfiguration\u0026#34;: [ 21 { 22 \u0026#34;classification\u0026#34;: \u0026#34;spark-defaults\u0026#34;, 23 \u0026#34;properties\u0026#34;: { 24 \u0026#34;spark.dynamicAllocation.enabled\u0026#34;:\u0026#34;false\u0026#34;, 25 \u0026#34;spark.kubernetes.executor.deleteOnTermination\u0026#34;: \u0026#34;true\u0026#34;, 26 \u0026#34;spark.kubernetes.driver.podTemplateFile\u0026#34;:\u0026#34;s3://\u0026#39;${DEFAULT_BUCKET_NAME}\u0026#39;/scripts/config/driver-template.yaml\u0026#34;, 27 \u0026#34;spark.kubernetes.executor.podTemplateFile\u0026#34;:\u0026#34;s3://\u0026#39;${DEFAULT_BUCKET_NAME}\u0026#39;/scripts/config/executor-template.yaml\u0026#34; 28 } 29 } 30 ] 31}\u0026#39; As indicated earlier, Karpenter can provide just-in-time compute resources to meet the Spark job\u0026rsquo;s requirements, and we see that 3 new nodes are added accordingly. Note that, unlike cluster autoscaler, Karpenter provision nodes without creating a node group.\nOnce the job completes, the new nodes are terminated as expected.\nBelow shows the event timeline of the Spark job. It adds all the 15 executors regardless of whether there are pending tasks or not. The DRA feature of Spark can be beneficial in this situation, and it’ll be discussed in the next section.\nWith Dynamic Resource Allocation (DRA) Here the initial number of executors is set to 1. With DRA enabled, the driver is expected to scale up the executors until it reaches the maximum number of executors if there are pending tasks.\n1export VIRTUAL_CLUSTER_ID=$(terraform -chdir=./infra output --raw emrcontainers_virtual_cluster_id) 2export EMR_ROLE_ARN=$(terraform -chdir=./infra output --json emr_on_eks_role_arn | jq \u0026#39;.[0]\u0026#39; -r) 3export DEFAULT_BUCKET_NAME=$(terraform -chdir=./infra output --raw default_bucket_name) 4export AWS_REGION=$(aws ec2 describe-availability-zones --query \u0026#39;AvailabilityZones[0].[RegionName]\u0026#39; --output text) 5 6## with DRA 7aws emr-containers start-job-run \\ 8--virtual-cluster-id $VIRTUAL_CLUSTER_ID \\ 9--name threadsleep-karpenter-w-dra \\ 10--execution-role-arn $EMR_ROLE_ARN \\ 11--release-label emr-6.7.0-latest \\ 12--region $AWS_REGION \\ 13--job-driver \u0026#39;{ 14 \u0026#34;sparkSubmitJobDriver\u0026#34;: { 15 \u0026#34;entryPoint\u0026#34;: \u0026#34;s3://\u0026#39;${DEFAULT_BUCKET_NAME}\u0026#39;/scripts/src/threadsleep.py\u0026#34;, 16 \u0026#34;sparkSubmitParameters\u0026#34;: \u0026#34;--conf spark.executor.instances=1 --conf spark.executor.memory=1G --conf spark.executor.cores=1 --conf spark.driver.cores=1\u0026#34; 17 } 18 }\u0026#39; \\ 19--configuration-overrides \u0026#39;{ 20 \u0026#34;applicationConfiguration\u0026#34;: [ 21 { 22 \u0026#34;classification\u0026#34;: \u0026#34;spark-defaults\u0026#34;, 23 \u0026#34;properties\u0026#34;: { 24 \u0026#34;spark.dynamicAllocation.enabled\u0026#34;:\u0026#34;true\u0026#34;, 25 \u0026#34;spark.dynamicAllocation.shuffleTracking.enabled\u0026#34;:\u0026#34;true\u0026#34;, 26 \u0026#34;spark.dynamicAllocation.minExecutors\u0026#34;:\u0026#34;1\u0026#34;, 27 \u0026#34;spark.dynamicAllocation.maxExecutors\u0026#34;:\u0026#34;10\u0026#34;, 28 \u0026#34;spark.dynamicAllocation.initialExecutors\u0026#34;:\u0026#34;1\u0026#34;, 29 \u0026#34;spark.dynamicAllocation.schedulerBacklogTimeout\u0026#34;: \u0026#34;1s\u0026#34;, 30 \u0026#34;spark.dynamicAllocation.executorIdleTimeout\u0026#34;: \u0026#34;5s\u0026#34;, 31 \u0026#34;spark.kubernetes.driver.podTemplateFile\u0026#34;:\u0026#34;s3://\u0026#39;${DEFAULT_BUCKET_NAME}\u0026#39;/scripts/config/driver-template.yaml\u0026#34;, 32 \u0026#34;spark.kubernetes.executor.podTemplateFile\u0026#34;:\u0026#34;s3://\u0026#39;${DEFAULT_BUCKET_NAME}\u0026#39;/scripts/config/executor-template.yaml\u0026#34; 33 } 34 } 35 ] 36}\u0026#39; As expected, the executors are added dynamically and removed subsequently as they are not needed.\nSummary In this post, it is discussed how to provision and manage Spark jobs on EMR on EKS with Terraform. Amazon EKS Blueprints for Terraform is used for provisioning EKS, EMR virtual cluster and related resources. Also, Karpenter is used to manage Spark job autoscaling and two Spark jobs with and without Dynamic Resource Allocation (DRA) are used for comparison. It is found that Karpenter manages transient nodes for Spark jobs to meet their scaling requirements effectively.\n","date":"August 26, 2022","img":"/blog/2022-08-26-emr-on-eks-with-terraform/featured.png","lang":"en","langName":"English","largeImg":"/blog/2022-08-26-emr-on-eks-with-terraform/featured_hu0db6c869a471dd7000f2d35dcf0e8ab0_67936_500x0_resize_box_3.png","permalink":"/blog/2022-08-26-emr-on-eks-with-terraform/","series":[],"smallImg":"/blog/2022-08-26-emr-on-eks-with-terraform/featured_hu0db6c869a471dd7000f2d35dcf0e8ab0_67936_180x0_resize_box_3.png","tags":[{"title":"AWS","url":"/tags/aws/"},{"title":"Amazon EKS","url":"/tags/amazon-eks/"},{"title":"Amazon EMR","url":"/tags/amazon-emr/"},{"title":"Apache Spark","url":"/tags/apache-spark/"},{"title":"Kubernetes","url":"/tags/kubernetes/"},{"title":"Terraform","url":"/tags/terraform/"}],"timestamp":1661472000,"title":"Manage EMR on EKS With Terraform"},{"categories":[{"title":"Data Engineering","url":"/categories/data-engineering/"}],"content":"Apache Airflow is a popular workflow management platform. A wide range of AWS services are integrated with the platform by Amazon AWS Operators. AWS Lambda is one of the integrated services, and it can be used to develop workflows efficiently. The current Lambda Operator, however, just invokes a Lambda function, and it can fail to report the invocation result of a function correctly and to record the exact error message from failure. In this post, we’ll discuss a custom Lambda operator that handles those limitations.\nArchitecture We’ll discuss a custom Lambda operator, and it extends the Lambda operator provided by AWS. When a DAG creates a task that invokes a Lambda function, it updates the Lambda payload with a correlation ID that uniquely identifies the task. The correlation ID is added to every log message that the Lambda function generates. Finally, the custom operator filters the associating CloudWatch log events, prints the log messages and raises a runtime error when an error message is found. In this setup, we are able to correctly identify the function invocation result and to point to the exact error message if it fails. The source of this post can be found in a GitHub repository.\nLambda Setup Lambda Function The Logger utility of the Lambda Powertools Python package is used to record log messages. The correlation ID is added to the event payload, and it is set to be injected with log messages by the logger.inject_lambda_context decorator. Note the Lambda Context would be a better place to add a correlation ID as we can add a custom client context object. However, it is not recognised when an invocation is made asynchronously, and we have to add it to the event payload. We use another decorator (middleware_before_after) and it logs messages before and after the function invocation. The latter message that indicates the end of a function is important as we can rely on it in order to identify whether a function is completed without an error. If a function finishes with an error, the last log message won’t be recorded. Also, we can check if a function fails by checking a log message where its level is ERROR, and it is created by the logger.exception method. The Lambda event payload has two extra attributes - n for setting-up the number of iteration and to_fail for determining whether to raise an error.\n1# lambda/src/lambda_function.py 2import time 3from aws_lambda_powertools import Logger 4from aws_lambda_powertools.utilities.typing import LambdaContext 5from aws_lambda_powertools.middleware_factory import lambda_handler_decorator 6 7logger = Logger(log_record_order=[\u0026#34;correlation_id\u0026#34;, \u0026#34;level\u0026#34;, \u0026#34;message\u0026#34;, \u0026#34;location\u0026#34;]) 8 9 10@lambda_handler_decorator 11def middleware_before_after(handler, event, context): 12 logger.info(\u0026#34;Function started\u0026#34;) 13 response = handler(event, context) 14 logger.info(\u0026#34;Function ended\u0026#34;) 15 return response 16 17 18@logger.inject_lambda_context(correlation_id_path=\u0026#34;correlation_id\u0026#34;) 19@middleware_before_after 20def lambda_handler(event: dict, context: LambdaContext): 21 num_iter = event.get(\u0026#34;n\u0026#34;, 10) 22 to_fail = event.get(\u0026#34;to_fail\u0026#34;, False) 23 logger.info(f\u0026#34;num_iter - {num_iter}, fail - {to_fail}\u0026#34;) 24 try: 25 for n in range(num_iter): 26 logger.info(f\u0026#34;iter - {n + 1}...\u0026#34;) 27 time.sleep(1) 28 if to_fail: 29 raise Exception 30 except Exception as e: 31 logger.exception(\u0026#34;Function invocation failed...\u0026#34;) 32 raise RuntimeError(\u0026#34;Unable to finish loop\u0026#34;) from e SAM Template The Serverless Application Model (SAM) framework is used to deploy the Lambda function. The Lambda Powertools Python package is added as a Lambda layer. The Lambda log group is configured so that messages are kept only for 1 day, and it can help reduce time to filter log events. By default, a Lambda function is invoked twice more on error when it is invoked asynchronously - the default retry attempts equals to 2. It is set to 0 as retry behaviour can be controlled by Airflow if necessary, and it can make it easier to track function invocation status.\n1# lambda/template.yml 2AWSTemplateFormatVersion: \u0026#34;2010-09-09\u0026#34; 3Transform: AWS::Serverless-2016-10-31 4Description: Lambda functions used to demonstrate Lambda invoke operator with S3 log extension 5 6Globals: 7 Function: 8 MemorySize: 128 9 Timeout: 30 10 Runtime: python3.8 11 Tracing: Active 12 Environment: 13 Variables: 14 POWERTOOLS_SERVICE_NAME: airflow 15 LOG_LEVEL: INFO 16 Tags: 17 Application: LambdaInvokeOperatorDemo 18Resources: 19 ExampleFunction: 20 Type: AWS::Serverless::Function 21 Properties: 22 FunctionName: example-lambda-function 23 Description: Example lambda function 24 CodeUri: src/ 25 Handler: lambda_function.lambda_handler 26 Layers: 27 - !Sub arn:aws:lambda:${AWS::Region}:017000801446:layer:AWSLambdaPowertoolsPython:26 28 ExampleFunctionAsyncConfig: 29 Type: AWS::Lambda::EventInvokeConfig 30 Properties: 31 FunctionName: !Ref ExampleFunction 32 MaximumRetryAttempts: 0 33 Qualifier: \u0026#34;$LATEST\u0026#34; 34 LogGroup: 35 Type: AWS::Logs::LogGroup 36 Properties: 37 LogGroupName: !Sub \u0026#34;/aws/lambda/${ExampleFunction}\u0026#34; 38 RetentionInDays: 1 39 40Outputs: 41 ExampleFunction: 42 Value: !Ref ExampleFunction 43 Description: Example lambda function ARN Lambda Operator Lambda Invoke Function Operator Below shows the source of the Lambda invoke function operator. After invoking a Lambda function, the _execute _method checks if the response status code indicates success and whether _FunctionError _is found in the response payload. When an invocation is made synchronously (RequestResponse invocation type), it can identify whether the invocation is successful or not because the response is returned after it finishes. However it reports a generic error message when it fails and we have to visit CloudWatch Logs if we want to check the exact error. It gets worse when it is invoked asynchronously (Event invocation type) because the response is made before the invocation finishes. In this case it is not even possible to check whether the invocation is successful.\n1... 2 3class AwsLambdaInvokeFunctionOperator(BaseOperator): 4 def __init__( 5 self, 6 *, 7 function_name: str, 8 log_type: Optional[str] = None, 9 qualifier: Optional[str] = None, 10 invocation_type: Optional[str] = None, 11 client_context: Optional[str] = None, 12 payload: Optional[str] = None, 13 aws_conn_id: str = \u0026#39;aws_default\u0026#39;, 14 **kwargs, 15 ): 16 super().__init__(**kwargs) 17 self.function_name = function_name 18 self.payload = payload 19 self.log_type = log_type 20 self.qualifier = qualifier 21 self.invocation_type = invocation_type 22 self.client_context = client_context 23 self.aws_conn_id = aws_conn_id 24 25 def execute(self, context: \u0026#39;Context\u0026#39;): 26 hook = LambdaHook(aws_conn_id=self.aws_conn_id) 27 success_status_codes = [200, 202, 204] 28 self.log.info(\u0026#34;Invoking AWS Lambda function: %s with payload: %s\u0026#34;, self.function_name, self.payload) 29 response = hook.invoke_lambda( 30 function_name=self.function_name, 31 invocation_type=self.invocation_type, 32 log_type=self.log_type, 33 client_context=self.client_context, 34 payload=self.payload, 35 qualifier=self.qualifier, 36 ) 37 self.log.info(\u0026#34;Lambda response metadata: %r\u0026#34;, response.get(\u0026#34;ResponseMetadata\u0026#34;)) 38 if response.get(\u0026#34;StatusCode\u0026#34;) not in success_status_codes: 39 raise ValueError(\u0026#39;Lambda function did not execute\u0026#39;, json.dumps(response.get(\u0026#34;ResponseMetadata\u0026#34;))) 40 payload_stream = response.get(\u0026#34;Payload\u0026#34;) 41 payload = payload_stream.read().decode() 42 if \u0026#34;FunctionError\u0026#34; in response: 43 raise ValueError( 44 \u0026#39;Lambda function execution resulted in error\u0026#39;, 45 {\u0026#34;ResponseMetadata\u0026#34;: response.get(\u0026#34;ResponseMetadata\u0026#34;), \u0026#34;Payload\u0026#34;: payload}, 46 ) 47 self.log.info(\u0026#39;Lambda function invocation succeeded: %r\u0026#39;, response.get(\u0026#34;ResponseMetadata\u0026#34;)) 48 return payload Custom Lambda Operator The custom Lambda operator extends the Lambda invoke function operator. It updates the Lambda payload by adding a correlation ID. The _execute _method is extended by the _log_processor _decorator function. As the name suggests, the decorator function filters all log messages that include the correlation ID and print them. This process loops over the lifetime of the invocation. While processing log events, it raises an error if an error message is found. And log event processing gets stopped when a message that indicates the end of the invocation is encountered. Finally, in order to handle the case where an invocation doesn’t finish within the timeout seconds, it raises an error at the end of the loop.\nThe main benefits of this approach are\nwe don’t have to rewrite Lambda invocation logic as we extend the Lambda invoke function operator, we can track a lambda invocation status regardless of its invocation type, and we are able to record all relevant log messages of an invocation 1# airflow/dags/lambda_operator.py 2... 3 4class CustomLambdaFunctionOperator(AwsLambdaInvokeFunctionOperator): 5 def __init__( 6 self, 7 *, 8 function_name: str, 9 log_type: Optional[str] = None, 10 qualifier: Optional[str] = None, 11 invocation_type: Optional[str] = None, 12 client_context: Optional[str] = None, 13 payload: Optional[str] = None, 14 aws_conn_id: str = \u0026#34;aws_default\u0026#34;, 15 correlation_id: str = str(uuid4()), 16 **kwargs, 17 ): 18 super().__init__( 19 function_name=function_name, 20 log_type=log_type, 21 qualifier=qualifier, 22 invocation_type=invocation_type, 23 client_context=client_context, 24 payload=json.dumps( 25 {**json.loads((payload or \u0026#34;{}\u0026#34;)), **{\u0026#34;correlation_id\u0026#34;: correlation_id}} 26 ), 27 aws_conn_id=aws_conn_id, 28 **kwargs, 29 ) 30 self.correlation_id = correlation_id 31 32 def log_processor(func): 33 @functools.wraps(func) 34 def wrapper_decorator(self, *args, **kwargs): 35 payload = func(self, *args, **kwargs) 36 function_timeout = self.get_function_timeout() 37 self.process_log_events(function_timeout) 38 return payload 39 40 return wrapper_decorator 41 42 @log_processor 43 def execute(self, context: \u0026#34;Context\u0026#34;): 44 return super().execute(context) 45 46 def get_function_timeout(self): 47 resp = boto3.client(\u0026#34;lambda\u0026#34;).get_function_configuration(FunctionName=self.function_name) 48 return resp[\u0026#34;Timeout\u0026#34;] 49 50 def process_log_events(self, function_timeout: int): 51 start_time = 0 52 for _ in range(function_timeout): 53 response_iterator = self.get_response_iterator( 54 self.function_name, self.correlation_id, start_time 55 ) 56 for page in response_iterator: 57 for event in page[\u0026#34;events\u0026#34;]: 58 start_time = event[\u0026#34;timestamp\u0026#34;] 59 message = json.loads(event[\u0026#34;message\u0026#34;]) 60 print(message) 61 if message[\u0026#34;level\u0026#34;] == \u0026#34;ERROR\u0026#34;: 62 raise RuntimeError(\u0026#34;ERROR found in log\u0026#34;) 63 if message[\u0026#34;message\u0026#34;] == \u0026#34;Function ended\u0026#34;: 64 return 65 time.sleep(1) 66 raise RuntimeError(\u0026#34;Lambda function end message not found after function timeout\u0026#34;) 67 68 @staticmethod 69 def get_response_iterator(function_name: str, correlation_id: str, start_time: int): 70 paginator = boto3.client(\u0026#34;logs\u0026#34;).get_paginator(\u0026#34;filter_log_events\u0026#34;) 71 return paginator.paginate( 72 logGroupName=f\u0026#34;/aws/lambda/{function_name}\u0026#34;, 73 filterPattern=f\u0026#39;\u0026#34;{correlation_id}\u0026#34;\u0026#39;, 74 startTime=start_time + 1, 75 ) Unit Testing Unit testing is performed for the main log processing function (process_log_events). Log events fixture is created by a closure function. Depending on the case argument, it returns a log events list that covers success, error or timeout error. It is used as the mock response of the _get_response_iterator _method. The 3 testing cases cover each of the possible scenarios.\n1# airflow/tests/test_lambda_operator.py 2import json 3import pytest 4from unittest.mock import MagicMock 5from dags.lambda_operator import CustomLambdaFunctionOperator 6 7 8@pytest.fixture 9def log_events(): 10 def _(case): 11 events = [ 12 { 13 \u0026#34;timestamp\u0026#34;: 1659296879605, 14 \u0026#34;message\u0026#34;: \u0026#39;{\u0026#34;correlation_id\u0026#34;:\u0026#34;2850fda4-9005-4375-aca8-88dfdda222ba\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;Function started\u0026#34;,\u0026#34;location\u0026#34;:\u0026#34;middleware_before_after:12\u0026#34;,\u0026#34;timestamp\u0026#34;:\u0026#34;2022-07-31 19:47:59,605+0000\u0026#34;,\u0026#34;service\u0026#34;:\u0026#34;airflow\u0026#34;, ...}\\n\u0026#39;, 15 }, 16 { 17 \u0026#34;timestamp\u0026#34;: 1659296879605, 18 \u0026#34;message\u0026#34;: \u0026#39;{\u0026#34;correlation_id\u0026#34;:\u0026#34;2850fda4-9005-4375-aca8-88dfdda222ba\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;num_iter - 10, fail - False\u0026#34;,\u0026#34;location\u0026#34;:\u0026#34;lambda_handler:23\u0026#34;,\u0026#34;timestamp\u0026#34;:\u0026#34;2022-07-31 19:47:59,605+0000\u0026#34;,\u0026#34;service\u0026#34;:\u0026#34;airflow\u0026#34;, ...}\\n\u0026#39;, 19 }, 20 { 21 \u0026#34;timestamp\u0026#34;: 1659296879605, 22 \u0026#34;message\u0026#34;: \u0026#39;{\u0026#34;correlation_id\u0026#34;:\u0026#34;2850fda4-9005-4375-aca8-88dfdda222ba\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;iter - 1...\u0026#34;,\u0026#34;location\u0026#34;:\u0026#34;lambda_handler:26\u0026#34;,\u0026#34;timestamp\u0026#34;:\u0026#34;2022-07-31 19:47:59,605+0000\u0026#34;,\u0026#34;service\u0026#34;:\u0026#34;airflow\u0026#34;, ...}\\n\u0026#39;, 23 }, 24 ] 25 if case == \u0026#34;success\u0026#34;: 26 events.append( 27 { 28 \u0026#34;timestamp\u0026#34;: 1659296889620, 29 \u0026#34;message\u0026#34;: \u0026#39;{\u0026#34;correlation_id\u0026#34;:\u0026#34;2850fda4-9005-4375-aca8-88dfdda222ba\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;Function ended\u0026#34;,\u0026#34;location\u0026#34;:\u0026#34;middleware_before_after:14\u0026#34;,\u0026#34;timestamp\u0026#34;:\u0026#34;2022-07-31 19:48:09,619+0000\u0026#34;,\u0026#34;service\u0026#34;:\u0026#34;airflow\u0026#34;, ...}\\n\u0026#39;, 30 } 31 ) 32 elif case == \u0026#34;error\u0026#34;: 33 events.append( 34 { 35 \u0026#34;timestamp\u0026#34;: 1659296889629, 36 \u0026#34;message\u0026#34;: \u0026#39;{\u0026#34;correlation_id\u0026#34;:\u0026#34;2850fda4-9005-4375-aca8-88dfdda222ba\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;ERROR\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;Function invocation failed...\u0026#34;,\u0026#34;location\u0026#34;:\u0026#34;lambda_handler:31\u0026#34;,\u0026#34;timestamp\u0026#34;:\u0026#34;2022-07-31 19:48:09,628+0000\u0026#34;,\u0026#34;service\u0026#34;:\u0026#34;airflow\u0026#34;, ..., \u0026#34;exception\u0026#34;:\u0026#34;Traceback (most recent call last):\\\\n File \\\\\u0026#34;/var/task/lambda_function.py\\\\\u0026#34;, line 29, in lambda_handler\\\\n raise Exception\\\\nException\u0026#34;,\u0026#34;exception_name\u0026#34;:\u0026#34;Exception\u0026#34;,\u0026#34;xray_trace_id\u0026#34;:\u0026#34;1-62e6dc6f-30b8e51d000de0ee5a22086b\u0026#34;}\\n\u0026#39;, 37 }, 38 ) 39 return [{\u0026#34;events\u0026#34;: events}] 40 41 return _ 42 43 44def test_process_log_events_success(log_events): 45 success_resp = log_events(\u0026#34;success\u0026#34;) 46 operator = CustomLambdaFunctionOperator( 47 task_id=\u0026#34;sync_w_error\u0026#34;, 48 function_name=\u0026#34;\u0026#34;, 49 invocation_type=\u0026#34;RequestResponse\u0026#34;, 50 payload=json.dumps({\u0026#34;n\u0026#34;: 1, \u0026#34;to_fail\u0026#34;: True}), 51 aws_conn_id=None, 52 ) 53 operator.get_response_iterator = MagicMock(return_value=success_resp) 54 assert operator.process_log_events(1) == None 55 56 57def test_process_log_events_fail_with_error(log_events): 58 fail_resp = log_events(\u0026#34;error\u0026#34;) 59 operator = CustomLambdaFunctionOperator( 60 task_id=\u0026#34;sync_w_error\u0026#34;, 61 function_name=\u0026#34;\u0026#34;, 62 invocation_type=\u0026#34;RequestResponse\u0026#34;, 63 payload=json.dumps({\u0026#34;n\u0026#34;: 1, \u0026#34;to_fail\u0026#34;: True}), 64 aws_conn_id=None, 65 ) 66 operator.get_response_iterator = MagicMock(return_value=fail_resp) 67 with pytest.raises(RuntimeError) as e: 68 operator.process_log_events(1) 69 assert \u0026#34;ERROR found in log\u0026#34; == str(e.value) 70 71 72def test_process_log_events_fail_by_timeout(log_events): 73 fail_resp = log_events(None) 74 operator = CustomLambdaFunctionOperator( 75 task_id=\u0026#34;sync_w_error\u0026#34;, 76 function_name=\u0026#34;\u0026#34;, 77 invocation_type=\u0026#34;RequestResponse\u0026#34;, 78 payload=json.dumps({\u0026#34;n\u0026#34;: 1, \u0026#34;to_fail\u0026#34;: True}), 79 aws_conn_id=None, 80 ) 81 operator.get_response_iterator = MagicMock(return_value=fail_resp) 82 with pytest.raises(RuntimeError) as e: 83 operator.process_log_events(1) 84 assert \u0026#34;Lambda function end message not found after function timeout\u0026#34; == str(e.value) Below shows the results of the testing.\n1$ pytest airflow/tests/test_lambda_operator.py -v 2============================================ test session starts ============================================= 3platform linux -- Python 3.8.10, pytest-7.1.2, pluggy-1.0.0 -- /home/jaehyeon/personal/revisit-lambda-operator/venv/bin/python3 4cachedir: .pytest_cache 5rootdir: /home/jaehyeon/personal/revisit-lambda-operator 6plugins: anyio-3.6.1 7collected 3 items 8 9airflow/tests/test_lambda_operator.py::test_process_log_events_success PASSED [ 33%] 10airflow/tests/test_lambda_operator.py::test_process_log_events_fail_with_error PASSED [ 66%] 11airflow/tests/test_lambda_operator.py::test_process_log_events_fail_by_timeout PASSED [100%] 12 13============================================= 3 passed in 1.34s ============================================== Compare Operators Docker Compose In order to compare the two operators, the Airflow Docker quick start guide is simplified into using the Local Executor. In this setup, both scheduling and task execution are handled by the airflow scheduler service. Instead of creating an AWS connection for invoking Lambda functions, the host AWS configuration is shared by volume-mapping (${HOME}/.aws to /home/airflow/.aws). Also, as I don’t use the default AWS profile but a profile named cevo, it is added to the scheduler service as an environment variable (AWS_PROFILE: \u0026ldquo;cevo\u0026rdquo;).\n1# airflow/docker-compose.yaml 2--- 3version: \u0026#34;3\u0026#34; 4x-airflow-common: \u0026amp;airflow-common 5 image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:2.3.3} 6 environment: airflow-common-env 7 AIRFLOW__CORE__EXECUTOR: LocalExecutor 8 AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow 9 # For backward compatibility, with Airflow \u0026lt;2.3 10 AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow 11 AIRFLOW__CORE__FERNET_KEY: \u0026#34;\u0026#34; 12 AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: \u0026#34;true\u0026#34; 13 AIRFLOW__CORE__LOAD_EXAMPLES: \u0026#34;false\u0026#34; 14 AIRFLOW__API__AUTH_BACKENDS: \u0026#34;airflow.api.auth.backend.basic_auth\u0026#34; 15 _PIP_ADDITIONAL_REQUIREMENTS: ${_PIP_ADDITIONAL_REQUIREMENTS:-} 16 volumes: 17 - ./dags:/opt/airflow/dags 18 - ./logs:/opt/airflow/logs 19 - ./plugins:/opt/airflow/plugins 20 - ${HOME}/.aws:/home/airflow/.aws 21 user: \u0026#34;${AIRFLOW_UID:-50000}:0\u0026#34; 22 depends_on: \u0026amp;airflow-common-depends-on 23 postgres: 24 condition: service_healthy 25 26services: 27 postgres: 28 image: postgres:13 29 ports: 30 - 5432:5432 31 environment: 32 POSTGRES_USER: airflow 33 POSTGRES_PASSWORD: airflow 34 POSTGRES_DB: airflow 35 volumes: 36 - postgres-db-volume:/var/lib/postgresql/data 37 healthcheck: 38 test: [\u0026#34;CMD\u0026#34;, \u0026#34;pg_isready\u0026#34;, \u0026#34;-U\u0026#34;, \u0026#34;airflow\u0026#34;] 39 interval: 5s 40 retries: 5 41 42 airflow-webserver: 43 \u0026lt;\u0026lt;: *airflow-common 44 command: webserver 45 ports: 46 - 8080:8080 47 depends_on: 48 \u0026lt;\u0026lt;: *airflow-common-depends-on 49 airflow-init: 50 condition: service_completed_successfully 51 52 airflow-scheduler: 53 \u0026lt;\u0026lt;: *airflow-common 54 command: scheduler 55 environment: 56 \u0026lt;\u0026lt;: *airflow-common-env 57 AWS_PROFILE: \u0026#34;cevo\u0026#34; 58 depends_on: 59 \u0026lt;\u0026lt;: *airflow-common-depends-on 60 airflow-init: 61 condition: service_completed_successfully 62 63 airflow-init: 64 \u0026lt;\u0026lt;: *airflow-common 65 entrypoint: /bin/bash 66 # yamllint disable rule:line-length 67 command: 68 - -c 69 - | 70 if [[ -z \u0026#34;${AIRFLOW_UID}\u0026#34; ]]; then 71 echo 72 echo -e \u0026#34;\\033[1;33mWARNING!!!: AIRFLOW_UID not set!\\e[0m\u0026#34; 73 echo \u0026#34;If you are on Linux, you SHOULD follow the instructions below to set \u0026#34; 74 echo \u0026#34;AIRFLOW_UID environment variable, otherwise files will be owned by root.\u0026#34; 75 echo \u0026#34;For other operating systems you can get rid of the warning with manually created .env file:\u0026#34; 76 echo \u0026#34; See: https://airflow.apache.org/docs/apache-airflow/stable/start/docker.html#setting-the-right-airflow-user\u0026#34; 77 echo 78 fi 79 mkdir -p /sources/logs /sources/dags /sources/plugins 80 chown -R \u0026#34;${AIRFLOW_UID}:0\u0026#34; /sources/{logs,dags,plugins} 81 exec /entrypoint airflow version 82 # yamllint enable rule:line-length 83 environment: 84 \u0026lt;\u0026lt;: *airflow-common-env 85 _AIRFLOW_DB_UPGRADE: \u0026#34;true\u0026#34; 86 _AIRFLOW_WWW_USER_CREATE: \u0026#34;true\u0026#34; 87 _AIRFLOW_WWW_USER_USERNAME: ${_AIRFLOW_WWW_USER_USERNAME:-airflow} 88 _AIRFLOW_WWW_USER_PASSWORD: ${_AIRFLOW_WWW_USER_PASSWORD:-airflow} 89 _PIP_ADDITIONAL_REQUIREMENTS: \u0026#34;\u0026#34; 90 user: \u0026#34;0:0\u0026#34; 91 volumes: 92 - .:/sources 93 94volumes: 95 postgres-db-volume: The quick start guide requires a number of steps to initialise an environment before starting the services and they are added to a single shell script shown below.\n1# airflow/init.sh 2#!/usr/bin/env bash 3 4## initialising environment 5# remove docker-compose services 6docker-compose down --volumes 7# create folders to mount 8rm -rf ./logs 9mkdir -p ./dags ./logs ./plugins ./tests 10# setting the right airflow user 11echo -e \u0026#34;AIRFLOW_UID=$(id -u)\u0026#34; \u0026gt; .env 12# initialise database 13docker-compose up airflow-init After finishing the initialisation steps, the docker compose services can be started by docker-compose up -d.\nLambda Invoke Function Operator Two tasks are created with the Lambda invoke function operator. The first is invoked synchronously (RequestResponse) while the latter is asynchronously (Event). Both are configured to raise an error after 10 seconds.\n1# airflow/dags/example_without_logging.py 2import os 3import json 4from datetime import datetime 5 6from airflow import DAG 7from airflow.providers.amazon.aws.operators.aws_lambda import AwsLambdaInvokeFunctionOperator 8 9LAMBDA_FUNCTION_NAME = os.getenv(\u0026#34;LAMBDA_FUNCTION_NAME\u0026#34;, \u0026#34;example-lambda-function\u0026#34;) 10 11 12def _set_payload(n: int = 10, to_fail: bool = True): 13 return json.dumps({\u0026#34;n\u0026#34;: n, \u0026#34;to_fail\u0026#34;: to_fail}) 14 15 16with DAG( 17 dag_id=\u0026#34;example_without_logging\u0026#34;, 18 schedule_interval=None, 19 start_date=datetime(2022, 1, 1), 20 max_active_runs=2, 21 concurrency=2, 22 tags=[\u0026#34;logging\u0026#34;], 23 catchup=False, 24) as dag: 25 [ 26 AwsLambdaInvokeFunctionOperator( 27 task_id=\u0026#34;sync_w_error\u0026#34;, 28 function_name=LAMBDA_FUNCTION_NAME, 29 invocation_type=\u0026#34;RequestResponse\u0026#34;, 30 payload=_set_payload(), 31 aws_conn_id=None, 32 ), 33 AwsLambdaInvokeFunctionOperator( 34 task_id=\u0026#34;async_w_error\u0026#34;, 35 function_name=LAMBDA_FUNCTION_NAME, 36 invocation_type=\u0026#34;Event\u0026#34;, 37 payload=_set_payload(), 38 aws_conn_id=None, 39 ), 40 ] As shown below the task by asynchronous invocation is incorrectly marked as success. It is because practically only the response status code is checked as it doesn\u0026rsquo;t wait until the invocation finishes. On the other hand, the task by synchronous invocation is indicated as failed. However, it doesn’t show the exact error that fails the invocation - see below for further details.\nThe error message is Lambda function execution resulted in error, and it is the generic message constructed by the Lambda invoke function operator.\nCustom Lambda Operator Five tasks are created with the custom Lambda operator The first four tasks cover success and failure by synchronous and asynchronous invocations. The last task is to check failure due to timeout.\n1# airflow/dags/example_with_logging.py 2import os 3import json 4from datetime import datetime 5 6from airflow import DAG 7from lambda_operator import CustomLambdaFunctionOperator 8 9LAMBDA_FUNCTION_NAME = os.getenv(\u0026#34;LAMBDA_FUNCTION_NAME\u0026#34;, \u0026#34;example-lambda-function\u0026#34;) 10 11 12def _set_payload(n: int = 10, to_fail: bool = True): 13 return json.dumps({\u0026#34;n\u0026#34;: n, \u0026#34;to_fail\u0026#34;: to_fail}) 14 15 16with DAG( 17 dag_id=\u0026#34;example_with_logging\u0026#34;, 18 schedule_interval=None, 19 start_date=datetime(2022, 1, 1), 20 max_active_runs=2, 21 concurrency=5, 22 tags=[\u0026#34;logging\u0026#34;], 23 catchup=False, 24) as dag: 25 [ 26 CustomLambdaFunctionOperator( 27 task_id=\u0026#34;sync_w_error\u0026#34;, 28 function_name=LAMBDA_FUNCTION_NAME, 29 invocation_type=\u0026#34;RequestResponse\u0026#34;, 30 payload=_set_payload(), 31 aws_conn_id=None, 32 ), 33 CustomLambdaFunctionOperator( 34 task_id=\u0026#34;async_w_error\u0026#34;, 35 function_name=LAMBDA_FUNCTION_NAME, 36 invocation_type=\u0026#34;Event\u0026#34;, 37 payload=_set_payload(), 38 aws_conn_id=None, 39 ), 40 CustomLambdaFunctionOperator( 41 task_id=\u0026#34;sync_wo_error\u0026#34;, 42 function_name=LAMBDA_FUNCTION_NAME, 43 invocation_type=\u0026#34;RequestResponse\u0026#34;, 44 payload=_set_payload(to_fail=False), 45 aws_conn_id=None, 46 ), 47 CustomLambdaFunctionOperator( 48 task_id=\u0026#34;async_wo_error\u0026#34;, 49 function_name=LAMBDA_FUNCTION_NAME, 50 invocation_type=\u0026#34;Event\u0026#34;, 51 payload=_set_payload(to_fail=False), 52 aws_conn_id=None, 53 ), 54 CustomLambdaFunctionOperator( 55 task_id=\u0026#34;async_timeout_error\u0026#34;, 56 function_name=LAMBDA_FUNCTION_NAME, 57 invocation_type=\u0026#34;Event\u0026#34;, 58 payload=_set_payload(n=40, to_fail=False), 59 aws_conn_id=None, 60 ), 61 ] As expected we see two success tasks and three failure tasks. The custom Lambda operator tracks Lambda function invocation status correctly.\nBelow shows log messages of the success task by asynchronous invocation. Each message includes the same correlation ID and the last message from the Lambda function is Function ended.\nThe failed task by asynchronous invocation also shows all log messages, and it is possible to check what caused the invocation to fail.\nThe case of failure due to timeout doesn’t show an error message from the Lambda invocation. However, we can treat it as failure because we don’t see the message of the function invocation ended within the function timeout.\nStill the failure by synchronous invocation doesn’t show the exact error message, and it is because an error is raised before the process log events function is executed. Because of this, I advise to invoke a Lambda function asynchronously.\nSummary In this post, we discussed limitations of the Lambda invoke function operator and created a custom Lambda operator. The custom operator reports the invocation result of a function correctly and records the exact error message from failure. A number of tasks are created to compare the results between the two operators, and it is shown that the custom operator handles those limitations successfully.\n","date":"August 6, 2022","img":"/blog/2022-08-06-revisit-lambda-operator/featured.png","lang":"en","langName":"English","largeImg":"/blog/2022-08-06-revisit-lambda-operator/featured_hudaeb25d8660dc3c0c83176a3034f7899_24814_500x0_resize_box_3.png","permalink":"/blog/2022-08-06-revisit-lambda-operator/","series":[],"smallImg":"/blog/2022-08-06-revisit-lambda-operator/featured_hudaeb25d8660dc3c0c83176a3034f7899_24814_180x0_resize_box_3.png","tags":[{"title":"AWS","url":"/tags/aws/"},{"title":"AWS Lambda","url":"/tags/aws-lambda/"},{"title":"Apache Airflow","url":"/tags/apache-airflow/"},{"title":"Python","url":"/tags/python/"},{"title":"Docker","url":"/tags/docker/"},{"title":"Docker Compose","url":"/tags/docker-compose/"}],"timestamp":1659744000,"title":"Revisit AWS Lambda Invoke Function Operator of Apache Airflow"},{"categories":[{"title":"Data Engineering","url":"/categories/data-engineering/"}],"content":"AWS Lambda provides serverless computing capabilities, and it can be used for performing validation or light processing/transformation of data. Moreover, with its integration with more than 140 AWS services, it facilitates building complex systems employing event-driven architectures. There are many ways to build serverless applications and one of the most efficient ways is using specialised frameworks such as the AWS Serverless Application Model (SAM) and Serverless Framework. In this post, I’ll demonstrate how to build a serverless data processing application using SAM.\nArchitecture When we create an application or pipeline with AWS Lambda, most likely we’ll include its event triggers and destinations. The AWS Serverless Application Model (SAM) facilitates building serverless applications by providing shorthand syntax with a number of custom resource types. Also, the AWS SAM CLI supports an execution environment that helps build, test, debug and deploy applications easily. Furthermore, the CLI can be integrated with full-pledged IaC tools such as the AWS Cloud Development Kit (CDK) and Terraform - note integration with the latter is in its roadmap. With the integration, serverless application development can be a lot easier with capabilities of local testing and building. An alternative tool is the Serverless Framework. It supports multiple cloud providers and broader event sources out-of-box but its integration with IaC tools is practically non-existent.\nIn this post, we’ll build a simple data pipeline using SAM where a Lambda function is triggered when an object (CSV file) is created in a S3 bucket. The Lambda function converts the object into parquet and AVRO files and saves to a destination S3 bucket. For simplicity, we’ll use a single bucket for the source and destination.\nSAM Application After installing the SAM CLI, I initialised an app with the Python 3.8 Lambda runtime from the hello world template (sam init --runtime python3.8). Then it is modified for the data pipeline app. The application is defined in the template.yaml and the source of the main Lambda function is placed in the _transform _folder. We need 3rd party packages for converting source files into the parquet and AVRO formats - AWS Data Wrangler and fastavro. Instead of packaging them together with the Lambda function, they are made available as Lambda layers. While using the AWS managed Lambda layer for the former, we only need to build the Lambda layer for the _fastavro _package, and it is located in the _fastavro _folder. The source of the app can be found in the GitHub repository of this post.\n1fastavro 2└── requirements.txt 3transform 4├── __init__.py 5├── app.py 6└── requirements.txt 7tests 8├── __init__.py 9└── unit 10 ├── __init__.py 11 └── test_handler.py 12template.yaml 13requirements-dev.txt 14test.csv In the resources section of the template, the Lambda layer for AVRO transformation (FastAvro), the main Lambda function (TransformFunction) and the source (and destination) S3 bucket (SourceBucket) are added. The layer can be built simply by adding the pip package name to the requirements.txt file. It is set to be compatible with Python 3.7 to 3.9. For the Lambda function, its source is configured to be built from the _transform _folder and the ARNs of the custom and AWS managed Lambda layers are added to the layers property. Also, an S3 bucket event is configured so that this Lambda function is triggered whenever a new object is created to the bucket. Finally, as it needs to have permission to read and write objects to the S3 bucket, its invocation policies are added from ready-made policy templates - _S3ReadPolicy _and S3WritePolicy.\n1# template.yaml 2AWSTemplateFormatVersion: \u0026#34;2010-09-09\u0026#34; 3Transform: AWS::Serverless-2016-10-31 4Description: \u0026gt; 5 sam-for-data-professionals 6 7 Sample SAM Template for sam-for-data-professionals 8 9Globals: 10 Function: 11 MemorySize: 256 12 Timeout: 20 13 14Resources: 15 FastAvro: 16 Type: AWS::Serverless::LayerVersion 17 Properties: 18 LayerName: fastavro-layer-py3 19 ContentUri: fastavro/ 20 CompatibleRuntimes: 21 - python3.7 22 - python3.8 23 - python3.9 24 Metadata: 25 BuildMethod: python3.8 26 TransformFunction: 27 Type: AWS::Serverless::Function 28 Properties: 29 CodeUri: transform/ 30 Handler: app.lambda_handler 31 Runtime: python3.8 32 Layers: 33 - !Ref FastAvro 34 - arn:aws:lambda:ap-southeast-2:336392948345:layer:AWSDataWrangler-Python38:8 35 Policies: 36 - S3ReadPolicy: 37 BucketName: sam-for-data-professionals-cevo 38 - S3WritePolicy: 39 BucketName: sam-for-data-professionals-cevo 40 Events: 41 BucketEvent: 42 Type: S3 43 Properties: 44 Bucket: !Ref SourceBucket 45 Events: 46 - \u0026#34;s3:ObjectCreated:*\u0026#34; 47 SourceBucket: 48 Type: AWS::S3::Bucket 49 Properties: 50 BucketName: sam-for-data-professionals-cevo 51 52Outputs: 53 FastAvro: 54 Description: \u0026#34;ARN of fastavro-layer-py3\u0026#34; 55 Value: !Ref FastAvro 56 TransformFunction: 57 Description: \u0026#34;Transform Lambda Function ARN\u0026#34; 58 Value: !GetAtt TransformFunction.Arn Lambda Function The transform function reads an input file from the S3 bucket and saves the records as the parquet and AVRO formats. Thanks to the Lambda layers, we can access the necessary 3rd party packages as well as reduce the size of uploaded deployment packages and make it faster to deploy it.\n1# transform/app.py 2import re 3import io 4from fastavro import writer, parse_schema 5import awswrangler as wr 6import pandas as pd 7import boto3 8 9s3 = boto3.client(\u0026#34;s3\u0026#34;) 10 11avro_schema = { 12 \u0026#34;doc\u0026#34;: \u0026#34;User details\u0026#34;, 13 \u0026#34;name\u0026#34;: \u0026#34;User\u0026#34;, 14 \u0026#34;namespace\u0026#34;: \u0026#34;user\u0026#34;, 15 \u0026#34;type\u0026#34;: \u0026#34;record\u0026#34;, 16 \u0026#34;fields\u0026#34;: [{\u0026#34;name\u0026#34;: \u0026#34;name\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;}, {\u0026#34;name\u0026#34;: \u0026#34;age\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;int\u0026#34;}], 17} 18 19 20def check_fields(df: pd.DataFrame, schema: dict): 21 if schema.get(\u0026#34;fields\u0026#34;) is None: 22 raise Exception(\u0026#34;missing fields in schema keys\u0026#34;) 23 if len(set(df.columns) - set([f[\u0026#34;name\u0026#34;] for f in schema[\u0026#34;fields\u0026#34;]])) \u0026gt; 0: 24 raise Exception(\u0026#34;missing columns in schema key of fields\u0026#34;) 25 26 27def check_data_types(df: pd.DataFrame, schema: dict): 28 dtypes = df.dtypes.to_dict() 29 for field in schema[\u0026#34;fields\u0026#34;]: 30 match_type = \u0026#34;object\u0026#34; if field[\u0026#34;type\u0026#34;] == \u0026#34;string\u0026#34; else field[\u0026#34;type\u0026#34;] 31 if re.search(match_type, str(dtypes[field[\u0026#34;name\u0026#34;]])) is None: 32 raise Exception(f\u0026#34;incorrect column type - {field[\u0026#39;name\u0026#39;]}\u0026#34;) 33 34 35def generate_avro_file(df: pd.DataFrame, schema: dict): 36 check_fields(df, schema) 37 check_data_types(df, schema) 38 buffer = io.BytesIO() 39 writer(buffer, parse_schema(schema), df.to_dict(\u0026#34;records\u0026#34;)) 40 buffer.seek(0) 41 return buffer 42 43 44def lambda_handler(event, context): 45 # get bucket and key values 46 record = next(iter(event[\u0026#34;Records\u0026#34;])) 47 bucket = record[\u0026#34;s3\u0026#34;][\u0026#34;bucket\u0026#34;][\u0026#34;name\u0026#34;] 48 key = record[\u0026#34;s3\u0026#34;][\u0026#34;object\u0026#34;][\u0026#34;key\u0026#34;] 49 file_name = re.sub(\u0026#34;.csv$\u0026#34;, \u0026#34;\u0026#34;, key.split(\u0026#34;/\u0026#34;)[-1]) 50 # read input csv as a data frame 51 input_path = f\u0026#34;s3://{bucket}/{key}\u0026#34; 52 input_df = wr.s3.read_csv([input_path]) 53 # write to s3 as a parquet file 54 wr.s3.to_parquet(df=input_df, path=f\u0026#34;s3://{bucket}/output/{file_name}.parquet\u0026#34;) 55 # write to s3 as an avro file 56 s3.upload_fileobj(generate_avro_file(input_df, avro_schema), bucket, f\u0026#34;output/{file_name}.avro\u0026#34;) Unit Testing We use a custom function to create AVRO files (generate_avro_file) while relying on the AWS Data Wrangler package for reading input files and writing to parquet files. Therefore, unit testing is performed for the custom function only. Mainly it tests whether the AVRO schema matches the input data fields and data types.\n1# tests/unit/test_handler.py 2import pytest 3import pandas as pd 4from transform import app 5 6 7@pytest.fixture 8def input_df(): 9 return pd.DataFrame.from_dict({\u0026#34;name\u0026#34;: [\u0026#34;Vrinda\u0026#34;, \u0026#34;Tracy\u0026#34;], \u0026#34;age\u0026#34;: [22, 28]}) 10 11 12def test_generate_avro_file_success(input_df): 13 avro_schema = { 14 \u0026#34;doc\u0026#34;: \u0026#34;User details\u0026#34;, 15 \u0026#34;name\u0026#34;: \u0026#34;User\u0026#34;, 16 \u0026#34;namespace\u0026#34;: \u0026#34;user\u0026#34;, 17 \u0026#34;type\u0026#34;: \u0026#34;record\u0026#34;, 18 \u0026#34;fields\u0026#34;: [{\u0026#34;name\u0026#34;: \u0026#34;name\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;}, {\u0026#34;name\u0026#34;: \u0026#34;age\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;int\u0026#34;}], 19 } 20 app.generate_avro_file(input_df, avro_schema) 21 assert True 22 23 24def test_generate_avro_file_fail_missing_fields(input_df): 25 avro_schema = { 26 \u0026#34;doc\u0026#34;: \u0026#34;User details\u0026#34;, 27 \u0026#34;name\u0026#34;: \u0026#34;User\u0026#34;, 28 \u0026#34;namespace\u0026#34;: \u0026#34;user\u0026#34;, 29 \u0026#34;type\u0026#34;: \u0026#34;record\u0026#34;, 30 } 31 with pytest.raises(Exception) as e: 32 app.generate_avro_file(input_df, avro_schema) 33 assert \u0026#34;missing fields in schema keys\u0026#34; == str(e.value) 34 35 36def test_generate_avro_file_fail_missing_columns(input_df): 37 avro_schema = { 38 \u0026#34;doc\u0026#34;: \u0026#34;User details\u0026#34;, 39 \u0026#34;name\u0026#34;: \u0026#34;User\u0026#34;, 40 \u0026#34;namespace\u0026#34;: \u0026#34;user\u0026#34;, 41 \u0026#34;type\u0026#34;: \u0026#34;record\u0026#34;, 42 \u0026#34;fields\u0026#34;: [{\u0026#34;name\u0026#34;: \u0026#34;name\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;}], 43 } 44 with pytest.raises(Exception) as e: 45 app.generate_avro_file(input_df, avro_schema) 46 assert \u0026#34;missing columns in schema key of fields\u0026#34; == str(e.value) 47 48 49def test_generate_avro_file_fail_incorrect_age_type(input_df): 50 avro_schema = { 51 \u0026#34;doc\u0026#34;: \u0026#34;User details\u0026#34;, 52 \u0026#34;name\u0026#34;: \u0026#34;User\u0026#34;, 53 \u0026#34;namespace\u0026#34;: \u0026#34;user\u0026#34;, 54 \u0026#34;type\u0026#34;: \u0026#34;record\u0026#34;, 55 \u0026#34;fields\u0026#34;: [{\u0026#34;name\u0026#34;: \u0026#34;name\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;}, {\u0026#34;name\u0026#34;: \u0026#34;age\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;}], 56 } 57 with pytest.raises(Exception) as e: 58 app.generate_avro_file(input_df, avro_schema) 59 assert f\u0026#34;incorrect column type - age\u0026#34; == str(e.value) Build and Deploy The app has to be built before deployment. It can be done by sam build.\nThe deployment can be done with and without a guide. For the latter, we need to specify additional parameters such as the Cloudformation stack name, capabilities (as we create an IAM role for Lambda) and a flag to automatically determine an S3 bucket to store build artifacts.\n1sam deploy \\ 2 --stack-name sam-for-data-professionals \\ 3 --capabilities CAPABILITY_IAM \\ 4 --resolve-s3 Trigger Lambda Function We can simply trigger the Lambda function by uploading a source file to the S3 bucket. Once it is uploaded, we are able to see that the output parquet and AVRO files are saved as expected.\n1$ aws s3 cp test.csv s3://sam-for-data-professionals-cevo/input/ 2upload: ./test.csv to s3://sam-for-data-professionals-cevo/input/test.csv 3 4$ aws s3 ls s3://sam-for-data-professionals-cevo/output/ 52022-07-17 17:33:21 403 test.avro 62022-07-17 17:33:21 2112 test.parquet Summary In this post, it is illustrated how to build a serverless data processing application using SAM. A Lambda function is developed, which is triggered whenever an object is created in a S3 bucket. It converts input CSV files into the parquet and AVRO formats before saving into the destination bucket. For the format conversion, it uses 3rd party packages, and they are made available by Lambda layers. The application is built and deployed and the function triggering is checked.\n","date":"July 18, 2022","img":"/blog/2022-07-18-sam-for-data-professionals/featured.png","lang":"en","langName":"English","largeImg":"/blog/2022-07-18-sam-for-data-professionals/featured_hu896f95b0db9041cf59826714af2f73f7_22838_500x0_resize_box_3.png","permalink":"/blog/2022-07-18-sam-for-data-professionals/","series":[],"smallImg":"/blog/2022-07-18-sam-for-data-professionals/featured_hu896f95b0db9041cf59826714af2f73f7_22838_180x0_resize_box_3.png","tags":[{"title":"AWS","url":"/tags/aws/"},{"title":"AWS Lambda","url":"/tags/aws-lambda/"},{"title":"AWS SAM","url":"/tags/aws-sam/"}],"timestamp":1658102400,"title":"Serverless Application Model (SAM) for Data Professionals"},{"categories":[],"content":"Since v1.0.0-alpha.1, HBS supports much more image processing methods. Such as Crop, Fit and Fill images. You can also apply filters on an image.\nSee also Image Processing.\n","date":"July 8, 2022","img":"/news/2022/07/more-image-processing-methods/featured-sample.webp","lang":"en","langName":"English","largeImg":"/news/2022/07/more-image-processing-methods/featured-sample_huc6bcc14d597e300fd9ab4aae536c68a5_498412_500x0_resize_q75_h2_box_2.webp","permalink":"/news/2022/07/more-image-processing-methods/","series":[],"smallImg":"/news/2022/07/more-image-processing-methods/featured-sample_huc6bcc14d597e300fd9ab4aae536c68a5_498412_180x0_resize_q75_h2_box_2.webp","tags":[],"timestamp":1657251287,"title":"More Image Processing Methods"},{"categories":[{"title":"Data Engineering","url":"/categories/data-engineering/"}],"content":"Unlike traditional Data Lake, new table formats (Iceberg, Hudi and Delta Lake) support features that can be used to apply data warehousing patterns, which can bring a way to be rescued from Data Swamp. In this post, we\u0026rsquo;ll discuss how to implement ETL using retail analytics data. It has two dimension data (user and product) and a single fact data (order). The dimension data sets have different ETL strategies depending on whether to track historical changes. For the fact data, the primary keys of the dimension data are added to facilitate later queries. We\u0026rsquo;ll use Iceberg for data storage/management and Spark for data processing. Instead of provisioning an EMR cluster, a local development environment will be used. Finally, the ETL results will be queried by Athena for verification.\nEMR Local Environment In one of my earlier posts, we discussed how to develop and test Apache Spark apps for EMR locally using Docker (and/or VSCode). Instead of provisioning an EMR cluster, we can quickly build an ETL app using the local environment. For this post, a new local environment is created based on the Docker image of the latest EMR 6.6.0 release. Check the GitHub repository for this post for further details.\nApache Iceberg is supported by EMR 6.5.0 or later, and it requires iceberg-defaults configuration classification that enables Iceberg. The latest EMR Docker release (emr-6.6.0-20220411), however, doesn\u0026rsquo;t support that configuration classification and I didn\u0026rsquo;t find the _iceberg _folder (/usr/share/aws/iceberg) within the Docker container. Therefore, the project\u0026rsquo;s AWS integration example is used instead and the following script (run.sh) is an update of the example script that allows to launch the Pyspark shell or to submit a Spark application.\n1# run.sh 2#!/usr/bin/env bash 3 4# add Iceberg dependency 5ICEBERG_VERSION=0.13.2 6DEPENDENCIES=\u0026#34;org.apache.iceberg:iceberg-spark3-runtime:$ICEBERG_VERSION\u0026#34; 7 8# add AWS dependency 9AWS_SDK_VERSION=2.17.131 10AWS_MAVEN_GROUP=software.amazon.awssdk 11AWS_PACKAGES=( 12 \u0026#34;bundle\u0026#34; 13 \u0026#34;url-connection-client\u0026#34; 14) 15for pkg in \u0026#34;${AWS_PACKAGES[@]}\u0026#34;; do 16 DEPENDENCIES+=\u0026#34;,$AWS_MAVEN_GROUP:$pkg:$AWS_SDK_VERSION\u0026#34; 17done 18 19# execute pyspark or spark-submit 20execution=$1 21app_path=$2 22if [ -z $execution ]; then 23 echo \u0026#34;missing execution type. specify either pyspark or spark-submit\u0026#34; 24 exit 1 25fi 26 27if [ $execution == \u0026#34;pyspark\u0026#34; ]; then 28 pyspark --packages $DEPENDENCIES \\ 29 --conf spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions \\ 30 --conf spark.sql.catalog.demo=org.apache.iceberg.spark.SparkCatalog \\ 31 --conf spark.sql.catalog.demo.warehouse=s3://iceberg-etl-demo \\ 32 --conf spark.sql.catalog.demo.catalog-impl=org.apache.iceberg.aws.glue.GlueCatalog \\ 33 --conf spark.sql.catalog.demo.io-impl=org.apache.iceberg.aws.s3.S3FileIO 34elif [ $execution == \u0026#34;spark-submit\u0026#34; ]; then 35 if [ -z $app_path ]; then 36 echo \u0026#34;pyspark application is mandatory\u0026#34; 37 exit 1 38 else 39 spark-submit --packages $DEPENDENCIES \\ 40 --deploy-mode client \\ 41 --master local[*] \\ 42 --conf spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions \\ 43 --conf spark.sql.catalog.demo=org.apache.iceberg.spark.SparkCatalog \\ 44 --conf spark.sql.catalog.demo.warehouse=s3://iceberg-etl-demo \\ 45 --conf spark.sql.catalog.demo.catalog-impl=org.apache.iceberg.aws.glue.GlueCatalog \\ 46 --conf spark.sql.catalog.demo.io-impl=org.apache.iceberg.aws.s3.S3FileIO \\ 47 $app_path 48 fi 49fi Here is an example of using the script.\n1# launch pyspark shell 2$ ./run.sh pyspark 3 4# execute spark-submit, requires pyspark application (etl.py) as the second argument 5$ ./run.sh spark-submit path-to-app.py ETL Strategy Sample Data We use the retail analytics sample database from YugaByteDB to get the ETL sample data. Records from the following 3 tables are used to run ETL on its own ETL strategy.\nThe main focus of the demo ETL application is to show how to track product price changes over time and to apply those changes to the order data. Normally ETL is performed daily, but it\u0026rsquo;ll be time-consuming to execute daily incremental ETL with the order data because it includes records spanning for 5 calendar years. Moreover, as it is related to the user and product data, splitting the corresponding dimension records will be quite difficult. Instead, I chose to run yearly incremental ETL. I first grouped orders in 4 groups where the first group (year 0) includes orders in 2016 and 2017. And each of the remaining groups (year 1 to 3) keeps records of a whole year from 2018 to 2020. Then I created 4 product groups in order to match the order groups and to execute incremental ETL together with the order data. The first group (year 0) keeps the original data and the product price is set to be increased by 5% in the following years until the last group (year 3). Note, with this setup, it is expected that orders for a given product tend to be mapped to a higher product price over time. On the other hand, the ETL strategy of the user data is not to track historical data so that it is used as it is. The sample data files used for the ETL app are listed below, and they can be found in the data folder of the GitHub repository.\n1$ tree data 2data 3├── orders_year_0.csv 4├── orders_year_1.csv 5├── orders_year_2.csv 6├── orders_year_3.csv 7├── products_year_0.csv 8├── products_year_1.csv 9├── products_year_2.csv 10├── products_year_3.csv 11└── users.csv Users Slowly changing dimension (SCD) type 1 is implemented for the user data. This method basically _upsert_s records by comparing the primary key values and therefore doesn\u0026rsquo;t track historical data. The data has the natural key of _id _and its md5 hash is used as the surrogate key named user_sk - this column is used as the primary key of the table. The table is configured to be partitioned by its surrogate key in 20 buckets. The table creation statement can be found below.\n1CREATE TABLE demo.dwh.users ( 2\tuser_sk string, 3\tid bigint, 4\tname string, 5\temail string, 6\taddress string, 7\tcity string, 8\tstate string, 9\tzip string, 10\tbirth_date date, 11\tsource string, 12\tcreated_at timestamp) 13USING iceberg 14PARTITIONED BY (bucket(20, user_sk)) Products Slowly changing dimension (SCD) type 2 is taken for product data. This method tracks historical data by adding multiple records for a given natural key. Same as the user data, the id column is the natural key. Each record for the same natural key will be given a different surrogate key and the md5 hash of a combination of the id and _created_at _columns is used as the surrogate key named prod_sk. Each record has its own effect period and it is determined by the eff_from and _eff_to _columns and the latest record is marked as 1 for its curr_flag value. The table is also configured to be partitioned by its surrogate key in 20 buckets. The table creation statement is shown below.\n1CREATE TABLE demo.dwh.products ( 2\tprod_sk string, 3\tid bigint, 4\tcategory string, 5\tprice decimal(6,3), 6\ttitle string, 7\tvendor string, 8\tcurr_flag int, 9\teff_from timestamp, 10\teff_to timestamp, 11\tcreated_at timestamp) 12USING iceberg 13PARTITIONED BY (bucket(20, prod_sk)) Orders The orders table has a composite primary key of the surrogate keys of the dimension tables - users_sk and prod_sk. Those columns don\u0026rsquo;t exist in the source data and are added during transformation. The table is configured to be partitioned by the date part of the created_at column. The table creation statement can be found below.\n1CREATE TABLE demo.dwh.orders ( 2\tuser_sk string, 3\tprod_sk string, 4\tid bigint, 5\tdiscount decimal(4,2), 6\tquantity integer, 7\tcreated_at timestamp) 8USING iceberg 9PARTITIONED BY (days(created_at)) ETL Implementation Users In the transformation phase, a source dataframe is created by creating the surrogate key (user_sk), changing data types of relevant columns and selecting columns in the same order as the table is created. Then a view (users_tbl) is created from the source dataframe and it is used to execute MERGE operation by comparing the surrogate key values of the source view with those of the target users table.\n1# src.py 2def etl_users(file_path: str, spark_session: SparkSession): 3 print(\u0026#34;users - transform records...\u0026#34;) 4 src_df = ( 5 spark_session.read.option(\u0026#34;header\u0026#34;, \u0026#34;true\u0026#34;) 6 .csv(file_path) 7 .withColumn(\u0026#34;user_sk\u0026#34;, md5(\u0026#34;id\u0026#34;)) 8 .withColumn(\u0026#34;id\u0026#34;, expr(\u0026#34;CAST(id AS bigint)\u0026#34;)) 9 .withColumn(\u0026#34;created_at\u0026#34;, to_timestamp(\u0026#34;created_at\u0026#34;)) 10 .withColumn(\u0026#34;birth_date\u0026#34;, to_date(\u0026#34;birth_date\u0026#34;)) 11 .select( 12 \u0026#34;user_sk\u0026#34;, 13 \u0026#34;id\u0026#34;, 14 \u0026#34;name\u0026#34;, 15 \u0026#34;email\u0026#34;, 16 \u0026#34;address\u0026#34;, 17 \u0026#34;city\u0026#34;, 18 \u0026#34;state\u0026#34;, 19 \u0026#34;zip\u0026#34;, 20 \u0026#34;birth_date\u0026#34;, 21 \u0026#34;source\u0026#34;, 22 \u0026#34;created_at\u0026#34;, 23 ) 24 ) 25 print(\u0026#34;users - upsert records...\u0026#34;) 26 src_df.createOrReplaceTempView(\u0026#34;users_tbl\u0026#34;) 27 spark_session.sql( 28 \u0026#34;\u0026#34;\u0026#34; 29 MERGE INTO demo.dwh.users t 30 USING (SELECT * FROM users_tbl ORDER BY user_sk) s 31 ON s.user_sk = t.user_sk 32 WHEN MATCHED THEN UPDATE SET * 33 WHEN NOT MATCHED THEN INSERT * 34 \u0026#34;\u0026#34;\u0026#34; 35 ) Products The source dataframe is created by adding the surrogate key while concatenating the id and _created_at _columns, followed by changing data types of relevant columns and selecting columns in the same order as the table is created. The view (products_tbl) that is created from the source dataframe is used to query all the records that have the product ids in the source table - see products_to_update. Note we need data from the products table in order to update eff_from, _eff_to _and _current_flag _column values. Then eff_lead is added to the result set, which is the next record\u0026rsquo;s created_at value for a given product id - see products_updated. The final result set is created by determining the _curr_flag _and _eff_to _column value. Note that the _eff_to _value of the last record for a product is set to ‘9999-12-31 00:00:00\u0026rsquo; in order to make it easy to query the relevant records. The updated records are updated/inserted by executing MERGE operation by comparing the surrogate key values to those of the target products table\n1# src.py 2def etl_products(file_path: str, spark_session: SparkSession): 3 print(\u0026#34;products - transform records...\u0026#34;) 4 src_df = ( 5 spark_session.read.option(\u0026#34;header\u0026#34;, \u0026#34;true\u0026#34;) 6 .csv(file_path) 7 .withColumn(\u0026#34;prod_sk\u0026#34;, md5(concat(\u0026#34;id\u0026#34;, \u0026#34;created_at\u0026#34;))) 8 .withColumn(\u0026#34;id\u0026#34;, expr(\u0026#34;CAST(id AS bigint)\u0026#34;)) 9 .withColumn(\u0026#34;price\u0026#34;, expr(\u0026#34;CAST(price AS decimal(6,3))\u0026#34;)) 10 .withColumn(\u0026#34;created_at\u0026#34;, to_timestamp(\u0026#34;created_at\u0026#34;)) 11 .withColumn(\u0026#34;curr_flag\u0026#34;, expr(\u0026#34;CAST(NULL AS int)\u0026#34;)) 12 .withColumn(\u0026#34;eff_from\u0026#34;, col(\u0026#34;created_at\u0026#34;)) 13 .withColumn(\u0026#34;eff_to\u0026#34;, expr(\u0026#34;CAST(NULL AS timestamp)\u0026#34;)) 14 .select( 15 \u0026#34;prod_sk\u0026#34;, 16 \u0026#34;id\u0026#34;, 17 \u0026#34;category\u0026#34;, 18 \u0026#34;price\u0026#34;, 19 \u0026#34;title\u0026#34;, 20 \u0026#34;vendor\u0026#34;, 21 \u0026#34;curr_flag\u0026#34;, 22 \u0026#34;eff_from\u0026#34;, 23 \u0026#34;eff_to\u0026#34;, 24 \u0026#34;created_at\u0026#34;, 25 ) 26 ) 27 print(\u0026#34;products - upsert records...\u0026#34;) 28 src_df.createOrReplaceTempView(\u0026#34;products_tbl\u0026#34;) 29 products_update_qry = \u0026#34;\u0026#34;\u0026#34; 30 WITH products_to_update AS ( 31 SELECT l.* 32 FROM demo.dwh.products AS l 33 JOIN products_tbl AS r ON l.id = r.id 34 UNION 35 SELECT * 36 FROM products_tbl 37 ), products_updated AS ( 38 SELECT *, 39 LEAD(created_at) OVER (PARTITION BY id ORDER BY created_at) AS eff_lead 40 FROM products_to_update 41 ) 42 SELECT prod_sk, 43 id, 44 category, 45 price, 46 title, 47 vendor, 48 (CASE WHEN eff_lead IS NULL THEN 1 ELSE 0 END) AS curr_flag, 49 eff_from, 50 COALESCE(eff_lead, to_timestamp(\u0026#39;9999-12-31 00:00:00\u0026#39;)) AS eff_to, 51 created_at 52 FROM products_updated 53 ORDER BY prod_sk 54 \u0026#34;\u0026#34;\u0026#34; 55 spark_session.sql( 56 f\u0026#34;\u0026#34;\u0026#34; 57 MERGE INTO demo.dwh.products t 58 USING ({products_update_qry}) s 59 ON s.prod_sk = t.prod_sk 60 WHEN MATCHED THEN UPDATE SET * 61 WHEN NOT MATCHED THEN INSERT * 62 \u0026#34;\u0026#34;\u0026#34; 63 ) Orders After transformation, a view (orders_tbl) is created from the source dataframe. The relevant user (user_sk) and product (prod_sk) surrogate keys are added to source data by joining the users and products dimension tables. The users table is SCD type 1 so matching the _user_id _alone is enough for the join condition. On the other hand, additional join condition based on the _eff_from _and _eff_to _columns is necessary for the products table as it is SCD type 2 and records in that table have their own effective periods. Note that ideally we should be able to apply INNER JOIN but the sample data is not clean and some product records are not matched by that operation. For example, an order whose id is 15 is made at 2018-06-26 02:24:38 with a product whose id is 116. However the earliest record of that product is created at 2018-09-12 15:23:05 and it\u0026rsquo;ll be missed by INNER JOIN. Therefore LEFT JOIN is applied to create the initial result set (orders_updated) and, for those products that are not matched, the surrogate keys of the earliest records are added instead. Finally the updated order records are appended using the DataFrameWriterV2 API.\n1# src.py 2def etl_orders(file_path: str, spark_session: SparkSession): 3 print(\u0026#34;orders - transform records...\u0026#34;) 4 src_df = ( 5 spark_session.read.option(\u0026#34;header\u0026#34;, \u0026#34;true\u0026#34;) 6 .csv(file_path) 7 .withColumn(\u0026#34;id\u0026#34;, expr(\u0026#34;CAST(id AS bigint)\u0026#34;)) 8 .withColumn(\u0026#34;user_id\u0026#34;, expr(\u0026#34;CAST(user_id AS bigint)\u0026#34;)) 9 .withColumn(\u0026#34;product_id\u0026#34;, expr(\u0026#34;CAST(product_id AS bigint)\u0026#34;)) 10 .withColumn(\u0026#34;discount\u0026#34;, expr(\u0026#34;CAST(discount AS decimal(4,2))\u0026#34;)) 11 .withColumn(\u0026#34;quantity\u0026#34;, expr(\u0026#34;CAST(quantity AS int)\u0026#34;)) 12 .withColumn(\u0026#34;created_at\u0026#34;, to_timestamp(\u0026#34;created_at\u0026#34;)) 13 ) 14 print(\u0026#34;orders - append records...\u0026#34;) 15 src_df.createOrReplaceTempView(\u0026#34;orders_tbl\u0026#34;) 16 spark_session.sql( 17 \u0026#34;\u0026#34;\u0026#34; 18 WITH src_products AS ( 19 SELECT * FROM demo.dwh.products 20 ), orders_updated AS ( 21 SELECT o.*, u.user_sk, p.prod_sk 22 FROM orders_tbl o 23 LEFT JOIN demo.dwh.users u 24 ON o.user_id = u.id 25 LEFT JOIN src_products p 26 ON o.product_id = p.id 27 AND o.created_at \u0026gt;= p.eff_from 28 AND o.created_at \u0026lt; p.eff_to 29 ), products_tbl AS ( 30 SELECT prod_sk, 31 id, 32 ROW_NUMBER() OVER (PARTITION BY id ORDER BY eff_from) AS rn 33 FROM src_products 34 ) 35 SELECT o.user_sk, 36 COALESCE(o.prod_sk, p.prod_sk) AS prod_sk, 37 o.id, 38 o.discount, 39 o.quantity, 40 o.created_at 41 FROM orders_updated AS o 42 JOIN products_tbl AS p ON o.product_id = p.id 43 WHERE p.rn = 1 44 ORDER BY o.created_at 45 \u0026#34;\u0026#34;\u0026#34; 46 ).writeTo(\u0026#34;demo.dwh.orders\u0026#34;).append() Run ETL The ETL script begins with creating all the tables - users, products and orders. Then the ETL for the users table is executed. Note that, although it is executed as initial loading, the code can also be applied to incremental ETL. Finally, incremental ETL is executed for the products and orders tables. The application can be submitted by ./run.sh spark-submit etl.py. Note to create the following environment variables before submitting the application.\nAWS_ACCESS_KEY_ID AWS_SECRET_ACCESS_KEY AWS_SESSION_TOKEN Note it is optional and required if authentication is made via assume role AWS_REGION Note it is NOT AWS_DEFAULT_REGION 1# etl.py 2from pyspark.sql import SparkSession 3from src import create_tables, etl_users, etl_products, etl_orders 4 5spark = SparkSession.builder.appName(\u0026#34;Iceberg ETL Demo\u0026#34;).getOrCreate() 6 7## create all tables - demo.dwh.users, demo.dwh.products and demo.dwh.orders 8create_tables(spark_session=spark) 9 10## users etl - assuming SCD type 1 11etl_users(\u0026#34;./data/users.csv\u0026#34;, spark) 12 13## incremental ETL 14for yr in range(0, 4): 15 print(f\u0026#34;processing year {yr}\u0026#34;) 16 ## products etl - assuming SCD type 2 17 etl_products(f\u0026#34;./data/products_year_{yr}.csv\u0026#34;, spark) 18 ## orders etl - relevant user_sk and prod_sk are added during transformation 19 etl_orders(f\u0026#34;./data/orders_year_{yr}.csv\u0026#34;, spark) Once the application completes, we\u0026rsquo;re able to query the iceberg tables on Athena. The following query returns all products whose id is 1. It is shown that the price increases over time and the relevant columns (curr_flag, eff_from and eff_to) for SCD type 2 are created as expected.\n1SELECT * 2FROM dwh.products 3WHERE id = 1 4ORDER BY eff_from The following query returns sample order records that bought the product. It can be checked that the product surrogate key matches the products dimension records.\n1WITH src_orders AS ( 2 SELECT o.user_sk, o.prod_sk, o.id, p.title, p.price, o.discount, o.quantity, o.created_at, 3 ROW_NUMBER() OVER (PARTITION BY p.price ORDER BY o.created_at) AS rn 4 FROM dwh.orders AS o 5 JOIN dwh.products AS p ON o.prod_sk = p.prod_sk 6 WHERE p.id = 1 7) 8SELECT * 9FROM src_orders 10WHERE rn = 1 11ORDER BY created_at Summary In this post, we discussed how to implement ETL using retail analytics data. In transformation, SCD type 1 and SCD type 2 are applied to the user and product data respectively. For the order data, the corresponding surrogate keys of the user and product data are added. A Pyspark application that implements ETL against Iceberg tables is used for demonstration in an EMR location environment. Finally, the ETL results will be queried by Athena for verification.\n","date":"June 26, 2022","img":"/blog/2022-06-26-iceberg-etl-demo/featured.png","lang":"en","langName":"English","largeImg":"/blog/2022-06-26-iceberg-etl-demo/featured_hu593bea0bbd3278708b1d2f8e8019b3ec_43604_500x0_resize_box_3.png","permalink":"/blog/2022-06-26-iceberg-etl-demo/","series":[],"smallImg":"/blog/2022-06-26-iceberg-etl-demo/featured_hu593bea0bbd3278708b1d2f8e8019b3ec_43604_180x0_resize_box_3.png","tags":[{"title":"AWS","url":"/tags/aws/"},{"title":"Amazon EMR","url":"/tags/amazon-emr/"},{"title":"Apache Spark","url":"/tags/apache-spark/"},{"title":"PySpark","url":"/tags/pyspark/"},{"title":"Apache Iceberg","url":"/tags/apache-iceberg/"},{"title":"ETL","url":"/tags/etl/"},{"title":"SCD","url":"/tags/scd/"},{"title":"Slowly Changing Dimension","url":"/tags/slowly-changing-dimension/"},{"title":"Docker","url":"/tags/docker/"},{"title":"Docker Compose","url":"/tags/docker-compose/"},{"title":"Visual Studio Code","url":"/tags/visual-studio-code/"}],"timestamp":1656201600,"title":"Data Warehousing ETL Demo With Apache Iceberg on EMR Local Environment"},{"categories":[{"title":"Data Engineering","url":"/categories/data-engineering/"}],"content":"Amazon EMR is a managed service that simplifies running Apache Spark on AWS. It has multiple deployment options that cover EC2, EKS, Outposts and Serverless. For development and testing, EMR Notebooks or EMR Studio can be an option. Both provide a Jupyter Notebook environment and the former is only available for EMR on EC2. There are cases, however, that development (and learning) is performed in a local environment more efficiently. The AWS Glue team understands this demand, and they illustrate how to make use of a custom Docker image for Glue in a recent blog post. However, we don’t hear similar news from the EMR team. In order to fill the gap, we’ll discuss how to create a Spark local development environment for EMR using Docker and/or VSCode. Typical Spark development examples will be demonstrated, which covers Spark Submit, Pytest, PySpark shell, Jupyter Notebook and Spark Structured Streaming. For the Spark Submit and Jupyter Notebook examples, Glue Catalog integration will be illustrated as well. And both the cases of utilising Visual Studio Code Remote - Containers extension and running as an isolated container will be covered in some key examples.\nCustom Docker Image While we may build a custom Spark Docker image from scratch, it’ll be tricky to configure the AWS Glue Data Catalog as the metastore for Spark SQL. Note that it is important to set up this feature because it can be used to integrate other AWS services such as Athena, Glue, Redshift Spectrum and so on. For example, with this feature, we can create a Glue table using a Spark application and the table can be queried by Athena or Redshift Spectrum.\nInstead, we can use one of the Docker images for EMR on EKS as a base image and build a custom image from it. As indicated in the EMR on EKS document, we can pull an EMR release image from ECR. Note to select the right AWS account ID as it is different from one region to another. After authenticating to the ECR repository, I pulled the latest EMR 6.5.0 release image.\n1## different aws region has a different account id 2$ aws ecr get-login-password --region ap-southeast-2 \\ 3 | docker login --username AWS --password-stdin 038297999601.dkr.ecr.ap-southeast-2.amazonaws.com 4## download the latest release (6.5.0) 5$ docker pull 038297999601.dkr.ecr.ap-southeast-2.amazonaws.com/spark/emr-6.5.0:20211119 In the Dockerfile, I updated the default user (hadoop) to have the admin privilege as it can be handy to modify system configuration if necessary. Then spark-defaults.conf and log4j.properties are copied to the Spark configuration folder - they’ll be discussed in detail below. Finally a number of python packages are installed. Among those, the ipykernel and python-dotenv packages are installed to work on Jupyter Notebooks and the pytest and pytest-cov packages are for testing. The custom Docker image is built with the following command: docker build -t=emr-6.5.0:20211119 .devcontainer/.\n1# .devcontainer/Dockerfile 2FROM 038297999601.dkr.ecr.ap-southeast-2.amazonaws.com/spark/emr-6.5.0:20211119 3 4USER root 5 6## Add hadoop to sudo 7RUN yum install -y sudo git \\ 8 \u0026amp;\u0026amp; echo \u0026#34;hadoop ALL=(ALL) NOPASSWD:ALL\u0026#34; \u0026gt;\u0026gt; /etc/sudoers 9 10## Update spark config and log4j properties 11COPY ./spark/spark-defaults.conf /usr/lib/spark/conf/spark-defaults.conf 12COPY ./spark/log4j.properties /usr/lib/spark/conf/log4j.properties 13 14## Install python packages 15COPY ./pkgs /tmp/pkgs 16RUN pip3 install -r /tmp/pkgs/requirements.txt 17 18USER hadoop:hadoop In the default spark configuration file (spark-defaults.conf) shown below, I commented out the following properties that are strictly related to EMR on EKS.\nspark.master spark.submit.deployMode spark.kubernetes.container.image.pullPolicy spark.kubernetes.pyspark.pythonVersion Then I changed the custom AWS credentials provider class from WebIdentityTokenCredentialsProvider to EnvironmentVariableCredentialsProvider. Note EMR jobs are run by a service account on EKS and authentication is managed by web identity token credentials. In a local environment, however, we don’t have an identity provider to authenticate so that access via environment variables can be an easy alternative option. We need the following environment variables to access AWS resources.\nAWS_ACCESS_KEY_ID AWS_SECRET_ACCESS_KEY AWS_SESSION_TOKEN note it is optional and required if authentication is made via assume role AWS_REGION note it is NOT AWS_DEFAULT_REGION Finally, I enabled Hive support and set AWSGlueDataCatalogHiveClientFactory as the Hive metastore factory class. When we start an EMR job, we can override application configuration to use AWS Glue Data Catalog as the metastore for Spark SQL and these are the relevant configuration changes for it.\n# .devcontainer/spark/spark-defaults.conf\r...\r#spark.master k8s://https://kubernetes.default.svc:443\r#spark.submit.deployMode cluster\rspark.hadoop.fs.defaultFS file:///\rspark.shuffle.service.enabled false\rspark.dynamicAllocation.enabled false\r#spark.kubernetes.container.image.pullPolicy Always\r#spark.kubernetes.pyspark.pythonVersion 3\rspark.hadoop.fs.s3.customAWSCredentialsProvider com.amazonaws.auth.EnvironmentVariableCredentialsProvider\rspark.hadoop.dynamodb.customAWSCredentialsProvider com.amazonaws.auth.EnvironmentVariableCredentialsProvider\rspark.authenticate true\r## for Glue catalog\rspark.sql.catalogImplementation hive\rspark.hadoop.hive.metastore.client.factory.class com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory Even if the credentials provider class is changed, it keeps showing long warning messages while fetching EC2 metadata. The following lines are added to the Log4j properties in order to disable those messages.\n# .devcontainer/spark/log4j.properties\r...\r## Ignore warn messages related to EC2 metadata access failure\rlog4j.logger.com.amazonaws.internal.InstanceMetadataServiceResourceFetcher=FATAL\rlog4j.logger.com.amazonaws.util.EC2MetadataUtils=FATAL VSCode Development Container We are able to run Spark Submit, pytest, PySpark shell examples as an isolated container using the custom Docker image. However it can be much more convenient if we are able to perform development inside the Docker container where our app is executed. The Visual Studio Code Remote - Containers extension allows you to open a folder inside a container and to use VSCode’s feature sets. It supports both a standalone container and Docker Compose. In this post, we’ll use the latter as we’ll discuss an example Spark Structured Streaming application and multiple services should run and linked together for it.\nDocker Compose The main service (container) is named spark and its command prevents it from being terminated. The current working directory is mapped to /home/hadoop/repo and it’ll be the container folder that we’ll open for development. The aws configuration folder is volume-mapped to the container user’s home directory. It is an optional configuration to access AWS services without relying on AWS credentials via environment variables. The remaining services are related to Kafka. The kafka and zookeeper services are to run a Kafka cluster and the kafka-ui allows us to access the cluster on a browser. The services share the same Docker network named spark. Note that the compose file includes other Kafka related services and their details can be found in one of my earlier posts.\n1# .devcontainer/docker-compose.yml 2version: \u0026#34;2\u0026#34; 3 4services: 5 spark: 6 image: emr-6.5.0:20211119 7 container_name: spark 8 command: /bin/bash -c \u0026#34;while sleep 1000; do :; done\u0026#34; 9 networks: 10 - spark 11 volumes: 12 - ${PWD}:/home/hadoop/repo 13 - ${HOME}/.aws:/home/hadoop/.aws 14 zookeeper: 15 image: bitnami/zookeeper:3.7.0 16 container_name: zookeeper 17 ports: 18 - \u0026#34;2181:2181\u0026#34; 19 networks: 20 - spark 21 environment: 22 - ALLOW_ANONYMOUS_LOGIN=yes 23 kafka: 24 image: bitnami/kafka:2.8.1 25 container_name: kafka 26 ports: 27 - \u0026#34;9092:9092\u0026#34; 28 networks: 29 - spark 30 environment: 31 - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181 32 - ALLOW_PLAINTEXT_LISTENER=yes 33 depends_on: 34 - zookeeper 35 kafka-ui: 36 image: provectuslabs/kafka-ui:0.3.3 37 container_name: kafka-ui 38 ports: 39 - \u0026#34;8080:8080\u0026#34; 40 networks: 41 - spark 42 environment: 43 KAFKA_CLUSTERS_0_NAME: local 44 KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:9092 45 KAFKA_CLUSTERS_0_ZOOKEEPER: zookeeper:2181 46 ... 47 depends_on: 48 - zookeeper 49 - kafka 50... 51 52networks: 53 spark: 54 name: spark Development Container The development container is configured to connect the _spark _service among the Docker Compose services. The _AWS_PROFILE _environment variable is optionally set for AWS configuration and additional folders are added to PYTHONPATH, which is to use the bundled pyspark and py4j packages of the Spark distribution. The port 4040 for Spark History Server is added to the forwarded ports array - I guess it’s optional as the port is made accessible in the compose file. The remaining sections are for installing VSCode extensions and adding editor configuration. Note we need the Python extension (ms-python.python) not only for code formatting but also for working on Jupyter Notebooks.\n1# .devcontainer/devcontainer.json 2{ 3 \u0026#34;name\u0026#34;: \u0026#34;Spark Development\u0026#34;, 4 \u0026#34;dockerComposeFile\u0026#34;: \u0026#34;docker-compose.yml\u0026#34;, 5 \u0026#34;service\u0026#34;: \u0026#34;spark\u0026#34;, 6 \u0026#34;runServices\u0026#34;: [ 7 \u0026#34;spark\u0026#34;, 8 \u0026#34;zookeeper\u0026#34;, 9 \u0026#34;kafka\u0026#34;, 10 \u0026#34;kafka-ui\u0026#34; 11 ], 12 \u0026#34;remoteEnv\u0026#34;: { 13 \u0026#34;AWS_PROFILE\u0026#34;: \u0026#34;cevo\u0026#34;, 14 \u0026#34;PYTHONPATH\u0026#34;: \u0026#34;/usr/lib/spark/python/lib/py4j-0.10.9-src.zip:/usr/lib/spark/python/\u0026#34; 15 }, 16 \u0026#34;workspaceFolder\u0026#34;: \u0026#34;/home/hadoop/repo\u0026#34;, 17 \u0026#34;extensions\u0026#34;: [\u0026#34;ms-python.python\u0026#34;, \u0026#34;esbenp.prettier-vscode\u0026#34;], 18 \u0026#34;forwardPorts\u0026#34;: [4040], 19 \u0026#34;settings\u0026#34;: { 20 \u0026#34;terminal.integrated.profiles.linux\u0026#34;: { 21 \u0026#34;bash\u0026#34;: { 22 \u0026#34;path\u0026#34;: \u0026#34;/bin/bash\u0026#34; 23 } 24 }, 25 \u0026#34;terminal.integrated.defaultProfile.linux\u0026#34;: \u0026#34;bash\u0026#34;, 26 \u0026#34;editor.formatOnSave\u0026#34;: true, 27 \u0026#34;editor.defaultFormatter\u0026#34;: \u0026#34;esbenp.prettier-vscode\u0026#34;, 28 \u0026#34;editor.tabSize\u0026#34;: 2, 29 \u0026#34;python.defaultInterpreterPath\u0026#34;: \u0026#34;python3\u0026#34;, 30 \u0026#34;python.testing.pytestEnabled\u0026#34;: true, 31 \u0026#34;python.linting.enabled\u0026#34;: true, 32 \u0026#34;python.linting.pylintEnabled\u0026#34;: false, 33 \u0026#34;python.linting.flake8Enabled\u0026#34;: false, 34 \u0026#34;python.formatting.provider\u0026#34;: \u0026#34;black\u0026#34;, 35 \u0026#34;python.formatting.blackPath\u0026#34;: \u0026#34;black\u0026#34;, 36 \u0026#34;python.formatting.blackArgs\u0026#34;: [\u0026#34;--line-length\u0026#34;, \u0026#34;100\u0026#34;], 37 \u0026#34;[python]\u0026#34;: { 38 \u0026#34;editor.tabSize\u0026#34;: 4, 39 \u0026#34;editor.defaultFormatter\u0026#34;: \u0026#34;ms-python.python\u0026#34; 40 } 41 } 42} We can open the current folder in the development container after launching the Docker Compose services by executing the following command in the command palette.\nRemote-Containers: Open Folder in Container\u0026hellip; Once the development container is ready, the current folder will be open within the spark service container. We are able to check the container’s current folder is /home/hadoop/repo and the container user is hadoop.\nFile Permission Management I use Ubuntu in WSL 2 for development and the user ID and group ID of my WSL user are 1000. On the other hand, the container user is hadoop and its user ID and group ID are 999 and 1000 respectively. When you create a file in the host, the user has the read and write permissions of the file while the group only has the read permission. Therefore, you can read the file inside the development container by the container user, but it is not possible to modify it due to lack of the write permission. This file permission issue will happen when a file is created by the container user and the WSL user tries to modify it in the host. A quick search shows this is a typical behaviour applicable only to Linux (not Mac or Windows).\nIn order to handle this file permission issue, we can update the file permission so that the read and write permissions are given to both the user and group. Note the host (WSL) user and container user have the same group ID and writing activities will be allowed at least by the group permission. Below shows an example. The read and write permissions for files in the project folder are given to both the user and group. Those that are created by the container user indicate the username while there are 2 files that are created by the WSL user, and it is indicated by the user ID because there is no user whose user ID is 1000 in the container.\n1bash-4.2$ ls -al | grep \u0026#39;^-\u0026#39; 2-rw-rw-r-- 1 hadoop hadoop 1086 Apr 12 22:23 .env 3-rw-rw-r-- 1 1000 hadoop 1855 Apr 12 19:45 .gitignore 4-rw-rw-r-- 1 1000 hadoop 66 Mar 30 22:39 README.md 5-rw-rw-r-- 1 hadoop hadoop 874 Apr 5 11:14 test_utils.py 6-rw-rw-r-- 1 hadoop hadoop 3882 Apr 12 22:24 tripdata.ipynb 7-rw-rw-r-- 1 hadoop hadoop 1653 Apr 24 13:09 tripdata_notify.py 8-rw-rw-r-- 1 hadoop hadoop 1101 Apr 24 01:22 tripdata.py 9-rw-rw-r-- 1 hadoop hadoop 664 Apr 12 19:45 utils.py Below is the same file list that is printed in the host. Note that the group name is changed into the WSL user’s group and those that are created by the container user are marked by the user ID.\n1jaehyeon@cevo:~/personal/emr-local-dev$ ls -al | grep \u0026#39;^-\u0026#39; 2-rw-rw-r-- 1 999 jaehyeon 1086 Apr 13 08:23 .env 3-rw-rw-r-- 1 jaehyeon jaehyeon 1855 Apr 13 05:45 .gitignore 4-rw-rw-r-- 1 jaehyeon jaehyeon 66 Mar 31 09:39 README.md 5-rw-rw-r-- 1 999 jaehyeon 874 Apr 5 21:14 test_utils.py 6-rw-rw-r-- 1 999 jaehyeon 3882 Apr 13 08:24 tripdata.ipynb 7-rw-rw-r-- 1 999 jaehyeon 1101 Apr 24 11:22 tripdata.py 8-rw-rw-r-- 1 999 jaehyeon 1653 Apr 24 23:09 tripdata_notify.py 9-rw-rw-r-- 1 999 jaehyeon 664 Apr 13 05:45 utils.py We can add the read or write permission of a single file or a folder easily as shown below - g+rw. Note the last example is for the AWS configuration folder and only the read access is given to the group. Note also that file permission change is not affected if the repository is cloned into a new place, and thus it only affects the local development environment.\n1# add write access of a file to the group 2sudo chmod g+rw /home/hadoop/repo/\u0026lt;file-name\u0026gt; 3# add write access of a folder to the group 4sudo chmod -R g+rw /home/hadoop/repo/\u0026lt;folder-name\u0026gt; 5# add read access of the .aws folder to the group 6sudo chmod -R g+r /home/hadoop/.aws Examples In this section, I’ll demonstrate typical Spark development examples. They’ll cover Spark Submit, Pytest, PySpark shell, Jupyter Notebook and Spark Structured Streaming. For the Spark Submit and Jupyter Notebook examples, Glue Catalog integration will be illustrated as well. And both the cases of utilising Visual Studio Code Remote - Containers extension and running as an isolated container will be covered in some key examples.\nSpark Submit It is a simple Spark application that reads a sample NY taxi trip dataset from a public S3 bucket. Once loaded, it converts the pick-up and drop-off datetime columns from string to timestamp followed by writing the transformed data to a destination S3 bucket. The destination bucket name (bucket_name) can be specified by a system argument or its default value is taken. It finishes by creating a Glue table and, similar to the destination bucket name, the table name (tblname) can be specified as well.\n1# tripdata.py 2import sys 3from pyspark.sql import SparkSession 4 5from utils import to_timestamp_df 6 7if __name__ == \u0026#34;__main__\u0026#34;: 8 spark = SparkSession.builder.appName(\u0026#34;Trip Data\u0026#34;).getOrCreate() 9 10 dbname = \u0026#34;tripdata\u0026#34; 11 tblname = \u0026#34;ny_taxi\u0026#34; if len(sys.argv) \u0026lt;= 1 else sys.argv[1] 12 bucket_name = \u0026#34;emr-local-dev\u0026#34; if len(sys.argv) \u0026lt;= 2 else sys.argv[2] 13 dest_path = f\u0026#34;s3://{bucket_name}/{tblname}/\u0026#34; 14 src_path = \u0026#34;s3://aws-data-analytics-workshops/shared_datasets/tripdata/\u0026#34; 15 # read csv 16 ny_taxi = spark.read.option(\u0026#34;inferSchema\u0026#34;, \u0026#34;true\u0026#34;).option(\u0026#34;header\u0026#34;, \u0026#34;true\u0026#34;).csv(src_path) 17 ny_taxi = to_timestamp_df(ny_taxi, [\u0026#34;lpep_pickup_datetime\u0026#34;, \u0026#34;lpep_dropoff_datetime\u0026#34;]) 18 ny_taxi.printSchema() 19 # write parquet 20 ny_taxi.write.mode(\u0026#34;overwrite\u0026#34;).parquet(dest_path) 21 # create glue table 22 ny_taxi.registerTempTable(tblname) 23 spark.sql(f\u0026#34;CREATE DATABASE IF NOT EXISTS {dbname}\u0026#34;) 24 spark.sql(f\u0026#34;USE {dbname}\u0026#34;) 25 spark.sql( 26 f\u0026#34;\u0026#34;\u0026#34;CREATE TABLE IF NOT EXISTS {tblname} 27 USING PARQUET 28 LOCATION \u0026#39;{dest_path}\u0026#39; 29 AS SELECT * FROM {tblname} 30 \u0026#34;\u0026#34;\u0026#34; 31 ) The Spark application can be submitted as shown below.\n1export AWS_ACCESS_KEY_ID=\u0026lt;AWS-ACCESS-KEY-ID\u0026gt; 2export AWS_SECRET_ACCESS_KEY=\u0026lt;AWS-SECRET-ACCESS-KEY\u0026gt; 3export AWS_REGION=\u0026lt;AWS-REGION\u0026gt; 4# optional 5export AWS_SESSION_TOKEN=\u0026lt;AWS-SESSION-TOKEN\u0026gt; 6 7$SPARK_HOME/bin/spark-submit \\ 8 --deploy-mode client \\ 9 --master local[*] \\ 10 tripdata.py Once it completes, the Glue table will be created, and we can query it using Athena as shown below.\nIf we want to submit the application as an isolated container, we can use the custom image directly. Below shows the equivalent Docker run command.\n1docker run --rm \\ 2 -e AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID \\ 3 -e AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY \\ 4 -e AWS_SESSION_TOKEN=$AWS_SESSION_TOKEN \\ # optional 5 -e AWS_REGION=$AWS_REGION \\ 6 -v $PWD:/usr/hadoop \\ 7 emr-6.5.0:20211119 \\ 8 /usr/lib/spark/bin/spark-submit --deploy-mode client --master local[*] /usr/hadoop/tripdata.py taxi emr-local-dev Pytest The Spark application in the earlier example uses a custom function that converts the data type of one or more columns from string to timestamp - to_timestamp_df(). The source of the function and the testing script of it can be found below.\n1# utils.py 2from typing import List, Union 3from pyspark.sql import DataFrame 4from pyspark.sql.functions import col, to_timestamp 5 6def to_timestamp_df( 7 df: DataFrame, fields: Union[List[str], str], format: str = \u0026#34;M/d/yy H:mm\u0026#34; 8) -\u0026gt; DataFrame: 9 fields = [fields] if isinstance(fields, str) else fields 10 for field in fields: 11 df = df.withColumn(field, to_timestamp(col(field), format)) 12 return df 13# test_utils.py 14import pytest 15import datetime 16from pyspark.sql import SparkSession 17from py4j.protocol import Py4JError 18 19from utils import to_timestamp_df 20 21@pytest.fixture(scope=\u0026#34;session\u0026#34;) 22def spark(): 23 return ( 24 SparkSession.builder.master(\u0026#34;local\u0026#34;) 25 .appName(\u0026#34;test\u0026#34;) 26 .config(\u0026#34;spark.submit.deployMode\u0026#34;, \u0026#34;client\u0026#34;) 27 .getOrCreate() 28 ) 29 30 31def test_to_timestamp_success(spark): 32 raw_df = spark.createDataFrame( 33 [(\u0026#34;1/1/17 0:01\u0026#34;,)], 34 [\u0026#34;date\u0026#34;], 35 ) 36 37 test_df = to_timestamp_df(raw_df, \u0026#34;date\u0026#34;, \u0026#34;M/d/yy H:mm\u0026#34;) 38 for row in test_df.collect(): 39 assert row[\u0026#34;date\u0026#34;] == datetime.datetime(2017, 1, 1, 0, 1) 40 41 42def test_to_timestamp_bad_format(spark): 43 raw_df = spark.createDataFrame( 44 [(\u0026#34;1/1/17 0:01\u0026#34;,)], 45 [\u0026#34;date\u0026#34;], 46 ) 47 48 with pytest.raises(Py4JError): 49 to_timestamp_df(raw_df, \u0026#34;date\u0026#34;, \u0026#34;M/d/yy HH:mm\u0026#34;).collect() As the test cases don’t access AWS services, they can be executed simply by the Pytest command (e.g. pytest -v).\nTesting can also be made in an isolated container as shown below. Note that we need to add the PYTHONPATH environment variable because we use the bundled Pyspark package.\n1docker run --rm \\ 2 -e PYTHONPATH=\u0026#34;/usr/lib/spark/python/lib/py4j-0.10.9-src.zip:/usr/lib/spark/python/\u0026#34; \\ 3 -v $PWD:/usr/hadoop \\ 4 emr-6.5.0:20211119 \\ 5 pytest /usr/hadoop -v PySpark Shell The PySpark shell can be launched as shown below.\n1$SPARK_HOME/bin/pyspark \\ 2 --deploy-mode client \\ 3 --master local[*] Also, below shows an example of launching it as an isolated container.\n1docker run --rm -it \\ 2 -e AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID \\ 3 -e AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY \\ 4 -e AWS_SESSION_TOKEN=$AWS_SESSION_TOKEN \\ # optional 5 -e AWS_REGION=$AWS_REGION \\ 6 -v $PWD:/usr/hadoop \\ 7 emr-6.5.0:20211119 \\ 8 /usr/lib/spark/bin/pyspark --deploy-mode client --master local[*] Jupyter Notebook Jupyter Notebook is a popular Spark application authoring tool, and we can create a notebook simply by creating a file with the ipynb extension in VSCode. Note we need the ipykernel package in order to run code cells, and it is already installed in the custom Docker image. For accessing AWS resources, we need the environment variables of AWS credentials mentioned earlier. We can use the python-dotenv package. Specifically we can create an .env file and add AWS credentials to it. Then we can add a code cell that loads the .env file at the beginning of the notebook.\nIn the next code cell, the app reads the Glue table and adds a column of trip duration followed by showing the summary statistics of key columns. We see some puzzling records that show zero trip duration or negative total amount. Among those, we find negative total amount records should be reported immediately and a Spark Structured Streaming application turns out to be a good option.\nSpark Streaming We need sample data that can be read by the Spark application. In order to generate it, the individual records are taken from the source CSV file and saved locally after being converted into json. Below script creates those json files in the data/json folder. Inside the development container, it can be executed as python3 data/generate.py.\n1# data/generate.py 2import shutil 3import io 4import json 5import csv 6from pathlib import Path 7import boto3 8 9BUCKET_NAME = \u0026#34;aws-data-analytics-workshops\u0026#34; 10KEY_NAME = \u0026#34;shared_datasets/tripdata/tripdata.csv\u0026#34; 11DATA_PATH = Path.joinpath(Path(__file__).parent, \u0026#34;json\u0026#34;) 12 13 14def recreate_data_path_if(data_path: Path, recreate: bool = True): 15 if recreate: 16 shutil.rmtree(data_path, ignore_errors=True) 17 data_path.mkdir() 18 19 20def write_to_json(bucket_name: str, key_name: str, data_path: Path, recreate: bool = True): 21 s3 = boto3.resource(\u0026#34;s3\u0026#34;) 22 data = io.BytesIO() 23 bucket = s3.Bucket(bucket_name) 24 bucket.download_fileobj(key_name, data) 25 contents = data.getvalue().decode(\u0026#34;utf-8\u0026#34;) 26 print(\u0026#34;download complete\u0026#34;) 27 reader = csv.DictReader(contents.split(\u0026#34;\\n\u0026#34;)) 28 recreate_data_path_if(data_path, recreate) 29 for c, row in enumerate(reader): 30 record_id = str(c).zfill(5) 31 data_path.joinpath(f\u0026#34;{record_id}.json\u0026#34;).write_text( 32 json.dumps({**{\u0026#34;record_id\u0026#34;: record_id}, **row}) 33 ) 34 35 36if __name__ == \u0026#34;__main__\u0026#34;: 37 write_to_json(BUCKET_NAME, KEY_NAME, DATA_PATH, True) In the Spark streaming application, the steam reader loads JSON files in the data/json folder and the data schema is provided by DDL statements. Then it generates the target dataframe that filters records whose total amount is negative. Note the target dataframe is structured to have the key and value columns, which is required by Kafka. Finally, it writes the records of the target dataframe to the _notifications _topics of the Kafka cluster.\n1# tripdata_notify.py 2from pyspark.sql import SparkSession 3from pyspark.sql.functions import col, to_json, struct 4from utils import remove_checkpoint 5 6if __name__ == \u0026#34;__main__\u0026#34;: 7 remove_checkpoint() 8 9 spark = ( 10 SparkSession.builder.appName(\u0026#34;Trip Data Notification\u0026#34;) 11 .config(\u0026#34;spark.streaming.stopGracefullyOnShutdown\u0026#34;, \u0026#34;true\u0026#34;) 12 .config(\u0026#34;spark.sql.shuffle.partitions\u0026#34;, 3) 13 .getOrCreate() 14 ) 15 16 tripdata_ddl = \u0026#34;\u0026#34;\u0026#34; 17 record_id STRING, 18 VendorID STRING, 19 lpep_pickup_datetime STRING, 20 lpep_dropoff_datetime STRING, 21 store_and_fwd_flag STRING, 22 RatecodeID STRING, 23 PULocationID STRING, 24 DOLocationID STRING, 25 passenger_count STRING, 26 trip_distance STRING, 27 fare_amount STRING, 28 extra STRING, 29 mta_tax STRING, 30 tip_amount STRING, 31 tolls_amount STRING, 32 ehail_fee STRING, 33 improvement_surcharge STRING, 34 total_amount STRING, 35 payment_type STRING, 36 trip_type STRING 37 \u0026#34;\u0026#34;\u0026#34; 38 39 ny_taxi = ( 40 spark.readStream.format(\u0026#34;json\u0026#34;) 41 .option(\u0026#34;path\u0026#34;, \u0026#34;data/json\u0026#34;) 42 .option(\u0026#34;maxFilesPerTrigger\u0026#34;, \u0026#34;1000\u0026#34;) 43 .schema(tripdata_ddl) 44 .load() 45 ) 46 47 target_df = ny_taxi.filter(col(\u0026#34;total_amount\u0026#34;) \u0026lt;= 0).select( 48 col(\u0026#34;record_id\u0026#34;).alias(\u0026#34;key\u0026#34;), to_json(struct(\u0026#34;*\u0026#34;)).alias(\u0026#34;value\u0026#34;) 49 ) 50 51 notification_writer_query = ( 52 target_df.writeStream.format(\u0026#34;kafka\u0026#34;) 53 .queryName(\u0026#34;notifications\u0026#34;) 54 .option(\u0026#34;kafka.bootstrap.servers\u0026#34;, \u0026#34;kafka:9092\u0026#34;) 55 .option(\u0026#34;topic\u0026#34;, \u0026#34;notifications\u0026#34;) 56 .outputMode(\u0026#34;append\u0026#34;) 57 .option(\u0026#34;checkpointLocation\u0026#34;, \u0026#34;.checkpoint\u0026#34;) 58 .start() 59 ) 60 61 notification_writer_query.awaitTermination() The streaming application can be submitted as shown below. Note the Kafak 0.10+ Source for Structured Streaming and its dependencies are added directly to the spark submit command as indicated by the official document.\n1$SPARK_HOME/bin/spark-submit \\ 2 --deploy-mode client \\ 3 --master local[*] \\ 4 --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2 \\ 5 tripdata_notify.py We can check the topic via Kafka UI on port 8080. We see the notifications topic has 50 messages, which matches to the number that we obtained from the notebook.\nWe can check the individual messages via the UI as well.\nSummary In this post, we discussed how to create a Spark local development environment for EMR using Docker and/or VSCode. A range of Spark development examples are demonstrated, and Glue Catalog integration is illustrated in some of them. And both the cases of utilising Visual Studio Code Remote - Containers extension and running as an isolated container are covered in some key examples.\n","date":"May 8, 2022","img":"/blog/2022-05-08-emr-local-dev/featured.png","lang":"en","langName":"English","largeImg":"/blog/2022-05-08-emr-local-dev/featured_hu653aa93a98a5139fededca231658be70_25693_500x0_resize_box_3.png","permalink":"/blog/2022-05-08-emr-local-dev/","series":[],"smallImg":"/blog/2022-05-08-emr-local-dev/featured_hu653aa93a98a5139fededca231658be70_25693_180x0_resize_box_3.png","tags":[{"title":"AWS","url":"/tags/aws/"},{"title":"Amazon EMR","url":"/tags/amazon-emr/"},{"title":"Apache Spark","url":"/tags/apache-spark/"},{"title":"PySpark","url":"/tags/pyspark/"},{"title":"Docker","url":"/tags/docker/"},{"title":"Docker Compose","url":"/tags/docker-compose/"},{"title":"Visual Studio Code","url":"/tags/visual-studio-code/"}],"timestamp":1651968000,"title":"Develop and Test Apache Spark Apps for EMR Locally Using Docker"},{"categories":[{"title":"Data Engineering","url":"/categories/data-engineering/"}],"content":"In the previous post, we discussed a Change Data Capture (CDC) solution with a schema registry. A local development environment is set up using Docker Compose. The Debezium and Confluent S3 connectors are deployed with the Confluent Avro converter and the Apicurio registry is used as the schema registry service. A quick example is shown to illustrate how schema evolution can be managed by the schema registry. In this post, we\u0026rsquo;ll build the solution on AWS using MSK, MSK Connect, Aurora PostgreSQL and ECS.\nPart 1 Local Development Part 2 MSK Deployment (this post) Architecture Below shows an updated CDC architecture with a schema registry. The Debezium connector talks to the schema registry first and checks if the schema is available. If it doesn\u0026rsquo;t exist, it is registered and cached in the schema registry. Then the producer serializes the data with the schema and sends it to the topic with the schema ID. When the sink connector consumes the message, it\u0026rsquo;ll read the schema with the ID and deserializes it. The schema registry uses a PostgreSQL database as an artifact store where multiple versions of schemas are kept. In this post, we\u0026rsquo;ll build it on AWS. An MSK cluster will be created and data will be pushed from a database deployed using Aurora PostgreSQL. The database has a schema called registry and schema metadata will be stored in it. The Apicurio registry will be deployed as an ECS service behind an internal load balancer.\nInfrastructure The main AWS resources will be deployed to private subnets of a VPC and connection between those will be managed by updating security group inbound rules. For example, the MSK connectors should have access to the registry service and the connectors\u0026rsquo; security group ID should be added to the inbound rule of the registry service. As multiple resources are deployed to private subnets, it\u0026rsquo;ll be convenient to set up VPN so that access to them can be made from the developer machine. It can improve developer experience significantly. We\u0026rsquo;ll use Terraform for managing the resources on AWS and how to set up VPC, VPN and Aurora PostgreSQL is discussed in detail in one of my earlier posts. In this post, I\u0026rsquo;ll illustrate those that are not covered in the article. The Terraform source can be found in the GitHub repository for this post.\nMSK Cluster As discussed in one of the earlier posts, we\u0026rsquo;ll create an MSK cluster with 2 brokers of the kafka.m5.large instance type in order to prevent the failed authentication error. 2 inbound rules are configured for the MSK\u0026rsquo;s security group. The first one is allowing all access from its own security group, and it is required for MSK connectors to have access to the MKS cluster. Note, when we create a connector from the AWS console, the cluster\u0026rsquo;s subnets and security group are selected for the connector by default. The second inbound rule is allowing the VPN\u0026rsquo;s security group at port 9098, which is the port of bootstrap servers for IAM authentication. Also, an IAM role is created, which can be assumed by MSK connectors in order to have permission on the cluster, topic and group. The Terraform file for the MSK cluster and related resources can be found in infra/msk.tf.\nSchema Registry The schema registry is deployed via ECS as a Fargate task. 2 tasks are served by an ECS service, and it can be accessed by an internal load balancer. The load balancer is configured to allow inbound traffic from the MSK cluster and VPN, and it has access to the individual tasks. Normally inbound traffic to the tasks should be allowed to the load balancer only but, for testing, it is set that they accept inbound traffic from VPN as well. The Terraform file for the schema registry and related resources can be found in infra/registry.tf.\nSetup Database In order for the schema registry to work properly, the database should have the appropriate schema named registry. Also, the database needs to have sample data loaded into the ods schema. Therefore, it is not possible to create all resources at once, and we need to skip creating the registry service at first. It can be done by setting the registry_create variable to false.\n1# infra/variables.tf 2variable \u0026#34;registry_create\u0026#34; { 3 description = \u0026#34;Whether to create an Apicurio registry service\u0026#34; 4 default = false 5} A simple python application is created to set up the database, and it can be run as shown below. Note do not forget to connect the VPN before executing the command.\n1(venv) $ python connect/data/load/main.py --help 2Usage: main.py [OPTIONS] 3 4Options: 5 -h, --host TEXT Database host [required] 6 -p, --port INTEGER Database port [default: 5432] 7 -d, --dbname TEXT Database name [required] 8 -u, --user TEXT Database user name [required] 9 --password TEXT Database user password [required] 10 --install-completion Install completion for the current shell. 11 --show-completion Show completion for the current shell, to copy it or 12 customize the installation. 13 --help Show this message and exit. 14 15(venv) $ python connect/data/load/main.py --help -h \u0026lt;db-host-name-or-ip\u0026gt; -d \u0026lt;dbname\u0026gt; -u \u0026lt;username\u0026gt; 16Password: 17To create database? [y/N]: y 18Database connection created 19Northwind SQL scripts executed Deploy Schema Registry Once the database setup is complete, we can apply the Terraform stack with the registry_create variable to true. When it\u0026rsquo;s deployed, we can check the APIs that the registry service supports as shown below. In line with the previous post, we\u0026rsquo;ll use the Confluent schema registry compatible API.\nKafka UI The Kafka UI supports MSK IAM Authentication and we can use it to monitor and manage MSK clusters and related objects/resources. My AWS credentials are mapped to the container and my AWS profile (cevo) is added to the SASL config environment variable. Note environment variables are used for the bootstrap server endpoint and registry host. It can be started as docker-compose -f kafka-ui.yml up.\n1# kafka-ui.yml 2version: \u0026#34;2\u0026#34; 3services: 4 kafka-ui: 5 image: provectuslabs/kafka-ui:0.3.3 6 container_name: kafka-ui 7 ports: 8 - \u0026#34;8080:8080\u0026#34; 9 # restart: always 10 volumes: 11 - $HOME/.aws:/root/.aws 12 environment: 13 KAFKA_CLUSTERS_0_NAME: msk 14 KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: $BS_SERVERS 15 KAFKA_CLUSTERS_0_PROPERTIES_SECURITY_PROTOCOL: SASL_SSL 16 KAFKA_CLUSTERS_0_PROPERTIES_SASL_MECHANISM: AWS_MSK_IAM 17 KAFKA_CLUSTERS_0_PROPERTIES_SASL_CLIENT_CALLBACK_HANDLER_CLASS: software.amazon.msk.auth.iam.IAMClientCallbackHandler 18 KAFKA_CLUSTERS_0_PROPERTIES_SASL_JAAS_CONFIG: software.amazon.msk.auth.iam.IAMLoginModule required awsProfileName=\u0026#34;cevo\u0026#34;; 19 KAFKA_CLUSTERS_0_SCHEMAREGISTRY: http://$REGISTRY_HOST/apis/ccompat/v6 The UI can be checked on a browser as shown below.\nCreate Connectors Creating custom plugins and connectors is illustrated in detail in one of my earlier posts. Here I\u0026rsquo;ll sketch key points only. The custom plugins for the source and sink connectors should include the Kafka Connect Avro Converter as well. The version 6.0.3 is used, and plugin packaging can be checked in connect/local/download-connectors.sh.\nThe Debezium Postgres Connector is used as the source connector. Here the main difference from the earlier post is using the Confluent Avro Converter class for key and value converter properties and adding the schema registry URL.\n# connect/msk/debezium.properties\rconnector.class=io.debezium.connector.postgresql.PostgresConnector\rtasks.max=1\rplugin.name=pgoutput\rpublication.name=cdc_publication\rslot.name=orders\rdatabase.hostname=analytics-db-cluster.cluster-ctrfy31kg8iq.ap-southeast-2.rds.amazonaws.com\rdatabase.port=5432\rdatabase.user=master\rdatabase.password=\u0026lt;database-user-password\u0026gt;\rdatabase.dbname=main\rdatabase.server.name=ord\rschema.include=ods\rtable.include.list=ods.cdc_events\rkey.converter=io.confluent.connect.avro.AvroConverter\rkey.converter.schema.registry.url=http://internal-analytics-registry-lb-754693167.ap-southeast-2.elb.amazonaws.com/apis/ccompat/v6\rvalue.converter=io.confluent.connect.avro.AvroConverter\rvalue.converter.schema.registry.url=http://internal-analytics-registry-lb-754693167.ap-southeast-2.elb.amazonaws.com/apis/ccompat/v6\rtransforms=unwrap\rtransforms.unwrap.type=io.debezium.transforms.ExtractNewRecordState\rtransforms.unwrap.drop.tombstones=false\rtransforms.unwrap.delete.handling.mode=rewrite\rtransforms.unwrap.add.fields=op,db,table,schema,lsn,source.ts_ms The sink connector also uses the Confluent Avro Converter class for key and value converter properties and the schema registry URL is added accordingly.\n# connect/msk/confluent.properties\rconnector.class=io.confluent.connect.s3.S3SinkConnector\rstorage.class=io.confluent.connect.s3.storage.S3Storage\rformat.class=io.confluent.connect.s3.format.avro.AvroFormat\rtasks.max=1\rtopics=ord.ods.cdc_events\rs3.bucket.name=analytics-data-590312749310-ap-southeast-2\rs3.region=ap-southeast-2\rflush.size=100\rrotate.schedule.interval.ms=60000\rtimezone=Australia/Sydney\rpartitioner.class=io.confluent.connect.storage.partitioner.DefaultPartitioner\rkey.converter=io.confluent.connect.avro.AvroConverter\rkey.converter.schema.registry.url=http://internal-analytics-registry-lb-754693167.ap-southeast-2.elb.amazonaws.com/apis/ccompat/v6\rvalue.converter=io.confluent.connect.avro.AvroConverter\rvalue.converter.schema.registry.url=http://internal-analytics-registry-lb-754693167.ap-southeast-2.elb.amazonaws.com/apis/ccompat/v6\rerrors.log.enable=true As with the previous post, we can check the key and value schemas are created once the source connector is deployed. Note we can check the details of the schemas by clicking the relevant schema items.\nWe can see the messages (key and value) are properly deserialized within the UI as we added the schema registry URL as an environment variable and it can be accessed from it.\nSchema Evolution The schema registry keeps multiple versions of schemas and we can check it by adding a column to the table and updating records.\n1--// add a column with a default value 2ALTER TABLE ods.cdc_events 3 ADD COLUMN employee_id int DEFAULT -1; 4 5--// update employee ID 6UPDATE ods.cdc_events 7 SET employee_id = (employee -\u0026gt;\u0026gt; \u0026#39;employee_id\u0026#39;)::INT 8WHERE customer_id = \u0026#39;VINET\u0026#39; Once the above queries are executed, we see a new version is added to the topic\u0026rsquo;s value schema, and it includes the new field.\nSummary In this post, we continued the discussion of a Change Data Capture (CDC) solution with a schema registry, and it is deployed to AWS. Multiple services including MSK, MSK Connect, Aurora PostgreSQL and ECS are used to build the solution. All major resources are deployed in private subnets and VPN is used to access them in order to improve developer experience. The Apicurio registry is used as the schema registry service, and it is deployed as an ECS service. In order for the connectors to have access to the registry, the Confluent Avro Converter is packaged together with the connector sources. The post ends with illustrating how schema evolution is managed by the schema registry.\n","date":"April 3, 2022","img":"/blog/2022-04-03-schema-registry-part2/featured.png","lang":"en","langName":"English","largeImg":"/blog/2022-04-03-schema-registry-part2/featured_hu55ef0aeef397b55768e961367d004bbc_59689_500x0_resize_box_3.png","permalink":"/blog/2022-04-03-schema-registry-part2/","series":[{"title":"Integrate Schema Registry with MSK Connect","url":"/series/integrate-schema-registry-with-msk-connect/"}],"smallImg":"/blog/2022-04-03-schema-registry-part2/featured_hu55ef0aeef397b55768e961367d004bbc_59689_180x0_resize_box_3.png","tags":[{"title":"AWS","url":"/tags/aws/"},{"title":"Amazon ECS","url":"/tags/amazon-ecs/"},{"title":"Amazon MSK","url":"/tags/amazon-msk/"},{"title":"Amazon MSK Connect","url":"/tags/amazon-msk-connect/"},{"title":"Apache Kafka","url":"/tags/apache-kafka/"},{"title":"Terraform","url":"/tags/terraform/"},{"title":"Docker","url":"/tags/docker/"},{"title":"Docker Compose","url":"/tags/docker-compose/"}],"timestamp":1648944000,"title":"Use External Schema Registry With MSK Connect – Part 2 MSK Deployment"},{"categories":[{"title":"Data Engineering","url":"/categories/data-engineering/"}],"content":"When we discussed a Change Data Capture (CDC) solution in one of the earlier posts, we used the JSON converter that comes with Kafka Connect. We optionally enabled the key and value schemas and the topic messages include those schemas together with payload. It seems to be convenient at first as the messages are saved into S3 on their own. However, it became cumbersome when we tried to use the DeltaStreamer utility. Specifically it requires the scheme of the files, but unfortunately we cannot use the schema that is generated by the default JSON converter - it returns the struct type, which is not supported by the Hudi utility. In order to handle this issue, we created a schema with the record type using the Confluent Avro converter and used it after saving on S3. However, as we aimed to manage a long-running process, generating a schema manually was not an optimal solution because, for example, we\u0026rsquo;re not able to handle schema evolution effectively. In this post, we\u0026rsquo;ll discuss an improved architecture that makes use of a schema registry that resides outside the Kafka cluster and allows the producers and consumers to reference the schemas externally.\nPart 1 Local Development (this post) Part 2 MSK Deployment Architecture Below shows an updated CDC architecture with a schema registry. The Debezium connector talks to the schema registry first and checks if the schema is available. If it doesn\u0026rsquo;t exist, it is registered and cached in the schema registry. Then the producer serializes the data with the schema and sends it to the topic with the schema ID. When the sink connector consumes the message, it\u0026rsquo;ll read the schema with the ID and deserializes it. The schema registry uses a PostgreSQL database as an artifact store where multiple versions of schemas are kept. In this post, we\u0026rsquo;ll build it locally using Docker Compose.\nLocal Services Earlier we discussed a local development environment for a Change Data Capture (CDC) solution using the Confluent platform - see this post for details. While it provides a quick and easy way of developing Kafka locally, it doesn\u0026rsquo;t seem to match MSK well. For example, its docker image already includes the Avro converter and schema registry client libraries. Because of that, while Kafka connectors with Avro serialization work in the platform without modification, they\u0026rsquo;ll fail on MSK Connect if they are deployed with the same configuration. Therefore, it\u0026rsquo;ll be better to use docker images from other open source projects instead of the Confluent platform while we can still use docker-compose to build a local development environment. The associating docker-compose file can be found in the GitHub repository for this post.\nKafka Cluster We can create a single node Kafka cluster as a docker-compose service with Zookeeper, which is used to store metadata about the cluster. The Bitnami images are used to build the services as shown below.\n1# docker-compose.yml 2services: 3 zookeeper: 4 image: bitnami/zookeeper:3.7.0 5 container_name: zookeeper 6 ports: 7 - \u0026#34;2181:2181\u0026#34; 8 environment: 9 - ALLOW_ANONYMOUS_LOGIN=yes 10 kafka: 11 image: bitnami/kafka:2.8.1 12 container_name: kafka 13 ports: 14 - \u0026#34;9092:9092\u0026#34; 15 environment: 16 - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181 17 - ALLOW_PLAINTEXT_LISTENER=yes 18 depends_on: 19 - zookeeper 20 ... Kafka Connect The same Bitnami image can be used to create a Kafka connect service. It is set to run in the distributed mode so that multiple connectors can be deployed together. Three Kafka connector sources are mapped to the /opt/connectors folder of the container - Debezium, Confluent S3 Sink and Voluble. Note that this folder is added to the plugin path of the connector configuration file (connect-distributed.properties) so that they can be discovered when it is requested to create those. Also, a script is created to download the connector sources (and related libraries) to the connect/local/src folder - it\u0026rsquo;ll be illustrated below. Finally, my AWS account is configured by AWS SSO so that temporary AWS credentials are passed as environment variables - it is necessary for the S3 sink connector.\n1# docker-compose.yml 2services: 3 ... 4 kafka-connect: 5 image: bitnami/kafka:2.8.1 6 container_name: connect 7 command: \u0026gt; 8 /opt/bitnami/kafka/bin/connect-distributed.sh 9 /opt/bitnami/kafka/config/connect-distributed.properties 10 ports: 11 - \u0026#34;8083:8083\u0026#34; 12 environment: 13 AWS_ACCESS_KEY_ID: $AWS_ACCESS_KEY_ID 14 AWS_SECRET_ACCESS_KEY: $AWS_SECRET_ACCESS_KEY 15 AWS_SESSION_TOKEN: $AWS_SESSION_TOKEN 16 volumes: 17 - \u0026#34;./connect/local/src/debezium-connector-postgres:/opt/connectors/debezium-postgres\u0026#34; 18 - \u0026#34;./connect/local/src/confluent-s3/lib:/opt/connectors/confluent-s3\u0026#34; 19 - \u0026#34;./connect/local/src/voluble/lib:/opt/connectors/voluble\u0026#34; 20 - \u0026#34;./connect/local/config/connect-distributed.properties:/opt/bitnami/kafka/config/connect-distributed.properties\u0026#34; 21 depends_on: 22 - zookeeper 23 - kafka 24 ... When the script (download-connectors.sh) runs, it downloads connector sources from Maven Central and Confluent Hub and decompresses. And the Kafka Connect Avro Converter is packaged together with connector sources, which is necessary for Avro serialization of messages and schema registry integration. Note that, if we run our own Kafka connect, we\u0026rsquo;d add it to one of the folders of the connect service and update its plugin path to enable class discovery. However, we don\u0026rsquo;t have such control on MSK Connect, and we should add the converter source to the individual connectors.\n1# connect/local/download-connectors.sh 2#!/usr/bin/env bash 3 4echo \u0026#34;Add avro converter? (Y/N)\u0026#34; 5read WITH_AVRO 6 7SCRIPT_DIR=\u0026#34;$(cd $(dirname \u0026#34;$0\u0026#34;); pwd)\u0026#34; 8 9SRC_PATH=${SCRIPT_DIR}/src 10rm -rf ${SCRIPT_DIR}/src \u0026amp;\u0026amp; mkdir -p ${SRC_PATH} 11 12## Debezium Source Connector 13echo \u0026#34;downloading debezium postgres connector...\u0026#34; 14DOWNLOAD_URL=https://repo1.maven.org/maven2/io/debezium/debezium-connector-postgres/1.8.1.Final/debezium-connector-postgres-1.8.1.Final-plugin.tar.gz 15 16curl -S -L ${DOWNLOAD_URL} | tar -C ${SRC_PATH} --warning=no-unknown-keyword -xzf - 17 18## Confluent S3 Sink Connector 19echo \u0026#34;downloading confluent s3 connector...\u0026#34; 20DOWNLOAD_URL=https://d1i4a15mxbxib1.cloudfront.net/api/plugins/confluentinc/kafka-connect-s3/versions/10.0.5/confluentinc-kafka-connect-s3-10.0.5.zip 21 22curl ${DOWNLOAD_URL} -o ${SRC_PATH}/confluent.zip \\ 23 \u0026amp;\u0026amp; unzip -qq ${SRC_PATH}/confluent.zip -d ${SRC_PATH} \\ 24 \u0026amp;\u0026amp; rm ${SRC_PATH}/confluent.zip \\ 25 \u0026amp;\u0026amp; mv ${SRC_PATH}/$(ls ${SRC_PATH} | grep confluentinc-kafka-connect-s3) ${SRC_PATH}/confluent-s3 26 27## Voluble Source Connector 28echo \u0026#34;downloading voluble connector...\u0026#34; 29DOWNLOAD_URL=https://d1i4a15mxbxib1.cloudfront.net/api/plugins/mdrogalis/voluble/versions/0.3.1/mdrogalis-voluble-0.3.1.zip 30 31curl ${DOWNLOAD_URL} -o ${SRC_PATH}/voluble.zip \\ 32 \u0026amp;\u0026amp; unzip -qq ${SRC_PATH}/voluble.zip -d ${SRC_PATH} \\ 33 \u0026amp;\u0026amp; rm ${SRC_PATH}/voluble.zip \\ 34 \u0026amp;\u0026amp; mv ${SRC_PATH}/$(ls ${SRC_PATH} | grep mdrogalis-voluble) ${SRC_PATH}/voluble 35 36if [ ${WITH_AVRO} == \u0026#34;Y\u0026#34; ]; then 37 echo \u0026#34;downloading kafka connect avro converter...\u0026#34; 38 DOWNLOAD_URL=https://d1i4a15mxbxib1.cloudfront.net/api/plugins/confluentinc/kafka-connect-avro-converter/versions/6.0.3/confluentinc-kafka-connect-avro-converter-6.0.3.zip 39 40 curl ${DOWNLOAD_URL} -o ${SRC_PATH}/avro.zip \\ 41 \u0026amp;\u0026amp; unzip -qq ${SRC_PATH}/avro.zip -d ${SRC_PATH} \\ 42 \u0026amp;\u0026amp; rm ${SRC_PATH}/avro.zip \\ 43 \u0026amp;\u0026amp; mv ${SRC_PATH}/$(ls ${SRC_PATH} | grep confluentinc-kafka-connect-avro-converter) ${SRC_PATH}/avro 44 45 echo \u0026#34;copying to connectors...\u0026#34; 46 cp -r ${SRC_PATH}/avro/lib/* ${SRC_PATH}/debezium-connector-postgres 47 cp -r ${SRC_PATH}/avro/lib/* ${SRC_PATH}/confluent-s3/lib 48 cp -r ${SRC_PATH}/avro/lib/* ${SRC_PATH}/voluble/lib 49fi Schema Registry The Confluent schema registry uses Kafka as the storage backend, and it is not sure whether it supports IAM authentication. Therefore, the Apicurio Registry is used instead as it supports a SQL database as storage as well. It provides multiple APIs and one of them is compatible with the Confluent schema registry (/apis/ccompat/v6). We\u0026rsquo;ll use this API as we plan to use the Confluent version of Avro converter and schema registry client. The PostgreSQL database will be used for the artifact store of the registry as well as the source database of the Debezium connector. For Debezium, the pgoutput plugin is used so that logical replication is enabled (wal_level=logical). The NorthWind database is used as the source database - see this post for details. The registry service expects database connection details from environment variables, and it is set to wait until the source database is up and running.\n1# docker-compose.yml 2services: 3 ... 4 postgres: 5 image: postgres:13 6 container_name: postgres 7 command: [\u0026#34;postgres\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;wal_level=logical\u0026#34;] 8 ports: 9 - 5432:5432 10 volumes: 11 - ./connect/data/sql:/docker-entrypoint-initdb.d 12 environment: 13 - POSTGRES_DB=main 14 - POSTGRES_USER=master 15 - POSTGRES_PASSWORD=password 16 registry: 17 image: apicurio/apicurio-registry-sql:2.2.0.Final 18 container_name: registry 19 command: bash -c \u0026#39;while !\u0026lt;/dev/tcp/postgres/5432; do sleep 1; done; /usr/local/s2i/run\u0026#39; 20 ports: 21 - \u0026#34;9090:8080\u0026#34; 22 environment: 23 REGISTRY_DATASOURCE_URL: \u0026#34;jdbc:postgresql://postgres/main?currentSchema=registry\u0026#34; 24 REGISTRY_DATASOURCE_USERNAME: master 25 REGISTRY_DATASOURCE_PASSWORD: password 26 depends_on: 27 - zookeeper 28 - kafka 29 - postgres 30 ... Once started, we see the Confluent schema registry compatible API from the API list, and we\u0026rsquo;ll use it for creating Kafka connectors.\nKafka UI Having a good user interface can make development much easier and pleasant. The Kafka UI is an open source application where we can monitor and manage Kafka brokers, related objects and resources. It supports MSK IAM authentication as well, and it is a good choice for developing applications on MSK. It allows adding details of one or more Kafka clusters as environment variables. We only have a single Kafka cluster and details of the cluster and related resources are added as shown below.\n1# docker-compose.yml 2services: 3 ... 4 kafka-ui: 5 image: provectuslabs/kafka-ui:0.3.3 6 container_name: kafka-ui 7 ports: 8 - \u0026#34;8080:8080\u0026#34; 9 environment: 10 KAFKA_CLUSTERS_0_NAME: local 11 KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:9092 12 KAFKA_CLUSTERS_0_ZOOKEEPER: zookeeper:2181 13 KAFKA_CLUSTERS_0_SCHEMAREGISTRY: http://registry:8080/apis/ccompat/v6 14 KAFKA_CLUSTERS_0_KAFKACONNECT_0_NAME: local 15 KAFKA_CLUSTERS_0_KAFKACONNECT_0_ADDRESS: http://kafka-connect:8083 16 depends_on: 17 - zookeeper 18 - kafka The UI is quite intuitive, and we can monitor (and manage) the Kafka cluster and related objects/resources comprehensively.\nCreate Connectors The Debezium source connector can be created as shown below. The configuration details can be found in one of the earlier posts. Here the main difference is the key and value converters are set to the Confluent Avro converter in order for the key and value to be serialized into the Avro format. Note the Confluent compatible API is added to the registry URL.\n1curl -i -X POST -H \u0026#34;Accept:application/json\u0026#34; -H \u0026#34;Content-Type:application/json\u0026#34; \\ 2 http://localhost:8083/connectors/ \\ 3 -d \u0026#39;{ 4 \u0026#34;name\u0026#34;: \u0026#34;orders-source\u0026#34;, 5 \u0026#34;config\u0026#34;: { 6 \u0026#34;connector.class\u0026#34;: \u0026#34;io.debezium.connector.postgresql.PostgresConnector\u0026#34;, 7 \u0026#34;tasks.max\u0026#34;: \u0026#34;1\u0026#34;, 8 \u0026#34;plugin.name\u0026#34;: \u0026#34;pgoutput\u0026#34;, 9 \u0026#34;publication.name\u0026#34;: \u0026#34;cdc_publication\u0026#34;, 10 \u0026#34;slot.name\u0026#34;: \u0026#34;orders\u0026#34;, 11 \u0026#34;database.hostname\u0026#34;: \u0026#34;postgres\u0026#34;, 12 \u0026#34;database.port\u0026#34;: \u0026#34;5432\u0026#34;, 13 \u0026#34;database.user\u0026#34;: \u0026#34;master\u0026#34;, 14 \u0026#34;database.password\u0026#34;: \u0026#34;password\u0026#34;, 15 \u0026#34;database.dbname\u0026#34;: \u0026#34;main\u0026#34;, 16 \u0026#34;database.server.name\u0026#34;: \u0026#34;ord\u0026#34;, 17 \u0026#34;schema.include\u0026#34;: \u0026#34;ods\u0026#34;, 18 \u0026#34;table.include.list\u0026#34;: \u0026#34;ods.cdc_events\u0026#34;, 19 \u0026#34;key.converter\u0026#34;: \u0026#34;io.confluent.connect.avro.AvroConverter\u0026#34;, 20 \u0026#34;key.converter.schema.registry.url\u0026#34;: \u0026#34;http://registry:8080/apis/ccompat/v6\u0026#34;, 21 \u0026#34;value.converter\u0026#34;: \u0026#34;io.confluent.connect.avro.AvroConverter\u0026#34;, 22 \u0026#34;value.converter.schema.registry.url\u0026#34;: \u0026#34;http://registry:8080/apis/ccompat/v6\u0026#34;, 23 \u0026#34;transforms\u0026#34;: \u0026#34;unwrap\u0026#34;, 24 \u0026#34;transforms.unwrap.type\u0026#34;: \u0026#34;io.debezium.transforms.ExtractNewRecordState\u0026#34;, 25 \u0026#34;transforms.unwrap.drop.tombstones\u0026#34;: \u0026#34;false\u0026#34;, 26 \u0026#34;transforms.unwrap.delete.handling.mode\u0026#34;: \u0026#34;rewrite\u0026#34;, 27 \u0026#34;transforms.unwrap.add.fields\u0026#34;: \u0026#34;op,db,table,schema,lsn,source.ts_ms\u0026#34; 28 } 29 }\u0026#39; The Confluent S3 sink connector is used instead of the Lenses S3 sink connector because the Lenses connector doesn\u0026rsquo;t work with the Kafka Connect Avro Converter. Here the key and value converters are updated to the Confluent Avro converter with the corresponding schema registry URL.\n1curl -i -X POST -H \u0026#34;Accept:application/json\u0026#34; -H \u0026#34;Content-Type:application/json\u0026#34; \\ 2 http://localhost:8083/connectors/ \\ 3 -d \u0026#39;{ 4 \u0026#34;name\u0026#34;: \u0026#34;orders-sink\u0026#34;, 5 \u0026#34;config\u0026#34;: { 6 \u0026#34;connector.class\u0026#34;: \u0026#34;io.confluent.connect.s3.S3SinkConnector\u0026#34;, 7 \u0026#34;storage.class\u0026#34;: \u0026#34;io.confluent.connect.s3.storage.S3Storage\u0026#34;, 8 \u0026#34;format.class\u0026#34;: \u0026#34;io.confluent.connect.s3.format.avro.AvroFormat\u0026#34;, 9 \u0026#34;tasks.max\u0026#34;: \u0026#34;1\u0026#34;, 10 \u0026#34;topics\u0026#34;:\u0026#34;ord.ods.cdc_events\u0026#34;, 11 \u0026#34;s3.bucket.name\u0026#34;: \u0026#34;analytics-data-590312749310-ap-southeast-2\u0026#34;, 12 \u0026#34;s3.region\u0026#34;: \u0026#34;ap-southeast-2\u0026#34;, 13 \u0026#34;flush.size\u0026#34;: \u0026#34;100\u0026#34;, 14 \u0026#34;rotate.schedule.interval.ms\u0026#34;: \u0026#34;60000\u0026#34;, 15 \u0026#34;timezone\u0026#34;: \u0026#34;Australia/Sydney\u0026#34;, 16 \u0026#34;partitioner.class\u0026#34;: \u0026#34;io.confluent.connect.storage.partitioner.DefaultPartitioner\u0026#34;, 17 \u0026#34;key.converter\u0026#34;: \u0026#34;io.confluent.connect.avro.AvroConverter\u0026#34;, 18 \u0026#34;key.converter.schema.registry.url\u0026#34;: \u0026#34;http://registry:8080/apis/ccompat/v6\u0026#34;, 19 \u0026#34;value.converter\u0026#34;: \u0026#34;io.confluent.connect.avro.AvroConverter\u0026#34;, 20 \u0026#34;value.converter.schema.registry.url\u0026#34;: \u0026#34;http://registry:8080/apis/ccompat/v6\u0026#34;, 21 \u0026#34;errors.log.enable\u0026#34;: \u0026#34;true\u0026#34; 22 } 23 }\u0026#39; Once the source connector is created, we can check that the key and value schemas are created as shown below. Note we can check the details of the schemas by clicking the relevant items.\nAs we added the schema registry URL as an environment variable, we see the records (key and value) are properly deserialized within the UI.\nSchema Evolution The schema registry keeps multiple versions of schemas and we can check it by adding a column to the table and updating records.\n1--// add a column with a default value 2ALTER TABLE ods.cdc_events 3 ADD COLUMN employee_id int DEFAULT -1; 4 5--// update employee ID 6UPDATE ods.cdc_events 7 SET employee_id = (employee -\u0026gt;\u0026gt; \u0026#39;employee_id\u0026#39;)::INT 8WHERE customer_id = \u0026#39;VINET\u0026#39; Once the above queries are executed, we see a new version is added to the topic\u0026rsquo;s value schema, and it includes the new field.\nSummary In this post, we discussed an improved architecture of a Change Data Capture (CDC) solution with a schema registry. A local development environment is set up using Docker Compose. The Debezium and Confluent S3 connectors are deployed with the Confluent Avro converter and the Apicurio registry is used as the schema registry service. A quick example is shown to illustrate how schema evolution can be managed by the schema registry. In the next post, it\u0026rsquo;ll be deployed to AWS mainly using MSK Connect, Aurora PostgreSQL and ECS.\n","date":"March 7, 2022","img":"/blog/2022-03-07-schema-registry-part1/featured.png","lang":"en","langName":"English","largeImg":"/blog/2022-03-07-schema-registry-part1/featured_hu55ef0aeef397b55768e961367d004bbc_59689_500x0_resize_box_3.png","permalink":"/blog/2022-03-07-schema-registry-part1/","series":[{"title":"Integrate Schema Registry with MSK Connect","url":"/series/integrate-schema-registry-with-msk-connect/"}],"smallImg":"/blog/2022-03-07-schema-registry-part1/featured_hu55ef0aeef397b55768e961367d004bbc_59689_180x0_resize_box_3.png","tags":[{"title":"AWS","url":"/tags/aws/"},{"title":"Amazon MSK","url":"/tags/amazon-msk/"},{"title":"Amazon MSK Connect","url":"/tags/amazon-msk-connect/"},{"title":"Apache Kafka","url":"/tags/apache-kafka/"},{"title":"Docker","url":"/tags/docker/"},{"title":"Docker Compose","url":"/tags/docker-compose/"}],"timestamp":1646611200,"title":"Use External Schema Registry With MSK Connect – Part 1 Local Development"},{"categories":[{"title":"Engineering","url":"/categories/engineering/"}],"content":"When I wrote my data lake demo series (part 1, part 2 and part 3) recently, I used an Aurora PostgreSQL, MSK and EMR cluster. All of them were deployed to private subnets and dedicated infrastructure was created using CloudFormation. Using the infrastructure as code (IaC) tool helped a lot, but it resulted in creating 7 CloudFormation stacks, which was a bit harder to manage in the end. Then I looked into how to simplify building infrastructure and managing resources on AWS and decided to use Terraform instead. I find it has useful constructs (e.g. meta-arguments) to make it simpler to create and manage resources. It also has a wide range of useful modules that facilitate development significantly. In this post, we’ll build an infrastructure for development on AWS with Terraform. A VPN server will also be included in order to improve developer experience by accessing resources in private subnets from developer machines.\nArchitecture The infrastructure that we’ll discuss in this post is shown below. The database is deployed in a private subnet, and it is not possible to access it from the developer machine. We can construct a PC-to-PC VPN with SoftEther VPN. The VPN server runs in a public subnet, and it is managed by an autoscaling group where only a single instance will be maintained. An elastic IP address is associated by a bootstrap script so that its public IP doesn\u0026rsquo;t change even if the EC2 instance is recreated. We can add users with the server manager program, and they can access the server with the client program. Access from the VPN server to the database is allowed by adding an inbound rule where the source security group ID is set to the VPN server’s security group ID. Note that another option is AWS Client VPN, but it is way more expensive. We’ll create 2 private subnets, and it’ll cost $0.30/hour for endpoint association in the Sydney region. It also charges $0.05/hour for each connection and the minimum charge will be $0.35/hour. On the other hand, the SorftEther VPN server runs in the t3.nano instance and its cost is only $0.0066/hour.\nEven developing a single database can result in a stack of resources and Terraform can be of great help to create and manage those resources. Also, VPN can improve developer experience significantly as it helps access them from developer machines. In this post, it’ll be illustrated how to access a database but access to other resources such as MSK, EMR, ECS and EKS can also be made.\nInfrastructure Terraform can be installed in multiple ways and the CLI has intuitive commands to manage AWS infrastructure. Key commands are\ninit - It is used to initialize a working directory containing Terraform configuration files. plan - It creates an execution plan, which lets you preview the changes that Terraform plans to make to your infrastructure. apply - It executes the actions proposed in a Terraform plan. destroy - It is a convenient way to destroy all remote objects managed by a particular Terraform configuration. The GitHub repository for this post has the following directory structure. Terraform resources are grouped into 4 files, and they’ll be discussed further below. The remaining files are supporting elements and their details can be found in the language reference.\n1$ tree 2. 3├── README.md 4├── _data.tf 5├── _outputs.tf 6├── _providers.tf 7├── _variables.tf 8├── aurora.tf 9├── keypair.tf 10├── scripts 11│ └── bootstrap.sh 12├── vpc.tf 13└── vpn.tf 14 151 directory, 10 files VPC We can use the AWS VPC module to construct a VPC. A Terraform module is a container for multiple resources, and it makes it easier to manage related resources. A VPC with 2 availability zones is defined and private/public subnets are configured to each of them. Optionally a NAT gateway is added only to a single availability zone.\n1# vpc.tf 2module \u0026#34;vpc\u0026#34; { 3 source = \u0026#34;terraform-aws-modules/vpc/aws\u0026#34; 4 5 name = \u0026#34;${local.resource_prefix}-vpc\u0026#34; 6 cidr = \u0026#34;10.${var.class_b}.0.0/16\u0026#34; 7 8 azs = [\u0026#34;${var.aws_region}a\u0026#34;, \u0026#34;${var.aws_region}b\u0026#34;] 9 private_subnets = [\u0026#34;10.${var.class_b}.0.0/19\u0026#34;, \u0026#34;10.${var.class_b}.32.0/19\u0026#34;] 10 public_subnets = [\u0026#34;10.${var.class_b}.64.0/19\u0026#34;, \u0026#34;10.${var.class_b}.96.0/19\u0026#34;] 11 12 enable_nat_gateway = true 13 single_nat_gateway = true 14 one_nat_gateway_per_az = false 15} Key Pair An optional key pair is created. It can be used to access an EC2 instance via SSH. The PEM file will be saved to the key-pair folder once created.\n1# keypair.tf 2resource \u0026#34;tls_private_key\u0026#34; \u0026#34;pk\u0026#34; { 3 count = var.key_pair_create ? 1 : 0 4 algorithm = \u0026#34;RSA\u0026#34; 5 rsa_bits = 4096 6} 7 8resource \u0026#34;aws_key_pair\u0026#34; \u0026#34;key_pair\u0026#34; { 9 count = var.key_pair_create ? 1 : 0 10 key_name = \u0026#34;${local.resource_prefix}-key\u0026#34; 11 public_key = tls_private_key.pk[0].public_key_openssh 12} 13 14resource \u0026#34;local_file\u0026#34; \u0026#34;pem_file\u0026#34; { 15 count = var.key_pair_create ? 1 : 0 16 filename = pathexpand(\u0026#34;${path.module}/key-pair/${local.resource_prefix}-key.pem\u0026#34;) 17 file_permission = \u0026#34;0400\u0026#34; 18 sensitive_content = tls_private_key.pk[0].private_key_pem 19} VPN The AWS Auto Scaling Group (ASG) module is used to manage the SoftEther VPN server. The ASG maintains a single EC2 instance in one of the public subnets. The user data script (bootstrap.sh) is configured to run at launch and it’ll be discussed below. Note that there are other resources that are necessary to make the VPN server to work correctly and those can be found in the vpn.tf. Also note that the VPN resource requires a number of configuration values. While most of them have default values or are automatically determined, the IPsec Pre-Shared key (vpn_psk) and administrator password (admin_password) do not have default values. They need to be specified while running the plan, apply and _destroy _commands. Finally, if the variable vpn_limit_ingress is set to true, the inbound rules of the VPN security group is limited to the running machine’s IP address.\n1# _variables.tf 2variable \u0026#34;vpn_create\u0026#34; { 3 description = \u0026#34;Whether to create a VPN instance\u0026#34; 4 default = true 5} 6 7variable \u0026#34;vpn_limit_ingress\u0026#34; { 8 description = \u0026#34;Whether to limit the CIDR block of VPN security group inbound rules.\u0026#34; 9 default = true 10} 11 12variable \u0026#34;vpn_use_spot\u0026#34; { 13 description = \u0026#34;Whether to use spot or on-demand EC2 instance\u0026#34; 14 default = false 15} 16 17variable \u0026#34;vpn_psk\u0026#34; { 18 description = \u0026#34;The IPsec Pre-Shared Key\u0026#34; 19 type = string 20 sensitive = true 21} 22 23variable \u0026#34;admin_password\u0026#34; { 24 description = \u0026#34;SoftEther VPN admin / database master password\u0026#34; 25 type = string 26 sensitive = true 27} 28 29locals { 30 ... 31 local_ip_address = \u0026#34;${chomp(data.http.local_ip_address.body)}/32\u0026#34; 32 vpn_ingress_cidr = var.vpn_limit_ingress ? local.local_ip_address : \u0026#34;0.0.0.0/0\u0026#34; 33 vpn_spot_override = [ 34 { instance_type: \u0026#34;t3.nano\u0026#34; }, 35 { instance_type: \u0026#34;t3a.nano\u0026#34; }, 36 ] 37} 38 39# vpn.tf 40module \u0026#34;vpn\u0026#34; { 41 source = \u0026#34;terraform-aws-modules/autoscaling/aws\u0026#34; 42 count = var.vpn_create ? 1 : 0 43 44 name = \u0026#34;${local.resource_prefix}-vpn-asg\u0026#34; 45 46 key_name = var.key_pair_create ? aws_key_pair.key_pair[0].key_name : null 47 vpc_zone_identifier = module.vpc.public_subnets 48 min_size = 1 49 max_size = 1 50 desired_capacity = 1 51 52 image_id = data.aws_ami.amazon_linux_2.id 53 instance_type = element([for s in local.vpn_spot_override: s.instance_type], 0) 54 security_groups = [aws_security_group.vpn[0].id] 55 iam_instance_profile_arn = aws_iam_instance_profile.vpn[0].arn 56 57 # Launch template 58 create_lt = true 59 update_default_version = true 60 61 user_data_base64 = base64encode(join(\u0026#34;\\n\u0026#34;, [ 62 \u0026#34;#cloud-config\u0026#34;, 63 yamlencode({ 64 # https://cloudinit.readthedocs.io/en/latest/topics/modules.html 65 write_files : [ 66 { 67 path : \u0026#34;/opt/vpn/bootstrap.sh\u0026#34;, 68 content : templatefile(\u0026#34;${path.module}/scripts/bootstrap.sh\u0026#34;, { 69 aws_region = var.aws_region, 70 allocation_id = aws_eip.vpn[0].allocation_id, 71 vpn_psk = var.vpn_psk, 72 admin_password = var.admin_password 73 }), 74 permissions : \u0026#34;0755\u0026#34;, 75 } 76 ], 77 runcmd : [ 78 [\u0026#34;/opt/vpn/bootstrap.sh\u0026#34;], 79 ], 80 }) 81 ])) 82 83 # Mixed instances 84 use_mixed_instances_policy = true 85 mixed_instances_policy = { 86 instances_distribution = { 87 on_demand_base_capacity = var.vpn_use_spot ? 0 : 1 88 on_demand_percentage_above_base_capacity = var.vpn_use_spot ? 0 : 100 89 spot_allocation_strategy = \u0026#34;capacity-optimized\u0026#34; 90 } 91 override = local.vpn_spot_override 92 } 93 94 tags_as_map = { 95 \u0026#34;Name\u0026#34; = \u0026#34;${local.resource_prefix}-vpn-asg\u0026#34; 96 } 97} 98 99resource \u0026#34;aws_eip\u0026#34; \u0026#34;vpn\u0026#34; { 100 count = var.vpn_create ? 1 : 0 101 tags = { 102 \u0026#34;Name\u0026#34; = \u0026#34;${local.resource_prefix}-vpn-eip\u0026#34; 103 } 104} 105 106... The bootstrap script associates the elastic IP address followed by starting the SoftEther VPN server by a Docker container. It accepts the pre-shared key (vpn_psk) and administrator password (admin_password) as environment variables. Also, the Virtual Hub name is set to DEFAULT.\n1# scripts/bootstrap.sh 2#!/bin/bash -ex 3 4## Allocate elastic IP and disable source/destination checks 5TOKEN=$(curl --silent --max-time 60 -X PUT http://169.254.169.254/latest/api/token -H \u0026#34;X-aws-ec2-metadata-token-ttl-seconds: 30\u0026#34;) 6INSTANCEID=$(curl --silent --max-time 60 -H \u0026#34;X-aws-ec2-metadata-token: $TOKEN\u0026#34; http://169.254.169.254/latest/meta-data/instance-id) 7aws --region ${aws_region} ec2 associate-address --instance-id $INSTANCEID --allocation-id ${allocation_id} 8aws --region ${aws_region} ec2 modify-instance-attribute --instance-id $INSTANCEID --source-dest-check \u0026#34;{\\\u0026#34;Value\\\u0026#34;: false}\u0026#34; 9 10## Start SoftEther VPN server 11yum update -y \u0026amp;\u0026amp; yum install docker -y 12systemctl enable docker.service \u0026amp;\u0026amp; systemctl start docker.service 13 14docker pull siomiz/softethervpn:debian 15docker run -d \\ 16 --cap-add NET_ADMIN \\ 17 --name softethervpn \\ 18 --restart unless-stopped \\ 19 -p 500:500/udp -p 4500:4500/udp -p 1701:1701/tcp -p 1194:1194/udp -p 5555:5555/tcp -p 443:443/tcp \\ 20 -e PSK=${vpn_psk} \\ 21 -e SPW=${admin_password} \\ 22 -e HPW=DEFAULT \\ 23 siomiz/softethervpn:debian Database An Aurora PostgreSQL cluster is created using the AWS RDS Aurora module. It is set to have only a single instance and is deployed to a private subnet. Note that a security group (vpn_access) is created that allows access from the VPN server, and it is added to vpc_security_group_ids.\n1# aurora.tf 2module \u0026#34;aurora\u0026#34; { 3 source = \u0026#34;terraform-aws-modules/rds-aurora/aws\u0026#34; 4 5 name = \u0026#34;${local.resource_prefix}-db-cluster\u0026#34; 6 engine = \u0026#34;aurora-postgresql\u0026#34; 7 engine_version = \u0026#34;13\u0026#34; 8 auto_minor_version_upgrade = false 9 10 instances = { 11 1 = { 12 instance_class = \u0026#34;db.t3.medium\u0026#34; 13 } 14 } 15 16 vpc_id = module.vpc.vpc_id 17 db_subnet_group_name = aws_db_subnet_group.aurora.id 18 create_db_subnet_group = false 19 create_security_group = true 20 vpc_security_group_ids = [aws_security_group.vpn_access.id] 21 22 iam_database_authentication_enabled = false 23 create_random_password = false 24 master_password = var.admin_password 25 database_name = local.database_name 26 27 apply_immediately = true 28 skip_final_snapshot = true 29 30 db_cluster_parameter_group_name = aws_rds_cluster_parameter_group.aurora.id 31 enabled_cloudwatch_logs_exports = [\u0026#34;postgresql\u0026#34;] 32 33 tags = { 34 Name = \u0026#34;${local.resource_prefix}-db-cluster\u0026#34; 35 } 36} 37 38resource \u0026#34;aws_db_subnet_group\u0026#34; \u0026#34;aurora\u0026#34; { 39 name = \u0026#34;${local.resource_prefix}-db-subnet-group\u0026#34; 40 subnet_ids = module.vpc.private_subnets 41 42 tags = { 43 Name = \u0026#34;${local.resource_prefix}-db-subnet-group\u0026#34; 44 } 45} 46 47... 48 49resource \u0026#34;aws_security_group\u0026#34; \u0026#34;vpn_access\u0026#34; { 50 name = \u0026#34;${local.resource_prefix}-db-security-group\u0026#34; 51 vpc_id = module.vpc.vpc_id 52 53 lifecycle { 54 create_before_destroy = true 55 } 56} 57 58resource \u0026#34;aws_security_group_rule\u0026#34; \u0026#34;aurora_vpn_inbound\u0026#34; { 59 count = var.vpn_create ? 1 : 0 60 type = \u0026#34;ingress\u0026#34; 61 description = \u0026#34;VPN access\u0026#34; 62 security_group_id = aws_security_group.vpn_access.id 63 protocol = \u0026#34;tcp\u0026#34; 64 from_port = \u0026#34;5432\u0026#34; 65 to_port = \u0026#34;5432\u0026#34; 66 source_security_group_id = aws_security_group.vpn[0].id 67} VPN Configuration Both the VPN Server Manager and Client can be obtained from the download centre. The server and client configuration are illustrated below.\nVPN Server We can begin with adding a new setting.\nWe need to fill in the input fields in the red boxes below. It’s possible to use the elastic IP address as the host name and the administrator password should match to what is used for Terraform.\nThen we can make a connection to the server by clicking the connect button.\nIf it’s the first attempt, we’ll see the following pop-up message and we can click yes to set up the IPsec.\nIn the dialog, we just need to enter the IPsec Pre-Shared key and click ok.\nOnce a connection is made successfully, we can manage the Virtual Hub by clicking the manage virtual hub button. Note that we created a Virtual Hub named DEFAULT and the session will be established on that Virtual Hub.\nWe can create a new user by clicking the manage users button.\nAnd clicking the new button.\nFor simplicity, we can use Password Authentication as the auth type and enter the username and password.\nA new user is created, and we can use the credentials on the client program to make a connection to the server.\nVPN Client We can add a VPN connection by clicking the menu shown below.\nWe’ll need to create a Virtual Network Adapter and should click the yes button.\nIn the new dialog, we can add the adapter name and hit ok. Note we should have the administrator privilege to create a new adapter.\nThen a new dialog box will be shown. We can add a connection by entering the input fields in the red boxes below. The VPN server details should match to what are created by Terraform and the user credentials that are created in the previous section can be used.\nOnce a connection is added, we can make a connection to the VPN server by right-clicking the item and clicking the connect menu.\nWe can see that the status is changed into connected.\nOnce the VPN server is connected, we can access the database that is deployed in the private subnet. A connection is tested by a database client, and it is shown that the connection is successful.\nSummary In this post, we discussed how to set up a development infrastructure on AWS with Terraform. Terraform is used as an effective way of managing resources on AWS. An Aurora PostgreSQL cluster is created in a private subnet and SoftEther VPN is configured to access the database from the developer machine.\n","date":"February 6, 2022","img":"/blog/2022-02-06-dev-infra-terraform/featured.png","lang":"en","langName":"English","largeImg":"/blog/2022-02-06-dev-infra-terraform/featured_hube8ddf9cf5c302b7f1d964b6331dc37c_46253_500x0_resize_box_3.png","permalink":"/blog/2022-02-06-dev-infra-terraform/","series":[],"smallImg":"/blog/2022-02-06-dev-infra-terraform/featured_hube8ddf9cf5c302b7f1d964b6331dc37c_46253_180x0_resize_box_3.png","tags":[{"title":"AWS","url":"/tags/aws/"},{"title":"Terraform","url":"/tags/terraform/"},{"title":"VPN","url":"/tags/vpn/"}],"timestamp":1644105600,"title":"Simplify Your Development on AWS With Terraform"},{"categories":[{"title":"Data Engineering","url":"/categories/data-engineering/"}],"content":"EMR on EKS provides a deployment option for Amazon EMR that allows you to automate the provisioning and management of open-source big data frameworks on Amazon EKS. While a wide range of open source big data components are available in EMR on EC2, only Apache Spark is available in EMR on EKS. It is more flexible, however, that applications of different EMR versions can be run in multiple availability zones on either EC2 or Fargate. Also, other types of containerized applications can be deployed on the same EKS cluster. Therefore, if you have or plan to have, for example, Apache Airflow, Apache Superset or Kubeflow as your analytics toolkits, it can be an effective way to manage big data (as well as non-big data) workloads. While Glue is more for ETL, EMR on EKS can also be used for other types of tasks such as machine learning. Moreover, it allows you to build a Spark application, not a Gluish Spark application. For example, while you have to use custom connectors for Hudi or Iceberg for Glue, you can use their native libraries with EMR on EKS. In this post, we\u0026rsquo;ll discuss EMR on EKS with simple and elaborated examples.\nCluster setup and configuration We\u0026rsquo;ll use command line utilities heavily. The following tools are required.\nAWS CLI V2 - it is the official command line interface that enables users to interact with AWS services. eksctl - it is a CLI tool for creating and managing clusters on EKS. kubectl - it is a command line utility for communicating with the cluster API server. Upload preliminary resources to S3 We need supporting files, and they are created/downloaded into the config and manifests folders using a setup script - the script can be found in the project GitHub repository. The generated files will be illustrated below.\n1export OWNER=jaehyeon 2export AWS_REGION=ap-southeast-2 3export CLUSTER_NAME=emr-eks-example 4export EMR_ROLE_NAME=${CLUSTER_NAME}-job-execution 5export S3_BUCKET_NAME=${CLUSTER_NAME}-${AWS_REGION} 6export LOG_GROUP_NAME=/${CLUSTER_NAME} 7 8## run setup script 9# - create config files, sample scripts and download necessary files 10./setup.sh 11 12tree -p config -p manifests 13# config 14# ├── [-rw-r--r--] cdc_events.avsc 15# ├── [-rw-r--r--] cdc_events_s3.properties 16# ├── [-rw-r--r--] driver_pod_template.yml 17# ├── [-rw-r--r--] executor_pod_template.yml 18# ├── [-rw-r--r--] food_establishment_data.csv 19# ├── [-rw-r--r--] health_violations.py 20# └── [-rw-r--r--] hudi-utilities-bundle_2.12-0.10.0.jar 21# manifests 22# ├── [-rw-r--r--] cluster.yaml 23# ├── [-rw-r--r--] nodegroup-spot.yaml 24# └── [-rw-r--r--] nodegroup.yaml We\u0026rsquo;ll configure logging on S3 and CloudWatch so that a S3 bucket and CloudWatch log group are created. Also a Glue database is created as I encountered an error to create a Glue table when the database doesn\u0026rsquo;t exist. Finally the files in the config folder are uploaded to S3.\n1#### create S3 bucket/log group/glue database and upload files to S3 2aws s3 mb s3://${S3_BUCKET_NAME} 3aws logs create-log-group --log-group-name=${LOG_GROUP_NAME} 4aws glue create-database --database-input \u0026#39;{\u0026#34;Name\u0026#34;: \u0026#34;datalake\u0026#34;}\u0026#39; 5 6## upload files to S3 7for f in $(ls ./config/) 8 do 9 aws s3 cp ./config/${f} s3://${S3_BUCKET_NAME}/config/ 10 done 11# upload: config/cdc_events.avsc to s3://emr-eks-example-ap-southeast-2/config/cdc_events.avsc 12# upload: config/cdc_events_s3.properties to s3://emr-eks-example-ap-southeast-2/config/cdc_events_s3.properties 13# upload: config/driver_pod_template.yml to s3://emr-eks-example-ap-southeast-2/config/driver_pod_template.yml 14# upload: config/executor_pod_template.yml to s3://emr-eks-example-ap-southeast-2/config/executor_pod_template.yml 15# upload: config/food_establishment_data.csv to s3://emr-eks-example-ap-southeast-2/config/food_establishment_data.csv 16# upload: config/health_violations.py to s3://emr-eks-example-ap-southeast-2/config/health_violations.py 17# upload: config/hudi-utilities-bundle_2.12-0.10.0.jar to s3://emr-eks-example-ap-southeast-2/config/hudi-utilities-bundle_2.12-0.10.0.jar Create EKS cluster and node group We can use either command line options or a config file when creating a cluster or node group using eksctl. We\u0026rsquo;ll use config files and below shows the corresponding config files.\n1# ./config/cluster.yaml 2--- 3apiVersion: eksctl.io/v1alpha5 4kind: ClusterConfig 5 6metadata: 7 name: emr-eks-example 8 region: ap-southeast-2 9 tags: 10 Owner: jaehyeon 11# ./config/nodegroup.yaml 12--- 13apiVersion: eksctl.io/v1alpha5 14kind: ClusterConfig 15 16metadata: 17 name: emr-eks-example 18 region: ap-southeast-2 19 tags: 20 Owner: jaehyeon 21 22managedNodeGroups: 23- name: nodegroup 24 desiredCapacity: 2 25 instanceType: m5.xlarge _eksctl _creates a cluster or node group via CloudFormation. Each command will create a dedicated CloudFormation stack and it\u0026rsquo;ll take about 15 minutes. Also it generates the default kubeconfig file in the $HOME/.kube folder. Once the node group is created, we can check it using the kubectl command.\n1#### create cluster, node group and configure 2eksctl create cluster -f ./manifests/cluster.yaml 3eksctl create nodegroup -f ./manifests/nodegroup.yaml 4 5kubectl get nodes 6# NAME STATUS ROLES AGE VERSION 7# ip-192-168-33-60.ap-southeast-2.compute.internal Ready \u0026lt;none\u0026gt; 5m52s v1.21.5-eks-bc4871b 8# ip-192-168-95-68.ap-southeast-2.compute.internal Ready \u0026lt;none\u0026gt; 5m49s v1.21.5-eks-bc4871b Set up Amazon EMR on EKS As described in the Amazon EMR on EKS development guide, Amazon EKS uses Kubernetes namespaces to divide cluster resources between multiple users and applications. A virtual cluster is a Kubernetes namespace that Amazon EMR is registered with. Amazon EMR uses virtual clusters to run jobs and host endpoints. The following steps are taken in order to set up for EMR on EKS.\nEnable cluster access for Amazon EMR on EKS After creating a Kubernetes namespace for EMR (spark), it is necessary to allow Amazon EMR on EKS to access the namespace. It can be automated by eksctl and specifically the following actions are performed.\nsetting up RBAC authorization by creating a Kubernetes role and binding the role to a Kubernetes user mapping the Kubernetes user to the EMR on EKS service-linked role 1kubectl create namespace spark 2eksctl create iamidentitymapping --cluster ${CLUSTER_NAME} \\ 3 --namespace spark --service-name \u0026#34;emr-containers\u0026#34; While the details of the role and role binding can be found in the development guide, we can see that the aws-auth ConfigMap is updated with the new Kubernetes user.\n1kubectl describe cm aws-auth -n kube-system 2# Name: aws-auth 3# Namespace: kube-system 4# Labels: \u0026lt;none\u0026gt; 5# Annotations: \u0026lt;none\u0026gt; 6 7# Data 8# ==== 9# mapRoles: 10# ---- 11# - groups: 12# - system:bootstrappers 13# - system:nodes 14# rolearn: arn:aws:iam::\u0026lt;AWS-ACCOUNT-ID\u0026gt;:role/eksctl-emr-eks-example-nodegroup-NodeInstanceRole-15J26FPOYH0AL 15# username: system:node:{{EC2PrivateDNSName}} 16# - rolearn: arn:aws:iam::\u0026lt;AWS-ACCOUNT-ID\u0026gt;:role/AWSServiceRoleForAmazonEMRContainers 17# username: emr-containers 18 19# mapUsers: 20# ---- 21# [] 22 23# Events: \u0026lt;none\u0026gt; Create an IAM OIDC identity provider for the EKS cluster We can associate an IAM role with a Kubernetes service account. This service account can then provide AWS permissions to the containers in any pod that uses that service account. Simply put, the service account for EMR will be allowed to assume the EMR job execution role by OIDC federation - see EKS user guide for details. The job execution role will be created below. In order for the OIDC federation to work, we need to set up an IAM OIDC provider for the EKS cluster.\n1eksctl utils associate-iam-oidc-provider \\ 2 --cluster ${CLUSTER_NAME} --approve 3 4aws iam list-open-id-connect-providers --query \u0026#34;OpenIDConnectProviderList[1]\u0026#34; 5# { 6# \u0026#34;Arn\u0026#34;: \u0026#34;arn:aws:iam::\u0026lt;AWS-ACCOUNT-ID\u0026gt;:oidc-provider/oidc.eks.ap-southeast-2.amazonaws.com/id/6F3C18F00D8610088272FEF11013B8C5\u0026#34; 7# } Create a job execution role The following job execution role is created for the examples of this post. The permissions are set up to perform tasks on S3 and Glue. We\u0026rsquo;ll also enable logging on S3 and CloudWatch so that the necessary permissions are added as well.\n1aws iam create-role \\ 2 --role-name ${EMR_ROLE_NAME} \\ 3 --assume-role-policy-document \u0026#39;{ 4 \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, 5 \u0026#34;Statement\u0026#34;: [ 6 { 7 \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, 8 \u0026#34;Principal\u0026#34;: { 9 \u0026#34;Service\u0026#34;: \u0026#34;elasticmapreduce.amazonaws.com\u0026#34; 10 }, 11 \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRole\u0026#34; 12 } 13 ] 14}\u0026#39; 15 16aws iam put-role-policy \\ 17 --role-name ${EMR_ROLE_NAME} \\ 18 --policy-name ${EMR_ROLE_NAME}-policy \\ 19 --policy-document \u0026#39;{ 20 \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, 21 \u0026#34;Statement\u0026#34;: [ 22 { 23 \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, 24 \u0026#34;Action\u0026#34;: [ 25 \u0026#34;s3:PutObject\u0026#34;, 26 \u0026#34;s3:GetObject\u0026#34;, 27 \u0026#34;s3:ListBucket\u0026#34;, 28 \u0026#34;s3:DeleteObject\u0026#34; 29 ], 30 \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; 31 }, 32 { 33 \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, 34 \u0026#34;Action\u0026#34;: [ 35 \u0026#34;glue:*\u0026#34; 36 ], 37 \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; 38 }, 39 { 40 \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, 41 \u0026#34;Action\u0026#34;: [ 42 \u0026#34;logs:PutLogEvents\u0026#34;, 43 \u0026#34;logs:CreateLogStream\u0026#34;, 44 \u0026#34;logs:DescribeLogGroups\u0026#34;, 45 \u0026#34;logs:DescribeLogStreams\u0026#34; 46 ], 47 \u0026#34;Resource\u0026#34;: [ 48 \u0026#34;arn:aws:logs:*:*:*\u0026#34; 49 ] 50 } 51 ] 52}\u0026#39; Update the trust policy of the job execution role As mentioned earlier, the EMR service account is allowed to assume the job execution role by OIDC federation. In order to enable it, we need to update the trust relationship of the role. We can update it as shown below.\n1aws emr-containers update-role-trust-policy \\ 2 --cluster-name ${CLUSTER_NAME} \\ 3 --namespace spark \\ 4 --role-name ${EMR_ROLE_NAME} 5 6aws iam get-role --role-name ${EMR_ROLE_NAME} --query \u0026#34;Role.AssumeRolePolicyDocument.Statement[1]\u0026#34; 7# { 8# \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, 9# \u0026#34;Principal\u0026#34;: { 10# \u0026#34;Federated\u0026#34;: \u0026#34;arn:aws:iam::\u0026lt;AWS-ACCOUNT-ID\u0026gt;:oidc-provider/oidc.eks.ap-southeast-2.amazonaws.com/id/6F3C18F00D8610088272FEF11013B8C5\u0026#34; 11# }, 12# \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRoleWithWebIdentity\u0026#34;, 13# \u0026#34;Condition\u0026#34;: { 14# \u0026#34;StringLike\u0026#34;: { 15# \u0026#34;oidc.eks.ap-southeast-2.amazonaws.com/id/6F3C18F00D8610088272FEF11013B8C5:sub\u0026#34;: \u0026#34;system:serviceaccount:spark:emr-containers-sa-*-*-\u0026lt;AWS-ACCOUNT-ID\u0026gt;-93ztm12b8wi73z7zlhtudeipd0vpa8b60gchkls78cj1q\u0026#34; 16# } 17# } 18# } Register Amazon EKS Cluster with Amazon EMR We can register the Amazon EKS cluster with Amazon EMR as shown below. We need to provide the EKS cluster name and namespace.\n1## register EKS cluster with EMR 2aws emr-containers create-virtual-cluster \\ 3 --name ${CLUSTER_NAME} \\ 4 --container-provider \u0026#39;{ 5 \u0026#34;id\u0026#34;: \u0026#34;\u0026#39;${CLUSTER_NAME}\u0026#39;\u0026#34;, 6 \u0026#34;type\u0026#34;: \u0026#34;EKS\u0026#34;, 7 \u0026#34;info\u0026#34;: { 8 \u0026#34;eksInfo\u0026#34;: { 9 \u0026#34;namespace\u0026#34;: \u0026#34;spark\u0026#34; 10 } 11 } 12}\u0026#39; 13 14aws emr-containers list-virtual-clusters --query \u0026#34;sort_by(virtualClusters, \u0026amp;createdAt)[-1]\u0026#34; 15# { 16# \u0026#34;id\u0026#34;: \u0026#34;9wvd1yhms5tk1k8chrn525z34\u0026#34;, 17# \u0026#34;name\u0026#34;: \u0026#34;emr-eks-example\u0026#34;, 18# \u0026#34;arn\u0026#34;: \u0026#34;arn:aws:emr-containers:ap-southeast-2:\u0026lt;AWS-ACCOUNT-ID\u0026gt;:/virtualclusters/9wvd1yhms5tk1k8chrn525z34\u0026#34;, 19# \u0026#34;state\u0026#34;: \u0026#34;RUNNING\u0026#34;, 20# \u0026#34;containerProvider\u0026#34;: { 21# \u0026#34;type\u0026#34;: \u0026#34;EKS\u0026#34;, 22# \u0026#34;id\u0026#34;: \u0026#34;emr-eks-example\u0026#34;, 23# \u0026#34;info\u0026#34;: { 24# \u0026#34;eksInfo\u0026#34;: { 25# \u0026#34;namespace\u0026#34;: \u0026#34;spark\u0026#34; 26# } 27# } 28# }, 29# \u0026#34;createdAt\u0026#34;: \u0026#34;2022-01-07T01:26:37+00:00\u0026#34;, 30# \u0026#34;tags\u0026#34;: {} 31# } We can also check the virtual cluster on the EMR console.\nExamples Food Establishment Inspection This example is from the getting started tutorial of the Amazon EMR management guide. The PySpark script executes a simple SQL statement that counts the top 10 restaurants with the most Red violations and saves the output to S3. The script and its data source are saved to S3.\nIn the job request, we specify the job name, virtual cluster ID and job execution role. Also, the spark submit details are specified in the job driver option where the entrypoint is set to the S3 location of the PySpark script, entry point arguments and spark submit parameters. Finally, S3 and CloudWatch monitoring configuration is specified.\n1export VIRTUAL_CLUSTER_ID=$(aws emr-containers list-virtual-clusters --query \u0026#34;sort_by(virtualClusters, \u0026amp;createdAt)[-1].id\u0026#34; --output text) 2export EMR_ROLE_ARN=$(aws iam get-role --role-name ${EMR_ROLE_NAME} --query Role.Arn --output text) 3 4## create job request 5cat \u0026lt;\u0026lt; EOF \u0026gt; ./request-health-violations.json 6{ 7 \u0026#34;name\u0026#34;: \u0026#34;health-violations\u0026#34;, 8 \u0026#34;virtualClusterId\u0026#34;: \u0026#34;${VIRTUAL_CLUSTER_ID}\u0026#34;, 9 \u0026#34;executionRoleArn\u0026#34;: \u0026#34;${EMR_ROLE_ARN}\u0026#34;, 10 \u0026#34;releaseLabel\u0026#34;: \u0026#34;emr-6.2.0-latest\u0026#34;, 11 \u0026#34;jobDriver\u0026#34;: { 12 \u0026#34;sparkSubmitJobDriver\u0026#34;: { 13 \u0026#34;entryPoint\u0026#34;: \u0026#34;s3://${S3_BUCKET_NAME}/config/health_violations.py\u0026#34;, 14 \u0026#34;entryPointArguments\u0026#34;: [ 15 \u0026#34;--data_source\u0026#34;, \u0026#34;s3://${S3_BUCKET_NAME}/config/food_establishment_data.csv\u0026#34;, 16 \u0026#34;--output_uri\u0026#34;, \u0026#34;s3://${S3_BUCKET_NAME}/output\u0026#34; 17 ], 18 \u0026#34;sparkSubmitParameters\u0026#34;: \u0026#34;--conf spark.executor.instances=2 \\ 19 --conf spark.executor.memory=2G \\ 20 --conf spark.executor.cores=1 \\ 21 --conf spark.driver.cores=1 \\ 22 --conf spark.driver.memory=2G\u0026#34; 23 } 24 }, 25 \u0026#34;configurationOverrides\u0026#34;: { 26 \u0026#34;monitoringConfiguration\u0026#34;: { 27 \u0026#34;cloudWatchMonitoringConfiguration\u0026#34;: { 28 \u0026#34;logGroupName\u0026#34;: \u0026#34;${LOG_GROUP_NAME}\u0026#34;, 29 \u0026#34;logStreamNamePrefix\u0026#34;: \u0026#34;health\u0026#34; 30 }, 31 \u0026#34;s3MonitoringConfiguration\u0026#34;: { 32 \u0026#34;logUri\u0026#34;: \u0026#34;s3://${S3_BUCKET_NAME}/logs/\u0026#34; 33 } 34 } 35 } 36} 37EOF 38 39aws emr-containers start-job-run \\ 40 --cli-input-json file://./request-health-violations.json Once a job run is started, it can be checked under the virtual cluster section of the EMR console.\nWhen we click the View logs link, it launches the Spark History Server on a new tab.\nAs configured, the container logs of the job can be found in CloudWatch.\nAlso, the logs for the containers (spark driver and executor) and control-logs (job runner) can be found in S3.\nOnce the job completes, we can check the output from S3 as shown below.\n1export OUTPUT_FILE=$(aws s3 ls s3://${S3_BUCKET_NAME}/output/ | grep .csv | awk \u0026#39;{print $4}\u0026#39;) 2aws s3 cp s3://${S3_BUCKET_NAME}/output/${OUTPUT_FILE} - | head -n 15 3# name,total_red_violations 4# SUBWAY,322 5# T-MOBILE PARK,315 6# WHOLE FOODS MARKET,299 7# PCC COMMUNITY MARKETS,251 8# TACO TIME,240 9# MCDONALD\u0026#39;S,177 10# THAI GINGER,153 11# SAFEWAY INC #1508,143 12# TAQUERIA EL RINCONSITO,134 13# HIMITSU TERIYAKI,128 Hudi DeltaStreamer In an earlier post, we discussed a Hudi table generation using the DeltaStreamer utility as part of a CDC-based data ingestion solution. In that exercise, we executed the spark job in an EMR cluster backed by EC2 instances. We can run the spark job in our EKS cluster.\nWe can configure to run the executors in spot instances in order to save cost. A spot node group can be created by the following configuration file.\n1# ./manifests/nodegroup-spot.yaml 2--- 3apiVersion: eksctl.io/v1alpha5 4kind: ClusterConfig 5 6metadata: 7 name: emr-eks-example 8 region: ap-southeast-2 9 tags: 10 Owner: jaehyeon 11 12managedNodeGroups: 13- name: nodegroup-spot 14 desiredCapacity: 3 15 instanceTypes: 16 - m5.xlarge 17 - m5a.xlarge 18 - m4.xlarge 19 spot: true Once the spot node group is created, we can see 3 instances are added to the EKS node with the SPOT capacity type.\n1eksctl create nodegroup -f ./manifests/nodegroup-spot.yaml 2 3kubectl get nodes \\ 4 --label-columns=eks.amazonaws.com/nodegroup,eks.amazonaws.com/capacityType \\ 5 --sort-by=.metadata.creationTimestamp 6# NAME STATUS ROLES AGE VERSION NODEGROUP CAPACITYTYPE 7# ip-192-168-33-60.ap-southeast-2.compute.internal Ready \u0026lt;none\u0026gt; 52m v1.21.5-eks-bc4871b nodegroup ON_DEMAND 8# ip-192-168-95-68.ap-southeast-2.compute.internal Ready \u0026lt;none\u0026gt; 51m v1.21.5-eks-bc4871b nodegroup ON_DEMAND 9# ip-192-168-79-20.ap-southeast-2.compute.internal Ready \u0026lt;none\u0026gt; 114s v1.21.5-eks-bc4871b nodegroup-spot SPOT 10# ip-192-168-1-57.ap-southeast-2.compute.internal Ready \u0026lt;none\u0026gt; 112s v1.21.5-eks-bc4871b nodegroup-spot SPOT 11# ip-192-168-34-249.ap-southeast-2.compute.internal Ready \u0026lt;none\u0026gt; 97s v1.21.5-eks-bc4871b nodegroup-spot SPOT The driver and executor pods should be created in different nodes, and it can be controlled by Pod Template. Below the driver and executor have a different node selector, and they\u0026rsquo;ll be assigned based on the capacity type label specified in the node selector.\n1# ./config/driver_pod_template.yml 2apiVersion: v1 3kind: Pod 4spec: 5 nodeSelector: 6 eks.amazonaws.com/capacityType: ON_DEMAND 7 8# ./config/executor_pod_template.yml 9apiVersion: v1 10kind: Pod 11spec: 12 nodeSelector: 13 eks.amazonaws.com/capacityType: SPOT The job request for the DeltaStreamer job can be found below. Note that, in the entrypoint, we specified the latest Hudi utilities bundle (0.10.0) from S3 instead of the pre-installed Hudi 0.8.0. It is because Hudi 0.8.0 supports JDBC based Hive sync only while Hudi 0.9.0+ supports multiple Hive sync modes including Hive metastore. EMR on EKS doesn\u0026rsquo;t run _HiveServer2 _so that JDBC based Hive sync doesn\u0026rsquo;t work. Instead, we can specify Hive sync based on Hive metastore because Glue data catalog can be used as Hive metastore. Therefore, we need a newer version of the Hudi library in order to register the resulting Hudi table to Glue data catalog. Also, in the application configuration, we configured to use Glue data catalog as the Hive metastore and the driver/executor pod template files are specified.\n1export VIRTUAL_CLUSTER_ID=$(aws emr-containers list-virtual-clusters --query \u0026#34;sort_by(virtualClusters, \u0026amp;createdAt)[-1].id\u0026#34; --output text) 2export EMR_ROLE_ARN=$(aws iam get-role --role-name ${EMR_ROLE_NAME} --query Role.Arn --output text) 3 4## create job request 5cat \u0026lt;\u0026lt; EOF \u0026gt; ./request-cdc-events.json 6{ 7 \u0026#34;name\u0026#34;: \u0026#34;cdc-events\u0026#34;, 8 \u0026#34;virtualClusterId\u0026#34;: \u0026#34;${VIRTUAL_CLUSTER_ID}\u0026#34;, 9 \u0026#34;executionRoleArn\u0026#34;: \u0026#34;${EMR_ROLE_ARN}\u0026#34;, 10 \u0026#34;releaseLabel\u0026#34;: \u0026#34;emr-6.4.0-latest\u0026#34;, 11 \u0026#34;jobDriver\u0026#34;: { 12 \u0026#34;sparkSubmitJobDriver\u0026#34;: { 13 \u0026#34;entryPoint\u0026#34;: \u0026#34;s3://${S3_BUCKET_NAME}/config/hudi-utilities-bundle_2.12-0.10.0.jar\u0026#34;, 14 \u0026#34;entryPointArguments\u0026#34;: [ 15 \u0026#34;--table-type\u0026#34;, \u0026#34;COPY_ON_WRITE\u0026#34;, 16 \u0026#34;--source-ordering-field\u0026#34;, \u0026#34;__source_ts_ms\u0026#34;, 17 \u0026#34;--props\u0026#34;, \u0026#34;s3://${S3_BUCKET_NAME}/config/cdc_events_s3.properties\u0026#34;, 18 \u0026#34;--source-class\u0026#34;, \u0026#34;org.apache.hudi.utilities.sources.JsonDFSSource\u0026#34;, 19 \u0026#34;--target-base-path\u0026#34;, \u0026#34;s3://${S3_BUCKET_NAME}/hudi/cdc-events/\u0026#34;, 20 \u0026#34;--target-table\u0026#34;, \u0026#34;datalake.cdc_events\u0026#34;, 21 \u0026#34;--schemaprovider-class\u0026#34;, \u0026#34;org.apache.hudi.utilities.schema.FilebasedSchemaProvider\u0026#34;, 22 \u0026#34;--enable-hive-sync\u0026#34;, 23 \u0026#34;--min-sync-interval-seconds\u0026#34;, \u0026#34;60\u0026#34;, 24 \u0026#34;--continuous\u0026#34;, 25 \u0026#34;--op\u0026#34;, \u0026#34;UPSERT\u0026#34; 26 ], 27 \u0026#34;sparkSubmitParameters\u0026#34;: \u0026#34;--class org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer \\ 28 --jars local:///usr/lib/spark/external/lib/spark-avro_2.12-3.1.2-amzn-0.jar,s3://${S3_BUCKET_NAME}/config/hudi-utilities-bundle_2.12-0.10.0.jar \\ 29 --conf spark.driver.cores=1 \\ 30 --conf spark.driver.memory=2G \\ 31 --conf spark.executor.instances=2 \\ 32 --conf spark.executor.memory=2G \\ 33 --conf spark.executor.cores=1 \\ 34 --conf spark.sql.catalogImplementation=hive \\ 35 --conf spark.serializer=org.apache.spark.serializer.KryoSerializer\u0026#34; 36 } 37 }, 38 \u0026#34;configurationOverrides\u0026#34;: { 39 \u0026#34;applicationConfiguration\u0026#34;: [ 40 { 41 \u0026#34;classification\u0026#34;: \u0026#34;spark-defaults\u0026#34;, 42 \u0026#34;properties\u0026#34;: { 43 \u0026#34;spark.hadoop.hive.metastore.client.factory.class\u0026#34;:\u0026#34;com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory\u0026#34;, 44 \u0026#34;spark.kubernetes.driver.podTemplateFile\u0026#34;:\u0026#34;s3://${S3_BUCKET_NAME}/config/driver_pod_template.yml\u0026#34;, 45 \u0026#34;spark.kubernetes.executor.podTemplateFile\u0026#34;:\u0026#34;s3://${S3_BUCKET_NAME}/config/executor_pod_template.yml\u0026#34; 46 } 47 } 48 ], 49 \u0026#34;monitoringConfiguration\u0026#34;: { 50 \u0026#34;cloudWatchMonitoringConfiguration\u0026#34;: { 51 \u0026#34;logGroupName\u0026#34;: \u0026#34;${LOG_GROUP_NAME}\u0026#34;, 52 \u0026#34;logStreamNamePrefix\u0026#34;: \u0026#34;cdc\u0026#34; 53 }, 54 \u0026#34;s3MonitoringConfiguration\u0026#34;: { 55 \u0026#34;logUri\u0026#34;: \u0026#34;s3://${S3_BUCKET_NAME}/logs/\u0026#34; 56 } 57 } 58 } 59} 60EOF 61 62aws emr-containers start-job-run \\ 63 --cli-input-json file://./request-cdc-events.json Once the job run is started, we can check it as shown below.\n1aws emr-containers list-job-runs --virtual-cluster-id ${VIRTUAL_CLUSTER_ID} --query \u0026#34;jobRuns[?name==\u0026#39;cdc-events\u0026#39;]\u0026#34; 2# [ 3# { 4# \u0026#34;id\u0026#34;: \u0026#34;00000002vhi9hivmjk5\u0026#34;, 5# \u0026#34;name\u0026#34;: \u0026#34;cdc-events\u0026#34;, 6# \u0026#34;virtualClusterId\u0026#34;: \u0026#34;9wvd1yhms5tk1k8chrn525z34\u0026#34;, 7# \u0026#34;arn\u0026#34;: \u0026#34;arn:aws:emr-containers:ap-southeast-2:\u0026lt;AWS-ACCOUNT-ID\u0026gt;:/virtualclusters/9wvd1yhms5tk1k8chrn525z34/jobruns/00000002vhi9hivmjk5\u0026#34;, 8# \u0026#34;state\u0026#34;: \u0026#34;RUNNING\u0026#34;, 9# \u0026#34;clientToken\u0026#34;: \u0026#34;63a707e4-e5bc-43e4-b11a-5dcfb4377fd3\u0026#34;, 10# \u0026#34;executionRoleArn\u0026#34;: \u0026#34;arn:aws:iam::\u0026lt;AWS-ACCOUNT-ID\u0026gt;:role/emr-eks-example-job-execution\u0026#34;, 11# \u0026#34;releaseLabel\u0026#34;: \u0026#34;emr-6.4.0-latest\u0026#34;, 12# \u0026#34;createdAt\u0026#34;: \u0026#34;2022-01-07T02:09:34+00:00\u0026#34;, 13# \u0026#34;createdBy\u0026#34;: \u0026#34;arn:aws:sts::\u0026lt;AWS-ACCOUNT-ID\u0026gt;:assumed-role/AWSReservedSSO_AWSFullAccountAdmin_fb6fa00561d5e1c2/jaehyeon.kim@cevo.com.au\u0026#34;, 14# \u0026#34;tags\u0026#34;: {} 15# } 16# ] With kubectl, we can check there are 1 driver, 2 executors and 1 job runner pods.\n1kubectl get pod -n spark 2# NAME READY STATUS RESTARTS AGE 3# pod/00000002vhi9hivmjk5-wf8vp 3/3 Running 0 14m 4# pod/delta-streamer-datalake-cdcevents-5397917e324dea27-exec-1 2/2 Running 0 12m 5# pod/delta-streamer-datalake-cdcevents-5397917e324dea27-exec-2 2/2 Running 0 12m 6# pod/spark-00000002vhi9hivmjk5-driver 2/2 Running 0 13m Also, we can see the driver pod runs in the on-demand node group while the executor and job runner pods run in the spot node group.\n1## driver runs in the on demand node 2for n in $(kubectl get nodes -l eks.amazonaws.com/capacityType=ON_DEMAND --no-headers | cut -d \u0026#34; \u0026#34; -f1) 3 do echo \u0026#34;Pods on instance ${n}:\u0026#34;;kubectl get pods -n spark --no-headers --field-selector spec.nodeName=${n} 4 echo 5 done 6# Pods on instance ip-192-168-33-60.ap-southeast-2.compute.internal: 7# No resources found in spark namespace. 8 9# Pods on instance ip-192-168-95-68.ap-southeast-2.compute.internal: 10# spark-00000002vhi9hivmjk5-driver 2/2 Running 0 17m 11 12## executor and job runner run in the spot node 13for n in $(kubectl get nodes -l eks.amazonaws.com/capacityType=SPOT --no-headers | cut -d \u0026#34; \u0026#34; -f1) 14 do echo \u0026#34;Pods on instance ${n}:\u0026#34;;kubectl get pods -n spark --no-headers --field-selector spec.nodeName=${n} 15 echo 16 done 17# Pods on instance ip-192-168-1-57.ap-southeast-2.compute.internal: 18# delta-streamer-datalake-cdcevents-5397917e324dea27-exec-2 2/2 Running 0 16m 19 20# Pods on instance ip-192-168-34-249.ap-southeast-2.compute.internal: 21# 00000002vhi9hivmjk5-wf8vp 3/3 Running 0 18m 22 23# Pods on instance ip-192-168-79-20.ap-southeast-2.compute.internal: 24# delta-streamer-datalake-cdcevents-5397917e324dea27-exec-1 2/2 Running 0 16m The Hudi utility will register a table in the Glue data catalog and it can be checked as shown below.\n1aws glue get-table --database-name datalake --name cdc_events \\ 2 --query \u0026#34;Table.[DatabaseName, Name, StorageDescriptor.Location, CreateTime, CreatedBy]\u0026#34; 3# [ 4# \u0026#34;datalake\u0026#34;, 5# \u0026#34;cdc_events\u0026#34;, 6# \u0026#34;s3://emr-eks-example-ap-southeast-2/hudi/cdc-events\u0026#34;, 7# \u0026#34;2022-01-07T13:18:49+11:00\u0026#34;, 8# \u0026#34;arn:aws:sts::590312749310:assumed-role/emr-eks-example-job-execution/aws-sdk-java-1641521928075\u0026#34; 9# ] Finally, the details of the table can be queried in Athena.\nClean up The resources that are created for this post can be deleted using aws cli and eksctl as shown below.\n1## delete virtual cluster 2export JOB_RUN_ID=$(aws emr-containers list-job-runs --virtual-cluster-id ${VIRTUAL_CLUSTER_ID} --query \u0026#34;jobRuns[?name==\u0026#39;cdc-events\u0026#39;].id\u0026#34; --output text) 3aws emr-containers cancel-job-run --id ${JOB_RUN_ID} \\ 4 --virtual-cluster-id ${VIRTUAL_CLUSTER_ID} 5aws emr-containers delete-virtual-cluster --id ${VIRTUAL_CLUSTER_ID} 6## delete s3 7aws s3 rm s3://${S3_BUCKET_NAME} --recursive 8aws s3 rb s3://${S3_BUCKET_NAME} --force 9## delete log group 10aws logs delete-log-group --log-group-name ${LOG_GROUP_NAME} 11## delete glue table/database 12aws glue delete-table --database-name datalake --name cdc_events 13aws glue delete-database --name datalake 14## delete iam role/policy 15aws iam delete-role-policy --role-name ${EMR_ROLE_NAME} --policy-name ${EMR_ROLE_NAME}-policy 16aws iam delete-role --role-name ${EMR_ROLE_NAME} 17## delete eks cluster 18eksctl delete cluster --name ${CLUSTER_NAME} Summary In this post, we discussed how to run spark jobs on EKS. First we created an EKS cluster and a node group using eksctl. Then we set up EMR on EKS. A simple PySpark job that shows the basics of EMR on EKS is illustrated and a more realistic example of running Hudi DeltaStreamer utility is demonstrated where the driver and executors are assigned in different node groups.\n","date":"January 17, 2022","img":"/blog/2022-01-17-emr-on-eks-by-example/featured.png","lang":"en","langName":"English","largeImg":"/blog/2022-01-17-emr-on-eks-by-example/featured_hu7346e7fea9bf42a33719962b2d46c84d_76740_500x0_resize_box_3.png","permalink":"/blog/2022-01-17-emr-on-eks-by-example/","series":[],"smallImg":"/blog/2022-01-17-emr-on-eks-by-example/featured_hu7346e7fea9bf42a33719962b2d46c84d_76740_180x0_resize_box_3.png","tags":[{"title":"AWS","url":"/tags/aws/"},{"title":"Amazon EMR","url":"/tags/amazon-emr/"},{"title":"Amazon EKS","url":"/tags/amazon-eks/"},{"title":"Apache Spark","url":"/tags/apache-spark/"},{"title":"Kubernetes","url":"/tags/kubernetes/"}],"timestamp":1642377600,"title":"EMR on EKS by Example"},{"categories":[{"title":"Data Engineering","url":"/categories/data-engineering/"}],"content":"In the previous post, we created a VPC that has private and public subnets in 2 availability zones in order to build and deploy the data lake solution on AWS. NAT instances are created to forward outbound traffic to the internet and a VPN bastion host is set up to facilitate deployment. An Aurora PostgreSQL cluster is deployed to host the source database and a Python command line app is used to create the database. To develop data ingestion using CDC, an Amazon MSK cluster is deployed and the Debezium source and Lenses S3 sink connectors are created on MSK Connect. We also confirmed the order creation and update events are captured as expected. As the last part of this series, we\u0026rsquo;ll build an Apache Hudi DeltaStreamer app on Amazon EMR and use the resulting Hudi table with Amazon Athena and Amazon Quicksight to build a dashboard.\nPart 1 Local Development Part 2 Implement CDC Part 3 Implement Data Lake (this post) Architecture As described in a Red Hat IT topics article, change data capture (CDC) is a proven data integration pattern to track when and what changes occur in data then alert other systems and services that must respond to those changes. Change data capture helps maintain consistency and functionality across all systems that rely on data.\nThe primary use of CDC is to enable applications to respond almost immediately whenever data in databases change. Specifically its use cases cover microservices integration, data replication with up-to-date data, building time-sensitive analytics dashboards, auditing and compliance, cache invalidation, full-text search and so on. There are a number of approaches for CDC - polling, dual writes and log-based CDC. Among those, log-based CDC has advantages to other approaches.\nBoth Amazon DMS and Debezium implement log-based CDC. While the former is a managed service, the latter can be deployed to a Kafka cluster as a (source) connector. It uses Apache Kafka as a messaging service to deliver database change notifications to the applicable systems and applications. Note that Kafka Connect is a tool for streaming data between Apache Kafka and other data systems by connectors in a scalable and reliable way. In AWS, we can use Amazon MSK and MSK Connect for building a Debezium based CDC solution.\nData replication to data lakes using CDC can be much more effective if data is stored to a format that supports atomic transactions and consistent updates. Popular choices are Apache Hudi, Apache Iceberg and Delta Lake. Among those, Apache Hudi can be a good option as it is well-integrated with AWS services.\nBelow shows the architecture of the data lake solution that we will be building in this series of posts.\nEmploying the transactional outbox pattern, the source database publishes change event records to the CDC event table. The event records are generated by triggers that listen to insert and update events on source tables. See the Source Database section of the first post of this series for details. CDC is implemented in a streaming environment and Amazon MSK is used to build the streaming infrastructure. In order to process the real-time CDC event records, a source and sink connectors are set up in Amazon MSK Connect. The Debezium connector for PostgreSQL is used as the source connector and the Lenses S3 connector is used as the sink connector. The sink connector pushes messages to a S3 bucket. Hudi DeltaStreamer is run on Amazon EMR. As a spark application, it reads files from the S3 bucket and upserts Hudi records to another S3 bucket. The Hudi table is created in the AWS Glue Data Catalog. The Hudi table is queried in Amazon Athena while the table is registered in the AWS Glue Data Catalog. Dashboards are created in Amazon Quicksight where the dataset is created using Amazon Athena. In this post, we\u0026rsquo;ll build a Hudi DeltaStreamer app on Amazon EMR and use the resulting Hudi table with Athena and Quicksight to build a dashboard.\nInfrastructure In the previous post, we created a VPC in the Sydney region, which has private and public subnets in 2 availability zones. We also created NAT instances in each availability zone to forward outbound traffic to the internet and a VPN bastion host to access resources in the private subnets. An EMR cluster will be deployed to one of the private subnets of the VPC.\nEMR Cluster We\u0026rsquo;ll create the EMR cluster with the following configurations.\nIt is created with the latest EMR release - semr-6.4.0. It has 1 master and 2 core instance groups - their instance types are m4.large. Both the instance groups have additional security groups that allow access from the VPN bastion host. It installs Hadoop, Hive, Spark, Presto, Hue and Livy. It uses the AWS Glue Data Catalog as the metastore for Hive and Spark. The last configuration is important to register the Hudi table to the Glue Data Catalog so that it can be accessed from other AWS services such as Athena and Quicksight. The cluster is created by CloudFormation and the template also creates a Glue database (datalake) in which the Hudi table will be created. The template can be found in the project GitHub repository.\nOnce the EMR cluster is ready, we can access the master instance as shown below. Note don\u0026rsquo;t forget to connect the VPN bastion host using the SoftEther VPN client program.\nHudi Table Source Schema Kafka only transfers data in byte format and data verification is not performed at the cluster level. As producers and consumers do not communicate with each other, we need a schema registry that sits outside a Kafka cluster and handles distribution of schemas. Although it is recommended to associate with a schema registry, we avoid using it because it requires either an external service or a custom server to host a schema registry. Ideally it\u0026rsquo;ll be good if we\u0026rsquo;re able to use the AWS Glue Schema Registry, but unfortunately it doesn\u0026rsquo;t support a REST interface and cannot be used at the moment.\nIn order to avoid having a schema registry, we use the built-in JSON converter (org.apache.kafka.connect.json.JsonConverter) as the key and value converters for the Debezium source and S3 sink connectors. The resulting value schema of our CDC event message is of the struct type, and it can be found in the project GitHub repository. However, it is not supported by the DeltaStreamer utility that we\u0026rsquo;ll be using to generate the Hudi table. A quick fix is replacing it with the Avro schema, and we can generate it with the local docker-compose environment that we discussed in the first post of this series. Once the local environment is up and running, we can create the Debezium source connector with the Avro converter (io.confluent.connect.avro.AvroConverter) as shown below.\n1curl -i -X POST -H \u0026#34;Accept:application/json\u0026#34; -H \u0026#34;Content-Type:application/json\u0026#34; \\ 2 http://localhost:8083/connectors/ -d @connector/local/source-debezium-avro.json Then we can download the value schema in the Schema tab of the topic.\nThe schema file of the CDC event messages can be found in the project GitHub repository. Note that, although we use it as the source schema file for the DeltaStreamer app, we keep using the JSON converter for the Kafka connectors as we don\u0026rsquo;t set up a schema registry\nDeltaStreamer The HoodieDeltaStreamer utility (part of hudi-utilities-bundle) provides the way to ingest from different sources such as DFS or Kafka, with the following capabilities.\nExactly once ingestion of new events from Kafka, incremental imports from Sqoop or output of HiveIncrementalPuller or files under a DFS folder Support JSON, AVRO or a custom record types for the incoming data Manage checkpoints, rollback \u0026amp; recovery Leverage AVRO schemas from DFS or Confluent schema registry. Support for plugging in transformations As shown below, it runs as a Spark application. Some important options are illustrated below.\nHudi-related jar files are specified directly because Amazon EMR release version 5.28.0 and later installs Hudi components by default. Hudi 0.8.0 is installed for EMR release 6.4.0. It is deployed by the cluster deploy mode where the driver and executor have 2G and 4G of memory respectively. Copy on Write (CoW) is configured as the storage type. Additional Hudi properties are saved in S3 (cdc_events_deltastreamer_s3.properties) - it\u0026rsquo;ll be discussed below. The JSON type is configured as the source file type - note we use the built-in JSON converter for the Kafka connectors. The S3 target base path indicates the place where the Hudi data is stored, and the target table configures the resulting table. As we enable the AWS Glue Data Catalog as the Hive metastore, it can be accessed in Glue. The file-based schema provider is configured. The Avro schema file is referred to as the source schema file in the additional Hudi property file. Hive sync is enabled, and the minimum sync interval is set to 5 seconds. It is set to run continuously and the default UPSERT operation is chosen. 1spark-submit --jars /usr/lib/spark/external/lib/spark-avro.jar,/usr/lib/hudi/hudi-utilities-bundle.jar \\ 2 --master yarn \\ 3 --deploy-mode cluster \\ 4 --driver-memory 2g \\ 5 --executor-memory 4g \\ 6 --conf spark.sql.catalogImplementation=hive \\ 7 --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \\ 8 --class org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer /usr/lib/hudi/hudi-utilities-bundle.jar \\ 9 --table-type COPY_ON_WRITE \\ 10 --source-ordering-field __source_ts_ms \\ 11 --props \u0026#34;s3://data-lake-demo-cevo/hudi/config/cdc_events_deltastreamer_s3.properties\u0026#34; \\ 12 --source-class org.apache.hudi.utilities.sources.JsonDFSSource \\ 13 --target-base-path \u0026#34;s3://data-lake-demo-cevo/hudi/cdc-events/\u0026#34; \\ 14 --target-table datalake.cdc_events \\ 15 --schemaprovider-class org.apache.hudi.utilities.schema.FilebasedSchemaProvider \\ 16 --enable-sync \\ 17 --min-sync-interval-seconds 5 \\ 18 --continuous \\ 19 --op UPSERT Below shows the additional Hudi properties. For Hive sync, the database, table, partition fields and JDBC URL are specified. Note that the private IP address of the master instance is added to the host of the JDBC URL. It is required when submitting the application by the cluster deploy mode. By default, the host is set to localhost and the connection failure error will be thrown if the app doesn\u0026rsquo;t run in the master. The remaining Hudi datasource properties are to configure the primary key of the Hudi table - every record in Hudi is uniquely identified by a pair of record key and partition path fields. The Hudi DeltaStreamer properties specify the source schema file and the S3 location where the source data files exist. More details about the configurations can be found in the Hudi website.\n# ./hudi/config/cdc_events_deltastreamer_s3.properties\r## base properties\rhoodie.upsert.shuffle.parallelism=2\rhoodie.insert.shuffle.parallelism=2\rhoodie.delete.shuffle.parallelism=2\rhoodie.bulkinsert.shuffle.parallelism=2\r## datasource properties\rhoodie.datasource.hive_sync.database=datalake\rhoodie.datasource.hive_sync.table=cdc_events\rhoodie.datasource.hive_sync.partition_fields=customer_id,order_id\rhoodie.datasource.hive_sync.partition_extractor_class=org.apache.hudi.hive.MultiPartKeysValueExtractor\rhoodie.datasource.hive_sync.jdbcurl=jdbc:hive2://10.100.22.160:10000\rhoodie.datasource.write.recordkey.field=order_id\rhoodie.datasource.write.partitionpath.field=customer_id,order_id\rhoodie.datasource.write.keygenerator.class=org.apache.hudi.keygen.ComplexKeyGenerator\rhoodie.datasource.write.hive_style_partitioning=true\r# only supported in Hudi 0.9.0+\r# hoodie.datasource.write.drop.partition.columns=true\r## deltastreamer properties\rhoodie.deltastreamer.schemaprovider.source.schema.file=s3://data-lake-demo-cevo/hudi/config/schema-msk.datalake.cdc_events.avsc\rhoodie.deltastreamer.source.dfs.root=s3://data-lake-demo-cevo/cdc-events/\r## file properties\r# 1,024 * 1,024 * 128 = 134,217,728 (128 MB)\rhoodie.parquet.small.file.limit=134217728 EMR Steps While it is possible to submit the application in the master instance, it can also be submitted as an EMR step. As an example, a simple version of the app is submitted as shown below. The JSON file that configures the step can be found in the project GitHub repository.\n1aws emr add-steps \\ 2 --cluster-id \u0026lt;cluster-id\u0026gt; \\ 3 --steps file://hudi/steps/cdc-events-simple.json Once the step is added, we can see its details on the EMR console.\nGlue Table Below shows the Glue tables that are generated by the DeltaStreamer apps. The main table (cdc_events) is by submitting the app on the master instance while the simple version is by the EMR step.\nWe can check the details of the table in Athena. When we run the describe query, it shows column and partition information. As expected, it has 2 partition columns and additional Hudi table meta information is included as extra columns.\nDashboard Using the Glue table, we can create dashboards in Quicksight. As an example, a dataset of order items is created using Athena as illustrated in the Quicksight documentation. As the order items column in the source table is JSON, custom SQL is chosen so that it can be preprocessed in a more flexible way. The following SQL statement is used to create the dataset. First it parses the order items column and then flattens the array elements into rows. Also, the revenue column is added as a calculated field.\n1WITH raw_data AS ( 2 SELECT 3 customer_id, 4 order_id, 5 transform( 6 CAST(json_parse(order_items) AS ARRAY(MAP(varchar, varchar))), 7 x -\u0026gt; CAST(ROW(x[\u0026#39;discount\u0026#39;], x[\u0026#39;quantity\u0026#39;], x[\u0026#39;unit_price\u0026#39;]) 8 AS ROW(discount decimal(6,2), quantity decimal(6,2), unit_price decimal(6,2))) 9 ) AS order_items 10 from datalake.cdc_events 11), flat_data AS ( 12\tSELECT customer_id, 13\torder_id, 14\titem 15\tFROM raw_data 16\tCROSS JOIN UNNEST(order_items) AS t(item) 17) 18SELECT customer_id, 19 order_id, 20 item.discount, 21 item.quantity, 22 item.unit_price 23FROM flat_data A demo dashboard is created using the order items dataset as shown below. The pie chart on the left indicates there are 3 big customers and the majority of revenue is earned by the top 20 customers. The scatter plot on the right shows a more interesting story. It marks the average quantity and revenue by customers and the dots are scaled by the number of orders - the more orders, the larger the size of the dot. While the 3 big customers occupy the top right area, 5 potentially profitable customers are identified. They do not purchase frequently but tend to buy expensive items, resulting in the average revenue being higher. We may investigate them further if a promotional event may be appropriate to make them purchase more frequently in the future.\nConclusion In this post, we created an EMR cluster and developed a DeltaStreamer app that can be used to upsert records to a Hudi table. Being sourced as an Athena dataset, the records of the table are used by a Quicksight dashboard. Over the series of posts we have built an effective end-to-end data lake solution while combining various AWS services and open source tools. The source database is hosted in an Aurora PostgreSQL cluster and a change data capture (CDC) solution is built on Amazon MSK and MSK Connect. With the CDC output files in S3, a DeltaStreamer app is developed on Amazon EMR to build a Hudi table. The resulting table is used to create a dashboard with Amazon Athena and Quicksight.\n","date":"December 19, 2021","img":"/blog/2021-12-19-datalake-demo-part3/featured.png","lang":"en","langName":"English","largeImg":"/blog/2021-12-19-datalake-demo-part3/featured_hua25eccd3824300d3b1ed87f56797248c_164526_500x0_resize_box_3.png","permalink":"/blog/2021-12-19-datalake-demo-part3/","series":[{"title":"Data Lake Demo Using Change Data Capture","url":"/series/data-lake-demo-using-change-data-capture/"}],"smallImg":"/blog/2021-12-19-datalake-demo-part3/featured_hua25eccd3824300d3b1ed87f56797248c_164526_180x0_resize_box_3.png","tags":[{"title":"AWS","url":"/tags/aws/"},{"title":"Amazon EMR","url":"/tags/amazon-emr/"},{"title":"Amazon MSK","url":"/tags/amazon-msk/"},{"title":"Amazon MSK Connect","url":"/tags/amazon-msk-connect/"},{"title":"Apache Spark","url":"/tags/apache-spark/"},{"title":"Apache Hudi","url":"/tags/apache-hudi/"},{"title":"Apache Kafka","url":"/tags/apache-kafka/"},{"title":"Change Data Capture","url":"/tags/change-data-capture/"},{"title":"Data Lake","url":"/tags/data-lake/"},{"title":"Docker","url":"/tags/docker/"},{"title":"Terraform","url":"/tags/terraform/"}],"timestamp":1639872000,"title":"Data Lake Demo Using Change Data Capture (CDC) on AWS – Part 3 Implement Data Lake"},{"categories":[{"title":"Data Engineering","url":"/categories/data-engineering/"}],"content":"In the previous post, we discussed a data lake solution where data ingestion is performed using change data capture (CDC) and the output files are upserted to an Apache Hudi table. Being registered to Glue Data Catalog, it can be used for ad-hoc queries and report/dashboard creation. The Northwind database is used as the source database and, following the transactional outbox pattern, order-related changes are _upserted _to an outbox table by triggers. The data ingestion is developed using Kafka connectors in the local Confluent platform where the Debezium for PostgreSQL is used as the source connector and the Lenses S3 sink connector is used as the sink connector. We confirmed the order creation and update events are captured as expected, and it is ready for production deployment. In this post, we\u0026rsquo;ll build the CDC part of the solution on AWS using Amazon MSK and MSK Connect.\nPart 1 Local Development Part 2 Implement CDC (this post) Part 3 Implement Data Lake Architecture As described in a Red Hat IT topics article, change data capture (CDC) is a proven data integration pattern to track when and what changes occur in data then alert other systems and services that must respond to those changes. Change data capture helps maintain consistency and functionality across all systems that rely on data.\nThe primary use of CDC is to enable applications to respond almost immediately whenever data in databases change. Specifically its use cases cover microservices integration, data replication with up-to-date data, building time-sensitive analytics dashboards, auditing and compliance, cache invalidation, full-text search and so on. There are a number of approaches for CDC - polling, dual writes and log-based CDC. Among those, log-based CDC has advantages to other approaches.\nBoth Amazon DMS and Debezium implement log-based CDC. While the former is a managed service, the latter can be deployed to a Kafka cluster as a (source) connector. It uses Apache Kafka as a messaging service to deliver database change notifications to the applicable systems and applications. Note that Kafka Connect is a tool for streaming data between Apache Kafka and other data systems by connectors in a scalable and reliable way. In AWS, we can use Amazon MSK and MSK Connect for building a Debezium based CDC solution.\nData replication to data lakes using CDC can be much more effective if data is stored to a format that supports atomic transactions and consistent updates. Popular choices are Apache Hudi, Apache Iceberg and Delta Lake. Among those, Apache Hudi can be a good option as it is well-integrated with AWS services.\nBelow shows the architecture of the data lake solution that we will be building in this series of posts.\nEmploying the transactional outbox pattern, the source database publishes change event records to the CDC event table. The event records are generated by triggers that listen to insert and update events on source tables. See the Source Database section of the previous post for details. CDC is implemented in a streaming environment and Amazon MSK is used to build the streaming infrastructure. In order to process the real-time CDC event records, a source and sink connectors are set up in Amazon MSK Connect. The Debezium connector for PostgreSQL is used as the source connector and the Lenses S3 connector is used as the sink connector. The sink connector pushes messages to a S3 bucket. Hudi DeltaStreamer is run on Amazon EMR. As a spark application, it reads files from the S3 bucket and upserts Hudi records to another S3 bucket. The Hudi table is created in the AWS Glue Data Catalog. The Hudi table is queried in Amazon Athena while the table is registered in the AWS Glue Data Catalog. Dashboards are created in Amazon Quicksight where the dataset is created using Amazon Athena. In this post, we\u0026rsquo;ll build the CDC part of the solution on AWS using Amazon MSK and MSK Connect.\nInfrastructure VPC We\u0026rsquo;ll build the data lake solution in a dedicated VPC. The VPC is created in the Sydney region, and it has private and public subnets in 2 availability zones. Note the source database, MSK cluster/connectors and EMR cluster will be deployed to the private subnets. The CloudFormation template can be found in the Free Templates for AWS CloudFormation- see the VPC section for details.\nNAT Instances NAT instances are created in each of the availability zone to forward outbound traffic to the internet. The CloudFormation template can be found in the VPC section of the site as well.\nVPN Bastion Host We can use a VPN bastion host to access resources in the private subnets. The free template site provides both SSH and VPN bastion host templates and I find the latter is more convenient. It can be used to access other resources via different ports as well as be used as an SSH bastion host. The CloudFormation template creates an EC2 instance in one of the public subnets and installs a SoftEther VPN server. The template requires you to add the pre-shared key and the VPN admin password, which can be used to manage the server and create connection settings for the VPN server. After creating the CloudFormation stack, we need to download the server manager from the download page and to install the admin tools.\nAfter that, we can create connection settings for the VPN server by providing the necessary details as marked in red boxes below. We should add the public IP address of the EC2 instance and the VPN admin password.\nThe template also has VPN username and password parameters and those are used to create a user account in the VPN bastion server. Using the credentials we can set up a VPN connection using the SoftEther VPN client program - it can be downloaded on the same download page. We should add the public IP address of the EC2 instance to the host name and select DEFAULT as the virtual hub name.\nAfter that, we can connect to the VPN bastion host and access resources in the private subnets.\nAurora PostgreSQL We create the source database with Aurora PostgreSQL. The CloudFormation template from the free template site creates database instances in multiple availability zones. We can simplify it by creating only a single instance. Also, as we\u0026rsquo;re going to use the pgoutput plugin for the Debezium source connector, we need to set rds:logical_replication value to \u0026ldquo;1\u0026rdquo; in the database cluster parameter group. Note to add the CloudFormation stack name of the VPN bastion host to the ParentSSHBastionStack parameter value so that the database can be accessed by the VPN bastion host. Also note that the template doesn\u0026rsquo;t include an inbound rule from the MSK cluster that\u0026rsquo;ll be created below. For now, we need to add the inbound rule manually. The updated template can be found in the project GitHub repository.\nSource Database As with the local development, we can create the source database by executing the db creation SQL scripts. A function is created in Python. After creating the datalake schema and setting the search path of the database (devdb) to it, it executes the SQL scripts.\n1# ./data/load/src.py 2def create_northwind_db(): 3 \u0026#34;\u0026#34;\u0026#34; 4 Create Northwind database by executing SQL scripts 5 \u0026#34;\u0026#34;\u0026#34; 6 try: 7 global conn 8 with conn: 9 with conn.cursor() as curs: 10 curs.execute(\u0026#34;CREATE SCHEMA datalake;\u0026#34;) 11 curs.execute(\u0026#34;SET search_path TO datalake;\u0026#34;) 12 curs.execute(\u0026#34;ALTER database devdb SET search_path TO datalake;\u0026#34;) 13 curs.execute(set_script_path(\u0026#34;01_northwind_ddl.sql\u0026#34;).open(\u0026#34;r\u0026#34;).read()) 14 curs.execute(set_script_path(\u0026#34;02_northwind_data.sql\u0026#34;).open(\u0026#34;r\u0026#34;).read()) 15 curs.execute(set_script_path(\u0026#34;03_cdc_events.sql\u0026#34;).open(\u0026#34;r\u0026#34;).read()) 16 conn.commit() 17 typer.echo(\u0026#34;Northwind SQL scripts executed\u0026#34;) 18 except (psycopg2.OperationalError, psycopg2.DatabaseError, FileNotFoundError) as err: 19 typer.echo(create_northwind_db.__name__, err) 20 close_conn() 21 exit(1) In order to facilitate the db creation, a simple command line application is created using the Typer library.\n1# ./data/load/main.py 2import typer 3from src import set_connection, create_northwind_db 4 5def main( 6 host: str = typer.Option(..., \u0026#34;--host\u0026#34;, \u0026#34;-h\u0026#34;, help=\u0026#34;Database host\u0026#34;), 7 port: int = typer.Option(5432, \u0026#34;--port\u0026#34;, \u0026#34;-p\u0026#34;, help=\u0026#34;Database port\u0026#34;), 8 dbname: str = typer.Option(..., \u0026#34;--dbname\u0026#34;, \u0026#34;-d\u0026#34;, help=\u0026#34;Database name\u0026#34;), 9 user: str = typer.Option(..., \u0026#34;--user\u0026#34;, \u0026#34;-u\u0026#34;, help=\u0026#34;Database user name\u0026#34;), 10 password: str = typer.Option(..., prompt=True, hide_input=True, help=\u0026#34;Database user password\u0026#34;), 11): 12 to_create = typer.confirm(\u0026#34;To create database?\u0026#34;) 13 if to_create: 14 params = {\u0026#34;host\u0026#34;: host, \u0026#34;port\u0026#34;: port, \u0026#34;dbname\u0026#34;: dbname, \u0026#34;user\u0026#34;: user, \u0026#34;password\u0026#34;: password} 15 set_connection(params) 16 create_northwind_db() 17 18if __name__ == \u0026#34;__main__\u0026#34;: 19 typer.run(main) It has a set of options to specify - database host, post, database name and username. The database password is set to be prompted, and an additional confirmation is required. If all options are provided, the app runs and creates the source database.\n1(venv) jaehyeon@cevo:~/data-lake-demo$ python data/load/main.py --help 2Usage: main.py [OPTIONS] 3 4Options: 5 -h, --host TEXT Database host [required] 6 -p, --port INTEGER Database port [default: 5432] 7 -d, --dbname TEXT Database name [required] 8 -u, --user TEXT Database user name [required] 9 --password TEXT Database user password [required] 10 --install-completion Install completion for the current shell. 11 --show-completion Show completion for the current shell, to copy it or 12 customize the installation. 13 --help Show this message and exit. 14 15(venv) jaehyeon@cevo:~/data-lake-demo$ python data/load/main.py -h \u0026lt;db-host-name-or-ip\u0026gt; -d \u0026lt;dbname\u0026gt; -u \u0026lt;username\u0026gt; 16Password: 17To create database? [y/N]: y 18Database connection created 19Northwind SQL scripts executed As illustrated thoroughly in the previous post, it inserts 829 order event records to the cdc_events table.\nMSK Cluster We\u0026rsquo;ll create an MSK cluster with 2 brokers (servers). The instance type of brokers is set to kafka.m5.large. Note that the smallest instance type of kafka.t3.small may look better for development, but we\u0026rsquo;ll have a failed authentication error when IAM Authentication is used for access control and connectors are created on MSK Connect. It is because the T3 instance type is limited to 1 TCP connection per broker per second and if the frequency is higher than the limit, that error is thrown. Note, while it\u0026rsquo;s possible to avoid it by updating reconnect.backoff.ms to 1000, it is not allowed on MSK Connect.\nWhen it comes to inbound rules of the cluster security group, we need to configure that all inbound traffic is allowed from its own security group. This is because connectors on MSK Connect are deployed with the same security group of the MSK cluster, and they should have access to it. Also, we need to allow port 9098 from the security group of the VPN bastion host - 9098 is the port for establishing the initial connection to the cluster when IAM Authentication is used.\nFinally, a cluster configuration is created manually as it\u0026rsquo;s not supported by CloudFormation and its ARN and revision number are added as parameters. The configuration is shown below.\nauto.create.topics.enable = true\rdelete.topic.enable = true The CloudFormation template can be found in the project GitHub repository.\nMSK Connect Role As we use IAM Authentication for access control, the connectors need to have permission on the cluster, topic and group. Also, the sink connector should have access to put output files to the S3 bucket. For simplicity, a single connector role is created for both source and sink connectors in the same template.\nCDC Development In order to create a connector on MSK Connect, we need to create a custom plugin and the connector itself. A custom plugin is a set of JAR files containing the implementation of one or more connectors, transforms, or converters and installed on the workers of the connect cluster where the connector is running. Both the resources are not supported by CloudFormation and can be created on AWS Console or using AWS SDK.\nCustom Plugins We need plugins for the Debezium source and S3 sink connectors. Plugin objects can be saved to S3 in either JAR or ZIP file format. We can download them from the relevant release/download pages of Debezium and Lenses Stream Reactor. Note to put contents at the root level of the zip archives. Once they are saved to S3, we can create them simply by specifying their S3 URIs.\nMSK Connectors Source Connector _Debezium\u0026rsquo;s PostgreSQL connector captures row-level changes in the schemas of a PostgreSQL database. PostgreSQL versions 9.6, 10, 11, 12 and 13 are supported. The first time it connects to a PostgreSQL server or cluster, the connector takes a consistent snapshot of all schemas. After that snapshot is complete, the connector continuously captures row-level changes that insert, update, and delete database content and that were committed to a PostgreSQL database. The connector generates data change event records and streams them to Kafka topics. For each table, the default behavior is that the connector streams all generated events to a separate Kafka topic for that table. Applications and services consume data change event records from that topic.\nThe connector has a number of connector properties including name, connector class, database connection details, key/value converter and so on - the full list of properties can be found in this page. The properties that need explanation are listed below.\nplugin.name - Using the logical decoding feature, an output plug-in enables clients to consume changes to the transaction log in a user-friendly manner. Debezium supports decoderbufs, wal2json and pgoutput plug-ins. Both wal2json and pgoutput are available in Amazon RDS for PostgreSQL and Amazon Aurora PostgreSQL. decoderbufs requires a separate installation, and it is excluded from the option. Among the 2 supported plug-ins, pgoutput is selected because it is the standard logical decoding output plug-in in PostgreSQL 10+ and has better performance for large transactions. publication.name - With the pgoutput plug-in, the Debezium connector creates a publication (if not exists) and sets publication.autocreate.mode to all_tables. It can cause an issue to update a record to a table that doesn\u0026rsquo;t have the primary key or replica identity. We can set the value to filtered where the connector adjusts the applicable tables by other property values. Alternatively we can create a publication on our own and add the name to publication.name property. I find creating a publication explicitly is easier to maintain. Note a publication alone is not sufficient to handle the issue. All affected tables by the publication should have the primary key or replica identity. In our example, the _orders _and _order_details _tables should meet the condition. In short, creating an explicit publication can prevent the event generation process from interrupting other processes by limiting the scope of CDC event generation. key.converter/value.converter - Although Avro serialization is recommended, JSON is a format that can be generated without schema registry and can be read by DeltaStreamer. transforms - A Debezium event data has a complex structure that provides a wealth of information. It can be quite difficult to process such a structure using DeltaStreamer. Debezium\u0026rsquo;s event flattening single message transformation (SMT) is configured to flatten the output payload. Note once the connector is deployed, the CDC event records will be published to msk.datalake.cdc_events topic.\n# ./connector/msk/source-debezium.properties\rconnector.class=io.debezium.connector.postgresql.PostgresConnector\rtasks.max=1\rplugin.name=pgoutput\rpublication.name=cdc_publication\rdatabase.hostname=\u0026lt;database-hostname-or-ip-address\u0026gt;\rdatabase.port=5432\rdatabase.user=\u0026lt;database-user\u0026gt;\rdatabase.password=\u0026lt;database-user-password\u0026gt;\rdatabase.dbname=devdb\rdatabase.server.name=msk\rschema.include=datalake\rtable.include.list=datalake.cdc_events\rkey.converter=org.apache.kafka.connect.json.JsonConverter\rvalue.converter=org.apache.kafka.connect.json.JsonConverter\rtransforms=unwrap\rtransforms.unwrap.type=io.debezium.transforms.ExtractNewRecordState\rtransforms.unwrap.drop.tombstones=false\rtransforms.unwrap.delete.handling.mode=rewrite\rtransforms.unwrap.add.fields=op,db,table,schema,lsn,source.ts_ms The connector can be created in multiple steps. The first step is to select the relevant custom plugin from the custom plugins list.\nIn the second step, we can configure connector properties. Applicable properties are\nConnector name and description Apache Kafka cluster (MSK or self-managed) with an authentication method Connector configuration Connector capacity - Autoscaled or Provisioned Worker configuration IAM role of the connector - it is created in the CloudFormation template In the third step, we configure cluster security. As we selected IAM Authentication, only TLS encryption is available.\nFinally, we can configure logging. A log group is created in the CloudFormation, and we can add its ARN.\nAfter reviewing, we can create the connector.\nSink Connector Lenses S3 Connector is a Kafka Connect sink connector for writing records from Kafka to AWS S3 Buckets. It extends the standard connect config adding a parameter for a SQL command (Lenses Kafka Connect Query Language or \u0026ldquo;KCQL\u0026rdquo;). This defines how to map data from the source (in this case Kafka) to the target (S3). Importantly, it also includes how data should be partitioned into S3, the bucket names and the serialization format (support includes JSON, Avro, Parquet, Text, CSV and binary).\nI find the Lenses S3 connector is more straightforward to configure than the Confluent S3 sink connector for its SQL-like syntax. The KCQL configuration indicates that object files are set to be\nmoved from a Kafka topic (msk.datalake.cdc_events) to an S3 bucket (data-lake-demo-cevo) with object prefix of _cdc-events-local, partitioned by customer_id and order_id e.g. customer_id=\u0026lt;customer-id\u0026gt;/order_id=\u0026lt;order-id\u0026gt;, stored as the JSON format and, flushed every 60 seconds or when there are 50 records. # ./connector/msk/sink-s3-lenses.properties\rconnector.class=io.lenses.streamreactor.connect.aws.s3.sink.S3SinkConnector\rtasks.max=1\rconnect.s3.kcql=INSERT INTO data-lake-demo-cevo:cdc-events SELECT * FROM msk.datalake.cdc_events PARTITIONBY customer_id,order_id STOREAS `json` WITH_FLUSH_INTERVAL = 60 WITH_FLUSH_COUNT = 50\raws.region=ap-southeast-2\raws.custom.endpoint=https://s3.ap-southeast-2.amazonaws.com/\rtopics=msk.datalake.cdc_events\rkey.converter.schemas.enable=false\rschema.enable=false\rerrors.log.enable=true\rkey.converter=org.apache.kafka.connect.json.JsonConverter\rvalue.converter=org.apache.kafka.connect.json.JsonConverter We can create the sink connector on AWS Console using the same steps to the source connector.\nUpdate/Insert Examples Kafka UI When we used the Confluent platform for local development in the previous post, we checked topics and messages on the control tower UI. For MSK, we can use Kafka UI. Below shows a docker-compose file for it. Note that the MSK cluster is secured by IAM Authentication so that _AWS_MSK_IAM _is specified as the SASL mechanism. Under the hood, it uses Amazon MSK Library for AWS IAM for authentication and AWS credentials are provided via volume mapping. Also don\u0026rsquo;t forget to connect the VPN bastion host using the SoftEther VPN client program.\n1# ./kafka-ui/docker-compose.yml 2version: \u0026#34;2\u0026#34; 3services: 4 kafka-ui: 5 image: provectuslabs/kafka-ui 6 container_name: kafka-ui 7 ports: 8 - \u0026#34;8080:8080\u0026#34; 9 # restart: always 10 volumes: 11 - $HOME/.aws:/root/.aws 12 environment: 13 KAFKA_CLUSTERS_0_NAME: msk 14 KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: $BS_SERVERS 15 KAFKA_CLUSTERS_0_PROPERTIES_SECURITY_PROTOCOL: \u0026#34;SASL_SSL\u0026#34; 16 KAFKA_CLUSTERS_0_PROPERTIES_SASL_MECHANISM: \u0026#34;AWS_MSK_IAM\u0026#34; 17 KAFKA_CLUSTERS_0_PROPERTIES_SASL_CLIENT_CALLBACK_HANDLER_CLASS: \u0026#34;software.amazon.msk.auth.iam.IAMClientCallbackHandler\u0026#34; 18 KAFKA_CLUSTERS_0_PROPERTIES_SASL_JAAS_CONFIG: \u0026#34;software.amazon.msk.auth.iam.IAMLoginModule required;\u0026#34; Once started, the UI can be accessed via http://localhost:8080. We see that the CDC topic is found in the topics list. There are 829 messages in the topic, and it matches the number of records in the cdc_events table at creation.\nWhen we click the topic name, it shows further details of it. When clicking the messages tab, we can see individual messages within the topic. The UI also has some other options, and it can be convenient to manage topics and messages.\nThe message records can be expanded, and we can check a message\u0026rsquo;s key, content and headers. Also, we can either copy the value to clipboard or save as a file.\nUpdate Event The order 11077 initially didn\u0026rsquo;t have the _shipped_date _value. When the value is updated later, a new output file will be generated with the updated value.\n1BEGIN TRANSACTION; 2 UPDATE orders 3 SET shipped_date = \u0026#39;1998-06-15\u0026#39;::date 4 WHERE order_id = 11077; 5COMMIT TRANSACTION; 6END; In S3, we see 2 JSON objects are included in the output file for the order entry and the shipped_date value is updated as expected. Note that the Debezium connector converts the DATE type to the INT32 type, which represents the number of days since the epoch.\nInsert Event When a new order is created, it\u0026rsquo;ll insert a record to the _orders _table as well as one or more order items to the _order_details _table. Therefore, we expect multiple event records will be created when a new order is created. We can check it by inserting an order and related order details items.\n1BEGIN TRANSACTION; 2 INSERT INTO orders VALUES (11075, \u0026#39;RICSU\u0026#39;, 8, \u0026#39;1998-05-06\u0026#39;, \u0026#39;1998-06-03\u0026#39;, NULL, 2, 6.19000006, \u0026#39;Richter Supermarkt\u0026#39;, \u0026#39;Starenweg 5\u0026#39;, \u0026#39;Genève\u0026#39;, NULL, \u0026#39;1204\u0026#39;, \u0026#39;Switzerland\u0026#39;); 3 INSERT INTO order_details VALUES (11075, 2, 19, 10, 0.150000006); 4 INSERT INTO order_details VALUES (11075, 46, 12, 30, 0.150000006); 5 INSERT INTO order_details VALUES (11075, 76, 18, 2, 0.150000006); 6COMMIT TRANSACTION; 7END; We can see the output file includes 4 JSON objects where the first object has NULL _order_items _and _products _value. We can also see that those values are expanded gradually in subsequent event records\nConclusion We created a VPC that has private and public subnets in 2 availability zones in order to build and deploy the data lake solution on AWS. NAT instances are created to forward outbound traffic to the internet and a VPN bastion host is set up to facilitate deployment. An Aurora PostgreSQL cluster is deployed to host the source database and a Python command line app is used to create the database. To develop data ingestion using CDC, an MSK cluster is deployed and the Debezium source and Lenses S3 sink connectors are created on MSK Connect. We also confirmed the order creation and update events are captured as expected with the scenarios used by local development. Using CDC event output files in S3, we are able to build an Apache Hudi table on EMR and use it for ad-hoc queries and report/dashboard generation. It\u0026rsquo;ll be covered in the next post.\n","date":"December 12, 2021","img":"/blog/2021-12-12-datalake-demo-part2/featured.png","lang":"en","langName":"English","largeImg":"/blog/2021-12-12-datalake-demo-part2/featured_hua25eccd3824300d3b1ed87f56797248c_164526_500x0_resize_box_3.png","permalink":"/blog/2021-12-12-datalake-demo-part2/","series":[{"title":"Data Lake Demo Using Change Data Capture","url":"/series/data-lake-demo-using-change-data-capture/"}],"smallImg":"/blog/2021-12-12-datalake-demo-part2/featured_hua25eccd3824300d3b1ed87f56797248c_164526_180x0_resize_box_3.png","tags":[{"title":"AWS","url":"/tags/aws/"},{"title":"Amazon EMR","url":"/tags/amazon-emr/"},{"title":"Amazon MSK","url":"/tags/amazon-msk/"},{"title":"Amazon MSK Connect","url":"/tags/amazon-msk-connect/"},{"title":"Apache Spark","url":"/tags/apache-spark/"},{"title":"Apache Hudi","url":"/tags/apache-hudi/"},{"title":"Apache Kafka","url":"/tags/apache-kafka/"},{"title":"Change Data Capture","url":"/tags/change-data-capture/"},{"title":"Data Lake","url":"/tags/data-lake/"},{"title":"Docker","url":"/tags/docker/"},{"title":"Terraform","url":"/tags/terraform/"}],"timestamp":1639267200,"title":"Data Lake Demo Using Change Data Capture (CDC) on AWS – Part 2 Implement CDC"},{"categories":[{"title":"Data Engineering","url":"/categories/data-engineering/"}],"content":"Change data capture (CDC) is a proven data integration pattern that has a wide range of applications. Among those, data replication to data lakes is a good use case in data engineering. Coupled with best-in-breed data lake formats such as Apache Hudi, we can build an efficient data replication solution. This is the first post of the data lake demo series. Over time, we\u0026rsquo;ll build a data lake that uses CDC. As a starting point, we\u0026rsquo;ll discuss the source database and CDC streaming infrastructure in the local environment.\nPart 1 Local Development (this post) Part 2 Implement CDC Part 3 Implement Data Lake Architecture As described in a Red Hat IT topics article, change data capture (CDC) is a proven data integration pattern to track when and what changes occur in data then alert other systems and services that must respond to those changes. Change data capture helps maintain consistency and functionality across all systems that rely on data.\nThe primary use of CDC is to enable applications to respond almost immediately whenever data in databases change. Specifically its use cases cover microservices integration, data replication with up-to-date data, building time-sensitive analytics dashboards, auditing and compliance, cache invalidation, full-text search and so on. There are a number of approaches for CDC - polling, dual writes and log-based CDC. Among those, log-based CDC has advantages to other approaches.\nBoth Amazon DMS and Debezium implement log-based CDC. While the former is a managed service, the latter can be deployed to a Kafka cluster as a (source) connector. It uses Apache Kafka as a messaging service to deliver database change notifications to the applicable systems and applications. Note that Kafka Connect is a tool for streaming data between Apache Kafka and other data systems by connectors in a scalable and reliable way. In AWS, we can use Amazon MSK and MSK Connect for building a Debezium based CDC solution.\nData replication to data lakes using CDC can be much more effective if data is stored to a format that supports atomic transactions and consistent updates. Popular choices are Apache Hudi, Apache Iceberg and Delta Lake. Among those, Apache Hudi can be a good option as it is well-integrated with AWS services.\nBelow shows the architecture of the data lake solution that we will be building in this series of posts.\nEmploying the transactional outbox pattern, the source database publishes change event records to the CDC event table. The event records are generated by triggers that listen to insert and update events on source tables. CDC is implemented in a streaming environment and Amazon MSK is used to build the streaming infrastructure. In order to process the real-time CDC event records, a source and sink connectors are set up in Amazon MSK Connect. The Debezium connector for PostgreSQL is used as the source connector and the Lenses S3 connector is used as the sink connector. The sink connector pushes messages to a S3 bucket. Hudi DeltaStreamer is run on Amazon EMR. As a spark application, it reads files from the S3 bucket and upserts Hudi records to another S3 bucket. The Hudi table is created in the AWS Glue Data Catalog. The Hudi table is queried in Amazon Athena while the table is registered in the AWS Glue Data Catalog. Dashboards are created in Amazon Quicksight where the dataset is created using Amazon Athena. As a starting point, we\u0026rsquo;ll discuss the source database and streaming infrastructure in the local environment.\nSource Database Data Model We will use the Northwind database as the source database. It was originally created by Microsoft and used by various tutorials of their database products. It contains the sales data for a fictitious company called Northwind Traders that deals with specialty foods from around the world. As shown in the following entity relationship diagram, it includes a schema for a small business ERP with customers, products, orders, employees and so on. The version that is ported to PostgreSQL is obtained from YugabyteDB sample datasets and the SQL scripts can be found in the project GitHub repository. For local development, a service is created using docker compose, and it\u0026rsquo;ll be illustrated in the next section.\nOutbox Table and Event Generation It is straightforward to capture changes from multiple tables in a database using Kafka connectors where a separate topic is created for each table. Data ingestion to Hudi, however, can be complicated if messages are stored in multiple Kafka topics. Note that we will use the DeltaStreamer utility, and it maps to a single topic. In order to simplify the data ingestion process, we can employ the transactional outbox pattern. Using this pattern, we can create an outbox table (cdc_events) and upsert a record to it when a new transaction is made. In this way, all database changes can be pushed to a single topic, resulting in one DeltaStreamer process to listen to the change events.\nBelow shows the table creation statement of the outbox table. It aims to store all details of an order entry in a row. The columns that have the JSONB data type store attributes of other entities. For example, the order_items column includes ordered product information. As multiple products can be purchased, it keeps an array of product ID, unit price, quantity and discount.\n1-- ./data/sql/03_cdc_events.sql 2CREATE TABLE cdc_events( 3 order_id SMALLINT NOT NULL PRIMARY KEY, 4 customer_id BPCHAR NOT NULL, 5 order_date DATE, 6 required_date DATE, 7 shipped_date DATE, 8 order_items JSONB, 9 products JSONB, 10 customer JSONB, 11 employee JSONB, 12 shipper JSONB, 13 shipment JSONB, 14 updated_at TIMESTAMPTZ 15); In order to create event records, triggers are added to the orders and order_details tables. They execute the fn_insert_order_event function after an INSERT or UPDATE action occurs to the respective tables. Note the DELETE action is not considered in the event generation process for simplicity. The trigger function basically collects details of an order entry and attempts to insert a new record to the outbox table. If a record with the same order ID exists, it updates the record instead.\n1-- ./data/sql/03_cdc_events.sql 2CREATE TRIGGER orders_triggered 3 AFTER INSERT OR UPDATE 4 ON orders 5 FOR EACH ROW 6 EXECUTE PROCEDURE fn_insert_order_event(); 7 8CREATE TRIGGER order_details_triggered 9 AFTER INSERT OR UPDATE 10 ON order_details 11 FOR EACH ROW 12 EXECUTE PROCEDURE fn_insert_order_event(); 13 14CREATE OR REPLACE FUNCTION fn_insert_order_event() 15 RETURNS TRIGGER 16 LANGUAGE PLPGSQL 17 AS 18$$ 19BEGIN 20 IF (TG_OP IN (\u0026#39;INSERT\u0026#39;, \u0026#39;UPDATE\u0026#39;)) THEN 21 WITH product_details AS ( 22 SELECT p.product_id, 23 row_to_json(p.*)::jsonb AS product_details 24 FROM ( 25 SELECT * 26 FROM products p 27 JOIN suppliers s ON p.supplier_id = s.supplier_id 28 JOIN categories c ON p.category_id = c.category_id 29 ) AS p 30 ), order_items AS ( 31 SELECT od.order_id, 32 jsonb_agg(row_to_json(od.*)::jsonb - \u0026#39;order_id\u0026#39;) AS order_items, 33 jsonb_agg(pd.product_details) AS products 34 FROM order_details od 35 JOIN product_details pd ON od.product_id = pd.product_id 36 WHERE od.order_id = NEW.order_id 37 GROUP BY od.order_id 38 ), emps AS ( 39 SELECT employee_id, 40 row_to_json(e.*)::jsonb AS details 41 FROM employees e 42 ), emp_territories AS ( 43 SELECT et.employee_id, 44 jsonb_agg( 45 row_to_json(t.*) 46 ) AS territories 47 FROM employee_territories et 48 JOIN ( 49 SELECT t.territory_id, t.territory_description, t.region_id, r.region_description 50 FROM territories t 51 JOIN region r ON t.region_id = r.region_id 52 ) AS t ON et.territory_id = t.territory_id 53 GROUP BY et.employee_id 54 ), emp_details AS ( 55 SELECT e.employee_id, 56 e.details || jsonb_build_object(\u0026#39;territories\u0026#39;, et.territories) AS details 57 FROM emps AS e 58 JOIN emp_territories AS et ON e.employee_id = et.employee_id 59 ) 60 INSERT INTO cdc_events 61 SELECT o.order_id, 62 o.customer_id, 63 o.order_date, 64 o.required_date, 65 o.shipped_date, 66 oi.order_items, 67 oi.products, 68 row_to_json(c.*)::jsonb AS customer, 69 ed.details::jsonb AS employee, 70 row_to_json(s.*)::jsonb AS shipper, 71 jsonb_build_object( 72 \u0026#39;freight\u0026#39;, o.freight, 73 \u0026#39;ship_name\u0026#39;, o.ship_name, 74 \u0026#39;ship_address\u0026#39;, o.ship_address, 75 \u0026#39;ship_city\u0026#39;, o.ship_city, 76 \u0026#39;ship_region\u0026#39;, o.ship_region, 77 \u0026#39;ship_postal_code\u0026#39;, o.ship_postal_code, 78 \u0026#39;ship_country\u0026#39;, o.ship_country 79 ) AS shipment, 80 now() 81 FROM orders o 82 LEFT JOIN order_items oi ON o.order_id = oi.order_id 83 JOIN customers c ON o.customer_id = c.customer_id 84 JOIN emp_details ed ON o.employee_id = ed.employee_id 85 JOIN shippers s ON o.ship_via = s.shipper_id 86 WHERE o.order_id = NEW.order_id 87 ON CONFLICT (order_id) 88 DO UPDATE 89 SET order_id = excluded.order_id, 90 customer_id = excluded.customer_id, 91 order_date = excluded.order_date, 92 required_date = excluded.required_date, 93 shipped_date = excluded.shipped_date, 94 order_items = excluded.order_items, 95 products = excluded.products, 96 customer = excluded.customer, 97 shipper = excluded.shipper, 98 shipment = excluded.shipment, 99 updated_at = excluded.updated_at; 100 END IF; 101 RETURN NULL; 102END 103$$; Create Initial Event Records In order to create event records for existing order entries, a stored procedure is created - usp_init_order_events. It is quite similar to the trigger function and can be checked in the project GitHub repository. The procedure is called at database initialization and a total of 829 event records are created by that.\n1-- ./data/sql/03_cdc_events.sql 2CALL usp_init_order_events(); Below shows a simplified event record, converted into JSON. For the order with ID 10248, 3 products are ordered by a customer whose ID is VINET.\n1{ 2 \u0026#34;order_id\u0026#34;: 10248, 3 \u0026#34;customer_id\u0026#34;: \u0026#34;VINET\u0026#34;, 4 \u0026#34;order_date\u0026#34;: \u0026#34;1996-07-04\u0026#34;, 5 \u0026#34;required_date\u0026#34;: \u0026#34;1996-08-01\u0026#34;, 6 \u0026#34;shipped_date\u0026#34;: \u0026#34;1996-07-16\u0026#34;, 7 \u0026#34;order_items\u0026#34;: [ 8 { \u0026#34;discount\u0026#34;: 0, \u0026#34;quantity\u0026#34;: 12, \u0026#34;product_id\u0026#34;: 11, \u0026#34;unit_price\u0026#34;: 14 }, 9 { \u0026#34;discount\u0026#34;: 0, \u0026#34;quantity\u0026#34;: 10, \u0026#34;product_id\u0026#34;: 42, \u0026#34;unit_price\u0026#34;: 9.8 }, 10 { \u0026#34;discount\u0026#34;: 0, \u0026#34;quantity\u0026#34;: 5, \u0026#34;product_id\u0026#34;: 72, \u0026#34;unit_price\u0026#34;: 34.8 } 11 ], 12 \u0026#34;products\u0026#34;: [ 13 { \u0026#34;product_id\u0026#34;: 11, \u0026#34;product_name\u0026#34;: \u0026#34;Queso Cabrales\u0026#34; }, 14 { \u0026#34;product_id\u0026#34;: 42, \u0026#34;product_name\u0026#34;: \u0026#34;Singaporean Hokkien Fried Mee\u0026#34; }, 15 { \u0026#34;product_id\u0026#34;: 72, \u0026#34;product_name\u0026#34;: \u0026#34;Mozzarella di Giovanni\u0026#34; } 16 ], 17 \u0026#34;customer\u0026#34;: { 18 \u0026#34;customer_id\u0026#34;: \u0026#34;VINET\u0026#34;, 19 \u0026#34;company_name\u0026#34;: \u0026#34;Vins et alcools Chevalier\u0026#34; 20 }, 21 \u0026#34;employee\u0026#34;: { 22 \u0026#34;title\u0026#34;: \u0026#34;Sales Manager\u0026#34;, 23 \u0026#34;last_name\u0026#34;: \u0026#34;Buchanan\u0026#34;, 24 \u0026#34;employee_id\u0026#34;: 5 25 }, 26 \u0026#34;shipper\u0026#34;: { 27 \u0026#34;company_name\u0026#34;: \u0026#34;Federal Shipping\u0026#34; 28 }, 29 \u0026#34;shipment\u0026#34;: { 30 \u0026#34;freight\u0026#34;: 32.38, 31 \u0026#34;ship_name\u0026#34;: \u0026#34;Vins et alcools Chevalier\u0026#34; 32 }, 33 \u0026#34;updated_at\u0026#34;: \u0026#34;2021-11-27T20:30:13.644579+11:00\u0026#34; 34} Create Publication As discussed further in the next section, we\u0026rsquo;ll be using the native pgoutput logical replication stream support. Debezium, Kafka source connector, automatically creates a publication that contains all tables if it doesn\u0026rsquo;t exist. It can cause trouble to update a record to a table that doesn\u0026rsquo;t have the primary key or replica identity. In order to handle such an issue, a publication that contains only the outbox table is created. This publication will be used when configuring the source connector.\n1-- ./data/sql/03_cdc_events.sql 2CREATE PUBLICATION cdc_publication 3 FOR TABLE cdc_events; CDC Development Docker Compose The Confluent platform can be handy for local development, although we\u0026rsquo;ll be deploying the solution using Amazon MSK and MSK Connect. The quick start guide provides a docker compose file that includes its various components in separate services. It also contains the control center, a graphical user interface, which helps check brokers, topics, messages and connectors easily.\nAdditionally, we need a PostgreSQL instance for the Northwind database and a service named postgres is added. The database is initialised by a set of SQL scripts. They are executed by volume-mapping to the initialisation folder - the scripts can be found in the project GitHub repository. Also, the Kafka Connect instance, running in the connect service, needs an update to include the source and sink connectors. It\u0026rsquo;ll be illustrated further below.\nBelow shows a cut-down version of the docker compose file that we use for local development. The complete file can be found in the project GitHub repository.\n1# ./docker-compose.yml 2version: \u0026#34;2\u0026#34; 3services: 4 postgres: 5 image: debezium/postgres:13 6 ... 7 ports: 8 - 5432:5432 9 volumes: 10 - ./data/sql:/docker-entrypoint-initdb.d 11 environment: 12 - POSTGRES_DB=devdb 13 - POSTGRES_USER=devuser 14 - POSTGRES_PASSWORD=password 15 zookeeper: 16 image: confluentinc/cp-zookeeper:6.2.1 17 ... 18 ports: 19 - \u0026#34;2181:2181\u0026#34; 20 environment: 21 ... 22 broker: 23 image: confluentinc/cp-server:6.2.1 24 ... 25 depends_on: 26 - zookeeper 27 ports: 28 - \u0026#34;9092:9092\u0026#34; 29 - \u0026#34;9101:9101\u0026#34; 30 environment: 31 ... 32 KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:29092,PLAINTEXT_HOST://localhost:9092 33 ... 34 schema-registry: 35 image: confluentinc/cp-schema-registry:6.2.1 36 ... 37 depends_on: 38 ... 39 ports: 40 - \u0026#34;8081:8081\u0026#34; 41 environment: 42 ... 43 connect: 44 build: .connector/local/cp-server-connect-datagen 45 ... 46 depends_on: 47 ... 48 ports: 49 - \u0026#34;8083:8083\u0026#34; 50 volumes: 51 - ${HOME}/.aws:/home/appuser/.aws 52 environment: 53 CONNECT_BOOTSTRAP_SERVERS: \u0026#34;broker:29092\u0026#34; 54 CONNECT_REST_ADVERTISED_HOST_NAME: connect 55 CONNECT_REST_PORT: 8083 56 ... 57 # include /usr/local/share/kafka/plugins for community connectors 58 CONNECT_PLUGIN_PATH: \u0026#34;/usr/share/java,/usr/share/confluent-hub-components,/usr/local/share/kafka/plugins\u0026#34; 59 ... 60 control-center: 61 image: confluentinc/cp-enterprise-control-center:6.2.1 62 ... 63 depends_on: 64 ... 65 ports: 66 - \u0026#34;9021:9021\u0026#34; 67 environment: 68 CONTROL_CENTER_BOOTSTRAP_SERVERS: \u0026#34;broker:29092\u0026#34; 69 CONTROL_CENTER_CONNECT_CONNECT-DEFAULT_CLUSTER: \u0026#34;connect:8083\u0026#34; 70 ... 71 PORT: 9021 72 rest-proxy: 73 image: confluentinc/cp-kafka-rest:6.2.1 74 ... 75 depends_on: 76 ... 77 ports: 78 - 8082:8082 79 environment: 80 ... Install Connectors We use the Debezium connector for PostgreSQL as the source connector and Lenses S3 Connector as the sink connector. The source connector is installed via the confluent hub client while the sink connector is added as a community connector. Note that the environment variable of CONNECT_PLUGIN_PATH is updated to include the kafka plugin folder (/usr/local/share/kafka/plugins).\n1# .connector/local/cp-server-connect-datagen/Dockerfile 2FROM cnfldemos/cp-server-connect-datagen:0.5.0-6.2.1 3 4# install debezium postgresql connector from confluent hub 5RUN confluent-hub install --no-prompt debezium/debezium-connector-postgresql:1.7.1 6 7# install lenses S3 connector as a community connector - https://docs.confluent.io/home/connect/community.html 8USER root 9RUN mkdir -p /usr/local/share/kafka/plugins/kafka-connect-aws-s3 \u0026amp;\u0026amp; \\ 10 curl -SsL https://github.com/lensesio/stream-reactor/releases/download/3.0.0/kafka-connect-aws-s3-3.0.0-2.5.0-all.tar.gz \\ 11 | tar -C /usr/local/share/kafka/plugins/kafka-connect-aws-s3 --warning=no-unknown-keyword -xzf - 12 13# update connect plugin path 14ENV CONNECT_PLUGIN_PATH=$CONNECT_PLUGIN_PATH,/usr/local/share/kafka/plugins 15USER appuser Start Services After starting the docker compose services, we can check a local Kafka cluster in the control center via http://localhost:9021. We can go to the cluster overview page by clicking the cluster card item.\nWe can check an overview of the cluster in the page. For example, it shows the cluster has 1 broker and there is no running connector. On the left side, there are menus for individual components. Among those, Topics and Connect will be our main interest in this post.\nCreate Connectors Source Connector Debezium\u0026rsquo;s PostgreSQL connector captures row-level changes in the schemas of a PostgreSQL database. PostgreSQL versions 9.6, 10, 11, 12 and 13 are supported. The first time it connects to a PostgreSQL server or cluster, the connector takes a consistent snapshot of all schemas. After that snapshot is complete, the connector continuously captures row-level changes that insert, update, and delete database content and that were committed to a PostgreSQL database. The connector generates data change event records and streams them to Kafka topics. For each table, the default behavior is that the connector streams all generated events to a separate Kafka topic for that table. Applications and services consume data change event records from that topic.\nThe connector has a number of connector properties including name, connector class, database connection details, key/value converter and so on - the full list of properties can be found in this page. The properties that need explanation are listed below.\nplugin.name - Using the logical decoding feature, an output plug-in enables clients to consume changes to the transaction log in a user-friendly manner. Debezium supports decoderbufs, wal2json and pgoutput plug-ins. Both wal2json and pgoutput are available in Amazon RDS for PostgreSQL and Amazon Aurora PostgreSQL. decoderbufs requires a separate installation, and it is excluded from the option. Among the 2 supported plug-ins, pgoutput is selected because it is the standard logical decoding output plug-in in PostgreSQL 10+ and has better performance for large transactions. publication.name - With the pgoutput plug-in, the Debezium connector creates a publication (if not exists) and sets publication.autocreate.mode to all_tables. It can cause an issue to update a record to a table that doesn\u0026rsquo;t have the primary key or replica identity. We can set the value to filtered where the connector adjusts the applicable tables by other property values. Alternatively we can create a publication on our own and add the name to publication.name property. I find creating a publication explicitly is easier to maintain. Note a publication alone is not sufficient to handle the issue. All affected tables by the publication should have the primary key or replica identity. In our example, the _orders _and _order_details _tables should meet the condition. In short, creating an explicit publication can prevent the event generation process from interrupting other processes by limiting the scope of CDC event generation. key.converter/value.converter - Although Avro serialization is recommended, JSON is a format that can be generated without schema registry and can be read by DeltaStreamer. transforms - A Debezium event data has a complex structure that provides a wealth of information. It can be quite difficult to process such a structure using DeltaStreamer. Debezium\u0026rsquo;s event flattening single message transformation (SMT) is configured to flatten the output payload. Note once the connector is deployed, the CDC event records will be published to demo.datalake.cdc_events topic.\n1// ./connector/local/source-debezium.json 2{ 3 \u0026#34;name\u0026#34;: \u0026#34;orders-source\u0026#34;, 4 \u0026#34;config\u0026#34;: { 5 \u0026#34;connector.class\u0026#34;: \u0026#34;io.debezium.connector.postgresql.PostgresConnector\u0026#34;, 6 \u0026#34;tasks.max\u0026#34;: \u0026#34;1\u0026#34;, 7 \u0026#34;plugin.name\u0026#34;: \u0026#34;pgoutput\u0026#34;, 8 \u0026#34;publication.name\u0026#34;: \u0026#34;cdc_publication\u0026#34;, 9 \u0026#34;database.hostname\u0026#34;: \u0026#34;postgres\u0026#34;, 10 \u0026#34;database.port\u0026#34;: \u0026#34;5432\u0026#34;, 11 \u0026#34;database.user\u0026#34;: \u0026#34;devuser\u0026#34;, 12 \u0026#34;database.password\u0026#34;: \u0026#34;password\u0026#34;, 13 \u0026#34;database.dbname\u0026#34;: \u0026#34;devdb\u0026#34;, 14 \u0026#34;database.server.name\u0026#34;: \u0026#34;demo\u0026#34;, 15 \u0026#34;schema.include\u0026#34;: \u0026#34;datalake\u0026#34;, 16 \u0026#34;table.include.list\u0026#34;: \u0026#34;datalake.cdc_events\u0026#34;, 17 \u0026#34;key.converter\u0026#34;: \u0026#34;org.apache.kafka.connect.json.JsonConverter\u0026#34;, 18 \u0026#34;value.converter\u0026#34;: \u0026#34;org.apache.kafka.connect.json.JsonConverter\u0026#34;, 19 \u0026#34;transforms\u0026#34;: \u0026#34;unwrap\u0026#34;, 20 \u0026#34;transforms.unwrap.type\u0026#34;: \u0026#34;io.debezium.transforms.ExtractNewRecordState\u0026#34;, 21 \u0026#34;transforms.unwrap.drop.tombstones\u0026#34;: \u0026#34;false\u0026#34;, 22 \u0026#34;transforms.unwrap.delete.handling.mode\u0026#34;: \u0026#34;rewrite\u0026#34;, 23 \u0026#34;transforms.unwrap.add.fields\u0026#34;: \u0026#34;op,db,table,schema,lsn,source.ts_ms\u0026#34; 24 } 25} The source connector is created using the API and its status can be checked as shown below.\n1## create debezium source connector 2curl -i -X POST -H \u0026#34;Accept:application/json\u0026#34; -H \u0026#34;Content-Type:application/json\u0026#34; \\ 3 http://localhost:8083/connectors/ -d @connector/local/source-debezium.json 4 5## check connector status 6curl http://localhost:8083/connectors/orders-source/status 7 8#{ 9# \u0026#34;name\u0026#34;: \u0026#34;orders-source\u0026#34;, 10# \u0026#34;connector\u0026#34;: { \u0026#34;state\u0026#34;: \u0026#34;RUNNING\u0026#34;, \u0026#34;worker_id\u0026#34;: \u0026#34;connect:8083\u0026#34; }, 11# \u0026#34;tasks\u0026#34;: [{ \u0026#34;id\u0026#34;: 0, \u0026#34;state\u0026#34;: \u0026#34;RUNNING\u0026#34;, \u0026#34;worker_id\u0026#34;: \u0026#34;connect:8083\u0026#34; }], 12# \u0026#34;type\u0026#34;: \u0026#34;source\u0026#34; 13#} Sink Connector Lenses S3 Connector is a Kafka Connect sink connector for writing records from Kafka to AWS S3 Buckets. It extends the standard connect config adding a parameter for a SQL command (Lenses Kafka Connect Query Language or \u0026ldquo;KCQL\u0026rdquo;). This defines how to map data from the source (in this case Kafka) to the target (S3). Importantly, it also includes how data should be partitioned into S3, the bucket names and the serialization format (support includes JSON, Avro, Parquet, Text, CSV and binary).\nI find the Lenses S3 connector is more straightforward to configure than the Confluent S3 sink connector for its SQL-like syntax. The KCQL configuration indicates that object files are set to be\nmoved from a Kafka topic (demo.datalake.cdc_events) to an S3 bucket (data-lake-demo-cevo) with object prefix of _cdc-events-local, partitioned by customer_id and order_id e.g. customer_id=\u0026lt;customer-id\u0026gt;/order_id=\u0026lt;order-id\u0026gt;, stored as the JSON format and, flushed every 60 seconds or when there are 50 records. 1// ./connector/local/sink-s3.json 2{ 3 \u0026#34;name\u0026#34;: \u0026#34;orders-sink\u0026#34;, 4 \u0026#34;config\u0026#34;: { 5 \u0026#34;connector.class\u0026#34;: \u0026#34;io.lenses.streamreactor.connect.aws.s3.sink.S3SinkConnector\u0026#34;, 6 \u0026#34;tasks.max\u0026#34;: \u0026#34;1\u0026#34;, 7 \u0026#34;connect.s3.kcql\u0026#34;: \u0026#34;INSERT INTO data-lake-demo-cevo:cdc-events-local SELECT * FROM demo.datalake.cdc_events PARTITIONBY customer_id,order_id STOREAS `json` WITH_FLUSH_INTERVAL = 60 WITH_FLUSH_COUNT = 50\u0026#34;, 8 \u0026#34;aws.region\u0026#34;: \u0026#34;ap-southeast-2\u0026#34;, 9 \u0026#34;aws.custom.endpoint\u0026#34;: \u0026#34;https://s3.ap-southeast-2.amazonaws.com/\u0026#34;, 10 \u0026#34;topics\u0026#34;: \u0026#34;demo.datalake.cdc_events\u0026#34;, 11 \u0026#34;key.converter.schemas.enable\u0026#34;: \u0026#34;false\u0026#34;, 12 \u0026#34;schema.enable\u0026#34;: \u0026#34;false\u0026#34;, 13 \u0026#34;errors.log.enable\u0026#34;: \u0026#34;true\u0026#34;, 14 \u0026#34;key.converter\u0026#34;: \u0026#34;org.apache.kafka.connect.json.JsonConverter\u0026#34;, 15 \u0026#34;value.converter\u0026#34;: \u0026#34;org.apache.kafka.connect.json.JsonConverter\u0026#34; 16 } 17} The sink connector is created using the API and its status can be checked as shown below.\n1### create s3 sink connector 2curl -i -X POST -H \u0026#34;Accept:application/json\u0026#34; -H \u0026#34;Content-Type:application/json\u0026#34; \\ 3 http://localhost:8083/connectors/ -d @connector/local/sink-s3.json 4 5## check connector status 6curl http://localhost:8083/connectors/orders-sink/status 7 8#{ 9# \u0026#34;name\u0026#34;: \u0026#34;orders-sink\u0026#34;, 10# \u0026#34;connector\u0026#34;: { \u0026#34;state\u0026#34;: \u0026#34;RUNNING\u0026#34;, \u0026#34;worker_id\u0026#34;: \u0026#34;connect:8083\u0026#34; }, 11# \u0026#34;tasks\u0026#34;: [{ \u0026#34;id\u0026#34;: 0, \u0026#34;state\u0026#34;: \u0026#34;RUNNING\u0026#34;, \u0026#34;worker_id\u0026#34;: \u0026#34;connect:8083\u0026#34; }], 12# \u0026#34;type\u0026#34;: \u0026#34;sink\u0026#34; 13#} We can also check the details of the connectors from the control center.\nIn the Topics menu, we are able to see that the demo.datalake.cdc_events topic is created by the Debezium connector.\nWe can check messages of a topic by clicking the topic name. After adding an offset value (e.g. 0) to the input element, we are able to see messages of the topic. We can check message fields on the left-hand side or download a message in the JSON or CSV format.\nCheck Event Output Files We can check the output files that are processed by the sink connector in S3. Below shows an example record where _customer_id _is RATTC and _order_id _is 11077. As configured, the objects are prefixed by cdc-events-local and further partitioned by _customer_id _and order_id. The naming convention of output files is \u0026lt;topic-name\u0026gt;(partition_offset).ext.\nUpdate Event Example The above order record has a NULL shipped_date value. When we update it using the following SQL statement, we should be able to see a new output file with the updated value.\n1BEGIN TRANSACTION; 2 UPDATE orders 3 SET shipped_date = \u0026#39;1998-06-15\u0026#39;::date 4 WHERE order_id = 11077; 5COMMIT TRANSACTION; 6END; In S3, we are able to see that a new output file is stored. In the new output file, the _shipped_date _value is updated to 10392. Note that the Debezium connector converts the DATE type to the INT32 type, which represents the number of days since the epoch.\nInsert Event Example When a new order is created, it\u0026rsquo;ll insert a record to the _orders _table as well as one or more order items to the _order_details _table. Therefore, we expect multiple event records will be created when a new order is created. We can check it by inserting an order and related order details items.\n1BEGIN TRANSACTION; 2 INSERT INTO orders VALUES (11075, \u0026#39;RICSU\u0026#39;, 8, \u0026#39;1998-05-06\u0026#39;, \u0026#39;1998-06-03\u0026#39;, NULL, 2, 6.19000006, \u0026#39;Richter Supermarkt\u0026#39;, \u0026#39;Starenweg 5\u0026#39;, \u0026#39;Genève\u0026#39;, NULL, \u0026#39;1204\u0026#39;, \u0026#39;Switzerland\u0026#39;); 3 INSERT INTO order_details VALUES (11075, 2, 19, 10, 0.150000006); 4 INSERT INTO order_details VALUES (11075, 46, 12, 30, 0.150000006); 5 INSERT INTO order_details VALUES (11075, 76, 18, 2, 0.150000006); 6COMMIT TRANSACTION; 7END; We can see the output file includes 4 JSON objects where the first object has NULL _order_items _and _products _value. We can also see that those values are expanded gradually in subsequent event records.\nConclusion We discussed a data lake solution where data ingestion is performed using change data capture (CDC) and the output files are upserted to a Hudi table. Being registered to Glue Data Catalog, it can be used for ad-hoc queries and report/dashboard creation. For the solution, the Northwind database is used as the source database and, following the transactional outbox pattern, order-related changes are upserted to an outbox table by triggers. The data ingestion is developed in the local Confluent platform where the Debezium for PostgreSQL is used as the source connector and the Lenses S3 sink connector is used as the sink connector. We confirmed order creation and update events are captured as expected. In the next post, we\u0026rsquo;ll build the data ingestion part of the solution with Amazon MSK and MSK Connect.\n","date":"December 5, 2021","img":"/blog/2021-12-05-datalake-demo-part1/featured.png","lang":"en","langName":"English","largeImg":"/blog/2021-12-05-datalake-demo-part1/featured_hua25eccd3824300d3b1ed87f56797248c_164526_500x0_resize_box_3.png","permalink":"/blog/2021-12-05-datalake-demo-part1/","series":[{"title":"Data Lake Demo Using Change Data Capture","url":"/series/data-lake-demo-using-change-data-capture/"}],"smallImg":"/blog/2021-12-05-datalake-demo-part1/featured_hua25eccd3824300d3b1ed87f56797248c_164526_180x0_resize_box_3.png","tags":[{"title":"AWS","url":"/tags/aws/"},{"title":"Amazon EMR","url":"/tags/amazon-emr/"},{"title":"Amazon MSK","url":"/tags/amazon-msk/"},{"title":"Amazon MSK Connect","url":"/tags/amazon-msk-connect/"},{"title":"Apache Spark","url":"/tags/apache-spark/"},{"title":"Apache Hudi","url":"/tags/apache-hudi/"},{"title":"Apache Kafka","url":"/tags/apache-kafka/"},{"title":"Change Data Capture","url":"/tags/change-data-capture/"},{"title":"Data Lake","url":"/tags/data-lake/"},{"title":"Docker","url":"/tags/docker/"},{"title":"Docker Compose","url":"/tags/docker-compose/"}],"timestamp":1638662400,"title":"Data Lake Demo Using Change Data Capture (CDC) on AWS – Part 1 Local Development"},{"categories":[{"title":"Data Engineering","url":"/categories/data-engineering/"}],"content":"In an earlier post, I demonstrated how to set up a local development environment for AWS Glue 1.0 and 2.0 using a docker image that is published by the AWS Glue team and the Visual Studio Code Remote – Containers extension. Recently AWS Glue 3.0 was released, but a docker image for this version is not published. In this post, I\u0026rsquo;ll illustrate how to create a development environment for AWS Glue 3.0 (and later versions) by building a custom docker image.\nGlue Base Docker Image The Glue base images are built while referring to the official AWS Glue Python local development documentation. For example, the latest image that targets Glue 3.0 is built on top of the official Python image on the latest stable Debian version (python:3.7.12-bullseye). After installing utilities (zip and AWS CLI V2), Open JDK 8 is installed. Then Maven, Spark and Glue Python libraries (aws-glue-libs) are added to the /opt directory and Glue dependencies are downloaded by sourcing glue-setup.sh. It ends up downloading default Python packages and updating the _GLUE_HOME _and PYTHONPATH environment variables. The Dockerfile can be shown below, and it can also be found in the project GitHub repository.\n1## glue-base/3.0/Dockerfile 2FROM python:3.7.12-bullseye 3 4## Install utils 5RUN apt-get update \u0026amp;\u0026amp; apt-get install -y zip 6 7RUN curl \u0026#34;https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\u0026#34; -o \u0026#34;awscliv2.zip\u0026#34; \\ 8 \u0026amp;\u0026amp; unzip awscliv2.zip \u0026amp;\u0026amp; ./aws/install 9 10## Install Open JDK 8 11RUN apt-get update \\ 12 \u0026amp;\u0026amp; apt-get install -y software-properties-common \\ 13 \u0026amp;\u0026amp; apt-add-repository \u0026#39;deb http://security.debian.org/debian-security stretch/updates main\u0026#39; \\ 14 \u0026amp;\u0026amp; apt-get update \\ 15 \u0026amp;\u0026amp; apt-get install -y openjdk-8-jdk 16 17## Create environment variables 18ENV M2_HOME=/opt/apache-maven-3.6.0 19ENV SPARK_HOME=/opt/spark-3.1.1-amzn-0-bin-3.2.1-amzn-3 20ENV PATH=\u0026#34;${PATH}:${M2_HOME}/bin\u0026#34; 21 22## Add Maven, Spark and AWS Glue Libs to /opt 23RUN curl -SsL https://aws-glue-etl-artifacts.s3.amazonaws.com/glue-common/apache-maven-3.6.0-bin.tar.gz \\ 24 | tar -C /opt --warning=no-unknown-keyword -xzf - 25RUN curl -SsL https://aws-glue-etl-artifacts.s3.amazonaws.com/glue-3.0/spark-3.1.1-amzn-0-bin-3.2.1-amzn-3.tgz \\ 26 | tar -C /opt --warning=no-unknown-keyword -xf - 27RUN curl -SsL https://github.com/awslabs/aws-glue-libs/archive/refs/tags/v3.0.tar.gz \\ 28 | tar -C /opt --warning=no-unknown-keyword -xzf - 29 30# Install Glue dependencies 31RUN cd /opt/evoaustraliaaws-glue-libs-3.0/bin/ \\ 32 \u0026amp;\u0026amp; bash -c \u0026#34;source glue-setup.sh\u0026#34; 33 34## Add default Python packages 35COPY ./requirements.txt /tmp/requirements.txt 36RUN pip install -r /tmp/requirements.txt 37 38## Update Python path 39ENV GLUE_HOME=/opt/aws-glue-libs-3.0 40ENV PYTHONPATH=$GLUE_HOME:$SPARK_HOME/python/lib/pyspark.zip:$SPARK_HOME/python/lib/py4j-0.10.9-src.zip:$SPARK_HOME/python 41 42EXPOSE 4040 43 44CMD [\u0026#34;bash\u0026#34;] It is published to the glue-base repository of Cevo Australia\u0026rsquo;s public ECR registry with the following tags. Later versions of Glue base images will be published with relevant tags.\npublic.ecr.aws/cevoaustralia/glue-base:latest public.ecr.aws/cevoaustralia/glue-base:3.0 Usage The Glue base image can be used for running a Pyspark shell or submitting a spark application as shown below. For the spark application, I assume the project repository is mapped to the container\u0026rsquo;s /tmp folder. The Glue Python libraries also support Pytest, and it\u0026rsquo;ll be discussed later in the post.\n1docker run --rm -it \\ 2 -v $HOME/.aws:/root/.aws \\ 3 public.ecr.aws/cevoaustralia/glue-base bash -c \u0026#34;/opt/aws-glue-libs-3.0/bin/gluepyspark\u0026#34; 4 5docker run --rm -it \\ 6 -v $HOME/.aws:/root/.aws \\ 7 -v $PWD:/tmp/glue-vscode \\ 8 public.ecr.aws/cevoaustralia/glue-base bash -c \u0026#34;/opt/aws-glue-libs-3.0/bin/gluesparksubmit /tmp/glue-vscode/example.py\u0026#34; Extend Glue Base Image We can extend the Glue base image using the Visual Studio Code Dev Containers extension. The configuration for the extension can be found in the .devcontainer folder. The folder includes the Dockerfile for the development docker image and remote container configuration file (devcontainer.json). The other contents include the source for the Glue base image and materials for Pyspark, spark-submit and Pytest demonstrations. These will be illustrated below.\n1. 2├── .devcontainer 3│ ├── pkgs 4│ │ └── dev.txt 5│ ├── Dockerfile 6│ └── devcontainer.json 7├── .gitignore 8├── README.md 9├── example.py 10├── execute.sh 11├── glue-base 12│ └── 3.0 13│ ├── Dockerfile 14│ └── requirements.txt 15├── src 16│ └── utils.py 17└── tests 18 ├── __init__.py 19 ├── conftest.py 20 └── test_utils.py Development Docker Image The Glue base Docker image runs as the root user, and it is not convenient to write code with it. Therefore, a non-root user is created whose username corresponds to the logged-in user\u0026rsquo;s username - the _USERNAME _argument will be set accordingly in devcontainer.json. Next the sudo program is installed and the non-root user is added to the Sudo group. Note the Python Glue library\u0026rsquo;s executables are configured to run with the root user so that the sudo program is necessary to run those executables. Finally, it installs additional development Python packages.\n1## .devcontainer/Dockerfile 2FROM public.ecr.aws/i0m5p1b5/glue-base:3.0 3 4ARG USERNAME 5ARG USER_UID 6ARG USER_GID 7 8## Create non-root user 9RUN groupadd --gid $USER_GID $USERNAME \\ 10 \u0026amp;\u0026amp; useradd --uid $USER_UID --gid $USER_GID -m $USERNAME 11 12## Add sudo support in case we need to install software after connecting 13RUN apt-get update \\ 14 \u0026amp;\u0026amp; apt-get install -y sudo nano \\ 15 \u0026amp;\u0026amp; echo $USERNAME ALL=\\(root\\) NOPASSWD:ALL \u0026gt; /etc/sudoers.d/$USERNAME \\ 16 \u0026amp;\u0026amp; chmod 0440 /etc/sudoers.d/$USERNAME 17 18## Install Python packages 19COPY ./pkgs /tmp/pkgs 20RUN pip install -r /tmp/pkgs/dev.txt Container Configuration The development container will be created by building an image from the Dockerfile illustrated above. The logged-in user\u0026rsquo;s username is provided to create a non-root user and the container is set to run as the user as well. And 2 Visual Studio Code extensions are installed - Python and Prettier. Also, the current folder is mounted to the container\u0026rsquo;s workspace folder and 2 additional folders are mounted - they are to share AWS credentials and SSH keys. Note that AWS credentials are mounted to /root/.aws because the Python Glue library\u0026rsquo;s executables will be run as the root user. Then the port 4040 is set to be forwarded, which is used for the Spark UI. Finally, additional editor settings are added at the end.\n1// .devcontainer/devcontainer.json 2{ 3 \u0026#34;name\u0026#34;: \u0026#34;glue\u0026#34;, 4 \u0026#34;build\u0026#34;: { 5 \u0026#34;dockerfile\u0026#34;: \u0026#34;Dockerfile\u0026#34;, 6 \u0026#34;args\u0026#34;: { 7 \u0026#34;USERNAME\u0026#34;: \u0026#34;${localEnv:USER}\u0026#34;, 8 \u0026#34;USER_UID\u0026#34;: \u0026#34;1000\u0026#34;, 9 \u0026#34;USER_GID\u0026#34;: \u0026#34;1000\u0026#34; 10 } 11 }, 12 \u0026#34;containerUser\u0026#34;: \u0026#34;${localEnv:USER}\u0026#34;, 13 \u0026#34;extensions\u0026#34;: [ 14 \u0026#34;ms-python.python\u0026#34;, 15 \u0026#34;esbenp.prettier-vscode\u0026#34; 16 ], 17 \u0026#34;workspaceMount\u0026#34;: \u0026#34;source=${localWorkspaceFolder},target=${localEnv:HOME}/glue-vscode,type=bind,consistency=cached\u0026#34;, 18 \u0026#34;workspaceFolder\u0026#34;: \u0026#34;${localEnv:HOME}/glue-vscode\u0026#34;, 19 \u0026#34;forwardPorts\u0026#34;: [4040], 20 \u0026#34;mounts\u0026#34;: [ 21 \u0026#34;source=${localEnv:HOME}/.aws,target=/root/.aws,type=bind,consistency=cached\u0026#34;, 22 \u0026#34;source=${localEnv:HOME}/.ssh,target=${localEnv:HOME}/.ssh,type=bind,consistency=cached\u0026#34; 23 ], 24 \u0026#34;settings\u0026#34;: { 25 \u0026#34;terminal.integrated.profiles.linux\u0026#34;: { 26 \u0026#34;bash\u0026#34;: { 27 \u0026#34;path\u0026#34;: \u0026#34;/bin/bash\u0026#34; 28 } 29 }, 30 \u0026#34;terminal.integrated.defaultProfile.linux\u0026#34;: \u0026#34;bash\u0026#34;, 31 \u0026#34;editor.formatOnSave\u0026#34;: true, 32 \u0026#34;editor.defaultFormatter\u0026#34;: \u0026#34;esbenp.prettier-vscode\u0026#34;, 33 \u0026#34;editor.tabSize\u0026#34;: 2, 34 \u0026#34;python.testing.pytestEnabled\u0026#34;: true, 35 \u0026#34;python.linting.enabled\u0026#34;: true, 36 \u0026#34;python.linting.pylintEnabled\u0026#34;: false, 37 \u0026#34;python.linting.flake8Enabled\u0026#34;: false, 38 \u0026#34;python.formatting.provider\u0026#34;: \u0026#34;black\u0026#34;, 39 \u0026#34;python.formatting.blackPath\u0026#34;: \u0026#34;black\u0026#34;, 40 \u0026#34;python.formatting.blackArgs\u0026#34;: [\u0026#34;--line-length\u0026#34;, \u0026#34;100\u0026#34;], 41 \u0026#34;[python]\u0026#34;: { 42 \u0026#34;editor.tabSize\u0026#34;: 4, 43 \u0026#34;editor.defaultFormatter\u0026#34;: \u0026#34;ms-python.python\u0026#34; 44 } 45 } 46} Launch Container The development container can be run by executing the following command in the command palette.\nRemote-Containers: Open Folder in Container\u0026hellip; Once the development container is ready, the workspace folder will be open within the container.\nExamples I\u0026rsquo;ve created a script (execute.sh) to run the executables easily. The main command indicates which executable to run and possible values are pyspark, spark-submit and pytest. Below shows some example commands.\n1./execute.sh pyspark # pyspark 2./execute.sh spark-submit example.py # spark submit 3./execute.sh pytest -svv # pytest 1## execute.sh 2#!/usr/bin/env bash 3 4## remove first argument 5execution=$1 6echo \u0026#34;execution type - $execution\u0026#34; 7 8shift 1 9echo $@ 10 11## set up command 12if [ $execution == \u0026#39;pyspark\u0026#39; ]; then 13 sudo su -c \u0026#34;$GLUE_HOME/bin/gluepyspark\u0026#34; 14elif [ $execution == \u0026#39;spark-submit\u0026#39; ]; then 15 sudo su -c \u0026#34;$GLUE_HOME/bin/gluesparksubmit $@\u0026#34; 16elif [ $execution == \u0026#39;pytest\u0026#39; ]; then 17 sudo su -c \u0026#34;$GLUE_HOME/bin/gluepytest $@\u0026#34; 18else 19 echo \u0026#34;unsupported execution type - $execution\u0026#34; 20 exit 1 21fi Pyspark Using the script above, we can launch PySpark. A screenshot of the PySpark shell can be found below.\n1./execute.sh pyspark Spark Submit Below shows one of the Python samples in the Glue documentation. It pulls 3 data sets from a database called legislators. Then they are joined to create a history data set (l_history) and saved into S3.\n1./execute.sh spark-submit example.py 1## example.py 2from awsglue.dynamicframe import DynamicFrame 3from awsglue.transforms import Join 4from awsglue.utils import getResolvedOptions 5from pyspark.context import SparkContext 6from awsglue.context import GlueContext 7 8glueContext = GlueContext(SparkContext.getOrCreate()) 9 10DATABASE = \u0026#34;legislators\u0026#34; 11OUTPUT_PATH = \u0026#34;s3://glue-python-samples-fbe445ee/output_dir\u0026#34; 12 13## create dynamic frames from data catalog 14persons: DynamicFrame = glueContext.create_dynamic_frame.from_catalog( 15 database=DATABASE, table_name=\u0026#34;persons_json\u0026#34; 16) 17 18memberships: DynamicFrame = glueContext.create_dynamic_frame.from_catalog( 19 database=DATABASE, table_name=\u0026#34;memberships_json\u0026#34; 20) 21 22orgs: DynamicFrame = glueContext.create_dynamic_frame.from_catalog( 23 database=DATABASE, table_name=\u0026#34;organizations_json\u0026#34; 24) 25 26## manipulate data 27orgs = ( 28 orgs.drop_fields([\u0026#34;other_names\u0026#34;, \u0026#34;identifiers\u0026#34;]) 29 .rename_field(\u0026#34;id\u0026#34;, \u0026#34;org_id\u0026#34;) 30 .rename_field(\u0026#34;name\u0026#34;, \u0026#34;org_name\u0026#34;) 31) 32 33l_history: DynamicFrame = Join.apply( 34 orgs, Join.apply(persons, memberships, \u0026#34;id\u0026#34;, \u0026#34;person_id\u0026#34;), \u0026#34;org_id\u0026#34;, \u0026#34;organization_id\u0026#34; 35) 36l_history = l_history.drop_fields([\u0026#34;person_id\u0026#34;, \u0026#34;org_id\u0026#34;]) 37 38l_history.printSchema() 39 40## write to s3 41glueContext.write_dynamic_frame.from_options( 42 frame=l_history, 43 connection_type=\u0026#34;s3\u0026#34;, 44 connection_options={\u0026#34;path\u0026#34;: f\u0026#34;{OUTPUT_PATH}/legislator_history\u0026#34;}, 45 format=\u0026#34;parquet\u0026#34;, 46) When the execution completes, we can see the joined data set is stored as a parquet file in the output S3 bucket.\nNote that we can monitor and inspect Spark job executions in the Spark UI on port 4040.\nPytest We can test a function that deals with a DynamicFrame. Below shows a test case for a simple function that filters a DynamicFrame based on a column value.\n1./execute.sh pytest -svv 1## src/utils.py 2from awsglue.dynamicframe import DynamicFrame 3 4def filter_dynamic_frame(dyf: DynamicFrame, column_name: str, value: int): 5 return dyf.filter(f=lambda x: x[column_name] \u0026gt; value) 6 7## tests/conftest.py 8from pyspark.context import SparkContext 9from awsglue.context import GlueContext 10import pytest 11 12@pytest.fixture(scope=\u0026#34;session\u0026#34;) 13def glueContext(): 14 sparkContext = SparkContext() 15 glueContext = GlueContext(sparkContext) 16 yield glueContext 17 sparkContext.stop() 18 19 20## tests/test_utils.py 21from typing import List 22from awsglue.dynamicframe import DynamicFrame 23import pandas as pd 24from src.utils import filter_dynamic_frame 25 26def _get_sorted_data_frame(pdf: pd.DataFrame, columns_list: List[str] = None): 27 if columns_list is None: 28 columns_list = list(pdf.columns.values) 29 return pdf.sort_values(columns_list).reset_index(drop=True) 30 31 32def test_filter_dynamic_frame_by_value(glueContext): 33 spark = glueContext.spark_session 34 35 input = spark.createDataFrame( 36 [(\u0026#34;charly\u0026#34;, 15), (\u0026#34;fabien\u0026#34;, 18), (\u0026#34;sam\u0026#34;, 21), (\u0026#34;sam\u0026#34;, 25), (\u0026#34;nick\u0026#34;, 19), (\u0026#34;nick\u0026#34;, 40)], 37 [\u0026#34;name\u0026#34;, \u0026#34;age\u0026#34;], 38 ) 39 40 expected_output = spark.createDataFrame( 41 [(\u0026#34;sam\u0026#34;, 25), (\u0026#34;sam\u0026#34;, 21), (\u0026#34;nick\u0026#34;, 40)], 42 [\u0026#34;name\u0026#34;, \u0026#34;age\u0026#34;], 43 ) 44 45 real_output = filter_dynamic_frame(DynamicFrame.fromDF(input, glueContext, \u0026#34;output\u0026#34;), \u0026#34;age\u0026#34;, 20) 46 47 pd.testing.assert_frame_equal( 48 _get_sorted_data_frame(real_output.toDF().toPandas(), [\u0026#34;name\u0026#34;, \u0026#34;age\u0026#34;]), 49 _get_sorted_data_frame(expected_output.toPandas(), [\u0026#34;name\u0026#34;, \u0026#34;age\u0026#34;]), 50 check_like=True, 51 ) Conclusion In this post, I demonstrated how to build local development environments for AWS Glue 3.0 and later using a custom docker image and the Visual Studio Code Remote - Containers extension. Then examples of launching Pyspark shells, submitting an application and running a test are shown. I hope this post is useful to develop and test Glue ETL scripts locally.\n","date":"November 14, 2021","img":"/blog/2021-11-14-glue-3-local-development/featured.png","lang":"en","langName":"English","largeImg":"/blog/2021-11-14-glue-3-local-development/featured_hu689859cb443a98b1c98384fedaa00395_30923_500x0_resize_box_3.png","permalink":"/blog/2021-11-14-glue-3-local-development/","series":[],"smallImg":"/blog/2021-11-14-glue-3-local-development/featured_hu689859cb443a98b1c98384fedaa00395_30923_180x0_resize_box_3.png","tags":[{"title":"AWS","url":"/tags/aws/"},{"title":"AWS Glue","url":"/tags/aws-glue/"},{"title":"Docker","url":"/tags/docker/"},{"title":"Apache Spark","url":"/tags/apache-spark/"},{"title":"PySpark","url":"/tags/pyspark/"},{"title":"Python","url":"/tags/python/"},{"title":"Visual Studio Code","url":"/tags/visual-studio-code/"}],"timestamp":1636848000,"title":"Local Development of AWS Glue 3.0 and Later"},{"categories":[{"title":"Engineering","url":"/categories/engineering/"}],"content":"Triggering a Lambda function by an EventBridge Events rule can be used as a _serverless _replacement of cron job. The highest frequency of it is one invocation per minute so that it cannot be used directly if you need to schedule a Lambda function more frequently. For example, it may be refreshing an application with real time metrics from an Amazon Connect instance where some metrics are updated every 15 seconds. There is a post in the AWS Architecture Blog, and it suggests using AWS Step Functions. Or a usual recommendation is using Amazon EC2. Albeit being serverless, the former gets a bit complicated especially in order to handle the hard quota of 25,000 entries in the execution history. And the latter is not an option if you look for a serverless solution. In this post, I’ll demonstrate another serverless solution of scheduling a Lambda function at a sub-minute frequency using Amazon SQS.\nArchitecture The solution contains 2 Lambda functions and each of them has its own event source: EventBridge Events rule and SQS.\nA Lambda function (sender) is invoked every minute by an EventBridge Events rule. The function sends messages to a queue with different delay seconds values. For example, if we want to invoke the consumer Lambda function every 10 seconds, we can send 6 messages with delay seconds values of 0, 10, 20, 30, 40 and 50. The consumer is invoked after the delay seconds as the messages are visible. I find this architecture is simpler than other options.\nLambda Functions The sender Lambda function sends messages with different delay second values to a queue. An array of those values are generated by generateDelaySeconds(), given an interval value. Note that this function works well if the interval value is less than or equal to 30. If we want to set up a higher interval value, we should update the function together with the EventBridge Event rule. The source can be found in the GitHub repository.\n1// src/sender.js 2const AWS = require(\u0026#34;aws-sdk\u0026#34;); 3 4const sqs = new AWS.SQS({ 5 apiVersion: \u0026#34;2012-11-05\u0026#34;, 6 region: process.env.AWS_REGION || \u0026#34;us-east-1\u0026#34;, 7}); 8 9/** 10 * Generate delay seconds by an interval value. 11 * 12 * @example 13 * // returns [ 0, 30 ] 14 * generateDelaySeconds(30) 15 * // returns [ 0, 20, 40 ] 16 * generateDelaySeconds(20) 17 * // returns [ 0, 15, 30, 45 ] 18 * generateDelaySeconds(15) 19 * // returns [ 0, 10, 20, 30, 40, 50 ] 20 * generateDelaySeconds(10) 21 */ 22const generateDelaySeconds = (interval) =\u0026gt; { 23 const numElem = Math.round(60 / interval); 24 const array = Array.apply(0, Array(numElem + 1)).map((_, index) =\u0026gt; { 25 return index; 26 }); 27 const min = Math.min(...array); 28 const max = Math.max(...array); 29 return array 30 .map((a) =\u0026gt; Math.round(((a - min) / (max - min)) * 60)) 31 .filter((a) =\u0026gt; a \u0026lt; 60); 32}; 33 34const handler = async () =\u0026gt; { 35 const interval = process.env.SCHEDULE_INTERVAL || 30; 36 const delaySeconds = generateDelaySeconds(interval); 37 for (const ds of delaySeconds) { 38 const params = { 39 MessageBody: JSON.stringify({ delaySecond: ds }), 40 QueueUrl: process.env.QUEUE_URL, 41 DelaySeconds: ds, 42 }; 43 await sqs.sendMessage(params).promise(); 44 } 45 console.log(`send messages, delay seconds - ${delaySeconds.join(\u0026#34;, \u0026#34;)}`); 46}; 47 48module.exports = { handler }; The consumer Lambda function simply polls the messages. It is set to finish after 1 second followed by logging the delay second value.\n1// src/consumer.js 2const sleep = (ms) =\u0026gt; { 3 return new Promise((resolve) =\u0026gt; { 4 setTimeout(resolve, ms); 5 }); 6}; 7 8const handler = async (event) =\u0026gt; { 9 for (const rec of event.Records) { 10 const body = JSON.parse(rec.body); 11 console.log(`delay second - ${body.delaySecond}`); 12 await sleep(1000); 13 } 14}; 15 16module.exports = { handler }; Serverless Service Two Lambda functions (sender and consumer) and a queue are created by Serverless Framework. As discussed earlier the sender function has an EventBridge Event rule trigger, and it invokes the function at the rate of 1 minute. The schedule interval is set to 10, which is used to create delay seconds values. The consumer is set to be triggered by the queue.\n1# serverless.yml 2service: ${self:custom.serviceName} 3 4plugins: 5 - serverless-iam-roles-per-function 6 7custom: 8 serviceName: lambda-schedule 9 scheduleInterval: 10 10 queue: 11 name: ${self:custom.serviceName}-queue-${self:provider.stage} 12 url: !Ref Queue 13 arn: !GetAtt Queue.Arn 14 15… 16 17provider: 18 name: aws 19 runtime: nodejs12.x 20 stage: ${opt:stage, \u0026#39;dev\u0026#39;} 21 region: ${opt:region, \u0026#39;us-east-1\u0026#39;} 22 lambdaHashingVersion: 20201221 23 memorySize: 128 24 logRetentionInDays: 7 25 deploymentBucket: 26 tags: 27 OWNER: ${env:owner} 28 stackTags: 29 OWNER: ${env:owner} 30 31functions: 32 sender: 33 handler: src/sender.handler 34 name: ${self:custom.serviceName}-sender-${self:provider.stage} 35 events: 36 - eventBridge: 37 schedule: rate(1 minute) 38 enabled: true 39 environment: 40 SCHEDULE_INTERVAL: ${self:custom.scheduleInterval} 41 QUEUE_URL: ${self:custom.queue.url} 42 iamRoleStatements: 43 - Effect: Allow 44 Action: 45 - sqs:SendMessage 46 Resource: 47 - ${self:custom.queue.arn} 48 consumer: 49 handler: src/consumer.handler 50 name: ${self:custom.serviceName}-consumer-${self:provider.stage} 51 events: 52 - sqs: 53 arn: ${self:custom.queue.arn} 54 55resources: 56 Resources: 57 Queue: 58 Type: AWS::SQS::Queue 59 Properties: 60 QueueName: ${self:custom.queue.name} Performance We can filter the log of the consumer function in the CloudWatch page. The function is invoked as expected, but I see the interval gets shortened periodically especially when the delay second value is 0. We’ll have a closer look at that below.\nI created a chart that shows delay (milliseconds) by invocation. It shows periodic downward spikes, and they correspond to the invocations where the delay seconds value is 0. For some early invocations, the delay values are more than 1000 milliseconds, which means that the consumer function’s intervals are less than 9 seconds. The delays get stable at or after the 200th invocation. The table in the right-hand side shows the summary statistics of delays after that invocation. It shows the consumer invocation delays spread in a range of 300 milliseconds in general.\nCaveats An EventBridge Events rule can be triggered more than once and a message in an Amazon SQS queue can be delivered more than once as well. Therefore, it is important to design the consumer Lambda function to be idempotent.\nConclusion In this post, I demonstrated a serverless solution for scheduling a Lambda function at a sub-minute frequency with Amazon SQS. The architecture of the serverless solution is simpler than other options and its performance is acceptable in spite of some negative delays. Due to the at-least-once delivery feature of EventBridge Events and Amazon SQS, it is important to design the application to be idempotent. I hope this post is useful to build a Lambda scheduling solution.\n","date":"October 13, 2021","img":"/blog/2021-10-13-lambda-schedule/featured.png","lang":"en","langName":"English","largeImg":"/blog/2021-10-13-lambda-schedule/featured_huea874edbd388125a335a7aabc342a4f8_46921_500x0_resize_box_3.png","permalink":"/blog/2021-10-13-lambda-schedule/","series":[],"smallImg":"/blog/2021-10-13-lambda-schedule/featured_huea874edbd388125a335a7aabc342a4f8_46921_180x0_resize_box_3.png","tags":[{"title":"AWS Lambda","url":"/tags/aws-lambda/"},{"title":"Amazon SQS","url":"/tags/amazon-sqs/"},{"title":"Node.js","url":"/tags/node.js/"}],"timestamp":1634083200,"title":"Yet Another Serverless Solution for Invoking AWS Lambda at a Sub-Minute Frequency"},{"categories":[{"title":"Data Engineering","url":"/categories/data-engineering/"}],"content":"As described in the product page, AWS Glue is a serverless data integration service that makes it easy to discover, prepare, and combine data for analytics, machine learning, and application development. For development, a development endpoint is recommended, but it can be costly, inconvenient or unavailable (for Glue 2.0). The AWS Glue team published a Docker image that includes the AWS Glue binaries and all the dependencies packaged together. After inspecting it, I find some modifications are necessary in order to build a development environment on it. In this post, I\u0026rsquo;ll demonstrate how to build development environments for AWS Glue 1.0 and 2.0 using the Docker image and the Visual Studio Code Remote - Containers extension.\nConfiguration Although AWS Glue 1.0 and 2.0 have different dependencies and versions, the Python library (aws-glue-libs) shares the same branch (glue-1.0) and Spark version. On the other hand, AWS Glue 2.0 supports Python 3.7 and has different default python packages. Therefore, in order to set up an AWS Glue 2.0 development environment, it would be necessary to install Python 3.7 and the default packages while sharing the same Spark-related dependencies.\nThe Visual Studio Code Remote - Containers extension lets you use a Docker container as a full-featured development environment. It allows you to open any folder or repository inside a container and take advantage of Visual Studio Code\u0026rsquo;s full feature set. The development container configuration (devcontainer.json) and associating files can be found in the .devcontainer folder of the GitHub repository for this post. Apart from the configuration file, the folder includes a Dockerfile, files to keep Python packages to install and a custom Pytest executable for AWS Glue 2.0 (gluepytest2) - this executable will be explained later.\n1. 2├── .devcontainer 3│ ├── 3.6 4│ │ └── dev.txt 5│ ├── 3.7 6│ │ ├── default.txt 7│ │ └── dev.txt 8│ ├── Dockerfile 9│ ├── bin 10│ │ └── gluepytest2 11│ └── devcontainer.json 12├── .gitignore 13├── README.md 14├── example.py 15├── execute.sh 16├── src 17│ └── utils.py 18└── tests 19 ├── __init__.py 20 ├── conftest.py 21 └── test_utils.py Dockerfile The Docker image (amazon/aws-glue-libs:glue_libs_1.0.0_image_01) runs as the root user, and it is not convenient to write code with it. Therefore, a non-root user is created whose username corresponds to the logged-in user\u0026rsquo;s username - the USERNAME argument will be set accordingly in devcontainer.json. Next the sudo program is added in order to install other programs if necessary. More importantly, the Python Glue library\u0026rsquo;s executables are configured to run with the root user so that the sudo program is necessary to run those executables. Then the 3rd-party Python packages are installed for the Glue 1.0 and 2.0 development environments. Note that a virtual environment is created for the latter and the default Python packages and additional development packages are installed in it. Finally, a Pytest executable is copied to the Python Glue library\u0026rsquo;s executable path. It is because the Pytest path is hard-coded in the existing executable (gluepytest) and I just wanted to run test cases in the Glue 2.0 environment without touching existing ones - the Pytest path is set to /root/venv/bin/pytest_ in _gluepytest2.\n1## .devcontainer/Dockerfile 2FROM amazon/aws-glue-libs:glue_libs_1.0.0_image_01 3 4ARG USERNAME 5ARG USER_UID 6ARG USER_GID 7 8## Create non-root user 9RUN groupadd --gid $USER_GID $USERNAME \\ 10 \u0026amp;\u0026amp; useradd --uid $USER_UID --gid $USER_GID -m $USERNAME 11 12## Add sudo support in case we need to install software after connecting 13## Jessie is not the latest stable Debian release - jessie-backports is not available 14RUN rm -rf /etc/apt/sources.list.d/jessie-backports.list 15 16RUN apt-get update \\ 17 \u0026amp;\u0026amp; apt-get install -y sudo \\ 18 \u0026amp;\u0026amp; echo $USERNAME ALL=\\(root\\) NOPASSWD:ALL \u0026gt; /etc/sudoers.d/$USERNAME \\ 19 \u0026amp;\u0026amp; chmod 0440 /etc/sudoers.d/$USERNAME 20 21## Install extra packages for python 3.6 22COPY ./3.6 /tmp/3.6 23RUN pip install -r /tmp/3.6/dev.txt 24 25## Setup python 3.7 and install default and development packages to a virtual env 26RUN apt-get update \\ 27 \u0026amp;\u0026amp; apt-get install -y python3.7 python3.7-venv 28 29RUN python3.7 -m venv /root/venv 30 31COPY ./3.7 /tmp/3.7 32RUN /root/venv/bin/pip install -r /tmp/3.7/dev.txt 33 34## Copy pytest execution script to /aws-glue-libs/bin 35## in order to run pytest from the virtual env 36COPY ./bin/gluepytest2 /home/aws-glue-libs/bin/gluepytest2 Container Configuration The development container will be created by building an image from the Dockerfile illustrated above. The logged-in user\u0026rsquo;s username is provided to create a non-root user and the container is set to run as the user as well. And 2 Visual Studio Code extensions are installed - Python and Prettier. Also, the current folder is mounted to the container\u0026rsquo;s workspace folder and 2 additional folders are mounted - they are to share AWS credentials and SSH keys. Note that AWS credentials are mounted to /roo/.aws because the Python Glue library\u0026rsquo;s executables will be run as the root user. Then the port 4040 is set to be forwarded, which is used for the Spark UI. Finally, additional editor settings are added at the end.\n1// .devcontainer/devcontainer.json 2{ 3 \u0026#34;name\u0026#34;: \u0026#34;glue\u0026#34;, 4 \u0026#34;build\u0026#34;: { 5 \u0026#34;dockerfile\u0026#34;: \u0026#34;Dockerfile\u0026#34;, 6 \u0026#34;args\u0026#34;: { 7 \u0026#34;USERNAME\u0026#34;: \u0026#34;${localEnv:USER}\u0026#34;, 8 \u0026#34;USER_UID\u0026#34;: \u0026#34;1000\u0026#34;, 9 \u0026#34;USER_GID\u0026#34;: \u0026#34;1000\u0026#34; 10 } 11 }, 12 \u0026#34;containerUser\u0026#34;: \u0026#34;${localEnv:USER}\u0026#34;, 13 \u0026#34;extensions\u0026#34;: [ 14 \u0026#34;ms-python.python\u0026#34;, 15 \u0026#34;esbenp.prettier-vscode\u0026#34; 16 ], 17 \u0026#34;workspaceMount\u0026#34;: \u0026#34;source=${localWorkspaceFolder},target=${localEnv:HOME}/glue-vscode,type=bind,consistency=cached\u0026#34;, 18 \u0026#34;workspaceFolder\u0026#34;: \u0026#34;${localEnv:HOME}/glue-vscode\u0026#34;, 19 \u0026#34;forwardPorts\u0026#34;: [4040], 20 \u0026#34;mounts\u0026#34;: [ 21 \u0026#34;source=${localEnv:HOME}/.aws,target=/root/.aws,type=bind,consistency=cached\u0026#34;, 22 \u0026#34;source=${localEnv:HOME}/.ssh,target=${localEnv:HOME}/.ssh,type=bind,consistency=cached\u0026#34; 23 ], 24 \u0026#34;settings\u0026#34;: { 25 \u0026#34;terminal.integrated.profiles.linux\u0026#34;: { 26 \u0026#34;bash\u0026#34;: { 27 \u0026#34;path\u0026#34;: \u0026#34;/bin/bash\u0026#34; 28 } 29 }, 30 \u0026#34;terminal.integrated.defaultProfile.linux\u0026#34;: \u0026#34;bash\u0026#34;, 31 \u0026#34;editor.formatOnSave\u0026#34;: true, 32 \u0026#34;editor.defaultFormatter\u0026#34;: \u0026#34;esbenp.prettier-vscode\u0026#34;, 33 \u0026#34;editor.tabSize\u0026#34;: 2, 34 \u0026#34;python.testing.pytestEnabled\u0026#34;: true, 35 \u0026#34;python.linting.enabled\u0026#34;: true, 36 \u0026#34;python.linting.pylintEnabled\u0026#34;: false, 37 \u0026#34;python.linting.flake8Enabled\u0026#34;: false, 38 \u0026#34;python.formatting.provider\u0026#34;: \u0026#34;black\u0026#34;, 39 \u0026#34;python.formatting.blackPath\u0026#34;: \u0026#34;black\u0026#34;, 40 \u0026#34;python.formatting.blackArgs\u0026#34;: [\u0026#34;--line-length\u0026#34;, \u0026#34;100\u0026#34;], 41 \u0026#34;[python]\u0026#34;: { 42 \u0026#34;editor.tabSize\u0026#34;: 4, 43 \u0026#34;editor.defaultFormatter\u0026#34;: \u0026#34;ms-python.python\u0026#34; 44 } 45 } 46} Launch Container The development container can be run by executing the following command in the command palette.\nRemote-Containers: Open Folder in Container\u0026hellip; Once the development container is ready, the workspace folder will be open within the container. You will see 2 new images are created from the base Glue image and a container is run from the latest image.\nExamples I\u0026rsquo;ve created a simple script (execute.sh) to run the executables easily. The main command indicates which executable to run and possible values are pyspark, spark-submit and pytest. Note that the IPython notebook is available, but it is not added because I don\u0026rsquo;t think a notebook is good for development. However, you may try by just adding it. Below shows some example commands.\n1# pyspark 2version=1 ./execute.sh pyspark OR version=2 ./execute.sh pyspark 3# spark submit 4version=1 ./execute.sh spark-submit example.py OR version=2 ./execute.sh spark-submit example.py 5# pytest 6version=1 ./execute.sh pytest -svv OR version=2 ./execute.sh pytest -svv 1# ./execute.sh 2#!/usr/bin/env bash 3 4## configure python runtime 5if [ \u0026#34;$version\u0026#34; == \u0026#34;1\u0026#34; ]; then 6 pyspark_python=python 7elif [ \u0026#34;$version\u0026#34; == \u0026#34;2\u0026#34; ]; then 8 pyspark_python=/root/venv/bin/python 9else 10 echo \u0026#34;unsupported version - $version, only 1 or 2 is accepted\u0026#34; 11 exit 1 12fi 13echo \u0026#34;pyspark python - $pyspark_python\u0026#34; 14 15execution=$1 16echo \u0026#34;execution type - $execution\u0026#34; 17 18## remove first argument 19shift 1 20echo $@ 21 22## set up command 23if [ $execution == \u0026#39;pyspark\u0026#39; ]; then 24 sudo su -c \u0026#34;PYSPARK_PYTHON=$pyspark_python /home/aws-glue-libs/bin/gluepyspark\u0026#34; 25elif [ $execution == \u0026#39;spark-submit\u0026#39; ]; then 26 sudo su -c \u0026#34;PYSPARK_PYTHON=$pyspark_python /home/aws-glue-libs/bin/gluesparksubmit $@\u0026#34; 27elif [ $execution == \u0026#39;pytest\u0026#39; ]; then 28 if [ $version == \u0026#34;1\u0026#34; ]; then 29 sudo su -c \u0026#34;PYSPARK_PYTHON=$pyspark_python /home/aws-glue-libs/bin/gluepytest $@\u0026#34; 30 else 31 sudo su -c \u0026#34;PYSPARK_PYTHON=$pyspark_python /home/aws-glue-libs/bin/gluepytest2 $@\u0026#34; 32 fi 33else 34 echo \u0026#34;unsupported execution type - $execution\u0026#34; 35 exit 1 36fi Pyspark Using the script above, we can launch the PySpark shells for each of the environments. Python 3.6.10 is associated with the AWS Glue 1.0 while Python 3.7.3 in a virtual environment is with the AWS Glue 2.0.\nSpark Submit Below shows one of the Python samples in the Glue documentation. It pulls 3 data sets from a database called legislators. Then they are joined to create a history data set (l_history) and saved into S3.\n1# ./example.py 2from awsglue.dynamicframe import DynamicFrame 3from awsglue.transforms import Join 4from awsglue.utils import getResolvedOptions 5from pyspark.context import SparkContext 6from awsglue.context import GlueContext 7 8glueContext = GlueContext(SparkContext.getOrCreate()) 9 10DATABASE = \u0026#34;legislators\u0026#34; 11OUTPUT_PATH = \u0026#34;s3://glue-python-samples-fbe445ee/output_dir\u0026#34; 12 13## create dynamic frames from data catalog 14persons: DynamicFrame = glueContext.create_dynamic_frame.from_catalog( 15 database=DATABASE, table_name=\u0026#34;persons_json\u0026#34; 16) 17 18memberships: DynamicFrame = glueContext.create_dynamic_frame.from_catalog( 19 database=DATABASE, table_name=\u0026#34;memberships_json\u0026#34; 20) 21 22orgs: DynamicFrame = glueContext.create_dynamic_frame.from_catalog( 23 database=DATABASE, table_name=\u0026#34;organizations_json\u0026#34; 24) 25 26## manipulate data 27orgs = ( 28 orgs.drop_fields([\u0026#34;other_names\u0026#34;, \u0026#34;identifiers\u0026#34;]) 29 .rename_field(\u0026#34;id\u0026#34;, \u0026#34;org_id\u0026#34;) 30 .rename_field(\u0026#34;name\u0026#34;, \u0026#34;org_name\u0026#34;) 31) 32 33l_history: DynamicFrame = Join.apply( 34 orgs, Join.apply(persons, memberships, \u0026#34;id\u0026#34;, \u0026#34;person_id\u0026#34;), \u0026#34;org_id\u0026#34;, \u0026#34;organization_id\u0026#34; 35) 36l_history = l_history.drop_fields([\u0026#34;person_id\u0026#34;, \u0026#34;org_id\u0026#34;]) 37 38l_history.printSchema() 39 40## write to s3 41glueContext.write_dynamic_frame.from_options( 42 frame=l_history, 43 connection_type=\u0026#34;s3\u0026#34;, 44 connection_options={\u0026#34;path\u0026#34;: f\u0026#34;{OUTPUT_PATH}/legislator_history\u0026#34;}, 45 format=\u0026#34;parquet\u0026#34;, 46) When the execution completes, we can see the joined data set is stored as a parquet file in the output S3 bucket.\nNote that we can monitor and inspect Spark job executions in the Spark UI on port 4040.\nPytest We can test a function that deals with a DynamicFrame. Below shows a test case for a simple function that filters a DynamicFrame based on a column value.\n1# ./src/utils.py 2from awsglue.dynamicframe import DynamicFrame 3 4def filter_dynamic_frame(dyf: DynamicFrame, column_name: str, value: int): 5 return dyf.filter(f=lambda x: x[column_name] \u0026gt; value) 6 7# ./tests/conftest.py 8from pyspark.context import SparkContext 9from awsglue.context import GlueContext 10import pytest 11 12@pytest.fixture(scope=\u0026#34;session\u0026#34;) 13def glueContext(): 14 sparkContext = SparkContext() 15 glueContext = GlueContext(sparkContext) 16 yield glueContext 17 sparkContext.stop() 18 19 20# ./tests/test_utils.py 21from typing import List 22from awsglue.dynamicframe import DynamicFrame 23import pandas as pd 24from src.utils import filter_dynamic_frame 25 26def _get_sorted_data_frame(pdf: pd.DataFrame, columns_list: List[str] = None): 27 if columns_list is None: 28 columns_list = list(pdf.columns.values) 29 return pdf.sort_values(columns_list).reset_index(drop=True) 30 31 32def test_filter_dynamic_frame_by_value(glueContext): 33 spark = glueContext.spark_session 34 35 input = spark.createDataFrame( 36 [(\u0026#34;charly\u0026#34;, 15), (\u0026#34;fabien\u0026#34;, 18), (\u0026#34;sam\u0026#34;, 21), (\u0026#34;sam\u0026#34;, 25), (\u0026#34;nick\u0026#34;, 19), (\u0026#34;nick\u0026#34;, 40)], 37 [\u0026#34;name\u0026#34;, \u0026#34;age\u0026#34;], 38 ) 39 40 expected_output = spark.createDataFrame( 41 [(\u0026#34;sam\u0026#34;, 25), (\u0026#34;sam\u0026#34;, 21), (\u0026#34;nick\u0026#34;, 40)], 42 [\u0026#34;name\u0026#34;, \u0026#34;age\u0026#34;], 43 ) 44 45 real_output = filter_dynamic_frame(DynamicFrame.fromDF(input, glueContext, \u0026#34;output\u0026#34;), \u0026#34;age\u0026#34;, 20) 46 47 pd.testing.assert_frame_equal( 48 _get_sorted_data_frame(real_output.toDF().toPandas(), [\u0026#34;name\u0026#34;, \u0026#34;age\u0026#34;]), 49 _get_sorted_data_frame(expected_output.toPandas(), [\u0026#34;name\u0026#34;, \u0026#34;age\u0026#34;]), 50 check_like=True, 51 ) Conclusion In this post, I demonstrated how to build local development environments for AWS Glue 1.0 and 2.0 using Docker and the Visual Studio Code Remote - Containers extension. Then examples of launching Pyspark shells, submitting an application and running a test are shown. I hope this post is useful to develop and test Glue ETL scripts locally.\n","date":"August 20, 2021","img":"/blog/2021-08-20-glue-local-development/featured.png","lang":"en","langName":"English","largeImg":"/blog/2021-08-20-glue-local-development/featured_hu248b4052e45f408d4fe80445a9d59f15_19535_500x0_resize_box_3.png","permalink":"/blog/2021-08-20-glue-local-development/","series":[],"smallImg":"/blog/2021-08-20-glue-local-development/featured_hu248b4052e45f408d4fe80445a9d59f15_19535_180x0_resize_box_3.png","tags":[{"title":"AWS","url":"/tags/aws/"},{"title":"AWS Glue","url":"/tags/aws-glue/"},{"title":"Docker","url":"/tags/docker/"},{"title":"Apache Spark","url":"/tags/apache-spark/"},{"title":"PySpark","url":"/tags/pyspark/"},{"title":"Python","url":"/tags/python/"},{"title":"Visual Studio Code","url":"/tags/visual-studio-code/"}],"timestamp":1629417600,"title":"AWS Glue Local Development With Docker and Visual Studio Code"},{"categories":[{"title":"Engineering","url":"/categories/engineering/"}],"content":"Authorization is the mechanism that controls who can do what on which resource in an application. Although it is a critical part of an application, there are limited resources available on how to build authorization into an app effectively. In this post, I\u0026rsquo;ll be illustrating how to set up authorization in a GraphQL API using a custom directive and Oso, an open-source authorization library. This tutorial covers the NodeJS variant of Oso, but it also supports Python and other languages.\nRequirements There are a number of users and each of them belongs to one or more user groups. The groups are guest, member and admin. Also, a user can be given escalated permission on one or more projects if he/she belongs to a certain project user group (e.g. contributor).\nDepending on the membership, users have varying levels of permission on user, project and indicator resources. Specifically\nUser\nAll users can be fetched if a user belongs to the admin user group. Project\nA project or all permitted projects can be queried if a user belongs to the _admin or member _user group or to the _contributor _user project group. For a project record, the contract_sum field can be queried only if a user belongs to the _admin _user group or contributor user project group. The project status can be updated if a user belongs to the admin user group or contributor user project group. Indicator\nAll permitted project indicators can be fetched if a user belongs to the admin user group or contributor user project group. Building Blocks Permission Specification on Directive A directive decorates part of a GraphQL schema or operation with additional configuration. Tools like Apollo Server (and Apollo Client) can read a GraphQL document\u0026rsquo;s directives and perform custom logic as appropriate.\nA directive can be useful to define permission. Below shows the type definitions used to meet the authorization requirements listed above. For example, the auth directive (@auth) is applied to the project query where admin and member are required for the user groups and contributor for the project user group.\n1// src/schema.js 2const typeDefs = gql` 3 directive @auth( 4 userGroups: [UserGroup] 5 projGroups: [ProjectGroup] 6 ) on OBJECT | FIELD_DEFINITION 7 8 ... 9 10 type User { 11 id: ID! 12 name: String 13 groups: [String] 14 } 15 16 type Project { 17 id: ID! 18 name: String 19 status: String 20 contract_sum: Int @auth(userGroups: [admin], projGroups: [contributor]) 21 } 22 23 type Indicator { 24 id: ID! 25 project_id: Int 26 risk: Int 27 quality: Int 28 } 29 30 type Query { 31 users: [User] @auth(userGroups: [admin]) 32 project(projectId: ID!): Project 33 @auth(userGroups: [admin, member], projGroups: [contributor]) 34 projects: [Project] 35 @auth(userGroups: [admin, member], projGroups: [contributor]) 36 indicators: [Indicator] 37 @auth(userGroups: [admin], projGroups: [contributor]) 38 } 39 40 type Mutation { 41 updateProjectStatus(projectId: ID!, status: String!): Project 42 @auth(userGroups: [admin], projGroups: [contributor]) 43 } 44`; Policy Building Using Oso Oso is a batteries-included library for building authorization in your application. Oso gives you a mental model and an authorization system – a set of APIs built on top of a declarative policy language called Polar, plus a debugger and REPL – to define who can do what in your application. You can express common concepts from “users can see their own data” and role-based access control, to others like multi-tenancy, organizations and teams, hierarchies and relationships.\nAn authorization policy is a set of logical rules for who is allowed to access what resources in an application. For example, the policy that describes the get:project action allows the actor (user) to perform it on the project resource if he/she belongs to required user or project groups. The actor and resource can be either a custom class or one of the built-in classes (Dictionary, List, String …). Note methods of a custom class can be used instead of built-in operations as well.\n1# src/polars/policy.polar 2allow(user: User, \u0026#34;list:users\u0026#34;, _: String) if 3 user.isRequiredUserGroup(); 4 5allow(user: User, \u0026#34;get:project\u0026#34;, project: Dictionary) if 6 user.isRequiredUserGroup() 7 or 8 user.isRequiredProjectGroup(project); 9 10allow(user: User, \u0026#34;update:project\u0026#34;, projectId: Integer) if 11 user.isRequiredUserGroup() 12 or 13 projectId in user.filterAllowedProjectIds(); 14 15allow(user: User, \u0026#34;list:indicators\u0026#34;, _: String) if 16 user.isRequiredUserGroup() 17 or 18 user.filterAllowedProjectIds().length \u0026gt; 0; Policy Enforcement Within Directive The auth directive collects the user and project group configuration on an object or field definition. Then it updates the user object in the context and passes it to the resolver. In this way, policy enforcement for queries and mutations can be performed within the resolver, and it is more manageable while the number of queries and mutations increases.\nOn the other hand, the policy of an object field (e.g. contract_sum) is enforced within the directive. It is because, once a query (e.g. project) or mutation is resolved, and its parent object is returned, the directive is executed for the field with different configuration values.\n1// src/utils/directive.js 2class AuthDirective extends SchemaDirectiveVisitor { 3 ... 4 5 ensureFieldsWrapped(objectType) { 6 if (objectType._authFieldsWrapped) return; 7 objectType._authFieldsWrapped = true; 8 9 const fields = objectType.getFields(); 10 11 Object.keys(fields).forEach((fieldName) =\u0026gt; { 12 const field = fields[fieldName]; 13 const { resolve = defaultFieldResolver } = field; 14 field.resolve = async function (...args) { 15 const userGroups = field._userGroups || objectType._userGroups; 16 const projGroups = field._projGroups || objectType._projGroups; 17 if (!userGroups \u0026amp;\u0026amp; !projGroups) { 18 return resolve.apply(this, args); 19 } 20 21 const context = args[2]; 22 context.user.requires = { userGroups, projGroups }; 23 24 // check permission of fields that have a specific parent type 25 if (args[3].parentType.name == \u0026#34;Project\u0026#34;) { 26 const user = User.clone(context.user); 27 if (!(await context.oso.isAllowed(user, \u0026#34;get:project\u0026#34;, args[0]))) { 28 throw new ForbiddenError( 29 JSON.stringify({ requires: user.requires, groups: user.groups }) 30 ); 31 } 32 } 33 34 return resolve.apply(this, args); 35 }; 36 }); 37 } 38} Within Resolver The Oso object is instantiated and stored in the context. Then a policy can be enforced with the corresponding actor, action and _resource _triples. For list endpoints, different strategies can be employed. For example, the projects query fetches all records, but returns only authorized records. On the other hand, the indicators query is set to fetch only permitted records, which is more effective when dealing with sensitive data or a large amount of data.\n1// src/resolvers.js 2const resolvers = { 3 Query: { 4 users: async (_, __, context) =\u0026gt; { 5 const user = User.clone(context.user); 6 if (await context.oso.isAllowed(user, \u0026#34;list:users\u0026#34;, \u0026#34;_\u0026#34;)) { 7 return await User.fetchUsers(); 8 } else { 9 throw new ForbiddenError( 10 JSON.stringify({ requires: user.requires, groups: user.groups }) 11 ); 12 } 13 }, 14 project: async (_, args, context) =\u0026gt; { 15 const user = User.clone(context.user); 16 const result = await Project.fetchProjects([args.projectId]); 17 if (await context.oso.isAllowed(user, \u0026#34;get:project\u0026#34;, result[0])) { 18 return result[0]; 19 } else { 20 throw new ForbiddenError(...); 21 } 22 }, 23 projects: async (_, __, context) =\u0026gt; { 24 const user = User.clone(context.user); 25 const results = await Project.fetchProjects(); 26 const authorizedResults = []; 27 for (const result of results) { 28 if (await context.oso.isAllowed(user, \u0026#34;get:project\u0026#34;, result)) { 29 authorizedResults.push(result); 30 } 31 } 32 return authorizedResults; 33 }, 34 indicators: async (_, __, context) =\u0026gt; { 35 const user = User.clone(context.user); 36 if (await context.oso.isAllowed(user, \u0026#34;list:indicators\u0026#34;, \u0026#34;_\u0026#34;)) { 37 let projectIds; 38 if (user.isRequiredUserGroup()) { 39 projectIds = []; 40 } else { 41 projectIds = user.filterAllowedProjectIds(); 42 if (projectIds.length == 0) { 43 throw new Error(\u0026#34;fails to populate project ids\u0026#34;); 44 } 45 } 46 return await Project.fetchProjectIndicators(projectIds); 47 } else { 48 throw new ForbiddenError(...); 49 } 50 }, 51 }, 52 Mutation: { 53 updateProjectStatus: async (_, args, context) =\u0026gt; { 54 const user = User.clone(context.user); 55 if ( 56 await context.oso.isAllowed( 57 User, \u0026#34;update:project\u0026#34;, parseInt(args.projectId) 58 ) 59 ) { 60 return Project.updateProjectStatus(args.projectId, args.status); 61 } else { 62 throw new ForbiddenError(...); 63 } 64 }, 65 }, 66}; Examples The application source can be found in this GitHub repository, and it can be started as follows.\n1docker-compose up 2# if first time 3docker-compose up --build Apollo Studio can be used to query the example API. Note the server is running on port 5000, and it is expected to have one of the following values in the name request header.\nguest-user user group: guest member-user user group: member user project group: contributor of project 1 and 3 admin-user user group: admin contributor-user user group: guest user project group: contributor of project 1, 3, 5, 8 and 12 The member user can query the project thanks to her user group membership. Also, as the user is a contributor of project 1 and 3, she has access to contract_sum.\nThe query returns an error if a project that she is not a contributor is requested. The project query is resolved because of her user group membership while contract_sum turns to null.\nThe contributor user can query all permitted projects without an error as shown below.\nConclusion In this post, it is illustrated how to build authorization in a GraphQL API using a custom directive and an open source authorization library, Oso. A custom directive is effective to define permission on a schema, to pass configuration to the resolver and even to enforce policies directly. The Oso library helps build policies in a declarative way while expressing common concepts. Although it’s not covered in this post, the library supports building common authorization models such as role-based access control, multi-tenancy, hierarchies and relationships. It has a huge potential! I hope you find this post useful when building authorization in an application.\n","date":"July 20, 2021","img":"/blog/2021-07-20-graphql-api-authorization/featured.png","lang":"en","langName":"English","largeImg":"/blog/2021-07-20-graphql-api-authorization/featured_hueddcdd7752048c40aa557a0cee455633_52143_500x0_resize_box_3.png","permalink":"/blog/2021-07-20-graphql-api-authorization/","series":[],"smallImg":"/blog/2021-07-20-graphql-api-authorization/featured_hueddcdd7752048c40aa557a0cee455633_52143_180x0_resize_box_3.png","tags":[{"title":"Apollo","url":"/tags/apollo/"},{"title":"Authorization","url":"/tags/authorization/"},{"title":"GraphQL","url":"/tags/graphql/"},{"title":"Node.js","url":"/tags/node.js/"}],"timestamp":1626739200,"title":"Adding Authorization to a Graphql API"},{"categories":[{"title":"Data Engineering","url":"/categories/data-engineering/"}],"content":"Apache Airflow is a popular open-source workflow management platform. Typically tasks run remotely by Celery workers for scalability. In AWS, however, scalability can also be achieved using serverless computing services in a simpler way. For example, the ECS Operator allows to run dockerized tasks and, with the Fargate launch type, they can run in a serverless environment.\nThe ECS Operator alone is not sufficient because it can take up to several minutes to pull a Docker image and to set up network interface (for the case of Fargate launch type). Due to its latency, it is not suitable for frequently-running tasks. On the other hand, the latency of a Lambda function is negligible so that it\u0026rsquo;s more suitable for managing such tasks.\nIn this post, it is demonstrated how AWS Lambda can be integrated with Apache Airflow using a custom operator inspired by the ECS Operator.\nHow it works The following shows steps when an Airflow task is executed by the ECS Operator.\nRunning the associating ECS task Waiting for the task ended Checking the task status The status of a task is checked by searching a stopped reason and raises AirflowException if the reason is considered to be failure. While checking the status, the associating CloudWatch log events are pulled and printed so that the ECS task\u0026rsquo;s container logs can be found in Airflow web server.\nThe key difference between ECS and Lambda is that the former sends log events to a dedicated CloudWatch Log Stream while the latter may reuse an existing Log Stream due to container reuse. Therefore it is not straightforward to pull execution logs for a specific Lambda invocation. It can be handled by creating a custom CloudWatch Log Group and sending log events to a CloudWatch Log Stream within the custom Log Group. For example, let say there is a Lambda function named as airflow-test. In order to pull log events for a specific Lambda invocation, a custom Log Group (eg /airflow/lambda/airflow-test) can be created and, inside the Lambda function, log events can be sent to a Log Stream within the custom Log Group. Note that the CloudWatch Log Stream name can be determined by the operator and sent to the Lambda function in the Lambda payload. In this way, the Lambda function can send log events to a Log Stream that Airflow knows. Then the steps of a custom Lambda Operator can be as following.\nInvoking the Lambda function Wating for function ended Checking the invocation status Lambda Operator Below shows a simplified version of the custom Lambda Operator - the full version can be found here. Note that the associating CloudWatch Log Group name is a required argument (awslogs_group) while the Log Stream name is determined by a combination of execution date, qualifier and UUID. These are sent to the Lambda function in the payload. Note also that, in _check_success_invocation(), whether a function invocation is failed or succeeded is identified by searching ERROR within message of log events. I find this gives a more stable outcome than Lambda invocation response.\n1import re, time, json, math, uuid 2from datetime import datetime 3from botocore import exceptions 4from airflow.exceptions import AirflowException 5from airflow.models import BaseOperator 6from airflow.utils import apply_defaults 7from airflow.contrib.hooks.aws_hook import AwsHook 8from airflow.contrib.hooks.aws_logs_hook import AwsLogsHook 9 10class LambdaOperator(BaseOperator): 11 @apply_defaults 12 def __init__( 13 self, function_name, awslogs_group, qualifier=\u0026#34;$LATEST\u0026#34;, 14 payload={}, aws_conn_id=None, region_name=None, *args, **kwargs 15 ): 16 super(LambdaOperator, self).__init__(**kwargs) 17 self.function_name = function_name 18 self.qualifier = qualifier 19 self.payload = payload 20 # log stream is created and added to payload 21 self.awslogs_group = awslogs_group 22 self.awslogs_stream = \u0026#34;{0}/[{1}]{2}\u0026#34;.format( 23 datetime.utcnow().strftime(\u0026#34;%Y/%m/%d\u0026#34;), 24 self.qualifier, 25 re.sub(\u0026#34;-\u0026#34;, \u0026#34;\u0026#34;, str(uuid.uuid4())), 26 ) 27 # lambda client and cloudwatch logs hook 28 self.client = AwsHook(aws_conn_id=aws_conn_id).get_client_type(\u0026#34;lambda\u0026#34;) 29 self.awslogs_hook = AwsLogsHook(aws_conn_id=aws_conn_id, region_name=region_name) 30 31 def execute(self, context): 32 # invoke - wait - check 33 payload = json.dumps( 34 { 35 **{\u0026#34;group_name\u0026#34;: self.awslogs_group, \u0026#34;stream_name\u0026#34;: self.awslogs_stream}, 36 **self.payload, 37 } 38 ) 39 invoke_opts = { 40 \u0026#34;FunctionName\u0026#34;: self.function_name, 41 \u0026#34;Qualifier\u0026#34;: self.qualifier, 42 \u0026#34;InvocationType\u0026#34;: \u0026#34;RequestResponse\u0026#34;, 43 \u0026#34;Payload\u0026#34;: bytes(payload, encoding=\u0026#34;utf8\u0026#34;), 44 } 45 try: 46 resp = self.client.invoke(**invoke_opts) 47 self.log.info(\u0026#34;Lambda function invoked - StatusCode {0}\u0026#34;.format(resp[\u0026#34;StatusCode\u0026#34;])) 48 except exceptions.ClientError as e: 49 raise AirflowException(e.response[\u0026#34;Error\u0026#34;]) 50 51 self._wait_for_function_ended() 52 53 self._check_success_invocation() 54 self.log.info(\u0026#34;Lambda Function has been successfully invoked\u0026#34;) 55 56 def _wait_for_function_ended(self): 57 waiter = self.client.get_waiter(\u0026#34;function_active\u0026#34;) 58 waiter.config.max_attempts = math.ceil( 59 self._get_function_timeout() / 5 60 ) # poll interval - 5 seconds 61 waiter.wait(FunctionName=self.function_name, Qualifier=self.qualifier) 62 63 def _check_success_invocation(self): 64 self.log.info(\u0026#34;Lambda Function logs output\u0026#34;) 65 has_message, invocation_failed = False, False 66 messages, max_trial, current_trial = [], 5, 0 67 # sometimes events are not retrieved, run for 5 times if so 68 while True: 69 current_trial += 1 70 for event in self.awslogs_hook.get_log_events(self.awslogs_group, self.awslogs_stream): 71 dt = datetime.fromtimestamp(event[\u0026#34;timestamp\u0026#34;] / 1000.0) 72 self.log.info(\u0026#34;[{}] {}\u0026#34;.format(dt.isoformat(), event[\u0026#34;message\u0026#34;])) 73 messages.append(event[\u0026#34;message\u0026#34;]) 74 if len(messages) \u0026gt; 0 or current_trial \u0026gt; max_trial: 75 break 76 time.sleep(2) 77 if len(messages) == 0: 78 raise AirflowException(\u0026#34;Fails to get log events\u0026#34;) 79 for m in reversed(messages): 80 if re.search(\u0026#34;ERROR\u0026#34;, m) != None: 81 raise AirflowException(\u0026#34;Lambda Function invocation is not successful\u0026#34;) 82 83 def _get_function_timeout(self): 84 resp = self.client.get_function(FunctionName=self.function_name, Qualifier=self.qualifier) 85 return resp[\u0026#34;Configuration\u0026#34;][\u0026#34;Timeout\u0026#34;] Lambda Function Below shows a simplified version of the Lambda function - the full version can be found here. CustomLogManager includes methods to create CloudWatch Log Stream and to put log events. LambdaDecorator manages actions before/after the function invocation as well as when an exception occurs - it\u0026rsquo;s used as a decorator and modified from the lambda_decorators package. Before an invocation, it initializes a custom Log Stream. Log events are put to the Log Stream after an invocation or there is an exception. Note that traceback is also sent to the Log Stream when there\u0026rsquo;s an exception. The Lambda function simply exits after a loop or raises an exception at random.\n1import time, re, random, logging, traceback, boto3 2from datetime import datetime 3from botocore import exceptions 4from io import StringIO 5from functools import update_wrapper 6 7# save logs to stream 8stream = StringIO() 9logger = logging.getLogger() 10log_handler = logging.StreamHandler(stream) 11formatter = logging.Formatter(\u0026#34;%(levelname)-8s %(asctime)-s %(name)-12s %(message)s\u0026#34;) 12log_handler.setFormatter(formatter) 13logger.addHandler(log_handler) 14logger.setLevel(logging.INFO) 15 16cwlogs = boto3.client(\u0026#34;logs\u0026#34;) 17 18class CustomLogManager(object): 19 # create log stream and send logs to it 20 def __init__(self, event): 21 self.group_name = event[\u0026#34;group_name\u0026#34;] 22 self.stream_name = event[\u0026#34;stream_name\u0026#34;] 23 24 def has_log_group(self): 25 group_exists = True 26 try: 27 resp = cwlogs.describe_log_groups(logGroupNamePrefix=self.group_name) 28 group_exists = len(resp[\u0026#34;logGroups\u0026#34;]) \u0026gt; 0 29 except exceptions.ClientError as e: 30 logger.error(e.response[\u0026#34;Error\u0026#34;][\u0026#34;Code\u0026#34;]) 31 group_exists = False 32 return group_exists 33 34 def create_log_stream(self): 35 is_created = True 36 try: 37 cwlogs.create_log_stream(logGroupName=self.group_name, logStreamName=self.stream_name) 38 except exceptions.ClientError as e: 39 logger.error(e.response[\u0026#34;Error\u0026#34;][\u0026#34;Code\u0026#34;]) 40 is_created = False 41 return is_created 42 43 def delete_log_stream(self): 44 is_deleted = True 45 try: 46 cwlogs.delete_log_stream(logGroupName=self.group_name, logStreamName=self.stream_name) 47 except exceptions.ClientError as e: 48 # ResourceNotFoundException is ok 49 codes = [ 50 \u0026#34;InvalidParameterException\u0026#34;, 51 \u0026#34;OperationAbortedException\u0026#34;, 52 \u0026#34;ServiceUnavailableException\u0026#34;, 53 ] 54 if e.response[\u0026#34;Error\u0026#34;][\u0026#34;Code\u0026#34;] in codes: 55 logger.error(e.response[\u0026#34;Error\u0026#34;][\u0026#34;Code\u0026#34;]) 56 is_deleted = False 57 return is_deleted 58 59 def init_log_stream(self): 60 if not all([self.has_log_group(), self.delete_log_stream(), self.create_log_stream()]): 61 raise Exception(\u0026#34;fails to create log stream\u0026#34;) 62 logger.info(\u0026#34;log stream created\u0026#34;) 63 64 def create_log_events(self, stream): 65 fmt = \u0026#34;%Y-%m-%d %H:%M:%S,%f\u0026#34; 66 log_events = [] 67 for m in [s for s in stream.getvalue().split(\u0026#34;\\n\u0026#34;) if s]: 68 match = re.search(r\u0026#34;\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2},\\d{3}\u0026#34;, m) 69 dt_str = match.group() if match else datetime.utcnow().strftime(fmt) 70 log_events.append( 71 {\u0026#34;timestamp\u0026#34;: int(datetime.strptime(dt_str, fmt).timestamp()) * 1000, \u0026#34;message\u0026#34;: m} 72 ) 73 return log_events 74 75 def put_log_events(self, stream): 76 try: 77 resp = cwlogs.put_log_events( 78 logGroupName=self.group_name, 79 logStreamName=self.stream_name, 80 logEvents=self.create_log_events(stream), 81 ) 82 logger.info(resp) 83 except exceptions.ClientError as e: 84 logger.error(e) 85 raise Exception(\u0026#34;fails to put log events\u0026#34;) 86 87class LambdaDecorator(object): 88 # keep functions to run before, after and on exception 89 # modified from lambda_decorators (https://lambda-decorators.readthedocs.io/en/latest/) 90 def __init__(self, handler): 91 update_wrapper(self, handler) 92 self.handler = handler 93 94 def __call__(self, event, context): 95 try: 96 self.event = event 97 self.log_manager = CustomLogManager(event) 98 return self.after(self.handler(*self.before(event, context))) 99 except Exception as exception: 100 return self.on_exception(exception) 101 102 def before(self, event, context): 103 # remove existing logs 104 stream.seek(0) 105 stream.truncate(0) 106 # create log stream 107 self.log_manager.init_log_stream() 108 logger.info(\u0026#34;Start Request\u0026#34;) 109 return event, context 110 111 def after(self, retval): 112 logger.info(\u0026#34;End Request\u0026#34;) 113 # send logs to stream 114 self.log_manager.put_log_events(stream) 115 return retval 116 117 def on_exception(self, exception): 118 logger.error(str(exception)) 119 # log traceback 120 logger.error(traceback.format_exc()) 121 # send logs to stream 122 self.log_manager.put_log_events(stream) 123 return str(exception) 124 125@LambdaDecorator 126def lambda_handler(event, context): 127 max_len = event.get(\u0026#34;max_len\u0026#34;, 6) 128 fails_at = random.randint(0, max_len * 2) 129 for i in range(max_len): 130 if i != fails_at: 131 logger.info(\u0026#34;current run {0}\u0026#34;.format(i)) 132 else: 133 raise Exception(\u0026#34;fails at {0}\u0026#34;.format(i)) 134 time.sleep(1) Run Lambda Task A simple demo task is created as following. It just runs the Lambda function every 30 seconds.\n1import airflow 2from airflow import DAG 3from airflow.utils.dates import days_ago 4from datetime import timedelta 5from dags.operators import LambdaOperator 6 7function_name = \u0026#34;airflow-test\u0026#34; 8 9demo_dag = DAG( 10 dag_id=\u0026#34;demo-dag\u0026#34;, 11 start_date=days_ago(1), 12 catchup=False, 13 max_active_runs=1, 14 concurrency=1, 15 schedule_interval=timedelta(seconds=30), 16) 17 18demo_task = LambdaOperator( 19 task_id=\u0026#34;demo-task\u0026#34;, 20 function_name=function_name, 21 awslogs_group=\u0026#34;/airflow/lambda/{0}\u0026#34;.format(function_name), 22 payload={\u0026#34;max_len\u0026#34;: 6}, 23 dag=demo_dag, 24) The task can be tested by the following docker compose services. Note that the web server and scheduler are split into separate services although it doesn\u0026rsquo;t seem to be recommended for Local Executor - I had an issue to launch Airflow when those are combined in ECS.\n1version: \u0026#34;3.7\u0026#34; 2services: 3 postgres: 4 image: postgres:11 5 container_name: airflow-postgres 6 networks: 7 - airflow-net 8 ports: 9 - 5432:5432 10 environment: 11 - POSTGRES_USER=airflow 12 - POSTGRES_PASSWORD=airflow 13 - POSTGRES_DB=airflow 14 webserver: 15 image: puckel/docker-airflow:1.10.6 16 container_name: webserver 17 command: webserver 18 networks: 19 - airflow-net 20 user: root # for DockerOperator 21 volumes: 22 - ${HOME}/.aws:/root/.aws # run as root user 23 - ./requirements.txt:/requirements.txt 24 - ./dags:/usr/local/airflow/dags 25 - ./config/airflow.cfg:/usr/local/airflow/config/airflow.cfg 26 - ./entrypoint.sh:/entrypoint.sh # override entrypoint 27 - /var/run/docker.sock:/var/run/docker.sock # for DockerOperator 28 - ./custom:/usr/local/airflow/custom 29 ports: 30 - 8080:8080 31 environment: 32 - AIRFLOW__CORE__EXECUTOR=LocalExecutor 33 - AIRFLOW__CORE__LOAD_EXAMPLES=False 34 - AIRFLOW__CORE__LOGGING_LEVEL=INFO 35 - AIRFLOW__CORE__FERNET_KEY=Gg3ELN1gITETZAbBQpLDBI1y2P0d7gHLe_7FwcDjmKc= 36 - AIRFLOW__CORE__REMOTE_LOGGING=True 37 - AIRFLOW__CORE__REMOTE_BASE_LOG_FOLDER=s3://airflow-lambda-logs 38 - AIRFLOW__CORE__ENCRYPT_S3_LOGS=True 39 - POSTGRES_HOST=postgres 40 - POSTGRES_USER=airflow 41 - POSTGRES_PASSWORD=airflow 42 - POSTGRES_DB=airflow 43 - AWS_DEFAULT_REGION=ap-southeast-2 44 restart: always 45 healthcheck: 46 test: [\u0026#34;CMD-SHELL\u0026#34;, \u0026#34;[ -f /usr/local/airflow/config/airflow-webserver.pid ]\u0026#34;] 47 interval: 30s 48 timeout: 30s 49 retries: 3 50 scheduler: 51 image: puckel/docker-airflow:1.10.6 52 container_name: scheduler 53 command: scheduler 54 networks: 55 - airflow-net 56 user: root # for DockerOperator 57 volumes: 58 - ${HOME}/.aws:/root/.aws # run as root user 59 - ./requirements.txt:/requirements.txt 60 - ./logs:/usr/local/airflow/logs 61 - ./dags:/usr/local/airflow/dags 62 - ./config/airflow.cfg:/usr/local/airflow/config/airflow.cfg 63 - ./entrypoint.sh:/entrypoint.sh # override entrypoint 64 - /var/run/docker.sock:/var/run/docker.sock # for DockerOperator 65 - ./custom:/usr/local/airflow/custom 66 environment: 67 - AIRFLOW__CORE__EXECUTOR=LocalExecutor 68 - AIRFLOW__CORE__LOAD_EXAMPLES=False 69 - AIRFLOW__CORE__LOGGING_LEVEL=INFO 70 - AIRFLOW__CORE__FERNET_KEY=Gg3ELN1gITETZAbBQpLDBI1y2P0d7gHLe_7FwcDjmKc= 71 - AIRFLOW__CORE__REMOTE_LOGGING=True 72 - AIRFLOW__CORE__REMOTE_BASE_LOG_FOLDER=s3://airflow-lambda-logs 73 - AIRFLOW__CORE__ENCRYPT_S3_LOGS=True 74 - POSTGRES_HOST=postgres 75 - POSTGRES_USER=airflow 76 - POSTGRES_PASSWORD=airflow 77 - POSTGRES_DB=airflow 78 - AWS_DEFAULT_REGION=ap-southeast-2 79 restart: always 80 81networks: 82 airflow-net: 83 name: airflow-network Below shows the demo DAG after running for a while.\nLambda logs (and traceback) are found for both succeeded and failed tasks.\n","date":"April 13, 2020","img":"/blog/2020-04-13-airflow-lambda-operator/featured.png","lang":"en","langName":"English","largeImg":"/blog/2020-04-13-airflow-lambda-operator/featured_hu1fb366ed8468cd95c289e8b53adf2b4c_44994_500x0_resize_box_3.png","permalink":"/blog/2020-04-13-airflow-lambda-operator/","series":[],"smallImg":"/blog/2020-04-13-airflow-lambda-operator/featured_hu1fb366ed8468cd95c289e8b53adf2b4c_44994_180x0_resize_box_3.png","tags":[{"title":"AWS","url":"/tags/aws/"},{"title":"AWS Lambda","url":"/tags/aws-lambda/"},{"title":"Apache Airflow","url":"/tags/apache-airflow/"},{"title":"Python","url":"/tags/python/"},{"title":"Docker","url":"/tags/docker/"},{"title":"Docker Compose","url":"/tags/docker-compose/"}],"timestamp":1586736000,"title":"Thoughts on Apache Airflow AWS Lambda Operator"},{"categories":[{"title":"Engineering","url":"/categories/engineering/"}],"content":"Ingress in Kubernetes exposes HTTP and HTTPS routes from outside the cluster to services within the cluster. By setting rules, it routes requests to appropriate services (precisely requests are sent to individual Pods by Ingress Controller). Rules can be set up dynamically and I find it\u0026rsquo;s more efficient compared to traditional reverse proxy.\nTraefik is a modern HTTP reverse proxy and load balancer and it can be used as a Kubernetes Ingress Controller. Moreover it supports other providers, which are existing infrastructure components such as orchestrators, container engines, cloud providers, or key-value stores. To name a few, Docker, Kubernetes, AWS ECS, AWS DynamoDB and Consul are supported providers. With Traefik, it is possible to configure routing dynamically. Another interesting feature is Forward Authentication where authentication can be handled by an external service. In this post, it\u0026rsquo;ll be demonstrated how path-based routing can be set up by Traefik with Docker. Also a centralized authentication will be illustrated with the Forward Authentication feature of Traefik.\nHow Traefik works Below shows an illustration of internal architecture of Traefik.\nThe Traefik website explains workflow of requests as following.\nIncoming requests end on entrypoints, as the name suggests, they are the network entry points into Traefik (listening port, SSL, traffic redirection\u0026hellip;). Traffic is then forwarded to a matching frontend. A frontend defines routes from entrypoints to backends. Routes are created using requests fields (Host, Path, Headers\u0026hellip;) and can match or not a request. The frontend will then send the request to a backend. A backend can be composed by one or more servers, and by a load-balancing strategy. Finally, the server will forward the request to the corresponding microservice in the private network. In this example, a HTTP entrypoint is setup on port 80. Requests through it are forwarded to 2 web services by the following frontend rules.\nHost is k8s-traefik.info and path is /pybackend Host is k8s-traefik.info and path is /rbackend As the paths of the rules suggest, requests to /pybackend are sent to a backend service, created with FastAPI. If the other rule is met, requests are sent to the Rserve backend service. Note that only requests from authenticated users are fowarded to relevant backends and it is configured in frontend rules as well. Below shows how authentication is handled.\nTraefik setup Here is the traefik service defined in the compose file of this example - the full version can be found here.\n1version: \u0026#34;3.7\u0026#34; 2services: 3 traefik: 4 image: \u0026#34;traefik:v1.7.19\u0026#34; 5 networks: 6 - traefik-net 7 command: \u0026gt; 8 --docker 9 --docker.domain=k8s-traefik.info 10 --docker.exposedByDefault=false 11 --docker.network=traefik-net 12 --defaultentrypoints=http 13 --entrypoints=\u0026#34;Name:http Address::80\u0026#34; 14 --api.dashboard 15 ports: 16 - 80:80 17 - 8080:8080 18 labels: 19 - \u0026#34;traefik.frontend.rule=Host:k8s-traefik.info\u0026#34; 20 - \u0026#34;traefik.port=8080\u0026#34; 21 volumes: 22 - /var/run/docker.sock:/var/run/docker.sock 23... 24networks: 25 traefik-net: 26 name: traefik-network In commands, the Docker provider is enabled (--docker) with a custom domain name (k8s-traefik.info). A dedicated network is created and it is used for this and the other services (trafic-net). A single HTTP entrypoint is enabled as the default entrypoint. Finally monitoring dashboard is enabled (--api.dashboard). In lables, it is set to be served via the custom domain (hostname) - port 80 is for individual services while 8080 is for the monitoring UI.\nIt is necessary to have a custom hostname when setting up rules that include multiple hosts or enabling a HTTPS entrypoint. Although neither is discussed in this post, a custom domain (k8s-traefik.info), which is accessible only in local environment, is added - another post may come later. The location of hosts file is\nWindows - %WINDIR%\\System32\\drivers\\etc\\hosts or C:\\Windows\\System32\\drivers\\etc\\hosts Linux - /etc/hosts And the following entry is added.\n1# using a virtual machine 2\u0026lt;VM-IP-ADDRESS\u0026gt; k8s-traefik.info 3# or in the same machine 40.0.0.0 k8s-traefik.info In order to show how routes are configured dynamically, only the Traefik service is started as following.\n1docker-compose up -d traefik When visiting the monitoring UI via http://k8s-traefik.info:8080/dashboard, it\u0026rsquo;s shown that no frontend and backend exists in the docker provider tab.\nServices The authentication service is just checking if there\u0026rsquo;s an authorization header and the JWT value is foobar. If so, it returns 200 response so that requests can be forward to relevant backends. The source is shown below.\n1import os 2from typing import Dict, List 3from fastapi import FastAPI, Depends, HTTPException 4from pydantic import BaseModel 5from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials 6from starlette.requests import Request 7from starlette.status import HTTP_401_UNAUTHORIZED 8 9app = FastAPI(title=\u0026#34;Forward Auth API\u0026#34;, docs_url=None, redoc_url=None) 10 11## authentication 12class JWTBearer(HTTPBearer): 13 def __init__(self, auto_error: bool = True): 14 super().__init__(scheme_name=\u0026#34;Novice JWT Bearer\u0026#34;, auto_error=auto_error) 15 16 async def __call__(self, request: Request) -\u0026gt; None: 17 credentials: HTTPAuthorizationCredentials = await super().__call__(request) 18 19 if credentials.credentials != \u0026#34;foobar\u0026#34;: 20 raise HTTPException(HTTP_401_UNAUTHORIZED, detail=\u0026#34;Invalid Token\u0026#34;) 21 22 23## response models 24class StatusResp(BaseModel): 25 status: str 26 27 28## service methods 29@app.get(\u0026#34;/auth\u0026#34;, response_model=StatusResp, dependencies=[Depends(JWTBearer())]) 30async def forward_auth(): 31 return {\u0026#34;status\u0026#34;: \u0026#34;ok\u0026#34;} The service is defined in the compose file as following.\n1... 2 forward-auth: 3 image: kapps/trafik-demo:pybackend 4 networks: 5 - traefik-net 6 depends_on: 7 - traefik 8 command: \u0026gt; 9 forward_auth:app 10 --host=0.0.0.0 11 --port=8000 12 --reload 13... The Python service has 3 endpoints. The app\u0026rsquo;s title and path value are returned when requests are made to / and /{p} - a variable path value. Those to /admission calls the Rserve service and relays results from it - see the Rseve service section for the request payload. Note that an authorization header is not necessary between services.\n1import os 2import httpx 3from fastapi import FastAPI 4from pydantic import BaseModel, Schema 5from typing import Optional 6 7APP_PREFIX = os.environ[\u0026#34;APP_PREFIX\u0026#34;] 8 9app = FastAPI(title=\u0026#34;{0} API\u0026#34;.format(APP_PREFIX), docs_url=None, redoc_url=None) 10 11## response models 12class NameResp(BaseModel): 13 title: str 14 15 16class PathResp(BaseModel): 17 title: str 18 path: str 19 20 21class AdmissionReq(BaseModel): 22 gre: int = Schema(None, ge=0, le=800) 23 gpa: float = Schema(None, ge=0.0, le=4.0) 24 rank: str = Schema(None) 25 26 27class AdmissionResp(BaseModel): 28 result: bool 29 30 31## service methods 32@app.get(\u0026#34;/\u0026#34;, response_model=NameResp) 33async def whoami(): 34 return {\u0026#34;title\u0026#34;: app.title} 35 36 37@app.post(\u0026#34;/admission\u0026#34;) 38async def admission(*, req: Optional[AdmissionReq]): 39 host = os.getenv(\u0026#34;RSERVE_HOST\u0026#34;, \u0026#34;localhost\u0026#34;) 40 port = os.getenv(\u0026#34;RSERVE_PORT\u0026#34;, \u0026#34;8000\u0026#34;) 41 async with httpx.AsyncClient() as client: 42 dat = req.json() if req else None 43 r = await client.post(\u0026#34;http://{0}:{1}/{2}\u0026#34;.format(host, port, \u0026#34;admission\u0026#34;), data=dat) 44 return r.json() 45 46 47@app.get(\u0026#34;/{p}\u0026#34;, response_model=PathResp) 48async def whichpath(p: str): 49 print(p) 50 return {\u0026#34;title\u0026#34;: app.title, \u0026#34;path\u0026#34;: p} The Python service is configured with lables. Traefik is enabled and the same docker network is used. In frontend rules,\nrequests are set to be forwarded if host is k8s-traefik.info and path is /pybackend - PathPrefixStrip is to allow the path and its subpaths. authentication service is called and its address is http://forward-auth:8080/auth. authorization header is set to be copied to request - it\u0026rsquo;s for adding a custom header to a request and this label is mistakenly added. If the frontend rules pass, requests are sent to pybackend backend on port 8000.\n1... 2 pybackend: 3 image: kapps/trafik-demo:pybackend 4 networks: 5 - traefik-net 6 depends_on: 7 - traefik 8 - forward-auth 9 - rbackend 10 command: \u0026gt; 11 main:app 12 --host=0.0.0.0 13 --port=8000 14 --reload 15 expose: 16 - 8000 17 labels: 18 - \u0026#34;traefik.enable=true\u0026#34; 19 - \u0026#34;traefik.docker.network=traefik-net\u0026#34; 20 - \u0026#34;traefik.frontend.rule=Host:k8s-traefik.info;PathPrefixStrip:/pybackend\u0026#34; 21 - \u0026#34;traefik.frontend.auth.forward.address=http://forward-auth:8000/auth\u0026#34; 22 - \u0026#34;traefik.frontend.auth.forward.authResponseHeaders=Authorization\u0026#34; 23 - \u0026#34;traefik.backend=pybackend\u0026#34; 24 - \u0026#34;traefik.port=8000\u0026#34; 25 environment: 26 APP_PREFIX: \u0026#34;Python Backend\u0026#34; 27 RSERVE_HOST: \u0026#34;rbackend\u0026#34; 28 RSERVE_PORT: \u0026#34;8000\u0026#34; 29... R Service whoami() that returns the service name is executed when a request is made to the base path (/) - see here for details. To /admission, an admission result of a graduate school is returned by fitting a simple logistic regression. The result is based on 3 fields - GRE (Graduate Record Exam scores), GPA (grade point average) and prestige of the undergraduate institution. It\u0026rsquo;s from UCLA Institute for Digital Research \u0026amp; Education. If a field is missing, the mean or majority level is selected.\n1DAT \u0026lt;- read.csv(\u0026#39;./binary.csv\u0026#39;) 2DAT$rank \u0026lt;- factor(DAT$rank) 3 4value_if_null \u0026lt;- function(v, DAT) { 5 if (class(DAT[[v]]) == \u0026#39;factor\u0026#39;) { 6 tt \u0026lt;- table(DAT[[v]]) 7 names(tt[tt==max(tt)]) 8 } else { 9 mean(DAT[[v]]) 10 } 11} 12 13set_newdata \u0026lt;- function(args_called) { 14 args_init \u0026lt;- list(gre=NULL, gpa=NULL, rank=NULL) 15 newdata \u0026lt;- lapply(names(args_init), function(n) { 16 if (is.null(args_called[[n]])) { 17 args_init[[n]] \u0026lt;- value_if_null(n, DAT) 18 } else { 19 args_init[[n]] \u0026lt;- args_called[[n]] 20 } 21 }) 22 names(newdata) \u0026lt;- names(args_init) 23 lapply(names(newdata), function(n) { 24 flog.info(sprintf(\u0026#34;%s - %s\u0026#34;, n, newdata[[n]])) 25 }) 26 newdata \u0026lt;- as.data.frame(newdata) 27 newdata$rank \u0026lt;- factor(newdata$rank, levels = levels(DAT$rank)) 28 newdata 29} 30 31admission \u0026lt;- function(gre=NULL, gpa=NULL, rank=NULL, ...) { 32 newdata \u0026lt;- set_newdata(args_called = as.list(sys.call())) 33 logit \u0026lt;- glm(admit ~ gre + gpa + rank, data = DAT, family = \u0026#34;binomial\u0026#34;) 34 resp \u0026lt;- predict(logit, newdata=newdata, type=\u0026#34;response\u0026#34;) 35 flog.info(sprintf(\u0026#34;resp - %s\u0026#34;, resp)) 36 list(result = resp \u0026gt; 0.5) 37} 38 39whoami \u0026lt;- function() { 40 list(title=sprintf(\u0026#34;%s API\u0026#34;, Sys.getenv(\u0026#34;APP_PREFIX\u0026#34;, \u0026#34;RSERVE\u0026#34;))) 41} The Rserve service is configured with lables as well.\n1... 2 rbackend: 3 image: kapps/trafik-demo:rbackend 4 networks: 5 - traefik-net 6 depends_on: 7 - traefik 8 - forward-auth 9 command: \u0026gt; 10 --slave 11 --RS-conf /home/app/rserve.conf 12 --RS-source /home/app/rserve-src.R 13 expose: 14 - 8000 15 labels: 16 - \u0026#34;traefik.enable=true\u0026#34; 17 - \u0026#34;traefik.docker.network=traefik-net\u0026#34; 18 - \u0026#34;traefik.frontend.rule=Host:k8s-traefik.info;PathPrefixStrip:/rbackend\u0026#34; 19 - \u0026#34;traefik.frontend.auth.forward.address=http://forward-auth:8000/auth\u0026#34; 20 - \u0026#34;traefik.frontend.auth.forward.authResponseHeaders=Authorization\u0026#34; 21 - \u0026#34;traefik.backend=rbackend\u0026#34; 22 - \u0026#34;traefik.port=8000\u0026#34; 23 environment: 24 APP_PREFIX: \u0026#34;R Backend\u0026#34; 25... In order to check dynamic routes configuration, the Python service is started as following. Note that, as it depends on the authentication and Rserve service, these are started as well.\n1docker-compose up -d pybackend Once those services are started, the frontends/backends of the Python and Rserve services appear in the monitoring UI.\nBelow shows some request examples.\n1#### Authentication failure responses from authentication server 2http http://k8s-traefik.info/pybackend 3# HTTP/1.1 403 Forbidden 4# ... 5# { 6# \u0026#34;detail\u0026#34;: \u0026#34;Not authenticated\u0026#34; 7# } 8 9http http://k8s-traefik.info/pybackend \u0026#34;Authorization: Bearer foo\u0026#34; 10# HTTP/1.1 401 Unauthorized 11# ... 12# { 13# \u0026#34;detail\u0026#34;: \u0026#34;Invalid Token\u0026#34; 14# } 15 16#### Successful responses from Python service 17http http://k8s-traefik.info/pybackend \u0026#34;Authorization: Bearer foobar\u0026#34; 18# { 19# \u0026#34;title\u0026#34;: \u0026#34;Python Backend API\u0026#34; 20# } 21 22http http://k8s-traefik.info/pybackend/foobar \u0026#34;Authorization: Bearer foobar\u0026#34; 23# { 24# \u0026#34;path\u0026#34;: \u0026#34;foobar\u0026#34;, 25# \u0026#34;title\u0026#34;: \u0026#34;Python Backend API\u0026#34; 26# } 27 28#### Succesesful responses from Rserve service 29http http://k8s-traefik.info/rbackend \u0026#34;Authorization: Bearer foobar\u0026#34; 30# { 31# \u0026#34;title\u0026#34;: \u0026#34;R Backend API\u0026#34; 32# } 33 34#### Successful responses from requests to /admission 35echo \u0026#39;{\u0026#34;gre\u0026#34;: 600, \u0026#34;rank\u0026#34;: \u0026#34;1\u0026#34;}\u0026#39; \\ 36 | http POST http://k8s-traefik.info/rbackend/admission \u0026#34;Authorization: Bearer foobar\u0026#34; 37# { 38# \u0026#34;result\u0026#34;: true 39# } 40 41echo \u0026#39;{\u0026#34;gre\u0026#34;: 600, \u0026#34;rank\u0026#34;: \u0026#34;1\u0026#34;}\u0026#39; \\ 42 | http POST http://k8s-traefik.info/pybackend/admission \u0026#34;Authorization: Bearer foobar\u0026#34; 43# { 44# \u0026#34;result\u0026#34;: true 45# } The HEALTH tab of the monitoring UI shows some request metrics. After running the following for a while, the page is updated as shown below.\n1while true; do echo \u0026#39;{\u0026#34;gre\u0026#34;: 600, \u0026#34;rank\u0026#34;: \u0026#34;1\u0026#34;}\u0026#39; \\ 2 | http POST http://k8s-traefik.info/pybackend/admission \u0026#34;Authorization: Bearer foobar\u0026#34;; sleep 1; done ","date":"November 29, 2019","img":"/blog/2019-11-29-traefik-example/featured.png","lang":"en","langName":"English","largeImg":"/blog/2019-11-29-traefik-example/featured_huf02131f0aec4656166a63d0e0c90fae1_139790_500x0_resize_box_3.png","permalink":"/blog/2019-11-29-traefik-example/","series":[],"smallImg":"/blog/2019-11-29-traefik-example/featured_huf02131f0aec4656166a63d0e0c90fae1_139790_180x0_resize_box_3.png","tags":[{"title":"Traefik","url":"/tags/traefik/"},{"title":"FastAPI","url":"/tags/fastapi/"},{"title":"Rserve","url":"/tags/rserve/"},{"title":"R","url":"/tags/r/"},{"title":"Python","url":"/tags/python/"},{"title":"Docker","url":"/tags/docker/"},{"title":"Docker Compose","url":"/tags/docker-compose/"}],"timestamp":1574985600,"title":"Dynamic Routing and Centralized Auth With Traefik, Python and R Example"},{"categories":[{"title":"Engineering","url":"/categories/engineering/"}],"content":"While I\u0026rsquo;m looking into Apache Airflow, a workflow management tool, I thought it would be beneficial to get some understanding of how Celery works. To do so, I built a simple web service that sends tasks to Celery workers and collects the results from them. FastAPI is used for developing the web service and Redis is used for the message broker and result backend. During the development, I thought it would be possible to implement similar functionality in R with Rserve. Therefore a Rserve worker is added as an example as well. Coupling a web service with distributed task queue is beneficial on its own as it helps the service be more responsive by offloading heavyweight and long running processes to task workers.\nIn this post, it\u0026rsquo;ll be illustrated how a web service is created using FastAPI framework where tasks are sent to multiple workers. The workers are built with Celery and Rserve. Redis is used as a message broker/result backend for Celery and a key-value store for Rserve. Demos can be run in both Docker Compose and Kubernetes.\nThe following diagram shows how the apps work together and the source can be found in this GitHub repository.\nCelery Worker The source of the Celery app and task is shown below - /queue_celery/tasks.py. The same Redis DB is used as the message broker and result backend. The task is nothing but iterating to total - the value is from a request. In each iteration, it updates its state (bind=True) followed by sleeping for 1 second and it is set that a task can be sent by referring to its name (name=\u0026quot;long_task\u0026quot;).\n1import os 2import math 3import time 4from celery import Celery 5 6redis_url = \u0026#34;redis://{0}:{1}/{2}\u0026#34;.format( 7 os.environ[\u0026#34;REDIS_HOST\u0026#34;], os.environ[\u0026#34;REDIS_PORT\u0026#34;], os.environ[\u0026#34;REDIS_DB\u0026#34;] 8) 9 10app = Celery(\u0026#34;tasks\u0026#34;, backend=redis_url, broker=redis_url) 11app.conf.update(broker_transport_options={\u0026#34;visibility_timeout\u0026#34;: 3600}) 12 13 14@app.task(bind=True, name=\u0026#34;long_task\u0026#34;) 15def long_task(self, total): 16 message = \u0026#34;\u0026#34; 17 for i in range(total): 18 message = \u0026#34;Percentage completion {0} ...\u0026#34;.format(math.ceil(i / total * 100)) 19 self.update_state(state=\u0026#34;PROGRESS\u0026#34;, meta={\u0026#34;current\u0026#34;: i, \u0026#34;total\u0026#34;: total, \u0026#34;status\u0026#34;: message}) 20 time.sleep(1) 21 return {\u0026#34;current\u0026#34;: total, \u0026#34;total\u0026#34;: total, \u0026#34;status\u0026#34;: \u0026#34;Task completed!\u0026#34;, \u0026#34;result\u0026#34;: total} Rserve Worker redux package, Redis client for R, and RSclient package, R-based client for Rserve, are used to set up the Rserve worker. The function RR() checks if a Redis DB is available and returns a hiredis object, which is an interface to Redis. The task (long_task()) is constructed to be similar to the Celery task. In order for the task to be executed asynchronously, a handler function (handle_long_task()) is used to receive a request from the main web service. Once called, the task function is sent to be evaluated by a Rserve client (RS.eval()) - note wait=FALSE and lazy=TRUE. Its evaluation is asynchronous as the task function is run by a separate forked process. Finally the status of a task can be obtained by get_task() and it pulls the status output from the Redis DB - note a R list is converted as binary. The source of the Rserve worker can be found in /queue_rserve/tasks.R.\n1RR \u0026lt;- function(check_conn_only = FALSE) { 2 redis_host \u0026lt;- Sys.getenv(\u0026#34;REDIS_HOST\u0026#34;, \u0026#34;localhost\u0026#34;) 3 redis_port \u0026lt;- Sys.getenv(\u0026#34;REDIS_PORT\u0026#34;, \u0026#34;6379\u0026#34;) 4 redis_db \u0026lt;- Sys.getenv(\u0026#34;REDIS_DB\u0026#34;, \u0026#34;1\u0026#34;) 5 info \u0026lt;- sprintf(\u0026#34;host %s, port %s, db %s\u0026#34;, redis_host, redis_port, redis_db) 6 ## check if redis is available 7 is_available = redis_available(host=redis_host, port=redis_port, db=redis_db) 8 if (is_available) { 9 flog.info(sprintf(\u0026#34;Redis is available - %s\u0026#34;, info)) 10 } else { 11 flog.error(sprintf(\u0026#34;Redis is not available - %s\u0026#34;, info)) 12 } 13 ## create an interface to redis 14 if (!check_conn_only) { 15 return(hiredis(host=redis_host, port=redis_port, db=redis_db)) 16 } 17} 18 19 20long_task \u0026lt;- function(task_id, total) { 21 rr \u0026lt;- RR() 22 for (i in seq.int(total)) { 23 is_total \u0026lt;- i == max(seq.int(total)) 24 state \u0026lt;- if (is_total) \u0026#34;SUCESS\u0026#34; else \u0026#34;PROGRESS\u0026#34; 25 msg \u0026lt;- sprintf(\u0026#34;Percent completion %s ...\u0026#34;, ceiling(i / total * 100)) 26 val \u0026lt;- list(state = state, current = i, total = total, status = msg) 27 if (is_total) { 28 val \u0026lt;- append(val, list(result = total)) 29 } 30 flog.info(sprintf(\u0026#34;task id: %s, message: %s\u0026#34;, task_id, msg)) 31 rr$SET(task_id, object_to_bin(val)) 32 Sys.sleep(1) 33 } 34} 35 36 37handle_long_task \u0026lt;- function(task_id, total) { 38 flog.info(sprintf(\u0026#34;task started, task_id - %s, total - %s\u0026#34;, task_id, total)) 39 conn \u0026lt;- RS.connect() 40 RS.eval(conn, library(redux)) 41 RS.eval(conn, library(futile.logger)) 42 RS.eval(conn, setwd(\u0026#34;/home/app\u0026#34;)) 43 RS.eval(conn, source(\u0026#34;./tasks.R\u0026#34;)) 44 RS.assign(conn, task_id) 45 RS.assign(conn, total) 46 RS.eval(conn, long_task(task_id, total), wait=FALSE, lazy=TRUE) 47 RS.close(conn) 48 flog.info(sprintf(\u0026#34;task executed, task_id - %s, total - %s\u0026#34;, task_id, total)) 49 list(task_id = task_id, status = \u0026#34;created\u0026#34;) 50} 51 52get_task \u0026lt;- function(task_id) { 53 rr \u0026lt;- RR() 54 val \u0026lt;- bin_to_object(rr$GET(task_id)) 55 flog.info(sprintf(\u0026#34;task id - %s\u0026#34;, task_id)) 56 flog.info(val) 57 val 58} Main Web Service The main service has 2 methods for each of the workers - POST for executing a task and GET for collecting its status. To execute a task, a value named total is required in request body. As soon as a task is sent or requested, it returns the task ID and status value - ExecuteResp. A task\u0026rsquo;s status can be obtained by calling the associating collect method with its ID in query string. The response is defined by ResultResp. The source of the Rserve worker can be found in /main.py.\n1import os 2import json 3import httpx 4from uuid import uuid4 5from fastapi import FastAPI, Body, HTTPException 6from pydantic import BaseModel, Schema 7 8from queue_celery.tasks import app as celery_app, long_task 9 10 11def set_rserve_url(fname): 12 return \u0026#34;http://{0}:{1}/{2}\u0026#34;.format( 13 os.getenv(\u0026#34;RSERVE_HOST\u0026#34;, \u0026#34;localhost\u0026#34;), os.getenv(\u0026#34;RSERVE_PORT\u0026#34;, \u0026#34;8000\u0026#34;), fname 14 ) 15 16 17app = FastAPI(title=\u0026#34;FastAPI Job Queue Example\u0026#34;, version=\u0026#34;0.0.1\u0026#34;) 18 19 20class ExecuteResp(BaseModel): 21 task_id: str 22 status: str 23 24 25class ResultResp(BaseModel): 26 current: int 27 total: int 28 status: str 29 result: int = None 30 31 32class ErrorResp(BaseModel): 33 detail: str 34 35 36@app.post(\u0026#34;/celery/execute\u0026#34;, response_model=ExecuteResp, status_code=202, tags=[\u0026#34;celery\u0026#34;]) 37async def execute_celery_task(total: int = Body(..., min=1, max=50, embed=True)): 38 task = celery_app.send_task(\u0026#34;long_task\u0026#34;, args=[total]) 39 return {\u0026#34;task_id\u0026#34;: task.id, \u0026#34;status\u0026#34;: \u0026#34;created\u0026#34;} 40 41 42@app.get( 43 \u0026#34;/celery/collect\u0026#34;, 44 response_model=ResultResp, 45 responses={500: {\u0026#34;model\u0026#34;: ErrorResp}}, 46 tags=[\u0026#34;celery\u0026#34;], 47) 48async def collect_celery_result(task_id: str): 49 resp = long_task.AsyncResult(task_id) 50 if resp.status == \u0026#34;FAILURE\u0026#34;: 51 raise HTTPException(status_code=500, detail=\u0026#34;Fails to collect result\u0026#34;) 52 return resp.result 53 54 55@app.post(\u0026#34;/rserve/execute\u0026#34;, response_model=ExecuteResp, status_code=202, tags=[\u0026#34;rserve\u0026#34;]) 56async def execute_rserve_task(total: int = Body(..., min=1, max=50, embed=True)): 57 jsn = json.dumps({\u0026#34;task_id\u0026#34;: str(uuid4()), \u0026#34;total\u0026#34;: total}) 58 async with httpx.AsyncClient() as client: 59 r = await client.post(set_rserve_url(\u0026#34;handle_long_task\u0026#34;), json=jsn) 60 return r.json() 61 62 63@app.get( 64 \u0026#34;/rserve/collect\u0026#34;, 65 response_model=ResultResp, 66 responses={500: {\u0026#34;model\u0026#34;: ErrorResp}}, 67 tags=[\u0026#34;rserve\u0026#34;], 68) 69async def collect_rserve_task(task_id: str): 70 jsn = json.dumps({\u0026#34;task_id\u0026#34;: task_id}) 71 async with httpx.AsyncClient() as client: 72 try: 73 r = await client.post(set_rserve_url(\u0026#34;get_task\u0026#34;), json=jsn) 74 return {k: v for k, v in r.json().items() if k != \u0026#34;state\u0026#34;} 75 except Exception: 76 raise HTTPException(status_code=500, detail=\u0026#34;Fails to collect result\u0026#34;) Docker Compose The apps can be started with Docker Compose as following - the compose file can be found here.\n1git clone https://github.com/jaehyeon-kim/k8s-job-queue.git 2cd k8s-job-queue 3docker-compose up -d The swagger document of the main web service can be visited via http://localhost:9000/docs or http://\u0026lt;vm-ip-address\u0026gt;:9000 if it\u0026rsquo;s started in a VM.\nA task can be started by clicking the Try it out button, followed by clicking the Execute button. Any value between 1 and 50 can be set as the value total.\nThe status of a task can be checked by adding its ID to query string.\nKubernetes 4 groups of resources are necessary to run the apps in Kubernetes and they can be found in /manifests.\nwebservice.yaml - main web service Deployment and Service queue_celery - Celery worker Deployment queue_rserve - Rserve worker Deployment and Service redis.yaml - Redis Deployment and Service In Kubernetes, Pod is one or more containers that work together. Deployment handles a replica of Pod (ReplicaSet), update strategy and so on. And Service allows to connect to a set of Pods from within and outside a Kubernetes cluster. Note that the Celery worker doesn\u0026rsquo;t have a Service resource as it is accessed by the Redis message broker/result backend.\nWith kubectl apply, the following resources are created as shown below. Note only the main web service is accessible by a client outside the cluster. The service is mapped to a specific node port (30000). In Minikube, it can be accessed by http://\u0026lt;node-ip-address\u0026gt;:3000. The node IP address can be found by minikube ip command.\n1## create resources 2kubectl apply -f manifests 3 4## get resources 5kubectl get po,rs,deploy,svc 6 7NAME READY STATUS RESTARTS AGE 8pod/celery-deployment-674d8fb968-2x97k 1/1 Running 0 25s 9pod/celery-deployment-674d8fb968-44lw4 1/1 Running 0 25s 10pod/main-deployment-79cf8fc5df-45w4p 1/1 Running 0 25s 11pod/main-deployment-79cf8fc5df-hkz6r 1/1 Running 0 25s 12pod/redis-deployment-5ff8646968-hcsbk 1/1 Running 0 25s 13pod/rserve-deployment-59dfbd955-db4v9 1/1 Running 0 25s 14pod/rserve-deployment-59dfbd955-fxfxn 1/1 Running 0 25s 15 16NAME DESIRED CURRENT READY AGE 17replicaset.apps/celery-deployment-674d8fb968 2 2 2 25s 18replicaset.apps/main-deployment-79cf8fc5df 2 2 2 25s 19replicaset.apps/redis-deployment-5ff8646968 1 1 1 25s 20replicaset.apps/rserve-deployment-59dfbd955 2 2 2 25s 21 22NAME READY UP-TO-DATE AVAILABLE AGE 23deployment.apps/celery-deployment 2/2 2 2 25s 24deployment.apps/main-deployment 2/2 2 2 25s 25deployment.apps/redis-deployment 1/1 1 1 25s 26deployment.apps/rserve-deployment 2/2 2 2 25s 27 28NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE 29service/main-service NodePort 10.98.60.194 \u0026lt;none\u0026gt; 80:30000/TCP 25s 30service/redis-service ClusterIP 10.99.52.18 \u0026lt;none\u0026gt; 6379/TCP 25s 31service/rserve-service ClusterIP 10.105.249.199 \u0026lt;none\u0026gt; 8000/TCP 25s The execute/collect pair of requests to the Celery worker are shown below. HttPie is used to make HTTP requests.\n1echo \u0026#39;{\u0026#34;total\u0026#34;: 30}\u0026#39; | http POST http://172.28.175.23:30000/celery/execute 2{ 3 \u0026#34;status\u0026#34;: \u0026#34;created\u0026#34;, 4 \u0026#34;task_id\u0026#34;: \u0026#34;87ae7a42-1ec0-4848-bf30-2f68175b38db\u0026#34; 5} 6 7export TASK_ID=87ae7a42-1ec0-4848-bf30-2f68175b38db 8http http://172.28.175.23:30000/celery/collect?task_id=$TASK_ID 9{ 10 \u0026#34;current\u0026#34;: 18, 11 \u0026#34;result\u0026#34;: null, 12 \u0026#34;status\u0026#34;: \u0026#34;Percentage completion 60 ...\u0026#34;, 13 \u0026#34;total\u0026#34;: 30 14} 15 16# after a while 17 18http http://172.28.175.23:30000/celery/collect?task_id=$TASK_ID 19{ 20 \u0026#34;current\u0026#34;: 30, 21 \u0026#34;result\u0026#34;: 30, 22 \u0026#34;status\u0026#34;: \u0026#34;Task completed!\u0026#34;, 23 \u0026#34;total\u0026#34;: 30 24} The following shows the execute/collect pair of the Rserve worker.\n1echo \u0026#39;{\u0026#34;total\u0026#34;: 30}\u0026#39; | http POST http://172.28.175.23:30000/rserve/execute 2{ 3 \u0026#34;status\u0026#34;: \u0026#34;created\u0026#34;, 4 \u0026#34;task_id\u0026#34;: \u0026#34;f5d46986-1e89-4322-9d4e-7c1da6454534\u0026#34; 5} 6 7export TASK_ID=f5d46986-1e89-4322-9d4e-7c1da6454534 8http http://172.28.175.23:30000/rserve/collect?task_id=$TASK_ID 9{ 10 \u0026#34;current\u0026#34;: 16, 11 \u0026#34;result\u0026#34;: null, 12 \u0026#34;status\u0026#34;: \u0026#34;Percent completion 54 ...\u0026#34;, 13 \u0026#34;total\u0026#34;: 30 14} 15 16# after a while 17 18http http://172.28.175.23:30000/rserve/collect?task_id=$TASK_ID 19{ 20 \u0026#34;current\u0026#34;: 30, 21 \u0026#34;result\u0026#34;: 30, 22 \u0026#34;status\u0026#34;: \u0026#34;Percent completion 100 ...\u0026#34;, 23 \u0026#34;total\u0026#34;: 30 24} ","date":"November 15, 2019","img":"/blog/2019-11-15-task-queue/featured.png","lang":"en","langName":"English","largeImg":"/blog/2019-11-15-task-queue/featured_hu7f23bb50744730950217f66f197f569b_51615_500x0_resize_box_3.png","permalink":"/blog/2019-11-15-task-queue/","series":[],"smallImg":"/blog/2019-11-15-task-queue/featured_hu7f23bb50744730950217f66f197f569b_51615_180x0_resize_box_3.png","tags":[{"title":"FastAPI","url":"/tags/fastapi/"},{"title":"Celery","url":"/tags/celery/"},{"title":"Redis","url":"/tags/redis/"},{"title":"Rserve","url":"/tags/rserve/"},{"title":"R","url":"/tags/r/"},{"title":"Python","url":"/tags/python/"},{"title":"Docker","url":"/tags/docker/"},{"title":"Docker Compose","url":"/tags/docker-compose/"},{"title":"Kubernetes","url":"/tags/kubernetes/"}],"timestamp":1573776000,"title":"Distributed Task Queue With Python and R Example"},{"categories":[{"title":"Engineering","url":"/categories/engineering/"}],"content":"I use Linux containers a lot for development. Having Windows computers at home and work, I used to use Linux VMs on VirtualBox or VMWare Workstation. It\u0026rsquo;s not a bad option but it requires a lot of resources. Recently, after my home computer was updated, I was not able to start my hypervisor anymore. Also I didn\u0026rsquo;t like huge resource consumption of it so that I began to look for a different development environment. A while ago, I played with Windows Subsystem for Linux (WSL) and it was alright. Also Visual Studio Code (VSCode), my favourite editor, now supports remote development. Initially I thought I would be able to create a new development environment with WSL and Docker for Windows. However it was until I tried a bigger app with Docker Compose that Docker for Windows has a number of issues especially when containers are started by Docker Compose in WSL. I didn\u0026rsquo;t like to spend too much time on fixing those issues as I concerned those might not be the only ones. Then I decided to install a Linux VM on Hyper-V. Luckly VSCode also supports a remote VM via SSH.\nWhat I want was a Linux VM where Docker is installed and it should be possible to access a remote folder from the host for development. Also, as I\u0026rsquo;m getting interested in Kubernetes more and more, another VM where Minikube is installed was necessary. In this post, it\u0026rsquo;ll be illustrated how the new development environment is created. Also an example app (Rserve web service with a sidecar container) will be demonstrated. The app\u0026rsquo;s main functionality is served by Rserve while the sidecar container handles authentication and relays requests from authenticated users. The sidecar container is built by FastAPI, which is a modern, performant and developer-friendly Python web framework.\nWindows Subsystem for Linux (WSL) In order to use WSL, it is necessary to enable Windows Subsystem for Linux in Windows features as following.\nThen a Linux distribution need to be installed from Windows Store. I chose Ubuntu 18.04.\nOnce installed, you can hit the Launch button and a new terminal will pop up as shown below. If it\u0026rsquo;s the first launch, you\u0026rsquo;d need to create a default user account. I set the username to be jaehyeon. The default terminal is not the only way to access to WSL. For example you can enter bash to access to it on PowerShell.\nRemote WSL Having WSL alone wouldn\u0026rsquo;t be of much help for development. What\u0026rsquo;s really important is, for example, editing or debugging code seamlessly from the host. The new VSCode extension, Remote WSL makes it possible.\nOn VSCode \u0026gt; Extensions, search and install Remote WSL. Then Remote Explorer tab will appear in the left sidebar. There you can open a folder in WSL.\nAs can be seen in the Extensions tab, some extensions are installed in the host (LOCAL) and some are in WSL. For example, in order for Python syntax highlighting or autocompletion to work, the Python extension should be installed in WSL where source code exists. One of the great features of VSCode is installation of extensions and configuration is automatic and easy. For some standard extensions, it\u0026rsquo;s sufficient to open a file of a certain file extension.\nYou may wonder why I set up Remote WSL although I\u0026rsquo;m going to install a VM on Hyper-V. It is because not all development can be done in a separate VM easily. For example, a single node Kubernetes cluster will be created by Minikube in a VM but I\u0026rsquo;m not sure how to connect to it from another VM. On the other hand, connection from WSL can be made without a problem.\nLinux Virtual Machine on Hyper-V A Linux VM creation can be started by Quick Create\u0026hellip; button in the right panel of Hyper-V Manager.\nI used a Ubuntu 18.04 server ISO image rather than using the desktop version in the default list. Clicking Local installation source will allow to select an ISO or virtual hard disk file. I named it as ubuntu and left Default Switch selected for Network - it allows to connect from host to guest.\nOnce installed, it\u0026rsquo;s possible to connect by clicking Connect\u0026hellip;. A separate window will pop up.\nIt\u0026rsquo;s possible to log in by the default username and password, which are set during installation. Once logged in, it\u0026rsquo;ll go into the user\u0026rsquo;s home directory. Keep the value of IP address for eth0 as it\u0026rsquo;ll be used for setting-up Remote SSH.\nFor public key authentication for SSH, check /etc/ssh/sshd_config if RSAAuthentication is enabled. You may need to add/update the following entries.\n1RSAAuthentication yes 2PubkeyAuthentication yes 3AuthorizedKeysFile %h/.ssh/authorized_keys Password authentication is enabled by default. If you don\u0026rsquo;t like it, it can be disabled as following.\n1PasswordAuthentication no Finally you need to add your RSA public key to ~/.ssh/authorized_keys followed by setting up necessary permissions to the file as well as its parent folder - see this page for further details.\nRemote SSH For Remote SSH, I installed VSCode Insiders as the guest OS is Ubuntu 18.04 - see System requirements. I\u0026rsquo;ve decided to keep both stable and insiders versions of VSCode - stable for WSL and insiders for SSH in order not to be confused. In the Remote Explorer, you can see Add New and Configure buttons.\nTo add a new SSH target, you can click the Add New button and enter the following SSH command.\n1# vm-ip-address - \u0026#39;IP address for eth0\u0026#39;. 2ssh \u0026lt;username\u0026gt;@\u0026lt;vm-ip-address\u0026gt; -A I find the VM IP address changes from time to time and a new IP address can be updated by clicking the Configure button followed by changing the IP address as shown below.\nCurrently R doesn\u0026rsquo;t seem to be supported well by VSCode and it may not be necessary thanks to RStudio IDE. I haven\u0026rsquo;t tried installing RStudio Server in WSL but it\u0026rsquo;ll definitely be possible to install it in a VM. Another way of accessing RStudio Server is via Docker. The Docker extension of VSCode can make it easier to run an existing image or to customize your own.\nConEmu ConEmu is a handy Windows terminal tool. By default it has multiple terminals pre-configured - PowerShell, Git Bash, Putty, Chocolatey and more. Also it\u0026rsquo;s possible to set up a custom terminal eg) for SSH.\nIn Setup tasks\u0026hellip;, I created 3 terminals and moved them to top for easy access.\nAlso it supports split windows. Below shows an example of 3 terminals in a single window. They are created by New console dialog\u0026hellip;.\nMinikube Minikube is a tool that makes it easy to run Kubernetes locally. Minikube runs a single-node Kubernetes cluster inside a Virtual Machine (VM) on your laptop for users looking to try out Kubernetes or develop with it day-to-day.\nAlthough Docer for Windows supports Kubernetes, I decided not to rely on it and Minikube supports Hyper-V as a VM driver. Using Chocolatey, a package manager for Windows, Minikube can be installed simply as following.\n1PS \u0026gt; choco install minikube It\u0026rsquo;ll also install Kubectl, a CLI tool for Kubenetes. ConEmu includes a Chocolatey terminal. If it\u0026rsquo;s the first time, it\u0026rsquo;ll inform what to do before executing the install command.\nOnce installed, a single node Kubernetes cluster can be created as following.\n1PS \u0026gt; minikube start --vm-driver hyperv --hyperv-virtual-switch \u0026#34;Default Switch\u0026#34; --v=7 --alsologtostderr Hyper-V is selected as the VM driver and a detailed logging option is added (--v=7 --alsologtostderr) because it takes quite some time at first for installation and configuration. For networking, I selected Default Switch. Note that many tutorials instruct to create an external Virtual Switch but it didn\u0026rsquo;t work on my computer at work. Also note that it\u0026rsquo;s not possible to create a Minikube cluster in WSL due to insufficient permission.\nIf it\u0026rsquo;s created successfully, you can check the status of the cluster.\n1PS \u0026gt; minikube status 2host: Running 3kubelet: Running 4apiserver: Running 5kubectl: Correctly Configured: pointing to minikube-vm at 172.28.175.24 During installation, Kubectl is configured in C:\\Users\\\u0026lt;username\u0026gt;\\.kube\\config as shown below.\n1apiVersion: v1 2clusters: 3- cluster: 4 certificate-authority: C:\\Users\\jakim\\.minikube\\ca.crt 5 server: https://172.28.175.24:8443 6 name: minikube 7contexts: 8- context: 9 cluster: minikube 10 user: minikube 11 name: minikube 12current-context: minikube 13kind: Config 14preferences: {} 15users: 16- name: minikube 17 user: 18 client-certificate: C:\\Users\\jakim\\.minikube\\client.crt 19 client-key: C:\\Users\\jakim\\.minikube\\client.key Although it\u0026rsquo;s possible to access the cluster with Kubectl on PowerShell, I\u0026rsquo;d like to do it on a linux terminal. I find a slight modification is enough on WSL. After installing Kubectl in WSL, I created another folder in C:\\Users\\\u0026lt;username\u0026gt;\\.kubewsl and added a WSL version of Kubectl config file.\n1apiVersion: v1 2clusters: 3- cluster: 4 certificate-authority: /c/Users/jakim/.minikube/ca.crt 5 server: https://172.28.175.24:8443 6 name: minikube 7contexts: 8- context: 9 cluster: minikube 10 user: minikube 11 name: minikube 12current-context: minikube 13kind: Config 14preferences: {} 15users: 16- name: minikube 17 user: 18 client-certificate: /c/Users/jakim/.minikube/client.crt 19 client-key: /c/Users/jakim/.minikube/client.key The only difference is path notation. For example, C:\\Users\\\u0026lt;username\u0026gt; to /c/Users/\u0026lt;username\u0026gt;. Note that your mount point is likely to be different because usually a host drive is mounted to WSL in /mnt as the root directory. In this case, it should be /mnt/c/Users/\u0026lt;username\u0026gt;. Or you may change the default mount point. After creating a /c folder in WSL, update WSL config to /etc/wsl.conf as shown below. You\u0026rsquo;d have to restart WSL afterwards.\n1[automount] 2root = / Then it\u0026rsquo;s necessary to update KUBECONFIG environment variable as following - you may add it to ~/.bashrc.\n1export KUBECONFIG=$KUBECONFIG:/c/Users/jakim/.kubewsl/config Once the setup is complete, cluser information can be checked on WSL as following.\n1$ kubectl cluster-info 2Kubernetes master is running at https://172.28.175.24:8443 3KubeDNS is running at https://172.28.175.24:8443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy 4 5To further debug and diagnose cluster problems, use \u0026#39;kubectl cluster-info dump\u0026#39;. Rserve Sidecar Example As demonstrated in my earlier post, Rserve is an effective tool to build a web service in R. It\u0026rsquo;ll be useful to serve an analytics model without modifying the core if it\u0026rsquo;s built in R. However equipping comprehensive features to other web frameworks may not be an easy job. Another way of utilising Rserve may be deploying it with one or more helper containers. For example, if the service should be secured by authentication but it\u0026rsquo;s not easy to implement it in R, a sidecar container may be used to handle authentication. In this setup, the sidecar container can sit before Rserve and relay requests from authenticated users. A simple example will be illustrated in this post. Another example may be using a helper container as an ambassador. For example, let say it\u0026rsquo;s required to communicate with an external system but there\u0026rsquo;s no reliable client library in R. In this case, it\u0026rsquo;s possible to set up so that the ambassador does the communication and provides outputs to R.\nThe example app is created by 2 containers. The main functionality is from the Rserve container while the sidecar is to handle authentication and relays requests from authentication users. The sidecar container is build with the FastAPI framework, which is a modern, performant and developer-friendly Python web framework. The source can be found in the sidecar branch of this GitHub repository.\nWith Docker Compose, it can be started as following. The compose file can be found here.\n1git clone https://github.com/jaehyeon-kim/k8s-rserve.git 2cd k8s-rserve 3git fetch \u0026amp;\u0026amp; git checkout sidecar 4docker-compose up -d The swagger document created by the sidecar web service can be visited via http://localhost:9000/docs or http://\u0026lt;vm-ip-address\u0026gt;:9000 if it\u0026rsquo;s started in a VM. It basically has a main POST method in /rserve/test.\nThe web service is secured by Bearer Authentication so that a JWT token needs to be added to requests. A token can be obtained in the /auth/debug/{username} endpoint and it can also be tried out in the swagger document as shown below.\nThe token can be added to the Authrization section.\nAfter that, a request to /rserve/test can be made as an authenticated user. It returns a JSON object that has 3 properties: n, wait and hostname. The first 2 is just returning values from the request while the last shows the hostname where Rserve container is hosted.\nContainerization makes development easier. When it comes to deployment and management of potentially a large number of containers, a container orchestration engin is quite important. Kubernetes, among other engins, does such jobs well. Developers and administrator can define what it should be and then Kubernetes achieves it. This declarative nature can make complicated jobs to be tractable.\nIn Kubernetes, Pod is one or more containers that work together - eg) Rserve with sidecar. Deployment handles a replica of Pod, update strategy and so on. And Service allows to connect to a replica of Pod from within and outside a Kubernetes Cluster. The following Kubernetes manifest creates 2 Kubernetes resources where the Rserve with sidecar web service can be managed (by Deployment) and accessed (by Service). The manifest file can be found here.\n1apiVersion: v1 2kind: Service 3metadata: 4 name: rserve-sidecar-service 5spec: 6 selector: # Pods with this label will be served 7 app: rserve-sidecar 8 type: NodePort 9 ports: 10 - protocol: TCP 11 port: 80 12 targetPort: 9000 13 nodePort: 30000 # Service will exposed at this port 14--- 15apiVersion: apps/v1 16kind: Deployment 17metadata: 18 name: rserve-sidecar-deployment 19 labels: 20 app: rserve 21spec: 22 selector: 23 matchLabels: # Pods with this label will be managed by Deployment 24 app: rserve-sidecar 25 replicas: 2 26 template: 27 metadata: 28 labels: 29 app: rserve-sidecar 30 spec: 31 containers: 32 - name: rserve 33 image: kapps/sidecar:rserve 34 ports: 35 - containerPort: 8000 36 - name: pyapi 37 image: kapps/sidecar:pyapi 38 env: 39 - name: RSERVE_HOST # containers share network interface 40 value: localhost 41 - name: RSERVE_PORT 42 value: \u0026#34;8000\u0026#34; 43 - name: JWT_SECRET # it\u0026#39;s not secure!! 44 value: chickenAndSons 45 ports: 46 - containerPort: 9000 With kubectl, it can be created as following. The --record flag is for keeping revision history, which can be particulary useful for updating the app.\n1$ kubectl apply -f manifest.yml --record 2# service/rserve-sidecar-service created 3# deployment.apps/rserve-sidecar-deployment created The resources can be listed as shown below.\n1$ kubectl get deployment,svc 2NAME READY UP-TO-DATE AVAILABLE AGE 3deployment.apps/rserve-sidecar-deployment 2/2 2 2 21s 4 5NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE 6service/rserve-sidecar-service NodePort 10.106.51.149 \u0026lt;none\u0026gt; 80:30000/TCP 21s The service is mapped to a specific node port (30000) and, as Minikube creates a single node cluster, the web service can be accessed by http://\u0026lt;node-ip-address\u0026gt;:3000. The node IP address can be found by minikube ip on PowerShell.\nFor authentication, the debug method can be called and it\u0026rsquo;ll return a token.\n1$ http http://172.28.175.24:30000/auth/debug/JAKIM 2 3# { 4# \u0026#34;token\u0026#34;: \u0026#34;eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJKQUtJTSIsImRlYnVnIjp0cnVlfQ.ddWXLcsB4IzQ5743vq-WVC2n-D9Z5yFIkSqjkpOAcs4\u0026#34; 5# } All requests to the main method need the token in the Authorization header. If authenticated, the sidecar will make a request to Rserve and the response from it will be returned back to the client. I made 2 requests using HttPie and the responses show different hostname values. It\u0026rsquo;s because requests are load-balanced by the Service.\n1export token=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJKQUtJTSIsImRlYnVnIjp0cnVlfQ.ddWXLcsB4IzQ5743vq-WVC2n-D9Z5yFIkSqjkpOAcs4 2 3$ echo \u0026#39;{\u0026#34;n\u0026#34;: 100, \u0026#34;wait\u0026#34;: 0.1}\u0026#39; \\ 4 | http POST \u0026#34;http://172.28.175.24:30000/rserve/test\u0026#34; \u0026#34;Authorization:Bearer $token\u0026#34; 5 6# { 7# \u0026#34;hostname\u0026#34;: \u0026#34;rserve-sidecar-deployment-5cbd6569f-ndjnb\u0026#34;, 8# \u0026#34;n\u0026#34;: 100.0, 9# \u0026#34;wait\u0026#34;: 0.1 10# } 11 12$ echo \u0026#39;{\u0026#34;n\u0026#34;: 100, \u0026#34;wait\u0026#34;: 0.1}\u0026#39; \\ 13 | http POST \u0026#34;http://172.28.175.24:30000/rserve/test\u0026#34; \u0026#34;Authorization:Bearer $token\u0026#34; 14 15# { 16# \u0026#34;hostname\u0026#34;: \u0026#34;rserve-sidecar-deployment-5cbd6569f-kv599\u0026#34;, 17# \u0026#34;n\u0026#34;: 100.0, 18# \u0026#34;wait\u0026#34;: 0.1 19# } ","date":"November 1, 2019","img":"/blog/2019-11-01-linux-on-windows/featured.png","lang":"en","langName":"English","largeImg":"/blog/2019-11-01-linux-on-windows/featured_huc10c59749850e2c6e75df8112745c889_187978_500x0_resize_box_3.png","permalink":"/blog/2019-11-01-linux-on-windows/","series":[],"smallImg":"/blog/2019-11-01-linux-on-windows/featured_huc10c59749850e2c6e75df8112745c889_187978_180x0_resize_box_3.png","tags":[{"title":"VSCode","url":"/tags/vscode/"},{"title":"WSL","url":"/tags/wsl/"},{"title":"Docker","url":"/tags/docker/"},{"title":"Docker Compose","url":"/tags/docker-compose/"},{"title":"Kubernetes","url":"/tags/kubernetes/"},{"title":"Minikube","url":"/tags/minikube/"},{"title":"Rserve","url":"/tags/rserve/"},{"title":"FastAPI","url":"/tags/fastapi/"},{"title":"R","url":"/tags/r/"},{"title":"Python","url":"/tags/python/"}],"timestamp":1572566400,"title":"Linux Dev Environment on Windows"},{"categories":[{"title":"Engineering","url":"/categories/engineering/"}],"content":"LocalStack provides an easy-to-use test/mocking framework for developing AWS applications. In this post, I\u0026rsquo;ll demonstrate how to utilize LocalStack for development using a web service.\nSpecifically a simple web service built with Flask-RestPlus is used. It supports simple CRUD operations against a database table. It is set that SQS and Lambda are used for creating and updating a record. When a POST or PUT request is made, the service sends a message to a SQS queue and directly returns 204 reponse. Once a message is received, a Lambda function is invoked and a relevant database operation is performed.\nThe source of this post can be found here.\nWeb Service As usual, the GET requests returns all records or a single record when an ID is provided as a path parameter. When an ID is not specified, it\u0026rsquo;ll create a new record (POST). Otherwise it\u0026rsquo;ll update an existing record (PUT). Note that both the POST and PUT method send a message and directly returns 204 response - the source can be found here.\n1ns = Namespace(\u0026#34;records\u0026#34;) 2 3@ns.route(\u0026#34;/\u0026#34;) 4class Records(Resource): 5 parser = ns.parser() 6 parser.add_argument(\u0026#34;message\u0026#34;, type=str, required=True) 7 8 def get(self): 9 \u0026#34;\u0026#34;\u0026#34; 10 Get all records 11 \u0026#34;\u0026#34;\u0026#34; 12 conn = conn_db() 13 cur = conn.cursor(real_dict_cursor=True) 14 cur.execute( 15 \u0026#34;\u0026#34;\u0026#34; 16 SELECT * FROM records ORDER BY created_on DESC 17 \u0026#34;\u0026#34;\u0026#34;) 18 19 records = cur.fetchall() 20 return jsonify(records) 21 22 @ns.expect(parser) 23 def post(self): 24 \u0026#34;\u0026#34;\u0026#34; 25 Create a record via queue 26 \u0026#34;\u0026#34;\u0026#34; 27 try: 28 body = { 29 \u0026#34;id\u0026#34;: None, 30 \u0026#34;message\u0026#34;: self.parser.parse_args()[\u0026#34;message\u0026#34;] 31 } 32 send_message(flask.current_app.config[\u0026#34;QUEUE_NAME\u0026#34;], json.dumps(body)) 33 return \u0026#34;\u0026#34;, 204 34 except Exception as e: 35 return \u0026#34;\u0026#34;, 500 36 37 38@ns.route(\u0026#34;/\u0026lt;string:id\u0026gt;\u0026#34;) 39class Record(Resource): 40 parser = ns.parser() 41 parser.add_argument(\u0026#34;message\u0026#34;, type=str, required=True) 42 43 def get(self, id): 44 \u0026#34;\u0026#34;\u0026#34; 45 Get a record given id 46 \u0026#34;\u0026#34;\u0026#34; 47 record = Record.get_record(id) 48 if record is None: 49 return {\u0026#34;message\u0026#34;: \u0026#34;No record\u0026#34;}, 404 50 return jsonify(record) 51 52 @ns.expect(parser) 53 def put(self, id): 54 \u0026#34;\u0026#34;\u0026#34; 55 Update a record via queue 56 \u0026#34;\u0026#34;\u0026#34; 57 record = Record.get_record(id) 58 if record is None: 59 return {\u0026#34;message\u0026#34;: \u0026#34;No record\u0026#34;}, 404 60 61 try: 62 message = { 63 \u0026#34;id\u0026#34;: record[\u0026#34;id\u0026#34;], 64 \u0026#34;message\u0026#34;: self.parser.parse_args()[\u0026#34;message\u0026#34;] 65 } 66 send_message(flask.current_app.config[\u0026#34;QUEUE_NAME\u0026#34;], json.dumps(message)) 67 return \u0026#34;\u0026#34;, 204 68 except Exception as e: 69 return \u0026#34;\u0026#34;, 500 70 71 @staticmethod 72 def get_record(id): 73 conn = conn_db() 74 cur = conn.cursor(real_dict_cursor=True) 75 cur.execute( 76 \u0026#34;\u0026#34;\u0026#34; 77 SELECT * FROM records WHERE id = %(id)s 78 \u0026#34;\u0026#34;\u0026#34;, {\u0026#34;id\u0026#34;: id}) 79 80 return cur.fetchone() Lambda The SQS queue that messages are sent by the web service is an event source of the following lambda function. It polls the queue and processes messages as shown below.\n1import os 2import logging 3import json 4import psycopg2 5 6logger = logging.getLogger() 7logger.setLevel(logging.INFO) 8 9try: 10 conn = psycopg2.connect(os.environ[\u0026#34;DB_CONNECT\u0026#34;], connect_timeout=5) 11except psycopg2.Error as e: 12 logger.error(e) 13 sys.exit() 14 15logger.info(\u0026#34;SUCCESS: Connection to DB\u0026#34;) 16 17def lambda_handler(event, context): 18 for r in event[\u0026#34;Records\u0026#34;]: 19 body = json.loads(r[\u0026#34;body\u0026#34;]) 20 logger.info(\u0026#34;Body: {0}\u0026#34;.format(body)) 21 with conn.cursor() as cur: 22 if body[\u0026#34;id\u0026#34;] is None: 23 cur.execute( 24 \u0026#34;\u0026#34;\u0026#34; 25 INSERT INTO records (message) VALUES (%(message)s) 26 \u0026#34;\u0026#34;\u0026#34;, {k:v for k,v in body.items() if v is not None}) 27 else: 28 cur.execute( 29 \u0026#34;\u0026#34;\u0026#34; 30 UPDATE records 31 SET message = %(message)s 32 WHERE id = %(id)s 33 \u0026#34;\u0026#34;\u0026#34;, body) 34 conn.commit() 35 36 logger.info(\u0026#34;SUCCESS: Processing {0} records\u0026#34;.format(len(event[\u0026#34;Records\u0026#34;]))) Database As RDS is not yet supported by LocalStack, a postgres db is created with Docker. The web service will do CRUD operations against the table named as records. The initialization SQL script is shown below.\n1CREATE DATABASE testdb; 2\\connect testdb; 3 4CREATE SCHEMA testschema; 5GRANT ALL ON SCHEMA testschema TO testuser; 6 7-- change search_path on a connection-level 8SET search_path TO testschema; 9 10-- change search_path on a database-level 11ALTER database \u0026#34;testdb\u0026#34; SET search_path TO testschema; 12 13CREATE TABLE testschema.records ( 14\tid serial NOT NULL, 15\tmessage varchar(30) NOT NULL, 16\tcreated_on timestamptz NOT NULL DEFAULT now(), 17\tCONSTRAINT records_pkey PRIMARY KEY (id) 18); 19 20INSERT INTO testschema.records (message) 21VALUES (\u0026#39;foo\u0026#39;), (\u0026#39;bar\u0026#39;), (\u0026#39;baz\u0026#39;); Launch Services Below shows the docker-compose file that creates local AWS services and postgres database.\n1version: \u0026#39;3.7\u0026#39; 2services: 3 localstack: 4 image: localstack/localstack 5 ports: 6 - \u0026#39;4563-4584:4563-4584\u0026#39; 7 - \u0026#39;8080:8080\u0026#39; 8 privileged: true 9 environment: 10 - SERVICES=s3,sqs,lambda 11 - DEBUG=1 12 - DATA_DIR=/tmp/localstack/data 13 - DEFAULT_REGION=ap-southeast-2 14 - LAMBDA_EXECUTOR=docker-reuse 15 - LAMBDA_REMOTE_DOCKER=false 16 - LAMBDA_DOCKER_NETWORK=play-localstack_default 17 - AWS_ACCESS_KEY_ID=foobar 18 - AWS_SECRET_ACCESS_KEY=foobar 19 - AWS_DEFAULT_REGION=ap-southeast-2 20 - DB_CONNECT=\u0026#39;postgresql://testuser:testpass@postgres:5432/testdb\u0026#39; 21 - TEST_QUEUE=test-queue 22 - TEST_LAMBDA=test-lambda 23 volumes: 24 - ./init/create-resources.sh:/docker-entrypoint-initaws.d/create-resources.sh 25 - ./init/lambda_package:/tmp/lambda_package 26 # - \u0026#39;./.localstack:/tmp/localstack\u0026#39; 27 - \u0026#39;/var/run/docker.sock:/var/run/docker.sock\u0026#39; 28 postgres: 29 image: postgres 30 ports: 31 - 5432:5432 32 volumes: 33 - ./init/db:/docker-entrypoint-initdb.d 34 depends_on: 35 - localstack 36 environment: 37 - POSTGRES_USER=testuser 38 - POSTGRES_PASSWORD=testpass For LocalStack, it\u0026rsquo;s easier to illustrate by the environment variables.\nSERVICES - S3, SQS and Lambda services are selected DEFAULT_REGION - Local AWS resources will be created in ap-southeast-2 by default LAMBDA_EXECUTOR - By selecting docker-reuse, Lambda function will be invoked by another container (based on lambci/lambda image). Once a container is created, it\u0026rsquo;ll be reused. Note that, in order to invoke a Lambda function in a separate Docker container, it should run in privileged mode (privileged: true) LAMBDA_REMOTE_DOCKER - It is set to false so that a Lambda function package can be added from a local path instead of a zip file. LAMBDA_DOCKER_NETWORK - Although the Lambda function is invoked in a separate container, it should be able to discover the database service (postgres). By default, Docker Compose creates a network (\u0026lt;parent-folder\u0026gt;_default) and, specifying the network name, the Lambda function can connect to the database with the DNS set by DB_CONNECT Actual AWS resources is created by create-resources.sh, which will be executed at startup. A SQS queue and Lambda function are created and the queue is mapped to be an event source of the Lambda function.\n1#!/bin/bash 2 3echo \u0026#34;Creating $TEST_QUEUE and $TEST_LAMBDA\u0026#34; 4 5aws --endpoint-url=http://localhost:4576 sqs create-queue \\ 6 --queue-name $TEST_QUEUE 7 8aws --endpoint-url=http://localhost:4574 lambda create-function \\ 9 --function-name $TEST_LAMBDA \\ 10 --code S3Bucket=\u0026#34;__local__\u0026#34;,S3Key=\u0026#34;/tmp/lambda_package\u0026#34; \\ 11 --runtime python3.6 \\ 12 --environment Variables=\u0026#34;{DB_CONNECT=$DB_CONNECT}\u0026#34; \\ 13 --role arn:aws:lambda:ap-southeast-2:000000000000:function:$TEST_LAMBDA \\ 14 --handler lambda_function.lambda_handler \\ 15 16aws --endpoint-url=http://localhost:4574 lambda create-event-source-mapping \\ 17 --function-name $TEST_LAMBDA \\ 18 --event-source-arn arn:aws:sqs:elasticmq:000000000000:$TEST_QUEUE The services can be launched as following.\n1docker-compose up -d Test Web Service Before testing the web service, it can be shown how the SQS and Lambda work by sending a message as following.\n1aws --endpoint-url http://localhost:4576 sqs send-message \\ 2 --queue-url http://localhost:4576/queue/test-queue \\ 3 --message-body \u0026#39;{\u0026#34;id\u0026#34;: null, \u0026#34;message\u0026#34;: \u0026#34;test\u0026#34;}\u0026#39; As shown in the image below, LocalStack invokes the Lambda function in a separate Docker container.\nThe web service can be started as following.\n1FLASK_APP=api FLASK_ENV=development flask run Using HttPie, the record created just before can be checked as following.\n1http http://localhost:5000/api/records/4 1{ 2 \u0026#34;created_on\u0026#34;: \u0026#34;2019-07-20T04:26:33.048841+00:00\u0026#34;, 3 \u0026#34;id\u0026#34;: 4, 4 \u0026#34;message\u0026#34;: \u0026#34;test\u0026#34; 5} For updating it,\n1echo \u0026#39;{\u0026#34;message\u0026#34;: \u0026#34;test put\u0026#34;}\u0026#39; | \\ 2 http PUT http://localhost:5000/api/records/4 3 4http http://localhost:5000/api/records/4 1{ 2 \u0026#34;created_on\u0026#34;: \u0026#34;2019-07-20T04:26:33.048841+00:00\u0026#34;, 3 \u0026#34;id\u0026#34;: 4, 4 \u0026#34;message\u0026#34;: \u0026#34;test put\u0026#34; 5} ","date":"July 20, 2019","img":"/blog/2019-07-20-aws-localstack/featured.png","lang":"en","langName":"English","largeImg":"/blog/2019-07-20-aws-localstack/featured_hu76363859e2b475a85e538ad8f869426e_164886_500x0_resize_box_3.png","permalink":"/blog/2019-07-20-aws-localstack/","series":[],"smallImg":"/blog/2019-07-20-aws-localstack/featured_hu76363859e2b475a85e538ad8f869426e_164886_180x0_resize_box_3.png","tags":[{"title":"AWS","url":"/tags/aws/"},{"title":"Docker","url":"/tags/docker/"},{"title":"Docker Compose","url":"/tags/docker-compose/"},{"title":"AWS Lambda","url":"/tags/aws-lambda/"},{"title":"LocalStack","url":"/tags/localstack/"},{"title":"S3","url":"/tags/s3/"},{"title":"SQS","url":"/tags/sqs/"},{"title":"Python","url":"/tags/python/"},{"title":"Flask","url":"/tags/flask/"},{"title":"Flask-RestPlus","url":"/tags/flask-restplus/"}],"timestamp":1563580800,"title":"AWS Local Development With LocalStack"},{"categories":[{"title":"Engineering","url":"/categories/engineering/"}],"content":"Accroding to the project GitHub repository,\nCronicle is a multi-server task scheduler and runner, with a web based front-end UI. It handles both scheduled, repeating and on-demand jobs, targeting any number of slave servers, with real-time stats and live log viewer.\nBy default, Cronicle is configured to launch a single master server - task scheduling is controlled by the master server. For high availability, it is important that another server takes the role of master when the existing master server fails.\nIn this post, multi-server configuration of Cronicle will be demonstrated with Docker and Nginx as load balancer. Specifically a single master and backup server will be set up and they will be served behind a load balancer - backup server is a slave server that can take the role of master when the master is not avaialble.\nThe source of this post can be found here.\nCronicle Docker Image There isn\u0026rsquo;t an official Docker image for Cronicle. I just installed it from python:3.6 image. The docker file can be found as following.\n1FROM python:3.6 2 3ARG CRONICLE_VERSION=v0.8.28 4ENV CRONICLE_VERSION=${CRONICLE_VERSION} 5 6# Node 7RUN curl -sL https://deb.nodesource.com/setup_8.x | bash - \\ 8 \u0026amp;\u0026amp; apt-get install -y nodejs \\ 9 \u0026amp;\u0026amp; curl -sL https://dl.yarnpkg.com/debian/pubkey.gpg | apt-key add - \\ 10 \u0026amp;\u0026amp; echo \u0026#34;deb https://dl.yarnpkg.com/debian/ stable main\u0026#34; | tee /etc/apt/sources.list.d/yarn.list \\ 11 \u0026amp;\u0026amp; apt-get update \u0026amp;\u0026amp; apt-get install -y yarn 12 13# Cronicle 14RUN curl -s \u0026#34;https://raw.githubusercontent.com/jhuckaby/Cronicle/${CRONICLE_VERSION}/bin/install.js\u0026#34; | node \\ 15 \u0026amp;\u0026amp; cd /opt/cronicle \\ 16 \u0026amp;\u0026amp; npm install aws-sdk 17 18EXPOSE 3012 19EXPOSE 3014 20 21ENTRYPOINT [\u0026#34;/docker-entrypoint.sh\u0026#34;] As Cronicle is written in Node.js, it should be installed as well. aws-sdk is not required strictly but it is added to test S3 integration later. Port 3012 is the default web port of Cronicle and 3014 is used for server auto-discovery via UDP broadcast - it may not be required.\ndocker-entrypoint.sh is used to start a Cronicle server. For master, one more step is necessary, which is initializing the storage system. An environment variable (IS_MASTER) will be used to control storage initialization.\n1#!/bin/bash 2 3set -e 4 5if [ \u0026#34;$IS_MASTER\u0026#34; = \u0026#34;0\u0026#34; ] 6then 7 echo \u0026#34;Running SLAVE server\u0026#34; 8else 9 echo \u0026#34;Running MASTER server\u0026#34; 10 /opt/cronicle/bin/control.sh setup 11fi 12 13/opt/cronicle/bin/control.sh start 14 15while true; 16do 17 sleep 30; 18 /opt/cronicle/bin/control.sh status 19done A custom docker image, cronicle-base, is built using as following.\n1docker build -t=cronicle-base . Load Balancer Nginx is used as a load balancer. The config file can be found as following. It listens port 8080 and passes a request to cronicle1:3012 or cronicle2:3012.\n1events { worker_connections 1024; } 2 3http { 4 upstream cronicles { 5 server cronicle1:3012; 6 server cronicle2:3012; 7 } 8 9 server { 10 listen 8080; 11 12 location / { 13 proxy_pass http://cronicles; 14 proxy_set_header Host $host; 15 } 16 } 17} In order for Cronicle servers to be served behind the load balancer, the following changes are made (complete config file can be found here).\n1{ 2\t\u0026#34;base_app_url\u0026#34;: \u0026#34;http://loadbalancer:8080\u0026#34;, 3 4 ... 5\t6\t\u0026#34;web_direct_connect\u0026#34;: true, 7 8 ...\t9} First, base_app_url should be changed to the load balancer URL instead of an individual server\u0026rsquo;s URL. Secondly web_direct_connect should be changed to true. According to the project repository,\nIf you set this parameter (web_direct_connect) to true, then the Cronicle web application will connect directly to your individual Cronicle servers. This is more for multi-server configurations, especially when running behind a load balancer with multiple backup servers. The Web UI must always connect to the master server, so if you have multiple backup servers, it needs a direct connection.\nLaunch Servers Docke Compose is used to launch 2 Cronicle servers (master and backup) and a load balancer. The service cronicle1 is for the master while cronicle2 is for the backup server. Note that both servers should have the same configuration file (config.json). Also, as the backup server will take the role of master, it should have access to the same data - ./backend/cronicle/data is mapped to both the servers. (Cronicle supports S3 or Couchbase as well.)\n1version: \u0026#39;3.2\u0026#39; 2 3services: 4 loadbalancer: 5 container_name: loadbalancer 6 hostname: loadbalancer 7 image: nginx 8 volumes: 9 - ./loadbalancer/nginx.conf:/etc/nginx/nginx.conf 10 tty: true 11 links: 12 - cronicle1 13 ports: 14 - 8080:8080 15 cronicle1: 16 container_name: cronicle1 17 hostname: cronicle1 18 image: cronicle-base 19 #restart: always 20 volumes: 21 - ./sample_conf/config.json:/opt/cronicle/conf/config.json 22 - ./sample_conf/emails:/opt/cronicle/conf/emails 23 - ./docker-entrypoint.sh:/docker-entrypoint.sh 24 - ./backend/cronicle/data:/opt/cronicle/data 25 entrypoint: /docker-entrypoint.sh 26 environment: 27 IS_MASTER: \u0026#34;1\u0026#34; 28 cronicle2: 29 container_name: cronicle2 30 hostname: cronicle2 31 image: cronicle-base 32 #restart: always 33 volumes: 34 - ./sample_conf/config.json:/opt/cronicle/conf/config.json 35 - ./sample_conf/emails:/opt/cronicle/conf/emails 36 - ./docker-entrypoint.sh:/docker-entrypoint.sh 37 - ./backend/cronicle/data:/opt/cronicle/data 38 entrypoint: /docker-entrypoint.sh 39 environment: 40 IS_MASTER: \u0026#34;0\u0026#34; It can be started as following.\n1docker-compose up -d Add Backup Server Once started, Cronicle web app will be accessible at http://localhost:8080 and it\u0026rsquo;s possible to log in as the admin user - username and password are all admin.\nIn Admin \u0026gt; Servers, it\u0026rsquo;s possible to see that the 2 Cronicle servers are shown. The master server is recognized as expected but the backup server (cronicle2) is not yet added.\nBy default, 2 server groups (All Servers and Master Group) are created and the backup server should be added to the Master Group. To do so, the Hostname Match regular expression is modified as following: ^(cronicle[1-2])$.\nThen it can be shown that the backup server is recognized correctly.\nCreate Event A test event is created in order to show that an event that\u0026rsquo;s created in the original master can be available in the backup server when it takes the role of master.\nCronicle has a web UI so that it is easy to manage/monitor scheduled events. It also has management API that many jobs can be performed programmatically. Here an event that runs a simple shell script is created.\nOnce created, it is listed in Schedule tab.\nBackup Becomes Master As mentioned earlier, the backup server will take the role of master when the master becomes unavailable. In order to see this, I removed the master server as following.\n1docker-compose rm -f cronicle1 After a while, it\u0026rsquo;s possible to see that the backup server becomes master.\nIt can also be checked in Admin \u0026gt; Activity Log.\nIn Schedule, the test event can be found.\n","date":"July 19, 2019","img":"/blog/2019-07-19-cronicle-multi-server-setup/featured.png","lang":"en","langName":"English","largeImg":"/blog/2019-07-19-cronicle-multi-server-setup/featured_huf31defd43c31d7741cd0651b807bf58f_64396_500x0_resize_box_3.png","permalink":"/blog/2019-07-19-cronicle-multi-server-setup/","series":[],"smallImg":"/blog/2019-07-19-cronicle-multi-server-setup/featured_huf31defd43c31d7741cd0651b807bf58f_64396_180x0_resize_box_3.png","tags":[{"title":"Cronicle","url":"/tags/cronicle/"},{"title":"Docker","url":"/tags/docker/"},{"title":"Nginx","url":"/tags/nginx/"}],"timestamp":1563494400,"title":"Cronicle Multi Server Setup"},{"categories":[{"title":"Web Development","url":"/categories/web-development/"}],"content":"In the last post, the async feature of Shiny was discussed. Although it is a remarkable step forward to web development in R, it is not to the full extent that a Javascript application can bring. In fact, (long running) requests of a user (or session) are not impacted by those of other users (or sessions) but, for a given user, all requests are handled sequentially. On the other hand, it is not the case for a Javascript-backed app where all requests are processed asynchronously.\nAlthough Javascript helps develop a more performant web app, for a Shiny developer, the downside is that key features that Shiny provides are no longer available. Some of them are built-in data binding, event handling and state management. For example, think about what reactive*() and observe*() do in a Shiny app. Although it is possible to implement those with plain Javascript or JQuery, it can be problemsome due to the aysnc nature of Javascript (eg Callback Hell) or it may be ending up with a slow app (eg Why do developers think the DOM is slow?).\nJavascript frameworks (Angular, React and Vue) support such key features effectively. Also they help avoid those development issues, together with the recent Javascript standard. In this post, it\u0026rsquo;ll be demonstrated how to render htmlwidgets in a Vue application as well as replacing htmlwidgets with native JavaScript libraries.\nWhat is Vue? According to the project website,\nVue (pronounced /vjuː/, like view) is a progressive framework for building user interfaces. Unlike other monolithic frameworks, Vue is designed from the ground up to be incrementally adoptable. The core library is focused on the view layer only, and is easy to pick up and integrate with other libraries or existing projects. On the other hand, Vue is also perfectly capable of powering sophisticated Single-Page Applications when used in combination with modern tooling and supporting libraries.\nSome of the key features mentioned earlier are supported in the core library.\ndata binding event handling And the other by an official library.\nstate management And even more\nrouting Vue is taken here among the popular Javascript frameworks because it is simpler to get jobs done and easier to learn. See this article for a quick comparison.\nBuilding Vue Apps Vue Setup Prerequisites of building a vue app are\nNode.js Javascript runtime built on Chrome\u0026rsquo;s V8 JavaScript engine version \u0026gt;=6.x or 8.x (preferred) npm package manager for Javascript and software registry version 3+ installed with Node.js Git vue-cli a simple CLI for scaffolding Vue.js projects provides templates and webpack-simple is used for the apps of this post install globally - npm install -g vue-cli Yarn fast, reliable and secure dependency management tool used instead of npm install globally - npm install -g yarn The apps can be found in vue-htmlwidgets and vue-native folders of this GitHub repository. They are built with webpack and can be started as following.\n1cd path-to-folder 2yarn install 3npm run dev Libraries for the Apps Common libraries User Interface Vuetify - Although Bootstrap is popular for user interface, I find most UI libraries that rely on Bootstrap also depend on JQuery. And it is possible the JQuery for htmlwidgets is incompatible with those for the UI libraries. Therefore Vuetify is used instead, which is inspired by Material Design. HTTP request axios - Promise based HTTP client for the browser and node.js For vue-native state management Vuex - Vuex is a state management pattern + library for Vue.js applications. It serves as a centralized store for all the components in an application, with rules ensuring that the state can only be mutated in a predictable fashion. data table Vuetify - The built-in data table component of Vuetify is used. plotly plotly - official library @statnett/vue-plotly - plotly as a vue component ify-loader and transform-loader - for building with webpack highcharts highcharts - official library highcharts-vue - highcharts as vue components Render Htmlwidgets index.html The entry point of the app is index.html and htmlwidgets dependencies need to be included in head followed by material fonts and icons. 3 components are created in src/components - DataTable.vue, Highchart.vue and Plotly.vue. These components are bundled into build.js and sourced by the app.\n1\u0026lt;!DOCTYPE html\u0026gt; 2\u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; 3 \u0026lt;head\u0026gt; 4 \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt; 5 \u0026lt;title\u0026gt;Vue - htmlwidgets\u0026lt;/title\u0026gt; 6 \u0026lt;meta http-equiv=\u0026#34;X-UA-Compatible\u0026#34; content=\u0026#34;IE=edge\u0026#34;\u0026gt; 7 \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width, initial-scale=1.0\u0026#34;\u0026gt; 8 \u0026lt;!-- necessary to control htmlwidgets --\u0026gt; 9 \u0026lt;script src=\u0026#34;/public/htmlwidgets/htmlwidgets.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; 10 \u0026lt;!-- need a shared JQuery lib --\u0026gt; 11 \u0026lt;script src=\u0026#34;/public/shared/jquery.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; 12 \u0026lt;!-- DT --\u0026gt; 13 \u0026lt;script src=\u0026#34;/public/datatables/datatables.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; 14 ... more DT dependencies 15 \u0026lt;!-- highchater --\u0026gt; 16 \u0026lt;script src=\u0026#34;/public/highcharter/lib/proj4js/proj4.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; 17 ... more highcharts dependencies 18 \u0026lt;script src=\u0026#34;/public/highcharter/highchart.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; 19 \u0026lt;!-- plotly --\u0026gt; 20 \u0026lt;script src=\u0026#34;/public/plotly/plotly.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; 21 ... more plotly dependencies 22 \u0026lt;!-- crosstalk --\u0026gt; 23 \u0026lt;script src=\u0026#34;/public/crosstalk/js/crosstalk.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; 24 ... more crosstalk depencencies 25 \u0026lt;!-- bootstrap, etc --\u0026gt; 26 \u0026lt;script src=\u0026#34;/public/shared/bootstrap/js/bootstrap.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; 27 ... more bootstrap, etc depencencies 28 \u0026lt;!-- vuetify --\u0026gt; 29 \u0026lt;link href=\u0026#39;https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Material+Icons\u0026#39; rel=\u0026#34;stylesheet\u0026#34; type=\u0026#34;text/css\u0026#34;\u0026gt; 30 \u0026lt;/head\u0026gt; 31 \u0026lt;body\u0026gt; 32 \u0026lt;div id=\u0026#34;app\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; 33 \u0026lt;script src=\u0026#34;./dist/build.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; 34 \u0026lt;/body\u0026gt; 35\u0026lt;/html\u0026gt; Components The widgets are constructed as single file components. The button (update table) listens on a button click event and it\u0026rsquo;ll trigger update() defined in the script of the component. Note v-html directive. This directive allows to render raw HTML.\nRecall that the body of a htmlwidgets object is\ndiv - widget container and element script - application/json for widget data script - application/htmlwidget-sizing for widget size And /widget resource of the API renders all of those if html is specified as type.\nAll the HTML elements is updated in this Vue application (type = html) while only the application/json script is appended/updated in the Javascript application (type = src).\n1\u0026lt;template\u0026gt; 2 \u0026lt;v-layout\u0026gt; 3 \u0026lt;v-flex xs12 sm6 offset-sm2\u0026gt; 4 \u0026lt;v-card\u0026gt; 5 \u0026lt;v-card-media height=\u0026#34;350px\u0026#34;\u0026gt; 6 \u0026lt;v-container\u0026gt; 7 \u0026lt;v-layout row wrap justify-center\u0026gt; 8 \u0026lt;div v-if=\u0026#34;!isLoading\u0026#34; v-html=\u0026#34;dat\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; 9 \u0026lt;/v-layout\u0026gt; 10 \u0026lt;/v-container\u0026gt; 11 \u0026lt;/v-card-media\u0026gt; 12 \u0026lt;v-card-actions\u0026gt; 13 \u0026lt;v-container\u0026gt; 14 \u0026lt;v-layout row wrap justify-center\u0026gt; 15 \u0026lt;v-btn 16 @click=\u0026#34;update\u0026#34; 17 color=\u0026#34;primary\u0026#34; 18 \u0026gt;update table\u0026lt;/v-btn\u0026gt; 19 \u0026lt;div v-if=\u0026#34;isLoading\u0026#34;\u0026gt; 20 \u0026lt;v-progress-circular indeterminate color=\u0026#34;info\u0026#34;\u0026gt;\u0026lt;/v-progress-circular\u0026gt; 21 \u0026lt;/div\u0026gt; 22 \u0026lt;/v-layout\u0026gt; 23 \u0026lt;/v-container\u0026gt; 24 \u0026lt;/v-card-actions\u0026gt; 25 \u0026lt;/v-card\u0026gt; 26 \u0026lt;/v-flex\u0026gt; 27 \u0026lt;/v-layout\u0026gt; 28\u0026lt;/template\u0026gt; Here the HTTP request is made with axios. The element is set to null at the beginning and updated by the output of the API followed by executing window.HTMLWidgets.staticRender().\n1\u0026lt;script\u0026gt; 2import axios from \u0026#39;axios\u0026#39; 3 4export default { 5 data: () =\u0026gt; ({ 6 dat: null, 7 isLoading: false 8 }), 9 methods: { 10 update() { 11 this.dat = null 12 this.isLoading = true 13 14 let params = { element_id: \u0026#39;dt_out\u0026#39;, type: \u0026#39;html\u0026#39; } 15 axios.post(\u0026#39;http://[hostname]:8000/widget\u0026#39;, params) 16 .then(res =\u0026gt; { 17 this.dat = res.data.replace(\u0026#39;width:960px;\u0026#39;, \u0026#39;width:100%\u0026#39;) 18 console.log(this.dat) 19 setTimeout(function() { 20 window.HTMLWidgets.staticRender() 21 }, 500) 22 this.isLoading = false 23 }) 24 .catch(err =\u0026gt; { 25 this.isLoading = false 26 console.log(err) 27 }) 28 } 29 } 30} 31\u0026lt;/script\u0026gt; Layout The application layout is setup in ./src/App.vue where the individual components are imported into content.\n1\u0026lt;template\u0026gt; 2 \u0026lt;v-app\u0026gt; 3 \u0026lt;v-toolbar dense color=\u0026#34;light-blue\u0026#34; dark fixed app\u0026gt; 4 \u0026lt;v-toolbar-title\u0026gt; 5 Vue - htmlwidgets 6 \u0026lt;/v-toolbar-title\u0026gt; 7 \u0026lt;/v-toolbar\u0026gt; 8 \u0026lt;v-content\u0026gt; 9 \u0026lt;app-data-table\u0026gt;\u0026lt;/app-data-table\u0026gt; 10 \u0026lt;app-high-chart\u0026gt;\u0026lt;/app-high-chart\u0026gt; 11 \u0026lt;app-plotly\u0026gt;\u0026lt;/app-plotly\u0026gt; 12 \u0026lt;/v-content\u0026gt; 13 \u0026lt;/v-app\u0026gt; 14\u0026lt;/template\u0026gt; 15 16\u0026lt;script\u0026gt; 17import DataTable from \u0026#39;./components/DataTable.vue\u0026#39; 18import HighChart from \u0026#39;./components/HighChart.vue\u0026#39; 19import Plotly from \u0026#39;./components/Plotly.vue\u0026#39; 20 21export default { 22 components: { 23 appDataTable: DataTable, 24 appHighChart: HighChart, 25 appPlotly: Plotly 26 } 27} 28\u0026lt;/script\u0026gt; The screen shot of the app is shown below.\nNative Libraries instead of Htmlwidgets If an app doesn\u0026rsquo;t rely on htmlwidgets, it only requires data to create charts and tables. The API has /hdata resource to return the iris data. Here the scenario is the iris data will be pulled at the beginning and 10 records are selected randomly when a user clicks a button, resulting in updating components. Note one of the key benefits of this structure is that components can communicate with each other - see what crosstalk is aimed for.\nindex.html The entry point of the app becomes quite simple without htmlwidgets dependency.\n1\u0026lt;!DOCTYPE html\u0026gt; 2\u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; 3 \u0026lt;head\u0026gt; 4 \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt; 5 \u0026lt;title\u0026gt;Vue - native\u0026lt;/title\u0026gt; 6 \u0026lt;meta http-equiv=\u0026#34;X-UA-Compatible\u0026#34; content=\u0026#34;IE=edge\u0026#34;\u0026gt; 7 \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width, initial-scale=1.0\u0026#34;\u0026gt; 8 \u0026lt;link href=\u0026#39;https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Material+Icons\u0026#39; rel=\u0026#34;stylesheet\u0026#34; type=\u0026#34;text/css\u0026#34;\u0026gt; 9 \u0026lt;/head\u0026gt; 10 \u0026lt;body\u0026gt; 11 \u0026lt;div id=\u0026#34;app\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; 12 \u0026lt;script src=\u0026#34;./dist/build.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; 13 \u0026lt;/body\u0026gt; 14\u0026lt;/html\u0026gt; State Management Normally a Vue app has multiple components so that it is important to keep changes in sync across components. Although Vue supports custom events and event bus, I find state management with Vuex is more straightforward (and better for larger apps).\nIn the store (./src/store.js), there are 3 state properties.\nrawData - iris data vizData - randomly selected records isLoading - indicator if data request is completed These state properties can be accessed by getters and modified by mutations. While mutations are synchronous, actions can be asynchronous. Therefore dispatching actions is better for something that requires some time. In this example, the HTTP request that returns the iris data is performed by dispatching getRawData() and, on success, the following mutations are commtted.\ngetRawData updateVisData toggleIsLoading 1import Vue from \u0026#39;vue\u0026#39; 2import Vuex from \u0026#39;vuex\u0026#39; 3 4import axios from \u0026#39;axios\u0026#39; 5axios.defaults.baseURL = \u0026#39;http://[hostname]:8000\u0026#39; 6 7Vue.use(Vuex) 8 9export default new Vuex.Store({ 10 state: { 11 rawData: [], 12 visData: [], 13 isLoading: false 14 }, 15 getters: { 16 rawData (state) { 17 return state.rawData 18 }, 19 visData (state) { 20 return state.visData 21 }, 22 isLoading (state) { 23 return state.isLoading 24 } 25 }, 26 mutations: { 27 getRawData (state, payload) { 28 state.rawData = payload 29 }, 30 updateVisData (state) { 31 state.visData = state.rawData.sort(() =\u0026gt; .5 - Math.random()).slice(0, 10) 32 }, 33 toggleIsLoading (state) { 34 state.isLoading = !state.isLoading 35 } 36 }, 37 actions: { 38 getRawData ({ commit }) { 39 commit(\u0026#39;toggleIsLoading\u0026#39;) 40 41 axios.post(\u0026#39;/hdata\u0026#39;) 42 .then(res =\u0026gt; { 43 commit(\u0026#39;getRawData\u0026#39;, res.data) 44 commit(\u0026#39;updateVisData\u0026#39;) 45 commit(\u0026#39;toggleIsLoading\u0026#39;) 46 }) 47 .catch(err =\u0026gt; { 48 console.log(\u0026#39;error\u0026#39;) 49 commit(\u0026#39;toggleIsLoading\u0026#39;) 50 console.log(err) 51 }) 52 } 53 } 54}) Components Here the source looks quite different from the DT object because it\u0026rsquo;s created by the built-in data table component of Vuetify - the other 2 compoents look rather similar. The headers of the table is predefined as data property while the records (visData) are obtained from the store - it keeps in sync as a computed property.\n1\u0026lt;template\u0026gt; 2 \u0026lt;v-data-table 3 :headers=\u0026#34;headers\u0026#34; 4 :items=\u0026#34;visData\u0026#34; 5 class=\u0026#34;elevation-1\u0026#34; 6 \u0026gt; 7 \u0026lt;template slot=\u0026#34;items\u0026#34; slot-scope=\u0026#34;props\u0026#34;\u0026gt; 8 \u0026lt;td class=\u0026#34;text-xs-right\u0026#34;\u0026gt;{{ props.item.SepalLength }}\u0026lt;/td\u0026gt; 9 \u0026lt;td class=\u0026#34;text-xs-right\u0026#34;\u0026gt;{{ props.item.SepalWidth }}\u0026lt;/td\u0026gt; 10 \u0026lt;td class=\u0026#34;text-xs-right\u0026#34;\u0026gt;{{ props.item.PetalLength }}\u0026lt;/td\u0026gt; 11 \u0026lt;td class=\u0026#34;text-xs-right\u0026#34;\u0026gt;{{ props.item.PetalWidth }}\u0026lt;/td\u0026gt; 12 \u0026lt;td class=\u0026#34;text-xs-right\u0026#34;\u0026gt;{{ props.item.Species }}\u0026lt;/td\u0026gt; 13 \u0026lt;/template\u0026gt; 14 \u0026lt;template slot=\u0026#34;pageText\u0026#34; slot-scope=\u0026#34;props\u0026#34;\u0026gt; 15 Lignes {{ props.pageStart }} - {{ props.pageStop }} of {{ props.itemsLength }} 16 \u0026lt;/template\u0026gt; 17 \u0026lt;/v-data-table\u0026gt; 18\u0026lt;/template\u0026gt; 19 20\u0026lt;script\u0026gt; 21export default { 22 data () { 23 return { 24 headers: [ 25 { text: \u0026#39;Sepal Length\u0026#39;, value: \u0026#39;SepalLength\u0026#39;}, 26 { text: \u0026#39;Sepal Width\u0026#39;, value: \u0026#39;SepalWidth\u0026#39;}, 27 { text: \u0026#39;Petal Length\u0026#39;, value: \u0026#39;PetalLength\u0026#39;}, 28 { text: \u0026#39;Petal Width\u0026#39;, value: \u0026#39;PetalWidth\u0026#39;}, 29 { text: \u0026#39;Species\u0026#39;, value: \u0026#39;Species\u0026#39;} 30 ] 31 } 32 }, 33 computed: { 34 visData() { 35 return this.$store.getters[\u0026#39;visData\u0026#39;] 36 } 37 } 38} 39\u0026lt;/script\u0026gt; Layout Instead of requesting individual htmlwidgets objects, charts/table are created by individual components. Also the components are updated by clicking the button. The conditional directives (v-if and v-else) controls which to render depending on the value of isLoading.\n1\u0026lt;template\u0026gt; 2 \u0026lt;v-app\u0026gt; 3 \u0026lt;v-toolbar dense color=\u0026#34;light-blue\u0026#34; dark fixed app\u0026gt; 4 \u0026lt;v-toolbar-title\u0026gt; 5 Vue - native 6 \u0026lt;/v-toolbar-title\u0026gt; 7 \u0026lt;/v-toolbar\u0026gt; 8 \u0026lt;v-content\u0026gt; 9 \u0026lt;div v-if=\u0026#34;isLoading\u0026#34; class=\u0026#34;centered\u0026#34;\u0026gt; 10 \u0026lt;v-progress-circular 11 indeterminate color=\u0026#34;info\u0026#34; 12 :size=\u0026#34;100\u0026#34; 13 :width=\u0026#34;10\u0026#34; 14 \u0026gt;\u0026lt;/v-progress-circular\u0026gt; 15 \u0026lt;/div\u0026gt; 16 \u0026lt;div v-else\u0026gt; 17 \u0026lt;v-btn @click=\u0026#34;update\u0026#34;\u0026gt;update data\u0026lt;/v-btn\u0026gt; 18 \u0026lt;v-container fluid\u0026gt; 19 \u0026lt;v-layout row wrap\u0026gt; 20 \u0026lt;v-flex xs12 sm12 md6\u0026gt; 21 \u0026lt;div style=\u0026#34;display: inline-block;\u0026#34;\u0026gt; 22 \u0026lt;app-data-table\u0026gt;\u0026lt;/app-data-table\u0026gt; 23 \u0026lt;/div\u0026gt; 24 \u0026lt;/v-flex\u0026gt; 25 \u0026lt;v-flex xs12 sm12 md6\u0026gt; 26 \u0026lt;div style=\u0026#34;display: inline-block;\u0026#34;\u0026gt; 27 \u0026lt;app-highchart\u0026gt;\u0026lt;/app-highchart\u0026gt; 28 \u0026lt;/div\u0026gt; 29 \u0026lt;/v-flex\u0026gt; 30 \u0026lt;v-flex xs12 sm12 md6\u0026gt; 31 \u0026lt;div style=\u0026#34;display: inline-block;\u0026#34;\u0026gt; 32 \u0026lt;app-plotly\u0026gt;\u0026lt;/app-plotly\u0026gt; 33 \u0026lt;/div\u0026gt; 34 \u0026lt;/v-flex\u0026gt; 35 \u0026lt;/v-layout\u0026gt; 36 \u0026lt;/v-container\u0026gt; 37 \u0026lt;/div\u0026gt; 38 \u0026lt;/v-content\u0026gt; 39 \u0026lt;/v-app\u0026gt; 40\u0026lt;/template\u0026gt; Upon creation of the component (created()), getRawData() is dispatched. While the request is being processed, the computed property of isLoading remains as true, resulting in rendering the loader. Once succeeded, the compoents are updated with the initial random records. If a user click the button, it\u0026rsquo;ll commit updateVisData(), resulting in compoent updates.\n1\u0026lt;script\u0026gt; 2import DataTable from \u0026#39;./components/DataTable.vue\u0026#39; 3import Highchart from \u0026#39;./components/HighChart.vue\u0026#39; 4import Plotly from \u0026#39;./components/Plotly.vue\u0026#39; 5 6export default { 7 components: { 8 appDataTable: DataTable, 9 appHighchart: Highchart, 10 appPlotly: Plotly 11 }, 12 computed: { 13 visData() { 14 return this.$store.getters[\u0026#39;visData\u0026#39;] 15 }, 16 isLoading() { 17 return this.$store.getters[\u0026#39;isLoading\u0026#39;] 18 } 19 }, 20 methods: { 21 update() { 22 this.$store.commit(\u0026#39;updateVisData\u0026#39;) 23 } 24 }, 25 created () { 26 this.$store.dispatch(\u0026#39;getRawData\u0026#39;) 27 } 28} 29\u0026lt;/script\u0026gt; 30 31\u0026lt;style scoped\u0026gt; 32.centered { 33 position: fixed; /* or absolute */ 34 top: 50%; 35 left: 50%; 36} 37\u0026lt;/style\u0026gt; The screen shot of the app is shown below.\n","date":"May 26, 2018","img":"/blog/2018-05-26-shiny-to-vue.js/featured.png","lang":"en","langName":"English","largeImg":"/blog/2018-05-26-shiny-to-vue.js/featured_hu0f69ef413c1c0e126895b33362437da9_247205_500x0_resize_box_3.png","permalink":"/blog/2018-05-26-shiny-to-vue.js/","series":[],"smallImg":"/blog/2018-05-26-shiny-to-vue.js/featured_hu0f69ef413c1c0e126895b33362437da9_247205_180x0_resize_box_3.png","tags":[{"title":"R","url":"/tags/r/"},{"title":"Shiny","url":"/tags/shiny/"},{"title":"JavaScript","url":"/tags/javascript/"},{"title":"Vue.js","url":"/tags/vue.js/"}],"timestamp":1527292800,"title":"Shiny to Vue.js"},{"categories":[{"title":"Web Development","url":"/categories/web-development/"}],"content":"A Shiny app is served by one (single-threaded blocking) process by Open Source Shiny Server. This causes a scalability issue because all requests are handled one by one in a queue. Recently the creator of Shiny introduced the promises package, which brings asynchronous programming capabilities to R. This is a remarkable step forward to web development in R.\nIn this post, it\u0026rsquo;ll be demonstrated how to implement the async feature of Shiny. Then its limitation will be discussed with an alternative app, which is built by JavaScript for the frontend and RServe for the backend.\nAsync Shiny and Its Limitation Brief Intro to Promises A basic idea of how the promises package works is that a (long running) process is passed to a forked process while it immediately returns a promise object. Then the result can be obtained once it\u0026rsquo;s finished (or failed) by handlers eg) onFulfilled and onRejected. The package also provides pipe operators (eg %...\u0026gt;%) for ease of use. Typically a promise object can be created with the future package. See this page for further details.\nShiny App A simple Shiny app is created for demonstration that renders 3 htmlwidgets: DT, highcharter and plotly. For this, the following async-compatible packages are necessary.\nShiny v1.1+ DT from rstudio/DT@async htmlwidgets from ramnathv/htmlwidgets@async plotly from jcheng5/plotly@joe/feature/async highcharter - supported by htmlwidgets At startup, it is necessary to allocate the number of workers (forked processes). Note it doesn\u0026rsquo;t necessarily be the same to the number of cores. Rather it may be better to set it higher if the machine has enough resource. This is because, if there are n workers and n+m requests, the m requests tend to be queued.\n1library(magrittr) 2library(DT) 3library(highcharter) 4library(plotly) 5 6library(shiny) 7 8library(promises) 9library(future) 10plan(multiprocess, workers = 100) Then a simple NavBar page is created as the UI. A widget will be rendered by clicking a button.\n1tab \u0026lt;- tabPanel( 2 title = \u0026#39;Demo\u0026#39;, 3 fluidPage( 4 fluidRow( 5 div( 6 style=\u0026#39;height: 400px;\u0026#39;, 7 column(2, actionButton(\u0026#39;dt\u0026#39;, \u0026#39;update data table\u0026#39;)), 8 column(10, dataTableOutput(\u0026#39;dt_out\u0026#39;)) 9 ) 10 ), 11 fluidRow( 12 div( 13 style=\u0026#39;height: 400px;\u0026#39;, 14 column(2, actionButton(\u0026#39;highchart\u0026#39;, \u0026#39;update highchart\u0026#39;)), 15 column(10, highchartOutput(\u0026#39;highchart_out\u0026#39;)) 16 ) 17 ), 18 fluidRow( 19 div( 20 style=\u0026#39;height: 400px;\u0026#39;, 21 column(2, actionButton(\u0026#39;plotly\u0026#39;, \u0026#39;update plotly\u0026#39;)), 22 column(10, plotlyOutput(\u0026#39;plotly_out\u0026#39;)) 23 ) 24 25 ) 26 ) 27) 28 29ui \u0026lt;- navbarPage(\u0026#34;Async Shiny\u0026#34;, tab) A future object is created by get_iris(), which returns 10 records randomly from the iris data after 2 seconds. evantReactive()s generate htmlwidget objects and they are passed to the relevant render functions.\n1server \u0026lt;- function(input, output, session) { 2 3 get_iris \u0026lt;- function() { 4 future({ Sys.sleep(2); iris[sample(1:nrow(iris), 10),] }) 5 } 6 7 dt_df \u0026lt;- eventReactive(input$dt, { 8 get_iris() %...\u0026gt;% 9 datatable(options = list( 10 pageLength = 5, 11 lengthMenu = c(5, 10) 12 )) 13 }) 14 15 highchart_df \u0026lt;- eventReactive(input$highchart, { 16 get_iris() 17 }) 18 19 plotly_df \u0026lt;- eventReactive(input$plotly, { 20 get_iris() 21 }) 22 23 output$dt_out \u0026lt;- renderDataTable(dt_df()) 24 25 output$highchart_out \u0026lt;- renderHighchart({ 26 highchart_df() %...\u0026gt;% 27 hchart(\u0026#39;scatter\u0026#39;, hcaes(x = \u0026#39;Sepal.Length\u0026#39;, y = \u0026#39;Sepal.Width\u0026#39;, group = \u0026#39;Species\u0026#39;)) %...\u0026gt;% 28 hc_title(text = \u0026#39;Iris Scatter\u0026#39;) 29 }) 30 31 output$plotly_out \u0026lt;- renderPlotly({ 32 plotly_df() %...\u0026gt;% 33 plot_ly(x = ~Sepal.Length, y = ~Sepal.Width, z = ~Petal.Length, color = ~Species) 34 }) 35} The app code can also be found in this GitHub repository.\nFor deployment, a Docker image is created that includes the async-compatible packages, Open Source Shiny Server and RServe. It is available as rockerextra/shiny-async-dev:3.4 and its Dockerfile can be found in this repo. The app can be deployed with Docker Compose as can be seen here.\nA screen shot of the Shiny app is seen below. It is possible to check the async feature by opening the app in 2 different browsers and hit buttons multiple times across.\nLimitation You may notice the htmlwidgets are rendered without delay across browsers but it\u0026rsquo;s not the case in the same browser. This is due to the way how Shiny\u0026rsquo;s flush cycle is implemented. Simply put, a user (or session) is not affected by other users (or sessions) for their async requests. However the async feature of Shiny is of little help for multiple async requests by a single user because all requests are processed one by one as its sync version.\nThis limitation can have a significant impact on developing a web application. In general, almost all events/actions are handled through the server in a Shiny app. However the async feature of the server is not the full extent that a typical JavaScript app can bring.\nAlternative Implementation In order to compare the async Shiny app to a typical web app, an app is created with JavaScript for the frontend and RServe for the backend. In the UI, JQuery will be used for AJAX requests by clicking buttons. Then the same htmlwidget elements will be rendered to the app. With this setup, it\u0026rsquo;s possible to make multiple requests concurrently in a session and they are all handled asynchronously by a JavaScript-backed app.\nRServe Backend So as to render htmlwidgets to UI, it is necessary to have a backend API. As discussed in API Development with R series (Part I, Part II), RServe can be a performant option for building an API.\nI don\u0026rsquo;t plan to use native JavaScript libraries for creating individual widgets. Rather I\u0026rsquo;m going to render widgets that are created by R. Therefore it is necessary to understand the structure of a widget. saveWidget() of the htmlwidgets package helps save a widget into a HTML file and it executes save_html() of the htmltools package.\nKey parts of a widget is\nhead - dependent JavaScript and CSS body div - widget container and element script - application/json for widget data script - application/htmlwidget-sizing for widget size For example,\n1\u0026lt;!DOCTYPE html\u0026gt; 2\u0026lt;html\u0026gt; 3 \u0026lt;head\u0026gt; 4 \u0026lt;meta charset=\u0026#39;utf-8\u0026#39;/\u0026gt; 5 \u0026lt;script src=\u0026#34;src-to/htmlwidgets.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; 6 \u0026lt;script src=\u0026#34;src-to/jquery.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; 7 \u0026lt;script src=\u0026#34;src-to/datatables.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; 8 ... more DT dependencies 9 \u0026lt;/head\u0026gt; 10 \u0026lt;body\u0026gt; 11 \u0026lt;div id=\u0026#34;htmlwidget_container\u0026#34;\u0026gt; 12 \u0026lt;div id=\u0026#34;htmlwidget\u0026#34; 13 style=\u0026#34;width:960px;height:500px;\u0026#34; 14 class=\u0026#34;datatables html-widget\u0026#34;\u0026gt; 15 \u0026lt;/div\u0026gt; 16 \u0026lt;/div\u0026gt; 17 \u0026lt;script type=\u0026#34;application/json\u0026#34; 18 data-for=\u0026#34;htmlwidget\u0026#34;\u0026gt;JSON DATA\u0026lt;/script\u0026gt; 19 \u0026lt;script type=\u0026#34;application/htmlwidget-sizing\u0026#34; 20 data-for=\u0026#34;htmlwidget\u0026#34;\u0026gt;SIGING INFO\u0026lt;/script\u0026gt; 21 \u0026lt;/body\u0026gt; 22\u0026lt;/html\u0026gt; write_widget() is a slight modification of saveWidget() and save_html(). Given the following arguments, it returns the necessary widget string (HTML or JSON) and it can be passed to the UI. In this post, src type will be used exclusively.\nw htmlwidget object see widget() illustrated below element_id DOM element id (eg dt_out) type json - JSON DATA only src - application/json script html - body elements all - entire html page 1library(htmlwidgets) 2 3write_widget \u0026lt;- function(w, element_id, type = NULL, 4 cdn = NULL, output_path = NULL) { 5 w$elementId \u0026lt;- sprintf(\u0026#39;htmlwidget_%s\u0026#39;, element_id) 6 toHTML \u0026lt;- utils::getFromNamespace(x = \u0026#39;toHTML\u0026#39;, ns = \u0026#39;htmlwidgets\u0026#39;) 7 html \u0026lt;- toHTML(w, standalone = TRUE, knitrOptions = list()) 8 9 type \u0026lt;- match.arg(type, c(\u0026#39;src\u0026#39;, \u0026#39;json\u0026#39;, \u0026#39;html\u0026#39;, \u0026#39;all\u0026#39;)) 10 if (type == \u0026#39;src\u0026#39;) { 11 out \u0026lt;- html[[2]] 12 } else if (type == \u0026#39;json\u0026#39;) { 13 bptn \u0026lt;- paste0(\u0026#39;\u0026lt;script type=\u0026#34;application/json\u0026#34; data-for=\u0026#34;htmlwidget_\u0026#39;, 14 element_id, \u0026#39;\u0026#34;\u0026gt;\u0026#39;) 15 eptn \u0026lt;- \u0026#39;\u0026lt;/script\u0026gt;\u0026#39; 16 out \u0026lt;- sub(eptn, \u0026#39;\u0026#39;, sub(bptn, \u0026#39;\u0026#39;, html[[2]])) 17 } else { 18 html_tags \u0026lt;- htmltools::renderTags(html) 19 html_tags$html \u0026lt;- sub(\u0026#39;htmlwidget_container\u0026#39;, 20 sprintf(\u0026#39;htmlwidget_container_%s\u0026#39;, element_id) , 21 html_tags$html) 22 if (type == \u0026#39;html\u0026#39;) { 23 out \u0026lt;- html_tags$html 24 } else { # all 25 libdir \u0026lt;- gsub(\u0026#39;\\\\\\\\\u0026#39;, \u0026#39;/\u0026#39;, tempdir()) 26 libdir \u0026lt;- gsub(\u0026#39;[[:space:]]|[A-Z]:\u0026#39;, \u0026#39;\u0026#39;, libdir) 27 28 deps \u0026lt;- lapply(html_tags$dependencies, update_dep_path, libdir = libdir) 29 deps \u0026lt;- htmltools::renderDependencies(dependencies = deps, 30 srcType = c(\u0026#39;hred\u0026#39;, \u0026#39;file\u0026#39;)) 31 deps \u0026lt;- ifelse(!is.null(cdn), gsub(libdir, cdn, deps), deps) 32 33 out \u0026lt;- c( 34 \u0026#34;\u0026lt;!DOCTYPE html\u0026gt;\u0026#34;, 35 \u0026#34;\u0026lt;html\u0026gt;\u0026#34;, 36 \u0026#34;\u0026lt;head\u0026gt;\u0026#34;, 37 \u0026#34;\u0026lt;meta charset=\u0026#39;utf-8\u0026#39;/\u0026gt;\u0026#34;, 38 deps, 39 html_tags$head, 40 \u0026#34;\u0026lt;/head\u0026gt;\u0026#34;, 41 \u0026#34;\u0026lt;body\u0026gt;\u0026#34;, 42 html_tags$html, 43 \u0026#34;\u0026lt;/body\u0026gt;\u0026#34;, 44 \u0026#34;\u0026lt;/html\u0026gt;\u0026#34;) 45 } 46 } 47 48 if (!is.null(output_path)) { 49 writeLines(out, output_path, useBytes = TRUE) 50 } else { 51 paste(out, collapse = \u0026#39;\u0026#39;) 52 } 53} 54 55update_dep_path \u0026lt;- function(dep, libdir = \u0026#39;lib\u0026#39;) { 56 dir \u0026lt;- dep$src$file 57 if (!is.null(dep$package)) 58 dir \u0026lt;- system.file(dir, package = dep$package) 59 60 if (length(libdir) != 1 || libdir %in% c(\u0026#34;\u0026#34;, \u0026#34;/\u0026#34;)) 61 stop(\u0026#34;libdir must be of length 1 and cannot be \u0026#39;\u0026#39; or \u0026#39;/\u0026#39;\u0026#34;) 62 63 target \u0026lt;- if (getOption(\u0026#39;htmltools.dir.version\u0026#39;, TRUE)) { 64 paste(dep$name, dep$version, sep = \u0026#39;-\u0026#39;) 65 } else { 66 dep$name 67 } 68 dep$src$file \u0026lt;- file.path(libdir, target) 69 dep 70} Essentially the API has 2 endpoints.\nwidget returns output from write_widget() given element_id and type hdata returns iris data as JSON 1widget \u0026lt;- function(element_id, type, get_all = FALSE, cdn = \u0026#39;public\u0026#39;, ...) { 2 dat \u0026lt;- get_iris(get_all) 3 if (grepl(\u0026#39;dt\u0026#39;, element_id)) { 4 w \u0026lt;- dat %\u0026gt;% 5 datatable(options = list( 6 pageLength = 5, 7 lengthMenu = c(5, 10) 8 )) 9 } else if (grepl(\u0026#39;highchart\u0026#39;, element_id)) { 10 w \u0026lt;- dat %\u0026gt;% 11 hchart(\u0026#39;scatter\u0026#39;, hcaes(x = \u0026#39;Sepal.Length\u0026#39;, y = \u0026#39;Sepal.Width\u0026#39;, group = \u0026#39;Species\u0026#39;)) %\u0026gt;% 12 hc_title(text = \u0026#39;Iris Scatter\u0026#39;) 13 } else if (grepl(\u0026#39;plotly\u0026#39;, element_id)) { 14 w \u0026lt;- dat %\u0026gt;% 15 plot_ly(x = ~Sepal.Length, y = ~Sepal.Width, z = ~Petal.Length, color = ~Species) 16 } else { 17 stop(\u0026#39;Unexpected element\u0026#39;) 18 } 19 write_widget(w, element_id, type, cdn) 20} 21 22hdata \u0026lt;- function() { 23 dat \u0026lt;- get_iris(TRUE) 24 names(dat) \u0026lt;- sub(\u0026#39;\\\\.\u0026#39;, \u0026#39;\u0026#39;, names(dat)) 25 dat %\u0026gt;% toJSON() 26 27} 28 29get_iris \u0026lt;- function(get_all = FALSE) { 30 Sys.sleep(2) 31 if (!get_all) { 32 iris[sample(1:nrow(iris), 10),] 33 } else { 34 iris 35 } 36} process_request() remains largely the same but needs some modification so that it can be used as a backend of a web app.\nCross-Origin Resource Sharing (CORS) requests from browser will fail without necessary headers and handling OPTIONS method Response content type depending on type, response content type will be either application/json or text/html See API Development with R series (Part I, Part II) for further details of process_request() and how RServe\u0026rsquo;s built-in HTTP server works.\n1process_request \u0026lt;- function(url, query, body, headers) { 2 #### building request object 3 request \u0026lt;- list(uri = url, method = \u0026#39;POST\u0026#39;, query = query, body = body) 4 5 ## parse headers 6 request$headers \u0026lt;- parse_headers(headers) 7 if (\u0026#34;request-method\u0026#34; %in% names(request$headers)) 8 request$method \u0026lt;- c(request$headers[\u0026#34;request-method\u0026#34;]) 9 10 set_headers \u0026lt;- function(...) { 11 paste(list(...), collapse = \u0026#39;\\r\\n\u0026#39;) 12 } 13 14 h1 \u0026lt;- \u0026#39;Access-Control-Allow-Headers: Content-Type\u0026#39; 15 h2 \u0026lt;- \u0026#39;Access-Control-Allow-Methods: POST,GET,OPTIONS\u0026#39; 16 h3 \u0026lt;- \u0026#39;Access-Control-Allow-Origin: *\u0026#39; 17 18 cors_headers \u0026lt;- set_headers(h1, h2, h3) 19 20 if (request$method == \u0026#39;OPTIONS\u0026#39;) { 21 return (list(\u0026#39;\u0026#39;, \u0026#39;text/plain\u0026#39;, cors_headers)) 22 } 23 24 request$pars \u0026lt;- list() 25 if (request$method == \u0026#39;POST\u0026#39;) { 26 if (!is.null(body)) { 27 if (is.raw(body)) 28 body \u0026lt;- rawToChar(body) 29 if (any(grepl(\u0026#39;application/json\u0026#39;, request$headers))) 30 body \u0026lt;- jsonlite::fromJSON(body) 31 request$pars \u0026lt;- as.list(body) 32 } 33 } else { 34 if (!is.null(query)) { 35 request$pars \u0026lt;- as.list(query) 36 } 37 } 38 39 if (\u0026#39;type\u0026#39; %in% names(request$pars)) { 40 if (request$pars$type == \u0026#39;json\u0026#39;) { 41 content_type \u0026lt;- \u0026#39;application/json; charset=utf-8\u0026#39; 42 } else { 43 content_type \u0026lt;- \u0026#39;text/html; charset=utf-8\u0026#39; 44 } 45 } else { 46 content_type \u0026lt;- \u0026#39;text/plain; charset=utf-8\u0026#39; 47 } 48 49 message(sprintf(\u0026#39;Header:\\n%s\u0026#39;, cors_headers)) 50 message(sprintf(\u0026#39;Content Type: %s\u0026#39;, content_type)) 51 message(\u0026#39;Params:\u0026#39;) 52 print(do.call(c, request$pars)) 53 54 #### building output object 55 matched_fun \u0026lt;- gsub(\u0026#39;^/\u0026#39;, \u0026#39;\u0026#39;, request$uri) 56 57 payload \u0026lt;- tryCatch({ 58 do.call(matched_fun, request$pars) 59 }, error = function(err) { 60 \u0026#39;Internal Server Error\u0026#39; 61 }) 62 63 return (list(payload, content_type, cors_headers)) 64} The source can be found in here and the API deployment is included in the docker compose.\nJavaScript Frontend The app will be kept in index.html and will be served by a simple python web server. Basically the same Bootstrap page is created.\nIt is important to keep all the widgets\u0026rsquo; dependent JavaScript and CSS in head. We have 3 htmlwidgets and they are wrapped by the htmlwidgets package. Therefore it depends on\nhtmlwidgets DataTables for DT Highcharts for highcharter Plotly for plotly CrossTalk for DT and plotly Bootstrap for layout JQuery for all Note that a htmlwidget package tends to rely on a specific JQuery library. For example, the DT package uses 1.12.4 while the highcharter uses 1.11.1. Therefore there is a chance to encounter version incompatibility if multiple htmlwidget packages rendered at the same time. The HTML source of Shiny can be helpful because it holds a JQuery lib that can be shared across all widget packages.\n1\u0026lt;head\u0026gt; 2 \u0026lt;meta charset=\u0026#39;utf-8\u0026#39;/\u0026gt; 3 \u0026lt;!-- necessary to control htmlwidgets --\u0026gt; 4 \u0026lt;script src=\u0026#34;/public/htmlwidgets/htmlwidgets.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; 5 \u0026lt;!-- need a shared JQuery lib --\u0026gt; 6 \u0026lt;script src=\u0026#34;/public/shared/jquery.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; 7 \u0026lt;!-- DT --\u0026gt; 8 \u0026lt;script src=\u0026#34;/public/datatables/datatables.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; 9 ... more DT dependencies 10 \u0026lt;!-- highchater --\u0026gt; 11 \u0026lt;script src=\u0026#34;/public/highcharter/lib/proj4js/proj4.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; 12 ... more highcharts dependencies 13 \u0026lt;script src=\u0026#34;/public/highcharter/highchart.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; 14 \u0026lt;!-- plotly --\u0026gt; 15 \u0026lt;script src=\u0026#34;/public/plotly/plotly.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; 16 ... more plotly dependencies 17 \u0026lt;!-- crosstalk --\u0026gt; 18 \u0026lt;script src=\u0026#34;/public/crosstalk/js/crosstalk.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; 19 ... more crosstalk depencencies 20 \u0026lt;!-- bootstrap, etc --\u0026gt; 21 \u0026lt;script src=\u0026#34;/public/shared/bootstrap/js/bootstrap.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; 22 ... more bootstrap, etc depencencies 23\u0026lt;/head\u0026gt; The widget containers/elements as well as sizing script are added in body. The naming rules for the container and element are\ncontainer - htmlwidget_container_[element_id] element - htmlwidget_[element_id] In this structure, widgets can be updated if their data (application/json script) is added/updated to the page.\n1\u0026lt;body\u0026gt; 2 ... NAV 3 \u0026lt;div class=\u0026#34;container-fluid\u0026#34;\u0026gt; 4 ... TAB 5 \u0026lt;div class=\u0026#34;container-fluid\u0026#34;\u0026gt; 6 \u0026lt;div class=\u0026#34;row\u0026#34;\u0026gt; 7 \u0026lt;div style=\u0026#34;height: 400px;\u0026#34;\u0026gt; 8 \u0026lt;div class=\u0026#34;col-sm-2\u0026#34;\u0026gt; 9 \u0026lt;button id=\u0026#34;dt\u0026#34; type=\u0026#34;button\u0026#34; 10 class=\u0026#34;btn btn-default action-button\u0026#34;\u0026gt; 11 update data table 12 \u0026lt;/button\u0026gt; 13 \u0026lt;/div\u0026gt; 14 \u0026lt;div class=\u0026#34;col-sm-10\u0026#34;\u0026gt; 15 \u0026lt;div id=\u0026#34;htmlwidget_container_dt_out\u0026#34;\u0026gt; 16 \u0026lt;div id=\u0026#34;htmlwidget_dt_out\u0026#34; 17 style=\u0026#34;width:100%;height:100%;\u0026#34; 18 class=\u0026#34;datatables html-widget\u0026#34;\u0026gt; 19 \u0026lt;/div\u0026gt; 20 \u0026lt;/div\u0026gt; 21 \u0026lt;/div\u0026gt; 22 \u0026lt;/div\u0026gt; 23 \u0026lt;/div\u0026gt; 24 ... highcharter wrapper 25 ... plotly wrapper 26 \u0026lt;/div\u0026gt; 27 28 \u0026lt;/div\u0026gt; 29 30\u0026lt;script type=\u0026#34;application/htmlwidget-sizing\u0026#34; 31 data-for=\u0026#34;htmlwidget_dt_out\u0026#34;\u0026gt; 32 {\u0026#34;browser\u0026#34;:{\u0026#34;width\u0026#34;:\u0026#34;100%\u0026#34;,\u0026#34;height\u0026#34;:400,\u0026#34;padding\u0026#34;:40,\u0026#34;fill\u0026#34;:true}} 33\u0026lt;/script\u0026gt; 34... hicharter sizing 35... plotly sizing As mentioned earlier, AJAX requests are made by clicking buttons and it\u0026rsquo;s implemented in req(). Key steps are\nremove html-widget-static-bound class from a widget makes a call with element_id and type=src note to change hostname append or replace application/json script for plotly, purge() chart. Otherwise traces added continuously execute window.HTMLWidgets.staticRender() 1\u0026lt;script type = \u0026#34;text/javascript\u0026#34; language = \u0026#34;javascript\u0026#34;\u0026gt; 2 function req(elem, tpe) { 3 var btn_id = \u0026#34;#\u0026#34; + elem; 4 var widget_id = \u0026#34;#htmlwidget_\u0026#34; + elem + \u0026#34;_out\u0026#34;; 5 var elem_id = elem + \u0026#34;_out\u0026#34;; 6 var data_for = \u0026#34;htmlwidget_\u0026#34; + elem + \u0026#34;_out\u0026#34;; 7 var scr_selector = \u0026#39;script[type=\u0026#34;application/json\u0026#34;][data-for=\u0026#34;\u0026#39; + 8 data_for + \u0026#39;\u0026#34;]\u0026#39;; 9 $.support.cors = true; 10 $(btn_id).prop(\u0026#34;disabled\u0026#34;, true); 11 $(widget_id).removeClass(\u0026#34;html-widget-static-bound\u0026#34;); 12 $.ajax({ 13 url: \u0026#34;http://[hostname]:8000/widget\u0026#34;, 14 data: { element_id: elem_id, type: tpe }, 15 error: function(err) { 16 $(btn_id).removeAttr(\u0026#39;disabled\u0026#39;); 17 }, 18 success: function(data) { 19 //console.log(data) 20 if($(scr_selector).length == 0) { 21 $(\u0026#39;body\u0026#39;).append(data) 22 } else { 23 if (elem.includes(\u0026#39;plotly\u0026#39;)) { 24 try { 25 //Plotly.deleteTraces(htmlwidget_plotly_out, [0]) 26 Plotly.purge(data_for); 27 } 28 catch(err) { 29 console.log(err); 30 } 31 } 32 $(scr_selector).replaceWith(data); 33 } 34 setTimeout(function(){ 35 window.HTMLWidgets.staticRender(); 36 }, 500); 37 $(btn_id).removeAttr(\u0026#39;disabled\u0026#39;); 38 } 39 }); 40 } 41 42 $(document).ready(function() { 43 $(\u0026#34;#dt\u0026#34;).click(function() { 44 req(\u0026#39;dt\u0026#39;, \u0026#39;src\u0026#39;); 45 }); 46 }); 47 48 ... button clicks for highcharter and plotly 49\u0026lt;/script\u0026gt; For comparison, the async Shiny app and the JavaScript frontend/backend are included in the docker compose. The JavaScript app can be accessed in port 7000. Once started, it\u0026rsquo;s possible to see widgets are rendered without delay when buttons are clicked multiple times.\nCompared to the async Shiny app, the JavaScript app is more effective in handling multiple requests. The downside of it is the benefits that Shiny provides are no longer available. Some of them are built-in data binding, event handling and state management. For example, think about what reactive*() and observe*() do for a Shiny app. Although it is possible to setup those with plain JavaScript or JQuery, life will be a lot easier if an app is built with one of the popular JavaScript frameworks: Angular, React and Vue. In the next post, it\u0026rsquo;ll be shown how to render htmlwidgets in a Vue application as well as building those with native JavaScript libraries.\n","date":"May 19, 2018","img":"/blog/2018-05-19-asyn-shiny-and-its-limitation/featured.png","lang":"en","langName":"English","largeImg":"/blog/2018-05-19-asyn-shiny-and-its-limitation/featured_hu0f69ef413c1c0e126895b33362437da9_247205_500x0_resize_box_3.png","permalink":"/blog/2018-05-19-asyn-shiny-and-its-limitation/","series":[],"smallImg":"/blog/2018-05-19-asyn-shiny-and-its-limitation/featured_hu0f69ef413c1c0e126895b33362437da9_247205_180x0_resize_box_3.png","tags":[{"title":"R","url":"/tags/r/"},{"title":"Shiny","url":"/tags/shiny/"},{"title":"JavaScript","url":"/tags/javascript/"}],"timestamp":1526688000,"title":"Async Shiny and Its Limitation"},{"categories":[{"title":"Serverless","url":"/categories/serverless/"}],"content":"In Part I, it is discussed how to serve an R function with plumber, Rserve and rApache. In this post, the APIs are deployed in a Docker container and, after showing example requests, their performance is compared. The rocker/r-ver:3.4 is used as the base image and each of the APIs is added to it. For simplicity, the APIs are served by Supervisor. For performance testing, Locust is used. The source of this post can be found in this GitHub repository.\nDeployment As can be seen in the Dockerfile below, plumber, Rserve and rApache are installed in order. Plumber is an R package so that it can be installed by install.packages(). The latest versions of Rserve and rApache are built after installing their dependencies. Note, for rApache, the Rook package is installed as well because the test function is served as a Rook application.\nThen the source files are copied to /home/docker/\u0026lt;api-name\u0026gt;. rApache requires extra configuration. First the Rook app (rapache-app.R) and site configuration file (rapache-site.conf) are symlinked to the necessary paths and the site is enabled.\nFinaly Suervisor is started with the config file that monitors/manages the APIs.\n1FROM rocker/r-ver:3.4 2MAINTAINER Jaehyeon Kim \u0026lt;dottami@gmail.com\u0026gt; 3 4RUN apt-get update \u0026amp;\u0026amp; apt-get install -y wget supervisor 5 6## Plumber 7RUN R -e \u0026#39;install.packages(c(\u0026#34;plumber\u0026#34;, \u0026#34;jsonlite\u0026#34;))\u0026#39; 8 9## Rserve 10RUN apt-get install -y libssl-dev 11RUN wget http://www.rforge.net/Rserve/snapshot/Rserve_1.8-5.tar.gz \\ 12 \u0026amp;\u0026amp; R CMD INSTALL Rserve_1.8-5.tar.gz 13 14## rApache 15RUN apt-get install -y \\ 16 libpcre3-dev liblzma-dev libbz2-dev libzip-dev libicu-dev 17RUN apt-get install -y apache2 apache2-dev 18RUN wget https://github.com/jeffreyhorner/rapache/archive/v1.2.8.tar.gz \\ 19 \u0026amp;\u0026amp; tar xvf v1.2.8.tar.gz \\ 20 \u0026amp;\u0026amp; cd rapache-1.2.8 \u0026amp;\u0026amp; ./configure \u0026amp;\u0026amp; make \u0026amp;\u0026amp; make install 21 22RUN R -e \u0026#39;install.packages(c(\u0026#34;Rook\u0026#34;, \u0026#34;rjson\u0026#34;))\u0026#39; 23 24RUN echo \u0026#39;/usr/local/lib/R/lib/\u0026#39; \u0026gt;\u0026gt; /etc/ld.so.conf.d/libR.conf \\ 25 \u0026amp;\u0026amp; ldconfig 26 27## copy sources to /home/docker 28RUN useradd docker \u0026amp;\u0026amp; mkdir /home/docker \\ 29\t\u0026amp;\u0026amp; chown docker:docker /home/docker 30 31RUN mkdir /home/docker/plumber /home/docker/rserve /home/docker/rapache 32COPY ./src/plumber /home/docker/plumber/ 33COPY ./src/rserve /home/docker/rserve/ 34COPY ./src/rapache /home/docker/rapache/ 35COPY ./src/api-supervisor.conf /home/docker/api-supervisor.conf 36RUN chmod -R 755 /home/docker 37 38RUN ln -s /home/docker/rapache/rapache-site.conf \\ 39 /etc/apache2/sites-available/rapache-site.conf \\ 40 \u0026amp;\u0026amp; ln -s /home/docker/rapache/rapache-app.R /var/www/rapache-app.R 41 42## config rApache 43RUN echo \u0026#39;ServerName localhost\u0026#39; \u0026gt;\u0026gt; /etc/apache2/apache2.conf \\ 44 \u0026amp;\u0026amp; /bin/bash -c \u0026#34;source /etc/apache2/envvars\u0026#34; \u0026amp;\u0026amp; mkdir -p /var/run/apache2 \\ 45 \u0026amp;\u0026amp; a2ensite rapache-site 46 47CMD [\u0026#34;/usr/bin/supervisord\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;/home/docker/api-supervisor.conf\u0026#34;] Plumber As can be seen in api-supervisor.conf, the plumber API can be started at port 9000 as following. (plumber-src.R and plumber-serve.R are discussed in Part I)\n1/usr/local/bin/Rscript /home/docker/plumber/plumber-serve.R Rserve In order to utilize the built-in HTTP server of Rserve, http.port should be specified in rserve.conf. Also it is necessary to set daemon disable to manage Rserve by Supervisor.\n1http.port 8000 2remote disable 3auth disable 4daemon disable 5control disable Then it is possible to start the Rserve API at port 8000 as shown below. (rserve-src.R is discussed in Part I.)\n1/usr/local/bin/R CMD Rserve --slave --RS-conf /home/docker/rserve/rserve.conf \\ 2 --RS-source /home/docker/rserve/rserve-src.R rApache The site config file of the rApache API is shown below.\n1LoadModule R_module /usr/lib/apache2/modules/mod_R.so 2\u0026lt;Location /test\u0026gt; 3 SetHandler r-handler 4 RFileEval /var/www/rapache-app.R:Rook::Server$call(test) 5\u0026lt;/Location\u0026gt; It is possible to start the rApache API at port 80 as following. (rapache-app.R is discussed in Part I.)\n1apache2ctl -DFOREGROUND This Docker container can be built and run as following. Note the container\u0026rsquo;s port 80 is mapped to the host\u0026rsquo;s port 7000 to prevent a possible conflict.\n1## build 2docker build -t=api ./api/. 3 4## run 5# rApache - 7000, Rserve - 8000, plumber - 9000 6# all APIs managed by supervisor 7docker run -d -p 7000:80 -p 8000:8000 -p 9000:9000 --name api api:latest Example Request Example requests to the APIs and their responses are shown below. When a request includes both n and wait parameters, the APIs return 200 response as expected. Only the Rserve API properly shows 400 response and the others need some modification.\n1library(httr) 2plumber200 \u0026lt;- POST(url = \u0026#39;http://localhost:9000/test\u0026#39;, encode = \u0026#39;json\u0026#39;, 3 body = list(n = 10, wait = 0.5)) 4unlist(c(api = \u0026#39;plumber\u0026#39;, status = status_code(plumber200), 5 content = content(plumber200))) 1## api status content.value 2## \u0026#34;plumber\u0026#34; \u0026#34;200\u0026#34; \u0026#34;10\u0026#34; 1rapache200 \u0026lt;- POST(url = \u0026#39;http://localhost:7000/test\u0026#39;, encode = \u0026#39;json\u0026#39;, 2 body = list(n = 10, wait = 0.5)) 3unlist(c(api = \u0026#39;rapache\u0026#39;, status = status_code(rapache200), 4 content = content(rapache200))) 1## api status content.value 2## \u0026#34;rapache\u0026#34; \u0026#34;200\u0026#34; \u0026#34;10\u0026#34; 1rserve200 \u0026lt;- POST(url = \u0026#39;http://localhost:8000/test\u0026#39;, encode = \u0026#39;json\u0026#39;, 2 body = list(n = 10, wait = 0.5)) 3unlist(c(api = \u0026#39;rserve\u0026#39;, status = status_code(rserve200), 4 content = content(rserve200))) 1## api status content.value 2## \u0026#34;rserve\u0026#34; \u0026#34;200\u0026#34; \u0026#34;10\u0026#34; 1rserve400 \u0026lt;- POST(url = \u0026#39;http://localhost:8000/test\u0026#39;, encode = \u0026#39;json\u0026#39;, 2 body = list(wait = 0.5)) 3unlist(c(api = \u0026#39;rserve\u0026#39;, status = status_code(rserve400), 4 content = content(rserve400))) 1## api status content.message 2## \u0026#34;rserve\u0026#34; \u0026#34;400\u0026#34; \u0026#34;Missing parameter - n\u0026#34; Performance Test A way to examine performance of an API is to look into how effectively it can serve multiple concurrent requests. For this, Locust, a Python based load testing tool, is used to simulate 1, 3 and 6 concurrent requests successively.\nThe test locust file is shown below.\n1import json 2from locust import HttpLocust, TaskSet, task 3 4class TestTaskSet(TaskSet): 5 6 @task 7 def test(self): 8 payload = {\u0026#39;n\u0026#39;:10, \u0026#39;wait\u0026#39;: 0.5} 9 headers = {\u0026#39;content-type\u0026#39;: \u0026#39;application/json\u0026#39;} 10 self.client.post(\u0026#39;/test\u0026#39;, data=json.dumps(payload), headers=headers) 11 12class MyLocust(HttpLocust): 13 min_wait = 0 14 max_wait = 0 15 task_set = TestTaskSet With this file, testing can be made as following (eg for 3 concurrent requests).\n1locust -f ./locustfile.py --host http://localhost:8000 --no-web -c 3 -r 3 When only 1 request is made successively, the average response time of the APIs is around 500ms. When there are multiple concurrent requests, however, the average response time of the plumber API increases significantly. This is because R is single threaded and requests are queued by httpuv. On the other hand, the average response time of the Rserve API stays the same and this is because Rserve handles concurrent requests by forked processes. The performance of the rApache API is in the middle. In practice, it is possible to boost the performance of the rApache API by enabling Prefork Multi-Processing Module although it will consume more memory.\nAs expected, the Rserve API handles considerably many requests per second.\nNote that the test function in this post is a bit unrealistic as it just waits before returning a value. In practice, R functions will consume more CPU and the average response time will tend to increase when multiple requests are made concurrently. Even in this case, the benefit of forking will persist.\nThis series investigate exposing R functions via an API. I hope you enjoy reading this series.\n","date":"November 19, 2017","img":"/blog/2017-11-19-api-development-with-r-2/featured.png","lang":"en","langName":"English","largeImg":"/blog/2017-11-19-api-development-with-r-2/featured_hu14becfc4256de54ed32f12a303a50027_367256_500x0_resize_box_3.png","permalink":"/blog/2017-11-19-api-development-with-r-2/","series":[{"title":"API development with R","url":"/series/api-development-with-r/"}],"smallImg":"/blog/2017-11-19-api-development-with-r-2/featured_hu14becfc4256de54ed32f12a303a50027_367256_180x0_resize_box_3.png","tags":[{"title":"R","url":"/tags/r/"},{"title":"Rserve","url":"/tags/rserve/"},{"title":"rApache","url":"/tags/rapache/"},{"title":"Plumber","url":"/tags/plumber/"}],"timestamp":1511049600,"title":"API Development With R Part II"},{"categories":[{"title":"Serverless","url":"/categories/serverless/"}],"content":"API is an effective way of distributing analysis outputs to external clients. When it comes to API development with R, however, there are not many choices. Probably development would be made with plumber, Rserve, rApache or OpenCPU if a client or bridge layer to R is not considered.\nThis is 2 part series in relation to API development with R. In this post, serving an R function with plumber, Rserve and rApache is discussed. OpenCPU is not discussed partly because it could be overkill for API. Also its performance may be similar to rApache with Prefork Multi-Processing Module enabled. Then deploying the APIs in a Docker container, making example HTTP requests and their performance will be discussed in Part II.\nPlumber The plumber package is the easiest way of exposing an R function via API and it is built on top of the httpuv package.\nHere a simple function named test is defined in plumber-src.R. test() returns a number after waiting the amount of seconds specified by wait. Note the details of HTTP methods and resource paths can be specified as the way how a function is documented. By default, the response is converted into a json string and it is set to be unboxed.\n1#\u0026#39; Test function 2#\u0026#39; @serializer unboxedJSON 3#\u0026#39; @get /test 4#\u0026#39; @post /test 5test \u0026lt;- function(n, wait = 0.5, ...) { 6 Sys.sleep(wait) 7 list(value = n) 8} The function can be served as shown below. Port 9000 is set for the plumber API.\n1library(plumber) 2r \u0026lt;- plumb(\u0026#34;path-to-plumber-src.R\u0026#34;) 3r$run(port=9000, host=\u0026#34;0.0.0.0\u0026#34;) Rserve According to its project site,\nRserve is a TCP/IP server which allows other programs to use facilities of R from various languages without the need to initialize R or link against R library.\nThere are a number of Rserve client libraries and a HTTP API can be developed with one of them. For example, it is possible to set up a client layer to invoke an R function using the pyRserve library while a Python web freamwork serves HTTP requests.\nSince Rserve 1.7-0, however, a client layer is not mandatory because it includes the built-in R HTTP server. Using the built-in server has a couple of benefits. First development can be simpler without a client or bridge layer. Also performance of the API can be improved. For example, pyRserve waits for 200ms upon connecting to Rserve and this kind of overhead can be reduced significantly if HTTP requests are handled directly.\nThe FastRWeb package relies on Rserve\u0026rsquo;s built-in HTTP server and basically it serves HTTP requests by sourcing an R script and executing a function named as run - all source scripts must have run() as can be checked in the source.\nI find the FastRWeb package is not convenient for API development for several reasons. First, as mentioned earlier, it sources an R script and executes run(). However, after looking into the source code, it doesn\u0026rsquo;t need to be that way. Rather a more flexible way can be executing a function that\u0026rsquo;s already loaded. Secondly application/json is a popular content type but it is not understood by the built-in server. Finally, while it mainly aims to serve R graphics objects and HTML pages, json string can be effective for HTTP responses. In this regards, some modifications are maded as discussed below.\ntest() is the same to the plumber API.\n1#### HTTP RESOURCES 2test \u0026lt;- function(n, wait = 0.5, ...) { 3 Sys.sleep(wait) 4 list(value = n) 5} In order to use the built-in server, a function named .http.request should be found. Here another function named process_request is created and it is used instead of .http.request() defined in the FastRWeb package. process_request() is basically divided into 2 parts: builing request object and building output object.\nbuilding request object - the request obeject is built so that the headers are parsed so as to identify the request method. Then the request parameters are parsed according to the request method and content type. building output object - the output object is a list of payload, content-type, headers and status-code. A function can be found by the request URL and it is checked if all function arguments are found in the request parameters. Then payload is obtained by executing the matching function if all arguments are found. Otherwise the 400 (Bad Request) error will be returned. 1#### PROCESS REQUEST 2process_request \u0026lt;- function(url, query, body, headers) { 3 #### building request object 4 ## not strictly necessary as in FastRWeb, 5 ## just to make clear of request related variables 6 request \u0026lt;- list(uri = url, method = \u0026#39;POST\u0026#39;, 7 query = query, body = body) 8 9 ## parse headers 10 request$headers \u0026lt;- parse_headers(headers) 11 if (\u0026#34;request-method\u0026#34; %in% names(request$headers)) 12 request$method \u0026lt;- c(request$headers[\u0026#34;request-method\u0026#34;]) 13 14 ## parse parameters (function arguments) 15 ## POST accept only 2 content types 16 ## - application/x-www-form-urlencoded by built-in server 17 ## - application/json 18 ## used below as do.call(function_name, request$pars) 19 request$pars \u0026lt;- list() 20 if (request$method == \u0026#39;POST\u0026#39;) { 21 if (!is.null(body)) { 22 if (is.raw(body)) 23 body \u0026lt;- rawToChar(body) 24 if (any(grepl(\u0026#39;application/json\u0026#39;, request$headers))) 25 body \u0026lt;- jsonlite::fromJSON(body) 26 request$pars \u0026lt;- as.list(body) 27 } 28 } else { 29 if (!is.null(query)) { 30 request$pars \u0026lt;- as.list(query) 31 } 32 } 33 34 #### building output object 35 ## list(payload, content-type, headers, status_code) 36 ## https://github.com/s-u/Rserve/blob/master/src/http.c#L358 37 payload \u0026lt;- NULL 38 content_type \u0026lt;- \u0026#39;application/json; charset=utf-8\u0026#39; 39 headers \u0026lt;- character(0) 40 status_code \u0026lt;- 200 41 42 ## generate payload (function output) 43 ## function name must match to resource path for now 44 matched_fun \u0026lt;- gsub(\u0026#39;^/\u0026#39;, \u0026#39;\u0026#39;, request$uri) 45 46 ## no resource path means no matching function 47 if (matched_fun == \u0026#39;\u0026#39;) { 48 payload \u0026lt;- list(api_version = \u0026#39;1.0\u0026#39;) 49 if (grepl(\u0026#39;application/json\u0026#39;, content_type)) 50 payload \u0026lt;- jsonlite::toJSON(payload, auto_unbox = TRUE) 51 return (list(payload, content_type, headers)) # default status 200 52 } 53 54 ## check if all defined arguments are supplied 55 defined_args \u0026lt;- formalArgs(matched_fun)[formalArgs(matched_fun) != \u0026#39;...\u0026#39;] 56 args_exist \u0026lt;- defined_args %in% names(request$pars) 57 if (!all(args_exist)) { 58 missing_args \u0026lt;- defined_args[!args_exist] 59 payload \u0026lt;- list(message = paste(\u0026#39;Missing parameter -\u0026#39;, 60 paste(missing_args, collapse = \u0026#39;, \u0026#39;))) 61 status_code \u0026lt;- 400 62 } 63 64 if (is.null(payload)) { 65 payload \u0026lt;- tryCatch({ 66 do.call(matched_fun, request$pars) 67 }, error = function(err) { 68 list(message = \u0026#39;Internal Server Error\u0026#39;) 69 }) 70 71 if (\u0026#39;message\u0026#39; %in% names(payload)) 72 status_code \u0026lt;- 500 73 } 74 75 if (grepl(\u0026#39;application/json\u0026#39;, content_type)) 76 payload \u0026lt;- jsonlite::toJSON(payload, auto_unbox = TRUE) 77 78 return (list(payload, content_type, headers, status_code)) 79} 80 81# parse headers in process_request() 82# https://github.com/s-u/FastRWeb/blob/master/R/run.R#L65 83parse_headers \u0026lt;- function(headers) { 84 ## process headers to pull out request method (if supplied) and cookies 85 if (is.raw(headers)) headers \u0026lt;- rawToChar(headers) 86 if (is.character(headers)) { 87 ## parse the headers into key/value pairs, collapsing multi-line values 88 h.lines \u0026lt;- unlist(strsplit(gsub(\u0026#34;[\\r\\n]+[ \\t]+\u0026#34;,\u0026#34; \u0026#34;, headers), \u0026#34;[\\r\\n]+\u0026#34;)) 89 h.keys \u0026lt;- tolower(gsub(\u0026#34;:.*\u0026#34;, \u0026#34;\u0026#34;, h.lines)) 90 h.vals \u0026lt;- gsub(\u0026#34;^[^:]*:[[:space:]]*\u0026#34;, \u0026#34;\u0026#34;, h.lines) 91 names(h.vals) \u0026lt;- h.keys 92 h.vals \u0026lt;- h.vals[grep(\u0026#34;^[^:]+:\u0026#34;, h.lines)] 93 return (h.vals) 94 } else { 95 return (NULL) 96 } 97} process_request() replaces .http.request() in the source script of Rserve - it\u0026rsquo;ll be explained futher in Part II.\n1## Rserve requires .http.request function for handling HTTP request 2.http.request \u0026lt;- process_request rApache rApache is a project supporting web application development using the R statistical language and environment and the Apache web server.\nrApache provides multiple ways to specify an R function that handles incoming HTTP requests - see the manual for details. Among the multiple RHandlers, I find using a Rook application can be quite effective.\nHere is the test function as a Rook application. As process_request(), it parses function arguments according to the request method and content type. Then a value is returned after wating the specified seconds. The response of a Rook application is a list of status, headers and body.\n1test \u0026lt;- function(env) { 2 req \u0026lt;- Request$new(env) 3 res \u0026lt;- Response$new() 4 5 request_method \u0026lt;- env[[\u0026#39;REQUEST_METHOD\u0026#39;]] 6 rook_input \u0026lt;- env[[\u0026#39;rook.input\u0026#39;]]$read() 7 content_type \u0026lt;- env[[\u0026#39;CONTENT_TYPE\u0026#39;]] 8 9 req_args \u0026lt;- if (request_method == \u0026#39;GET\u0026#39;) { 10 req$GET() 11 } else { 12 # only accept application/json 13 if (!grepl(\u0026#39;application/json\u0026#39;, content_type, ignore.case = TRUE)) { 14 NULL 15 } else if (length(rook_input) == 0) { 16 NULL 17 } else { 18 if (is.raw(rook_input)) 19 rook_input \u0026lt;- rawToChar(rook_input) 20 rjson::fromJSON(rook_input) 21 } 22 } 23 24 if (!is.null(req_args)) { 25 wait \u0026lt;- if (\u0026#39;wait\u0026#39; %in% names(req_args)) req_args$wait else 1 26 n \u0026lt;- if (\u0026#39;n\u0026#39; %in% names(req_args)) req_args$n else 10 27 Sys.sleep(wait) 28 list( 29 status = 200, 30 headers = list(\u0026#39;Content-Type\u0026#39; = \u0026#39;application/json\u0026#39;), 31 body = rjson::toJSON(list(value=n)) 32 ) 33 } else { 34 list( 35 status = 400, 36 headers = list(\u0026#39;Content-Type\u0026#39; = \u0026#39;application/json\u0026#39;), 37 body = rjson::toJSON(list(message=\u0026#39;No parameters specified\u0026#39;)) 38 ) 39 } 40} This is all for Part I. In Part II, it\u0026rsquo;ll be discussed how to deploy the APIs via a Docker container, how to make example requests and their performance. I hope this article is interesting.\n","date":"November 18, 2017","img":"/blog/2017-11-18-api-development-with-r-1/featured.png","lang":"en","langName":"English","largeImg":"/blog/2017-11-18-api-development-with-r-1/featured_hu14becfc4256de54ed32f12a303a50027_367256_500x0_resize_box_3.png","permalink":"/blog/2017-11-18-api-development-with-r-1/","series":[{"title":"API development with R","url":"/series/api-development-with-r/"}],"smallImg":"/blog/2017-11-18-api-development-with-r-1/featured_hu14becfc4256de54ed32f12a303a50027_367256_180x0_resize_box_3.png","tags":[{"title":"R","url":"/tags/r/"},{"title":"Rserve","url":"/tags/rserve/"},{"title":"rApache","url":"/tags/rapache/"},{"title":"Plumber","url":"/tags/plumber/"}],"timestamp":1510963200,"title":"API Development With R Part I"},{"categories":[{"title":"Serverless","url":"/categories/serverless/"}],"content":"In the previous posts, it is discussed how to package/deploy an R model with AWS Lambda and to expose the Lambda function via Amazon API Gateway. Main benefits of serverless architecture is cost-effectiveness and being hassle-free from provisioning/managing servers. While the API returns a predicted admission status value given GRE, GPA and Rank, there is an issue if it is served within a web application: Cross-Origin Resource Sharing (CORS). This post discusses how to resolve this issue by updating API configuration and the Lambda function handler with a simple web application. Also it is illustrated how to host the application in a serverless environment.\nBackend Part I - Packaging R ML Model for Lambda Part II - Deploying R ML Model via Lambda Part III - Exposing R ML Model via APIG Frontend Part IV - Serving R ML Model via S3 - this post Frontend A simple single page application is created using React. By clicking the Check! button after entering the GRE, GPA and Rank values, information of the expected admimission status pops up in a modal. The status value is fetched from the API of the POC application that is discussed in Part III. The code of this application can be found here.\nUpdate Lambda function hander CORS According to Wikipedia,\nCross-origin resource sharing (CORS) is a mechanism that allows restricted resources (e.g. fonts) on a web page to be requested from another domain outside the domain from which the first resource was served. A web page may freely embed cross-origin images, stylesheets, scripts, iframes, and videos. Certain \u0026ldquo;cross-domain\u0026rdquo; requests, notably Ajax requests, however are forbidden by default by the same-origin security policy.\nHere is an example from a Stack Overflow answer why it can be important to prevent CORS.\nYou go to website X and the author of website X has written an evil script which gets sent to your browser. That script running on your browser logs onto your bank website and does evil stuff and because it\u0026rsquo;s running as you in your browser it has permission to do so. Therefore your bank\u0026rsquo;s website needs some way to tell your browser if scripts on website X should be trusted to access pages at your bank. The domain name of the API is api.jaehyeon.me so that requests fail within the application. An example of the error is shown below.\n1Fetch API cannot load ... 2No \u0026#39;Access-Control-Allow-Origin\u0026#39; header is present on the requested resource. 3Origin \u0026#39;http://localhost:9090\u0026#39; is therefore not allowed access. The response had HTTP status code 403. Cofigure API API Gateway allows to enable CORS and it can be either on a resource or a method within a resource. The GET method is selected and Enable CORS is clicked after pulling down actions.\nSimply put, another method of OPTIONS is created and the following response headers are added to the GET and OPTIONS methods.\nAccess-Control-Allow-Methods: Added to OPTIONS only Access-Control-Allow-Headers: Added to OPTIONS only, X-API-Key is allowed Access-Control-Allow-Origin: Added to both GET and OPTIONS Here is all the steps that enables CORS in API Gateway. Note that the necessary headers are added to 200 response only, not to 400 response so that the above error can\u0026rsquo;t be eliminated for 400 response unless the headers are set separately.\nAfter that, the API needs to be deployed again and, as can be seen in the deployment history, the latest deployment is selected as the current stage.\nUpdate handler Despite enabling CORS, it was not possilbe to resolve the issue. After some search, a way is found in a Stack Overflow answer. It requires to update the Lambda function handler (handler.py) that extends the 200 response with headers elements - one for CORS support to work and the other for cookies, authorization headers with HTTPS. The original response is added to the body element of the new response. Note it\u0026rsquo;d be necessary to modify the case of 400 response in order to reduce the risk of encountering the error although it is not covered here.\n1def lambda_handler(event, context): 2 try: 3 gre = event[\u0026#34;gre\u0026#34;] 4 gpa = event[\u0026#34;gpa\u0026#34;] 5 rnk = event[\u0026#34;rank\u0026#34;] 6 can_be_admitted = pred_admit(gre, gpa, rnk) 7 res = { 8 \u0026#34;httpStatus\u0026#34;: 200, 9 \u0026#34;headers\u0026#34;: { 10 # Required for CORS support to work 11 \u0026#34;Access-Control-Allow-Origin\u0026#34; : \u0026#34;*\u0026#34;, 12 # Required for cookies, authorization headers with HTTPS 13 \u0026#34;Access-Control-Allow-Credentials\u0026#34; : True 14 }, 15 \u0026#34;body\u0026#34;: {\u0026#34;result\u0026#34;: can_be_admitted} 16 } 17 return res 18 except Exception as e: 19 logging.error(\u0026#39;Payload: {0}\u0026#39;.format(event)) 20 logging.error(\u0026#39;Error: {0}\u0026#39;.format(e.message)) 21 err = { 22 \u0026#39;errorType\u0026#39;: type(e).__name__, 23 \u0026#39;httpStatus\u0026#39;: 400, 24 \u0026#39;request_id\u0026#39;: context.aws_request_id, 25 \u0026#39;message\u0026#39;: e.message.replace(\u0026#39;\\n\u0026#39;, \u0026#39; \u0026#39;) 26 } 27 raise Exception(json.dumps(err)) The updated handler is packaged again and copied to S3.\n1# pull git repo 2cd serverless-poc/ 3git pull origin master 4 5# copy handler.py and create admission.zip 6cd .. 7export HANDLER=handler 8 9cp -v serverless-poc/poc-logit-handler/*.py $HOME/$HANDLER 10cd $HOME/$HANDLER 11zip -r9 $HOME/admission.zip * 12 13# copy to S3 14aws s3 cp $HOME/admission.zip s3://serverless-poc-handlers The AWS web console doesn\u0026rsquo;t have an option to update a Lambda function where the deployment package is in S3 so that aws cli is used instead.\n1#http://docs.aws.amazon.com/cli/latest/reference/lambda/update-function-code.html 2aws lambda update-function-code --function-name ServerlessPOCAdmission \\ 3\t--s3-bucket serverless-poc-handlers --s3-key admission.zip The test result shown below indicates the updated handler is executed correctly.\nIt can also be checked by the API and now the API can be served in a web application.\n1# updated response 2#curl -H \u0026#39;x-api-key:xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\u0026#39; \\ 3# \u0026#39;https://api.jaehyeon.me/poc/admit?gre=800\u0026amp;gpa=4\u0026amp;rank=1\u0026#39; 4r \u0026lt;- GET(\u0026#34;https://api.jaehyeon.me/poc/admit\u0026#34;, 5 add_headers(`x-api-key` = \u0026#39;xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\u0026#39;), 6 query = list(gre = 800, gpa = 4, rank = 1)) 7status_code(r) 8[1] 200 9 10content(r) 11$body 12$body$result 13[1] TRUE 14 15$headers 16$headers$`Access-Control-Allow-Origin` 17[1] \u0026#34;*\u0026#34; 18 19$headers$`Access-Control-Allow-Credentials` 20[1] TRUE 21 22$httpStatus 23[1] 200 Hosting Amazon S3 is one of the popular ways to store static web contents and it can be used as a way to host a static web site. The React application can be hosted on S3 as the backend logic of calling the API is bundled and accessible. 2 ways are illustrated in this section. The former is via the Static website hosting property of Amazon S3 Buckets while the latter is through Amazon CloudFront, which is a content delivery network (CDN) service. Note that only HTTP is avaialble if the application is hosted without CloudFront.\nSeparate S3 buckets are created to store the application as shown below.\npoc.jaehyeon.me - For hosting Static website hosting property web.jaehyeon.me - For hosting through CloudFront In AWS console, two folders are created: app and css. Then the application files are saved to each of the buckets as following.\n1app 2 bundle.js 3css 4 bootstrap.css 5 style.css 6index.html Static website hosting First read-access is given to all objects in the bucket (poc.jaehyeon.me). It is set in Bucket Policy of the permissions tab - Policy is discussed in Part II.\nThen, in the properties tab, static website hosting is enabled where index.html is set to be rendered for both the default and error document. Now it is possible to have access to the application by the endpoint.\nIn order to replace the endpoint with a custom domain name, a Canonical name (CNAME) record is created in Amazon Route 53. Note that the CNAME record (poc.jaehyeon.me) has to be the same to the bucket name. s3-website-us-east-1.amazonaws.com. is entered in Value, which is used to define the host name as an alias for the Amazon S3 bucket. Note the period at the end is necessary as it signifies the DNS root and, if it is not specified, a DNS resolver could append it\u0026rsquo;s default domain to the domain you provided. (See Customizing Amazon S3 URLs with CNAMEs for further details.) Now the application can be accessed using http://poc.jaehyeon.me.\nCloudFront It is possible to host the application using Amazon CloudFront which is a global content delivery network (CDN) service that accelerates delivery of websites, APIs, video content or other web assets.\nWeb is taken as the delivery method.\nThe S3 bucket (web.jaehyeon.me) is selected as the origin domain name. Note, unlike relying on the static website hosting property where all objects in the bucket are given read-access, in this way, access to the bucket is restricted only to CloudFront with a newly created identity. The updated bucket policy is shown below. (See Using an Origin Access Identity to Restrict Access to Your Amazon S3 Content for further details.)\n1{ 2 \u0026#34;Version\u0026#34;: \u0026#34;2008-10-17\u0026#34;, 3 \u0026#34;Id\u0026#34;: \u0026#34;PolicyForCloudFrontPrivateContent\u0026#34;, 4 \u0026#34;Statement\u0026#34;: [ 5 { 6 \u0026#34;Sid\u0026#34;: \u0026#34;1\u0026#34;, 7 \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, 8 \u0026#34;Principal\u0026#34;: { 9 \u0026#34;AWS\u0026#34;: \u0026#34;arn:aws:iam::cloudfront:user/CloudFront Origin Access Identity xxxxxxxxxxxxxx\u0026#34; 10 }, 11 \u0026#34;Action\u0026#34;: \u0026#34;s3:GetObject\u0026#34;, 12 \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::web.jaehyeon.me/*\u0026#34; 13 } 14 ] 15} In default cache behavior settings, Redirect HTTP to HTTPS is selected for the viewer protocol policy. All other options are left untouched - they are not shown.\nIn distribution settings, a CNAME record (web.jaehyeon.me) is created to be the same to the bucket name. The custom SSL certificate that is obtained from AWS Certificate Manager is chosen rather than the default CloudFront certificate - see Part III. Finally it is selected to support only clients that support server name indication (SNI). Note all the other options are left untouched - they are not shown.\nOnce the distribution is created, the distribution\u0026rsquo;s CloudFront domain name is created and it is possible to use it to create a custom domain.\nIn Route 53, a new record set is created and web.jaehyeon.me is entered in the name field, followed by selecting A - IPv4 address as the type. Alias is set to be yes and the distribution domain name is entered as the alias target.\nOnce it is ready, the application can be accessed using either http://web.jaehyeon.me or https://web.jaehyeon.me where HTTP is redirected to HTTPS.\nFinal thoughts This is the end of the Serverless Data Product POC series. I consider a good amount of information is shared in relation to serverless data product development and I hope you find the posts useful. For demonstration, I used the AWS web console but it wouldn\u0026rsquo;t be suitable in a production environment as it involves a lot of manual jobs as well as those jobs are not reproducible. There are a number of notable frameworks that help develop applications in serverless environment: Serverless Framework, Apex, Chalice and Zappa. I hope there will be another series that cover one of these frameworks.\n","date":"April 17, 2017","img":"/blog/2017-04-17-serverless-data-product-4/featured.png","lang":"en","langName":"English","largeImg":"/blog/2017-04-17-serverless-data-product-4/featured_huf7a77a5394e53f84146299d65bd38b83_225463_500x0_resize_box_3.png","permalink":"/blog/2017-04-17-serverless-data-product-4/","series":[{"title":"Serverless Data Product","url":"/series/serverless-data-product/"}],"smallImg":"/blog/2017-04-17-serverless-data-product-4/featured_huf7a77a5394e53f84146299d65bd38b83_225463_180x0_resize_box_3.png","tags":[{"title":"AWS","url":"/tags/aws/"},{"title":"AWS Lambda","url":"/tags/aws-lambda/"},{"title":"Amazon S3","url":"/tags/amazon-s3/"},{"title":"Amazon API Gateway","url":"/tags/amazon-api-gateway/"},{"title":"R","url":"/tags/r/"},{"title":"Python","url":"/tags/python/"},{"title":"CloudFront","url":"/tags/cloudfront/"},{"title":"Route53","url":"/tags/route53/"},{"title":"React","url":"/tags/react/"}],"timestamp":1492387200,"title":"Serverless Data Product POC Backend Part IV - Serving R ML Model via S3"},{"categories":[{"title":"Serverless","url":"/categories/serverless/"}],"content":"In Part I of this series, R and necessary libraries/packages together with a Lambda function handler are packaged and saved to Amazon S3. Then, in Part II, the package is deployed at AWS Lambda after creating and assigning a role to the Lambda function. Although the Lambda function can be called via the Invoke API, it\u0026rsquo;ll be much more useful if the function can be called as a web service (or API). In this post, it is discussed how to expose the Lambda function via Amazon API Gateway. After creating an API by integrating the Lambda function, it is protected with an API key. Finally a custom domain name is used as an alternative URL of the API.\nBackend Part I - Packaging R ML Model for Lambda Part II - Deploying R ML Model via Lambda Part III - Exposing R ML Model via APIG - this post Frontend Part IV - Serving R ML Model via S3 [EDIT 2017-04-17] The Lambda function hander (handler.py) has been modified to resolve an issue of Cross-Origin Resource Sharing (CORS). See Part IV for further details.\nCreate API It can be started by clicking the Get Started button if there\u0026rsquo;s no existing API or the Create API button if there is an existing one.\nAmazon API Gageway provides several options to create an API. New API is selected for the API of the POC application and the name of the API (ServerlessPOC) and description are entered.\nCreate resource and method According to Thoughts on RESTful API Design,\nIn any RESTful API, a resource is an object with a type, associated data, relationships to other resources, and a set of methods that operate on it.\nA resource is represented in the URL and, if the resource is named as admit, the resource URL becomes /admit (eg http://example.com/admit) and a client application can make a request to the URL.\nAs can be seen below, the Lambda function hander requires that the event object has 3 elements: gre, gpa and rank.\n1def lambda_handler(event, context): 2 try: 3 gre = event[\u0026#34;gre\u0026#34;] 4 gpa = event[\u0026#34;gpa\u0026#34;] 5 rnk = event[\u0026#34;rank\u0026#34;] 6 can_be_admitted = pred_admit(gre, gpa, rnk) 7 res = {\u0026#34;result\u0026#34;: can_be_admitted} 8 return res 9 except Exception as e: 10 logging.error(\u0026#39;Payload: {0}\u0026#39;.format(event)) 11 logging.error(\u0026#39;Error: {0}\u0026#39;.format(e.message)) 12 err = { 13 \u0026#39;errorType\u0026#39;: type(e).__name__, 14 \u0026#39;httpStatus\u0026#39;: 400, 15 \u0026#39;request_id\u0026#39;: context.aws_request_id, 16 \u0026#39;message\u0026#39;: e.message.replace(\u0026#39;\\n\u0026#39;, \u0026#39; \u0026#39;) 17 } 18 raise Exception(json.dumps(err)) In Amazon API Gateway, there are two ways to create the resource for the Lambda function of the POC application.\nQuery string\nIt is possible to create only a resource and the 3 elements can be added in query string. Then a request with the 3 elements can be made to /admit?gre=800\u0026amp;gpa=4\u0026amp;rank=1. 1/ 2 /admit Proxy resource\nProxy resources can be created by covering path parameters by brackets. Then the equivalent request can be made to /800/4/1/admit. 1/ 2 /{gre} 3 /{gpa} 4 /{rank} 5 /admit For the API of the POC application, the way with query string is used. First it is necessary to create a resource.\nThen the resource is named as Admit.\nAfter creating the resource, it is necessary to create one or more HTTP methods on it.\nOnly the GET method is created for this API.\nNow it is time to integrate the method with the Lambda function. Lambda Function is selected as the interation type and ServerlessPOCAdmission is selected - note that the region where the Lambda function is deployed should be selected first.\nConfigure method execution The lifecycle of a Lambda function is shown below. A Lambda function is called after Method Request and Integration Request. Also there are two steps until the result is returned back to the client: Method Response and Integration Response.\nMethod request As discussed earlier, only a single resource is created so that a request is made with query string. Therefore the 3 event elements (gre, gpa and rank) should be created in URL Query String Parameters. Note that API Key Required is set to be false and it is necessary to change it to be true if the API needs to be protected with an API key - it\u0026rsquo;ll be discussed further below. The other sections (HTTP Request Header, Request Body, \u0026hellip;) are not touched for this API.\nIntegration request It is possible to update the target backend or to modify data from the incoming request. It is not necessary to change the target backend as it is already set appropriately.\nAmong the 3 event elements (gre, gpa and rank), rank is a factor or, at least, it should be a string while the others can be either numbers or numeric strings. Therefore the Lambda function will complain if a numeric rank value is included in a query string (eg rank=1). Although it is possible to modify the Lambda function handler, an easier way is to modify data from the incoming request.\nIn Body Mapping Templates, the recommended option of When there are no templates defined (recommended) is selected in request body passthrough and application/json is added to Content-Type. Data from incoming request can be updated in the template that is shown by clicking the added content type (application/json). As shown below, rank is changed into a string before the Lambda function is called. Note Velocity Template Engine is used in Amazon API Gateway.\n1{ 2 \u0026#34;gre\u0026#34;: $input.params(\u0026#39;gre\u0026#39;), 3 \u0026#34;gpa\u0026#34;: $input.params(\u0026#39;gpa\u0026#39;), 4 \u0026#34;rank\u0026#34;: \u0026#34;$input.params(\u0026#39;rank\u0026#39;)\u0026#34; 5} Method response If a request is successful, the HTTP status code of 200 is returned. As can be seen in the code of the Lambda function handler above, the status code of 400 is planned to be returned if there is an error. Therefore it is necessary to add 400 response so that it is mapped in Integration Response.\nIntegration response The output of a response can be mapped in Body Mapping Templates. The body of the default 200 response doesn\u0026rsquo;t need modification as the Lambda function already returns a JSON string - {\u0026quot;result\u0026quot;: true} or {\u0026quot;result\u0026quot;: false}. If the function returns only True or False, however, the response can be modified as shown below. (Note that this is only for illustration and nothing is added to the content type.)\n1{ 2 \u0026#34;result\u0026#34;: $input.path(\u0026#39;$\u0026#39;) 3} For 400 response, the HTTP status is identified by .*\u0026quot;httpStatus\u0026quot;:400.* and the body is mapped as following.\n1#set ($errorMessageObj = $util.parseJson($input.path(\u0026#39;$.errorMessage\u0026#39;))) 2{ 3 \u0026#34;code\u0026#34; : $errorMessageObj.httpStatus, 4 \u0026#34;message\u0026#34; : \u0026#34;$errorMessageObj.message\u0026#34;, 5 \u0026#34;request-id\u0026#34; : \u0026#34;$errorMessageObj.request_id\u0026#34; 6} Test API The API can be tested by adding the 3 elements in query string. As expected, the response returns {\u0026quot;result\u0026quot;: true} with the HTTP status code of 200.\nIn order to test 400 response, the value of gre is set to be a string (gre). The status code of 400 is returned as expected but it fails to parse the message of the error into JSON. It is necessary to modify the message, referring to Error Handling Patterns in Amazon API Gateway and AWS Lambda.\n1 ... 2 3 err = { 4 \u0026#39;errorType\u0026#39;: type(e).__name__, 5 \u0026#39;httpStatus\u0026#39;: 400, 6 \u0026#39;request_id\u0026#39;: context.aws_request_id, 7 \u0026#39;message\u0026#39;: e.message.replace(\u0026#39;\\n\u0026#39;, \u0026#39; \u0026#39;) 8 } 9 ... Deploy API Once testing is done, it is ready to deploy the API.\nIt is possible to create a new stage by selecting [New Stage] or to update an existing one by selecting its name in deployment stage. Although it is recommended to create at least 2 stages (eg development and production stage), only a singe production stage is created for the POC application.\nOnce created, the invoke URL can be found when the relevant method (GET) is clicked. The default root URL is of the following format.\n1https://api-id.execute-api.region.amazonaws.com/stage The API has been deployed successfully and it is possible to make a request using curl and R\u0026rsquo;s httr package as following - note the API ID is hidden.\n1## no API Key 2#curl \u0026#39;https://xxxxxxxxxx.execute-api.us-east-1.amazonaws.com/prod/admit?gre=800\u0026amp;gpa=4\u0026amp;rank=1\u0026#39; 3r \u0026lt;- GET(\u0026#34;https://xxxxxxxxxx.execute-api.us-east-1.amazonaws.com/prod/admit\u0026#34;, 4 query = list(gre = 800, gpa = 4, rank = 1)) 5 6status_code(r) 7[1] 200 8 9content(r) 10$result 11[1] TRUE Protecting by API key Enable API key It is on individual methods whether to enable an API key or not. In order to enable an API key, select the GET method in the resources section and change API Key Required to true in Method Request. Note that the API has to be deployed again in order to have the change in effect.\nCreate usage plan A usage plan enforces Throttling (Rate and Burst) and Quota of an API and it associates API stages and keys. Since its launch on August 11, 2016, it is enabled in a region where API Gateway is used for the first time. The meaning of the throttling and quota values are as following.\nRate is the rate at which tokens are added to the Token Bucket and this value indicates the average number of requests per second over an extended period of time. Burst is the capacity of the Token Bucket. Quota is the total number of requests in a given time period. For further details, see Manage API Request Throttling and Token Bucket vs Leaky Bucket.\nA usage plan named ServerlessPOC is created where the rate, burst and quote are 10 requests per second, 20 requests and 500 requests per day respectively.\nThen the production stage (prod) of ServerlessPOC API is added to the plan.\nCreate API key An API key can be created in API Keys section of the Console. The key is named as ServerlessPOC and it is set to be auto-generated.\nThe usage plan created earlier is added to the API key.\nNow the API has been protected with an API key and it is possible to make a request using curl and R\u0026rsquo;s httr package as following. Note that the API key should be added with the key named x-api-key. Without the API key in the header, the request returns 403 Forbidden error. (Note also tick marks rather than single quotations in GET())\n1## API Key 2# 403 Forbidden without api key 3#curl -H \u0026#39;x-api-key:xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\u0026#39; \\ 4# \u0026#39;https://xxxxxxxxxx.execute-api.us-east-1.amazonaws.com/prod/admit?gre=800\u0026amp;gpa=4\u0026amp;rank=1\u0026#39; 5r \u0026lt;- GET(\u0026#34;https://xxxxxxxxxx.execute-api.us-east-1.amazonaws.com/prod/admit\u0026#34;, 6 add_headers(`x-api-key` = \u0026#39;xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\u0026#39;), 7 query = list(gre = 800, gpa = 4, rank = 1)) 8 9status_code(r) 10[1] 200 11 12content(r) 13$result 14[1] TRUE Using custom domain name The invoke URL generated by API Gateway can be difficult to recall and not user-friendly. In order to have a more inituitive URL for the API, it is possible to set up a custom domain name as the API\u0026rsquo;s host name and choose a base path to present an alternative URL of the API. For example, instead of using xxxxxxxxxx.execute-api.us-east-1.amazonaws.com, it is possible to use api.jaehyeon.me.\nThe prerequisites for using a custom dome name for an API are\nDomain name ACM Certificate (us-east-1 only) I registered a domain name (jaehyeon.me) in Amazon Route 53 and requested ACM Certificate through AWS Certificate Manager. It was quite quick to me and it took less than 1 day. See the following articles for how-to.\nRegistering Domain Names Using Amazon Route 53 Requesting and Managing ACM Certificates The domain name of the API is set to be api.jaehyeon.me and the approved ACM Certificate is selected. In Base Path Mappings, poc is added to the path and the production stage of the ServerlessPOC API is selected as the destination. In this way, it is possible to change the resource URL as following.\n1# default resource URL 2https://xxxxxxxxxx.execute-api.us-east-1.amazonaws.com/prod/admit 3 4# custom resource URL 5https://api.jaehyeon.me/poc/admit When clicking the save button above, a distribution domain name is assigned by Amazon CloudFront. This step takes up to 40 minutes to complete and, in the meantime, A-record alias for the API domain name is set up so that it can be mapped to the associated distribution domain name.\nIn Route 53, a new record set is created and api.jaehyeon.me is entered in the name field, followed by selecting A - IPv4 address as the type. Alias is set to be yes and the distribution domain name is entered as the alias target.\nOnce it is ready, the custom domain name can be used as an alternative domain name of the API and it is possible to make a request using curl and R\u0026rsquo;s httr package as following.\n1## custom domain name 2#curl -H \u0026#39;x-api-key:xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\u0026#39; \\ 3# \u0026#39;https://api.jaehyeon.me/poc/admit?gre=800\u0026amp;gpa=4\u0026amp;rank=1\u0026#39; 4r \u0026lt;- GET(\u0026#34;https://api.jaehyeon.me/poc/admit\u0026#34;, 5 add_headers(`x-api-key` = \u0026#39;xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\u0026#39;), 6 query = list(gre = 800, gpa = 4, rank = 1)) 7 8status_code(r) 9[1] 200 10 11content(r) 12$result 13[1] TRUE That\u0026rsquo;s it! This is all that I was planning to discuss with regard to exposing a Lambda function backed by a prediction model in R via an API. I hope this series of posts are useful to productionize your analysis.\n","date":"April 13, 2017","img":"/blog/2017-04-13-serverless-data-product-3/featured.png","lang":"en","langName":"English","largeImg":"/blog/2017-04-13-serverless-data-product-3/featured_hue5e1a794303feb35ce63690649b07766_173293_500x0_resize_box_3.png","permalink":"/blog/2017-04-13-serverless-data-product-3/","series":[{"title":"Serverless Data Product","url":"/series/serverless-data-product/"}],"smallImg":"/blog/2017-04-13-serverless-data-product-3/featured_hue5e1a794303feb35ce63690649b07766_173293_180x0_resize_box_3.png","tags":[{"title":"AWS","url":"/tags/aws/"},{"title":"AWS Lambda","url":"/tags/aws-lambda/"},{"title":"Amazon API Gateway","url":"/tags/amazon-api-gateway/"},{"title":"R","url":"/tags/r/"},{"title":"Python","url":"/tags/python/"}],"timestamp":1492041600,"title":"Serverless Data Product POC Backend Part III - Exposing R ML Model via APIG"},{"categories":[{"title":"Serverless","url":"/categories/serverless/"}],"content":"In the previous post, serverless event-driven application development is introduced. Also how to package R, necessary libraries/packages and a Lambda function handler is discussed. No need of provisioning/managing servers is one of the key benefits of the architecture. It is also a cost-effective way of delivering a data product as functions are executed on-demand rather than in servers that run 24/7. Furthermore AWS Lambda free tier includes 1M free requests per month and 400,000 GB-seconds of compute time per month, which is available to both existing and new AWS customers indefinitely. (GB-seconds is applicable when execution is made with 1 GB of memory.) Lowering the size of memory increases the execution time and thus 3.2M seconds or about 37 days are free with 128 MB of memory (1 GB divided by 8) - note that CPU power is proportional to allocated memory.\nInitially I was planning to discuss how to deploy a package at AWS Lambda and to expose it via Amazon API Gateway in this post. However it\u0026rsquo;d be too long with so many screenshots and I split them in Part II and III. Here is an updated series plan.\nBackend Part I - Packaging R ML Model for Lambda Part II - Deploying R ML Model via Lambda - this post Part III - Exposing R ML Model via APIG Frontend Part IV - Serving R ML Model via S3 [EDIT 2017-04-17] The Lambda function handler (handler.py) has been modified to resolve an issue of Cross-Origin Resource Sharing (CORS). See Part IV for further details.\nManaging security Before deploying a Lambda package, it is necessary to understand the security framework provided by AWS mostly based on AWS Identity and Access Management (IAM). To use AWS services (e.g. downloading a file from S3), AWS needs to identify the user or service (eg AWS Lambda) that makes the API call - this is the authentication. Also it needs to be checked whether the user or service has the permission - this is the authorization. For example, for this POC product development, I created a user and authentication is made by permanent credentials (Access Key ID and Secret Access Key) given to the user. Authorization is managed by policy and the following 3 AWS managed policies are attached to the user.\nAWSLambdaFullAccess AmazonAPIGatewayAdministrator AmazonCognitoPowerUser These policies define permissions to relevant AWS services such as Amazon S3, Amazon CloudWatch, AWS Lambda and Amazon API Gateway - note that they are user-based policies. The attached policies can be found in the Users section of IAM.\nUnlike users (and groups that can have one or more users with relevant permissions), AWS services (eg AWS Lambda) can assume a role, inheriting the permissions given to the role. Roles don\u0026rsquo;t have permanent credentials assigned to them but, when a service assumes a role, temporary credentials are assigned and they are used to authorize the service to do what\u0026rsquo;s defined in the (user-defined) policies attached to the role. Note that temporary credentials are made up of Access Key ID, Secret Access Key and Security Token and are generated by AWS Security Token Service (STS). In order to assume a role, a service needs another type of policy called trust policy, which defines who can assume a role. Therefore we need both types of policy so as to deploy the POC application at AWS Lambda.\nUser-based policy - to give permissions to AWS services Trust policy - to assume a role Note that this section is based on the Ch4 of AWS Lambda in Action and I find this book is quite a good material to learn AWS Lambda.\nCreating role and policy Before creating a role, it is necessary to create (user-based) policies so that the Lambda function can be given permissions to relevant AWS services. For the POC application, the function needs to download the model object (admission.rds) from S3 and to log messages to Amazon CloudWatch. For the former, a tailored policy (ServerlessPOC) is created while an AWS managed policy (AWSLambdaBasicExecutionRole) is used for the latter. In a policy, permissions are defined in statements and the main elements of statments are shown below.\nEffect - Allow or deny Action - What actions are in effect? Resource - On which resources? Principal - Who is allowed or denied access to a resource? (relevant to trust policies) ServerlessPOC is read-only permission to all keys in the serverless-poc-models bucket while AWSLambdaBasicExecutionRole is write permission to all resources in Amazon CloudWatch.\n1//ServerlessPOC 2{ 3 \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, 4 \u0026#34;Statement\u0026#34;: [ 5 { 6 \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, 7 \u0026#34;Action\u0026#34;: [ 8 \u0026#34;s3:GetObject\u0026#34; 9 ], 10 \u0026#34;Resource\u0026#34;: [ 11 \u0026#34;arn:aws:s3:::serverless-poc-models/*\u0026#34; 12 ] 13 } 14 ] 15} 16 17//AWSLambdaBasicExecutionRole 18{ 19 \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, 20 \u0026#34;Statement\u0026#34;: [ 21 { 22 \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, 23 \u0026#34;Action\u0026#34;: [ 24 \u0026#34;logs:CreateLogGroup\u0026#34;, 25 \u0026#34;logs:CreateLogStream\u0026#34;, 26 \u0026#34;logs:PutLogEvents\u0026#34; 27 ], 28 \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; 29 } 30 ] 31} Only ServerlessPOC needs to be created and it is created in AWS Console. In the Policies section of IAM, the last option (Create Your Own Policy) is selected as it is quite a simple policy.\nThen the name, description and policy document is completed - it is possible to validate the policy document by clicking the Validate Policy button.\nNow it is ready to create a role for the Lambda function while attaching the policies described above. A role can be created in the Roles section of IAM.\nStep 1 : Set Role Name Step 2 : Select Role Type Step 3 : Establish Trust Step 4 : Attach Policy Step 5 : Review Step 1 : Set Role Name This is simply setting the name of the role.\nStep 2 : Select Role Type AWS Lambda is selected in the AWS Service Roles group.\nStep 3 : Establish Trust This step passes automatically while the following trust policy is created, which allows to assume a role for AWS Lambda.\n1// Trust Relationship 2{ 3 \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, 4 \u0026#34;Statement\u0026#34;: [ 5 { 6 \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, 7 \u0026#34;Principal\u0026#34;: { 8 \u0026#34;Service\u0026#34;: \u0026#34;lambda.amazonaws.com\u0026#34; 9 }, 10 \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRole\u0026#34; 11 } 12 ] 13} Step 4 : Attach Policy Here ServerlessPOC and AWSLambdaBasicExecutionRole are attached to the role - it is possible to select multiple policies by checking tick boxes.\nStep 5 : Review After reviewing, it is possible to create the role.\nNow the role (ServerlessPOC) can be seen in the Roles section of IAM. The user-based and trust policies are found in the Permissions and Trust Relationships tabs respectively.\nDeployment Select blueprint Configure triggers Configure function Review Select blueprint Blueprints are sample configurations of event sources and Lambda functions. Blank Function is selected for the Lambda function of the POC application.\nConfigure triggers A Lambda function can be triggered by another AWS service and clicking the dashed box populates available services. No trigger is selected for the Lambda function of the POC application. Note that triggering a Lambda function by changes in another service is useful because an application can be structured in an event-driven way and there is no need to control everything in a big program/script.\nConfigure function Now it is time to configure the function. The function\u0026rsquo;s name (ServerlessPOCAdmission) and description are filled in followed by selecting the runtime - Python 2.7 is the only supported version of Python at the moment. Then code for the function needs to be provided and the following 3 options are avaialble.\nEdit code inline Upload a .ZIP file Upload a file from Amazon S3 As the deployment package (admission.zip) is copied to S3 already, the last option is selected and its S3 link URL is filled in.\nThe value of handler is set as handler.lambda_handler as the Lambda function handler is in handler.py and named as lambda_handler. Then the role named ServerlessPOC is chosen after selecting the value of role to be Choose an existing role among the options listed below.\nChoose an existing role Create new role from template(s) Create a custom role For memory and timeout, 128 MB and 6 seconds are chosen respectively. Note that they can be modified and the timeout is set back to the default 3 seconds while testing.\nThe remaining settings are left as they are.\nAfter clicking the Next button, it is possible to review and create the function.\nTesting and invoke API Testing Testing a Lambda function can be done on the Console. When clicking the Test button, an editor pops up and it is possible to enter an event. For the Lambda function, gre, gpa and rank are set to be \u0026ldquo;800\u0026rdquo;, \u0026ldquo;4\u0026rdquo; and \u0026ldquo;1\u0026rdquo; respectively. Testing can be done by clicking the Save and Test button at the bottom - not shown in the screen shot.\nThe testing output includes Execution result, Summary and Log output. With the event values specified earlier, it returns true. Note that the Python dictionary output of the hander function is coverted to JSON. Also an event specified as JSON is automatically converted to Python dictionary.\nAs shown in the summary, it took 658.93 ms to complete. If testing is made multiple times without delay, the duration decreases quite significantly to less than 200 ms thanks to container reuse in AWS Lambda - recall that the model object is not downloaded if it exists in /tmp. If latency is an issue, it is possible to call the function in every, let say, 5 minutes. As mentioned eariler, a Lambda function can be triggered by another AWS service and, if an Amazon CloudWatch event is set, the function call can be scheduled.\nInvoke API AWS Lambda provides the invoke API so that a Lambda function can be called programmatically as shown below.\n1$ aws lambda invoke --function-name ServerlessPOCAdmission \\ 2 --payload \u0026#39;{\u0026#34;gre\u0026#34;:\u0026#34;800\u0026#34;, \u0026#34;gpa\u0026#34;:\u0026#34;4\u0026#34;, \u0026#34;rank\u0026#34;:\u0026#34;1\u0026#34;}\u0026#39; output.txt 3{ 4 \u0026#34;StatusCode\u0026#34;: 200 5} 6 7$ cat output.txt | grep result 8{\u0026#34;result\u0026#34;: true} This is all for the part II. The POC application is successfully deployed at AWS Lambda and now it is ready to expose it via Amazon API Gateway. This will be discussed in the next post. I hope you enjoy reading this post.\n","date":"April 11, 2017","img":"/blog/2017-04-11-serverless-data-product-2/featured.png","lang":"en","langName":"English","largeImg":"/blog/2017-04-11-serverless-data-product-2/featured_huf27bf8f64e529c91b4fbc544b4ca4599_139725_500x0_resize_box_3.png","permalink":"/blog/2017-04-11-serverless-data-product-2/","series":[{"title":"Serverless Data Product","url":"/series/serverless-data-product/"}],"smallImg":"/blog/2017-04-11-serverless-data-product-2/featured_huf27bf8f64e529c91b4fbc544b4ca4599_139725_180x0_resize_box_3.png","tags":[{"title":"AWS","url":"/tags/aws/"},{"title":"AWS Lambda","url":"/tags/aws-lambda/"},{"title":"Amazon API Gateway","url":"/tags/amazon-api-gateway/"},{"title":"R","url":"/tags/r/"},{"title":"Python","url":"/tags/python/"}],"timestamp":1491868800,"title":"Serverless Data Product POC Backend Part II - Deploying R ML Model via Lambda"},{"categories":[{"title":"Serverless","url":"/categories/serverless/"}],"content":"Let say you\u0026rsquo;ve got a prediction model built in R and you\u0026rsquo;d like to productionize it, for example, by serving it in a web application. One way is exposing the model through an API that returns the predicted result as a web service. However there are many issues. Firstly R is not a language for API development although there may be some ways - eg the plumber package. More importantly developing an API is not the end of the story as the API can\u0026rsquo;t be served in a production system if it is not deployed/managed/upgraded/patched/\u0026hellip; appropriately in a server or if it is not scalable, protected via authentication/authorization and so on. Therefore it requires quite a vast range of skill sets that cover both development and DevOps (engineering).\nA developer can be relieved from the overwhelming DevOps stuff if his/her model is deployed in a serverless environment that is provided by cloud computing companies - Amazon Web Service, Microsoft Azure, Google Cloud Platform and IBM OpenWhisk. They provide FaaS (Function as a Service) and, simply put, it allows to run code on demand without provisioning or managing servers. Furthermore an application can be developed/managed in a more efficient way if the workflow is streamlined by events. Let say the model has to be updated periodically. It requires to save new raw data into a place, to export it to a database, to manipulate and save it back to another place for modelling\u0026hellip; This kind of workflow can be efficiently managed by events where a function is configured to subscribe a specific event and its code is run accordingly. In this regards, I find there is a huge potential for serverless event-driven architecture in data product development.\nThis is the first post of Serverless Data Product POC series and I\u0026rsquo;m planning to introduce a data product in a serverless environment. For the backend, a simple logistic regression model is packaged and tested for AWS Lambda - R is not included in Lambda runtime so that it is packaged and run via the Python rpy2 package. Then the model is deployed at AWS Lambda and the Lambda function is exposed via Amazon API Gateway. For the frontend, a simple single page application is served from Amazon S3.\nBackend Part I - Packaging R ML Model for Lambda - this post Part II - Deploying R ML Model via Lambda Part III - Exposing R ML Model via APIG Frontend Part IV - Serving R ML Model via S3 [EDIT 2017-04-11] Deploying at AWS Lambda and exposing via API Gateway are split into 2 posts (Part II and III).\n[EDIT 2017-04-17] The Lambda function hander (handler.py) has been modified to resolve an issue of Cross-Origin Resource Sharing (CORS). See Part IV for further details.\nModel The data is from the LOGIT REGRESSION - R DATA ANALYSIS EXAMPLES of UCLA: Statistical Consulting Group. It is hypothetical data about graduate school admission and has 3 featues (gre, gpa, rank) and 1 binary response (admit).\n1data \u0026lt;- read.csv(\u0026#34;http://www.ats.ucla.edu/stat/data/binary.csv\u0026#34;) 2data$rank \u0026lt;- as.factor(data$rank) 3summary(data) 1## admit gre gpa rank 2## Min. :0.0000 Min. :220.0 Min. :2.260 1: 61 3## 1st Qu.:0.0000 1st Qu.:520.0 1st Qu.:3.130 2:151 4## Median :0.0000 Median :580.0 Median :3.395 3:121 5## Mean :0.3175 Mean :587.7 Mean :3.390 4: 67 6## 3rd Qu.:1.0000 3rd Qu.:660.0 3rd Qu.:3.670 7## Max. :1.0000 Max. :800.0 Max. :4.000 GLM is fit to the data and the fitted object is saved as admission.rds. The choice of logistic regression is because it is included in the stats package, which is one of the default packages, and I\u0026rsquo;d like to have R as small as possible for this POC application. Note that AWS Lambda has limits in deployment package size (50MB compressed) so that it is important to keep a deployment package small - see AWS Lambda Limits for further details. Then the saved file is uploaded to S3 to a bucket named serverless-poc-models - the Lambda function handler will use this object for prediction as described in the next section.\n1fit \u0026lt;- glm(admit ~ ., data = data, family = \u0026#34;binomial\u0026#34;) 2saveRDS(fit, \u0026#34;admission.rds\u0026#34;) Note that, if data is transformed for better performance, a model object alone may not be sufficient as transformed records are necessary as well. A way to handle this situation is using the caret package. The package has preProcess() and associating predict() so that a separate object can be created to transform records for prediction - see this page for further details.\nLambda function handler Lambda function handler is a function that AWS Lambda can invoke when the service executes the code. In this example, it downloads the model objects from S3, predicts admission status and returns the result - handler.py and test_handler.py can be found in the GitHub repository.\nThis and the next sections are based on the following posts with necessary modifications.\nAnalyzing Genomics Data at Scale using R, AWS Lambda, and Amazon API Gateway Run ML predictions with R on AWS Lambda handler.py begins with importing packages and setting-up environment variables. The above posts indicate C shared libraries of R must be loaded. When I tested the handler while uncommenting the for-loop of loading those libraries, however, I encountered the following error - OSError: lib/libRrefblas.so: undefined symbol: xerbla_. It is only when the for-loop is commented out that the script runs through to the handler. I guess the necessary C shared libraries are loaded via Lambda environment variables although I\u0026rsquo;m not sure why manual loading creates such an error. According to Lambda Execution Environment and Available Libraries, the following environment variables are available.\nLAMBDA_TASK_ROOT - Contains the path to your Lambda function code. LD_LIBRARY_PATH - Contains /lib64, /usr/lib64, LAMBDA_TASK_ROOT, LAMBDA_TASK_ROOT/lib. Used to store helper libraries and function code. As can be seen in the next section, the shared libraries are saved in LAMBDA_TASK_ROOT/lib so that they are loaded appropriately.\n1import ctypes 2import json 3import os 4import boto3 5import logging 6 7# use python logging module to log to CloudWatch 8# http://docs.aws.amazon.com/lambda/latest/dg/python-logging.html 9logging.getLogger().setLevel(logging.DEBUG) 10 11################### load R 12# must load all shared libraries and set the 13# R environment variables before you can import rpy2 14# load R shared libraries from lib dir 15 16# for file in os.listdir(\u0026#39;lib\u0026#39;): 17# if os.path.isfile(os.path.join(\u0026#39;lib\u0026#39;, file)): 18# ctypes.cdll.LoadLibrary(os.path.join(\u0026#39;lib\u0026#39;, file)) 19# 20# # set R environment variables 21os.environ[\u0026#34;R_HOME\u0026#34;] = os.getcwd() 22os.environ[\u0026#34;R_LIBS\u0026#34;] = os.path.join(os.getcwd(), \u0026#39;site-library\u0026#39;) 23 24# windows only 25# os.environ[\u0026#34;R_USER\u0026#34;] = r\u0026#39;C:\\Users\\jaehyeon\u0026#39; 26 27import rpy2 28from rpy2 import robjects 29from rpy2.robjects import r 30################## end of loading R Then 3 functions are defined as following.\nget_file_path - Given a S3 object key, it returns a file name or file path. Note that only /tmp has write-access so that a file should be downloaded to this folder download_file - Given bucket and key names, it downloads the S3 object having the key (eg admission.rds). Note that it does nothing if the object file already exists pred_admit - Given gre, gpa and rank, it returns True or False depending on the predicted probablity 1BUCKET = \u0026#39;serverless-poc-models\u0026#39; 2KEY = \u0026#39;admission.rds\u0026#39; 3s3 = boto3.client(\u0026#39;s3\u0026#39;) 4 5def get_file_path(key, name_only=True): 6 file_name = key.split(\u0026#39;/\u0026#39;)[len(key.split(\u0026#39;/\u0026#39;))-1] 7 if name_only: 8 return file_name 9 else: 10 return \u0026#39;/tmp/\u0026#39; + file_name 11 12def download_file(bucket, key): 13 # caching strategies used to avoid the download of the model file every time from S3 14 file_name = get_file_path(key, name_only=True) 15 file_path = get_file_path(key, name_only=False) 16 if os.path.isfile(file_path): 17 logging.debug(\u0026#39;{} already downloaded\u0026#39;.format(file_name)) 18 return 19 else: 20 logging.debug(\u0026#39;attempt to download model object to {}\u0026#39;.format(file_path)) 21 try: 22 s3.download_file(bucket, key, file_path) 23 except Exception as e: 24 logging.error(\u0026#39;error downloading key {} from bucket {}\u0026#39;.format(key, bucket)) 25 logging.error(e) 26 raise e 27 28def pred_admit(gre, gpa, rnk, bucket=BUCKET, key=KEY): 29 download_file(bucket, key) 30 r.assign(\u0026#39;gre\u0026#39;, gre) 31 r.assign(\u0026#39;gpa\u0026#39;, gpa) 32 r.assign(\u0026#39;rank\u0026#39;, rnk) 33 mod_path = get_file_path(key, name_only=False) 34 r(\u0026#39;fit \u0026lt;- readRDS(\u0026#34;{}\u0026#34;)\u0026#39;.format(mod_path)) 35 r(\u0026#39;newdata \u0026lt;- data.frame(gre=as.numeric(gre),gpa=as.numeric(gpa),rank=rank)\u0026#39;) 36 r(\u0026#39;pred \u0026lt;- predict(fit, newdata=newdata, type=\u0026#34;response\u0026#34;)\u0026#39;) 37 return robjects.r(\u0026#39;pred\u0026#39;)[0] \u0026gt; 0.5 This is the Lambda function handler for this POC application. It returns a prediction result if there is no error. 400 HTTP error will be returned if there is an error.\n1def lambda_handler(event, context): 2 try: 3 gre = event[\u0026#34;gre\u0026#34;] 4 gpa = event[\u0026#34;gpa\u0026#34;] 5 rnk = event[\u0026#34;rank\u0026#34;] 6 can_be_admitted = pred_admit(gre, gpa, rnk) 7 res = {\u0026#34;result\u0026#34;: can_be_admitted} 8 return res 9 except Exception as e: 10 logging.error(\u0026#39;Payload: {0}\u0026#39;.format(event)) 11 logging.error(\u0026#39;Error: {0}\u0026#39;.format(e.message)) 12 err = { 13 \u0026#39;errorType\u0026#39;: type(e).__name__, 14 \u0026#39;httpStatus\u0026#39;: 400, 15 \u0026#39;request_id\u0026#39;: context.aws_request_id, 16 \u0026#39;message\u0026#39;: e.message.replace(\u0026#39;\\n\u0026#39;, \u0026#39; \u0026#39;) 17 } 18 raise Exception(json.dumps(err)) Optionally code of the test handler is shown below.\n1import unittest 2import handler 3 4class AdmitHandlerTest(unittest.TestCase): 5 def test_admit(self): 6 gre = \u0026#39;800\u0026#39; 7 gpa = \u0026#39;4\u0026#39; 8 ranks = {\u0026#39;1\u0026#39;: True, \u0026#39;4\u0026#39;: False} 9 for rnk in ranks.keys(): 10 self.assertEqual(handler.pred_admit(gre, gpa, rnk), ranks.get(rnk)) 11 12if __name__ == \u0026#34;__main__\u0026#34;: 13 unittest.main() Packaging According to Lambda Execution Environment and Available Libraries, Lambda functions run in AMI name: amzn-ami-hvm-2016.03.3.x86_64-gp2. A t2.medium EC2 instance is used from this AMI to create the Lambda deployment package. In order to use R in AWS Lambda, R, some of its C shared libraries, the Lambda function handler (handler.py) and the handler\u0026rsquo;s dependent packages should be included in a zip deployment package file.\nPreparation In this step, R and necessary libraries are installed followed by cloning the project repository. The package folder is created as $HOME/$HANDLER (i.e. /home/ec2-user/handler). The subfolder $HOME/$HANDLER/library is to copy necessary R default packages separately - remind that I\u0026rsquo;d like to have R as small as possible.\n1sudo yum -y update 2sudo yum -y upgrade 3 4# readline for rpy2 and fortran for R 5sudo yum install -y python27-devel python27-pip gcc gcc-c++ readline-devel libgfortran.x86_64 R.x86_64 6 7# install Git and clone repository 8sudo yum install -y git 9git clone https://github.com/jaehyeon-kim/serverless-poc.git 10 11# create folder to R and lambda handler 12# note R packages will be copied to $HOME/$HANDLER/library separately 13export HANDLER=handler 14mkdir -p $HOME/$HANDLER/library Copy R and shared libraries Firstly all files and folders in /usr/lib64/R except for library are copyed to the Lambda package folder. By default 29 packages are installed as can be seen in /usr/lib64/R/library but not all them are necessary. Actually only the 7 packages listed below are loaded at startup and 1 package is required additionally by the rpy2 package. Therefore only the 8 default R packages are copyed to $HOME/$HANDLER/library/.\nLoaded at startup - stats, graphics, grDevices, utils, datasets, methods, base Required by rpy2 - tools In relation to C shared libraries, the default installation includes 4 libraries as can be checked in /usr/lib64/R/lib and 1 library is required additionally by the rpy2 package - this additional library is for regex processing.\nDefault C shared libraries - libRblas.so, libRlapack.so, libRrefblas.so, libR.so Required by rpy2 - libtre.so.5 Together with the above 5 shared libraries, the following 3 libraries are added: libgomp.so.1, libgfortran.so.3 and libquadmath.so.0. Further investigation is necessary to what extent these are used. Note that the above posts inclue 2 libraries for linear algebra (libblas.so.3 and liblapack.so.3) but they are not added as equivalent libraries seem to exist - I guess they are necessary if R is build from source with the following options: \u0026ndash;with-blas and \u0026ndash;with-lapack. A total of 8 C shared libraries are added to the deployment package.\n1# copy R except for packages - minimum R packages are copied separately 2ls /usr/lib64/R | grep -v library | xargs -I \u0026#39;{}\u0026#39; cp -vr /usr/lib64/R/\u0026#39;{}\u0026#39; $HOME/$HANDLER/ 3 4# copy minimal default libraries 5# loaded at R startup - stats, graphics, grDevices, utils, datasets, methods and base 6# needed for Rpy2 - tools 7ls /usr/lib64/R/library | grep \u0026#39;stats$\\|graphics\\|grDevices\\|utils\\|datasets\\|methods\\|base\\|^tools\u0026#39; | \\ 8\txargs -I \u0026#39;{}\u0026#39; cp -vr /usr/lib64/R/library/\u0026#39;{}\u0026#39; $HOME/$HANDLER/library/ 9 10# copy shared libraries 11ldd /usr/lib64/R/bin/exec/R | grep \u0026#34;=\u0026gt; /\u0026#34; | awk \u0026#39;{print $3}\u0026#39; | \\ 12\tgrep \u0026#39;libgomp.so.1\\|libgfortran.so.3\\|libquadmath.so.0\\|libtre.so.5\u0026#39; | \\ 13\txargs -I \u0026#39;{}\u0026#39; cp -v \u0026#39;{}\u0026#39; $HOME/$HANDLER/lib/ Install rpy2 and copy to Lamdba package folder Python virtualenv is used to install the rpy2 package. The idea is straightforward but actually it was a bit tricky as the rpy2 and its dependent packages can be found in either site-packages or dist-packages folder even in a single EC2 instance - the AWS Doc doesn\u0026rsquo;t explain clearly. pip install rpy2 -t folder-path was tricky as well because the rpy2 package was not installed sometimes while its dependent packages were installed. One way to check is executing pip list in the virtualenv and, if the rpy2 package is not shown, it is in dist-packages.\n1virtualenv ~/env \u0026amp;\u0026amp; source ~/env/bin/activate 2pip install rpy2 3# either in site-packages or dist-packages 4# http://docs.aws.amazon.com/lambda/latest/dg/lambda-python-how-to-create-deployment-package.html 5export PY_PACK=dist-packages 6cp -vr $VIRTUAL_ENV/lib64/python2.7/$PY_PACK/rpy2* $HOME/$HANDLER 7cp -vr $VIRTUAL_ENV/lib/python2.7/$PY_PACK/singledispatch* $HOME/$HANDLER 8cp -vr $VIRTUAL_ENV/lib/python2.7/$PY_PACK/six* $HOME/$HANDLER 9deactivate Copy handler.py/test_handler.py, compress and copy to S3 bucket handler.py and test_handler.py are copied to the Lambda package folder and all contents in the folder are compressed. Note that handler.py should exist in the root of the compressed file so that it is necessary to run zip in the deployment package folder. The size of admission.zip is about 27MB so that it is good to deploy. Finally the package file is copied to a S3 bucket called serverless-poc-handlers - note that the aws cli should be configured to copy the file to S3.\n1cp -v serverless-poc/poc-logit-handler/*.py $HOME/$HANDLER 2 3cd $HOME/$HANDLER 4zip -r9 $HOME/admission.zip * 5# du -sh ~/admission.zip # check file size 6# 27M /home/ec2-user/admission.zip 7 8# aws s3 mb s3://serverless-poc-handlers # create bucket 9aws s3 cp $HOME/admission.zip s3://serverless-poc-handlers Testing For testing, an EC2 instance without R is necessary so that testing is made in a separate t2.micro instance from the same AMI. Configure the aws-cli and install the boto3 package - the boto3 package is avaialble in Lambda execution environment so that it doesn\u0026rsquo;t need to be added to the deployment package. LD_LIBRARY_PATH is an environment variable that points to the C shared libraries of the Lambda package. After downloading the package and decompressing it, testing can be made by running test_handler.py.\n1# configure aws-cli if necessary 2 3sudo pip install boto3 4export R_HOME=$HOME 5export LD_LIBRARY_PATH=$HOME/lib 6 7aws s3 cp s3://serverless-poc-handlers/admission.zip . 8unzip admission.zip 9 10python ./test_handler.py This is an example testing output where the model object has been downloaded already.\n1[ec2-user@ip-172-31-71-13 ~]$ python ./test_handler.py 2DEBUG:root:admission.rds already downloaded 3DEBUG:root:admission.rds already downloaded 4. 5---------------------------------------------------------------------- 6Ran 1 test in 0.024s 7 8OK This is all that I\u0026rsquo;ve prepared for this post and I hope you don\u0026rsquo;t feel bored. The next posts will be much more interesting as this package will be exposed via an API.\n","date":"April 8, 2017","img":"/blog/2017-04-08-serverless-data-product-1/featured.png","lang":"en","langName":"English","largeImg":"/blog/2017-04-08-serverless-data-product-1/featured_huf27bf8f64e529c91b4fbc544b4ca4599_139725_500x0_resize_box_3.png","permalink":"/blog/2017-04-08-serverless-data-product-1/","series":[{"title":"Serverless Data Product","url":"/series/serverless-data-product/"}],"smallImg":"/blog/2017-04-08-serverless-data-product-1/featured_huf27bf8f64e529c91b4fbc544b4ca4599_139725_180x0_resize_box_3.png","tags":[{"title":"AWS","url":"/tags/aws/"},{"title":"AWS Lambda","url":"/tags/aws-lambda/"},{"title":"Amazon API Gateway","url":"/tags/amazon-api-gateway/"},{"title":"R","url":"/tags/r/"},{"title":"Python","url":"/tags/python/"}],"timestamp":1491609600,"title":"Serverless Data Product POC Backend Part I - Packaging R ML Model for Lambda"},{"categories":[],"content":"","date":"January 1, 1","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/contact/","series":[],"smallImg":"","tags":[],"timestamp":-62135596800,"title":"Contact Me"},{"categories":[],"content":"","date":"January 1, 1","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/offline/","series":[],"smallImg":"","tags":[],"timestamp":-62135596800,"title":"Offline"},{"categories":[],"content":"","date":"January 1, 1","img":"","lang":"zh-cn","langName":"","largeImg":"","permalink":"/zh-cn/offline/","series":[],"smallImg":"","tags":[],"timestamp":-62135596800,"title":"Offline"},{"categories":[],"content":"","date":"January 1, 1","img":"","lang":"zh-cn","langName":"","largeImg":"","permalink":"/zh-cn/contact/","series":[],"smallImg":"","tags":[],"timestamp":-62135596800,"title":"联系我们"}]
