[{"categories":[{"title":"Apache Beam","url":"/categories/apache-beam/"}],"content":"In this series, we develop Apache Beam Python pipelines. The majority of them are from Building Big Data Pipelines with Apache Beam by Jan Lukavský. Mainly relying on the Java SDK, the book teaches fundamentals of Apache Beam using hands-on tasks, and we convert those tasks using the Python SDK. We focus on streaming pipelines, and they are deployed on a local (or embedded) Apache Flink cluster using the Apache Flink Runner. Beginning with setting up the development environment, we build two pipelines that obtain top K most frequent words and the word that has the longest word length in this post.\nPart 1 Calculate K Most Frequent Words and Max Word Length (this post) Part 2 Calculate Average Word Length with/without Fixed Look back Part 3 Build Sport Activity Tracker with/without SQL Part 4 Call RPC Service for Data Augmentation Part 5 Call RPC Service in Batch using Stateless DoFn Part 6 Call RPC Service in Batch with Defined Batch Size using Stateful DoFn Part 7 Separate Droppable Data into Side Output Part 8 Enhance Sport Activity Tracker with Runner Motivation Part 9 Develop Batch File Reader and PiSampler using Splittable DoFn Part 10 Develop Streaming File Reader using Splittable DoFn Development Environment The development environment requires an Apache Flink cluster, Apache Kafka cluster and gRPC Server. For Flink, we can use either an embedded cluster or a local cluster while Docker Compose is used for the rest. The source of this post can be found in this GitHub repository.\nFlink Cluster To set up a local Flink cluster, we should download a supported Flink release (e.g. 1.16.3, 1.17.2 or 1.18.1) - we use Flink 1.18.1 with Apache Beam 2.57.0 in this series. Once downloaded, we need to update the Flink configuration file, for example, to enable the Flink UI and to make the Flink binaries to be executable. The steps can be found below.\n1mkdir -p setup \u0026amp;\u0026amp; cd setup 2## 1. download flink binary and decompress in the same folder 3FLINK_VERSION=1.18.1 # change flink version eg) 1.16.3, 1.17.2, 1.18.1 ... 4wget https://dlcdn.apache.org/flink/flink-${FLINK_VERSION}/flink-${FLINK_VERSION}-bin-scala_2.12.tgz 5tar -zxf flink-${FLINK_VERSION}-bin-scala_2.12.tgz 6## 2. update flink configuration in eg) ./flink-${FLINK_VERSION}/conf/flink-conf.yaml 7## rest.port: 8081 # uncommented 8## rest.address: localhost # kept as is 9## rest.bind-address: 0.0.0.0 # changed from localhost 10## taskmanager.numberOfTaskSlots: 10 # updated from 1 11## 3. make flink binaries to be executable 12chmod -R +x flink-${FLINK_VERSION}/bin Kafka Cluster A Kafka cluster with 1 broker and 1 Zookeeper node is used together with a Kafka management app (kafka-ui). The details of setting up the resources can be found in my Kafka Development with Docker series: Part 1 Cluster Setup and Part 2 Management App.\nThose resources are deployed using Docker Compose with the following configuration file.\n1# setup/docker-compose.yml 2version: \u0026#34;3.5\u0026#34; 3 4services: 5 zookeeper: 6 image: bitnami/zookeeper:3.5 7 container_name: zookeeper 8 expose: 9 - 2181 10 networks: 11 - appnet 12 environment: 13 - ALLOW_ANONYMOUS_LOGIN=yes 14 volumes: 15 - zookeeper_data:/bitnami/zookeeper 16 kafka-0: 17 image: bitnami/kafka:2.8.1 18 container_name: kafka-0 19 expose: 20 - 9092 21 ports: 22 - \u0026#34;29092:29092\u0026#34; 23 networks: 24 - appnet 25 environment: 26 - ALLOW_PLAINTEXT_LISTENER=yes 27 - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181 28 - KAFKA_CFG_BROKER_ID=0 29 - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT 30 - KAFKA_CFG_LISTENERS=INTERNAL://:9092,EXTERNAL://:29092 31 - KAFKA_CFG_ADVERTISED_LISTENERS=INTERNAL://kafka-0:9092,EXTERNAL://localhost:29092 32 - KAFKA_CFG_INTER_BROKER_LISTENER_NAME=INTERNAL 33 - KAFKA_CFG_NUM_PARTITIONS=3 34 - KAFKA_CFG_DEFAULT_REPLICATION_FACTOR=1 35 volumes: 36 - kafka_0_data:/bitnami/kafka 37 depends_on: 38 - zookeeper 39 kafka-ui: 40 image: provectuslabs/kafka-ui:v0.7.1 41 container_name: kafka-ui 42 ports: 43 - \u0026#34;8080:8080\u0026#34; 44 networks: 45 - appnet 46 environment: 47 KAFKA_CLUSTERS_0_NAME: local 48 KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka-0:9092 49 KAFKA_CLUSTERS_0_ZOOKEEPER: zookeeper:2181 50 depends_on: 51 - zookeeper 52 - kafka-0 53 54networks: 55 appnet: 56 name: app-network 57 58volumes: 59 zookeeper_data: 60 driver: local 61 name: zookeeper_data 62 kafka_0_data: 63 driver: local 64 name: kafka_0_data Note that the Kafka bootstrap server is accessible on port 29092 outside the Docker network, and it can be accessed on localhost:29092 from the Docker host machine and on host.docker.internal:29092 from a Docker container that is launched with the host network. We use both types of the bootstrap server address - the former is used by a Kafka producer app that is discussed later and the latter by a Java IO expansion service, which is launched in a Docker container. Note further that, for the latter to work, we have to update the /etc/hosts file by adding an entry for host.docker.internal as shown below.\n1cat /etc/hosts | grep host.docker.internal 2# 127.0.0.1 host.docker.internal gRPC Server We develop several pipelines that call an external RPC service for data enrichment in later posts. A gRPC server is required for those pipelines, and it is created using Docker Compose. The details of developing the underlying service will be discussed in Part 4.\n1# setup/docker-compose-grpc.yml 2version: \u0026#34;3.5\u0026#34; 3 4services: 5 grpc-server: 6 build: 7 context: . 8 image: grpc-server 9 container_name: grpc-server 10 command: [\u0026#34;python\u0026#34;, \u0026#34;/app/server.py\u0026#34;] 11 ports: 12 - 50051:50051 13 networks: 14 - appnet 15 environment: 16 PYTHONUNBUFFERED: \u0026#34;1\u0026#34; 17 INSECURE_PORT: 0.0.0.0:50051 18 volumes: 19 - ../chapter3:/app 20 21networks: 22 appnet: 23 name: grpc-network Manage Resources The Flink and Kafka clusters and gRPC server are managed by bash scripts. Those scripts accept four flags: -f, -k and -g to start/stop individual resources or -a to manage all of them. We can add multiple flags to start/stop multiple resources. Note that the scripts assume Flink 1.18.1 by default, and we can specify a specific Flink version if it is different from it e.g. FLINK_VERSION=1.17.2 ./setup/start-flink-env.sh.\n1# setup/start-flink-env.sh 2#!/usr/bin/env bash 3 4while [[ \u0026#34;$#\u0026#34; -gt 0 ]]; do 5 case $1 in 6 -k|--kafka) start_kafka=true;; 7 -f|--flink) start_flink=true;; 8 -g|--grpc) start_grpc=true;; 9 -a|--all) start_all=true;; 10 *) echo \u0026#34;Unknown parameter passed: $1\u0026#34;; exit 1 ;; 11 esac 12 shift 13done 14 15if [ ! -z $start_all ] \u0026amp;\u0026amp; [ $start_all = true ]; then 16 start_kafka=true 17 start_flink=true 18 start_grpc=true 19fi 20 21SCRIPT_DIR=$(dirname \u0026#34;$(readlink -f \u0026#34;$0\u0026#34;)\u0026#34;) 22 23#### start kafka cluster in docker 24if [ ! -z $start_kafka ] \u0026amp;\u0026amp; [ $start_kafka = true ]; then 25 echo \u0026#34;start kafka...\u0026#34; 26 docker-compose -f ${SCRIPT_DIR}/docker-compose.yml up -d 27fi 28 29#### start grpc server in docker 30if [ ! -z $start_grpc ] \u0026amp;\u0026amp; [ $start_grpc = true ]; then 31 echo \u0026#34;start grpc server...\u0026#34; 32 docker-compose -f ${SCRIPT_DIR}/docker-compose-grpc.yml up -d 33fi 34 35#### start local flink cluster 36if [ ! -z $start_flink ] \u0026amp;\u0026amp; [ $start_flink = true ]; then 37 FLINK_VERSION=${FLINK_VERSION:-1.18.1} 38 echo \u0026#34;start flink ${FLINK_VERSION}...\u0026#34; 39 ${SCRIPT_DIR}/flink-${FLINK_VERSION}/bin/start-cluster.sh 40fi The teardown script stops applicable resources followed by removing all stopped containers. Comment out the container prune command if necessary (docker container prune -f).\n1# setup/stop-flink-env.sh 2#!/usr/bin/env bash 3 4while [[ \u0026#34;$#\u0026#34; -gt 0 ]]; do 5 case $1 in 6 -k|--kafka) stop_kafka=true;; 7 -f|--flink) stop_flink=true;; 8 -g|--grpc) stop_grpc=true;; 9 -a|--all) stop_all=true;; 10 *) echo \u0026#34;Unknown parameter passed: $1\u0026#34;; exit 1 ;; 11 esac 12 shift 13done 14 15if [ ! -z $stop_all ] \u0026amp;\u0026amp; [ $stop_all = true ]; then 16 stop_kafka=true 17 stop_flink=true 18 stop_grpc=true 19fi 20 21SCRIPT_DIR=$(dirname \u0026#34;$(readlink -f \u0026#34;$0\u0026#34;)\u0026#34;) 22 23#### stop kafka cluster in docker 24if [ ! -z $stop_kafka ] \u0026amp;\u0026amp; [ $stop_kafka = true ]; then 25 echo \u0026#34;stop kafka...\u0026#34; 26 docker-compose -f ${SCRIPT_DIR}/docker-compose.yml down -v 27fi 28 29#### stop grpc server in docker 30if [ ! -z $stop_grpc ] \u0026amp;\u0026amp; [ $stop_grpc = true ]; then 31 echo \u0026#34;stop grpc server...\u0026#34; 32 docker-compose -f ${SCRIPT_DIR}/docker-compose-grpc.yml down 33fi 34 35#### stop local flink cluster 36if [ ! -z $stop_flink ] \u0026amp;\u0026amp; [ $stop_flink = true ]; then 37 FLINK_VERSION=${FLINK_VERSION:-1.18.1} 38 echo \u0026#34;stop flink ${FLINK_VERSION}...\u0026#34; 39 ${SCRIPT_DIR}/flink-${FLINK_VERSION}/bin/stop-cluster.sh 40fi 41 42#### remove all stopped containers 43docker container prune -f Below shows how to start resources using the start-up script. We need to launch both the Flink and Kafka clusters if we deploy a Beam pipeline on a local Flink cluster. Otherwise, we can start the Kafka cluster only.\n1## start a local flink can kafka cluster 2./setup/start-flink-env.sh -f -k 3# start kafka... 4# [+] Running 6/6 5# ⠿ Network app-network Created 0.0s 6# ⠿ Volume \u0026#34;zookeeper_data\u0026#34; Created 0.0s 7# ⠿ Volume \u0026#34;kafka_0_data\u0026#34; Created 0.0s 8# ⠿ Container zookeeper Started 0.3s 9# ⠿ Container kafka-0 Started 0.5s 10# ⠿ Container kafka-ui Started 0.8s 11# start flink 1.18.1... 12# Starting cluster. 13# Starting standalonesession daemon on host \u0026lt;hostname\u0026gt;. 14# Starting taskexecutor daemon on host \u0026lt;hostname\u0026gt;. 15 16## start a local kafka cluster only 17./setup/start-flink-env.sh -k 18# start kafka... 19# [+] Running 6/6 20# ⠿ Network app-network Created 0.0s 21# ⠿ Volume \u0026#34;zookeeper_data\u0026#34; Created 0.0s 22# ⠿ Volume \u0026#34;kafka_0_data\u0026#34; Created 0.0s 23# ⠿ Container zookeeper Started 0.3s 24# ⠿ Container kafka-0 Started 0.5s 25# ⠿ Container kafka-ui Started 0.8s Kafka Text Producer We have two Kafka text producers as many pipelines read/process text messages from a Kafka topic. The first producer sends fake text from the Faker package. We can run the producer simply by executing the producer script.\n1# utils/faker_gen.py 2import time 3import argparse 4 5from faker import Faker 6from producer import TextProducer 7 8if __name__ == \u0026#34;__main__\u0026#34;: 9 parser = argparse.ArgumentParser(__file__, description=\u0026#34;Fake Text Data Generator\u0026#34;) 10 parser.add_argument( 11 \u0026#34;--bootstrap_servers\u0026#34;, 12 \u0026#34;-b\u0026#34;, 13 type=str, 14 default=\u0026#34;localhost:29092\u0026#34;, 15 help=\u0026#34;Comma separated string of Kafka bootstrap addresses\u0026#34;, 16 ) 17 parser.add_argument( 18 \u0026#34;--topic_name\u0026#34;, 19 \u0026#34;-t\u0026#34;, 20 type=str, 21 default=\u0026#34;input-topic\u0026#34;, 22 help=\u0026#34;Kafka topic name\u0026#34;, 23 ) 24 parser.add_argument( 25 \u0026#34;--delay_seconds\u0026#34;, 26 \u0026#34;-d\u0026#34;, 27 type=float, 28 default=1, 29 help=\u0026#34;The amount of time that a record should be delayed.\u0026#34;, 30 ) 31 args = parser.parse_args() 32 33 producer = TextProducer(args.bootstrap_servers, args.topic_name) 34 fake = Faker() 35 36 num_events = 0 37 while True: 38 num_events += 1 39 text = fake.text() 40 producer.send_to_kafka(text) 41 if num_events % 10 == 0: 42 print(f\u0026#34;{num_events} text sent. current - {text}\u0026#34;) 43 time.sleep(args.delay_seconds) The second producer accepts user inputs, and it can be used for controlling text messages as well as message creation timestamps. The latter feature is useful for testing late arrival data.\n1# utils/interactive_gen.py 2import re 3import time 4import argparse 5 6from producer import TextProducer 7 8 9def get_digit(shift: str, pattern: str = None): 10 try: 11 return int(re.sub(pattern, \u0026#34;\u0026#34;, shift).strip()) 12 except (TypeError, ValueError): 13 return 1 14 15 16def get_ts_shift(shift: str): 17 current = int(time.time() * 1000) 18 multiplier = 1 19 if shift.find(\u0026#34;m\u0026#34;) \u0026gt; 0: 20 multiplier = 60 * 1000 21 digit = get_digit(shift, r\u0026#34;m.+\u0026#34;) 22 elif shift.find(\u0026#34;s\u0026#34;) \u0026gt; 0: 23 multiplier = 1000 24 digit = get_digit(shift, r\u0026#34;s.+\u0026#34;) 25 else: 26 digit = get_digit(shift) 27 return { 28 \u0026#34;current\u0026#34;: current, 29 \u0026#34;shift\u0026#34;: int(digit) * multiplier, 30 \u0026#34;shifted\u0026#34;: current + int(digit) * multiplier, 31 } 32 33 34def parse_user_input(user_input: str): 35 if len(user_input.split(\u0026#34;;\u0026#34;)) == 2: 36 shift, text = tuple(user_input.split(\u0026#34;;\u0026#34;)) 37 shift_info = get_ts_shift(shift) 38 msg = \u0026#34; | \u0026#34;.join( 39 [f\u0026#34;{k}: {v}\u0026#34; for k, v in {**{\u0026#34;text\u0026#34;: text.strip()}, **shift_info}.items()] 40 ) 41 print(f\u0026#34;\u0026gt;\u0026gt; {msg}\u0026#34;) 42 return {\u0026#34;text\u0026#34;: text.strip(), \u0026#34;timestamp_ms\u0026#34;: shift_info[\u0026#34;shifted\u0026#34;]} 43 print(f\u0026#34;\u0026gt;\u0026gt; text: {user_input}\u0026#34;) 44 return {\u0026#34;text\u0026#34;: user_input} 45 46 47if __name__ == \u0026#34;__main__\u0026#34;: 48 parser = argparse.ArgumentParser( 49 __file__, description=\u0026#34;Interactive Text Data Generator\u0026#34; 50 ) 51 parser.add_argument( 52 \u0026#34;--bootstrap_servers\u0026#34;, 53 \u0026#34;-b\u0026#34;, 54 type=str, 55 default=\u0026#34;localhost:29092\u0026#34;, 56 help=\u0026#34;Comma separated string of Kafka bootstrap addresses\u0026#34;, 57 ) 58 parser.add_argument( 59 \u0026#34;--topic_name\u0026#34;, 60 \u0026#34;-t\u0026#34;, 61 type=str, 62 default=\u0026#34;input-topic\u0026#34;, 63 help=\u0026#34;Kafka topic name\u0026#34;, 64 ) 65 args = parser.parse_args() 66 67 producer = TextProducer(args.bootstrap_servers, args.topic_name) 68 69 while True: 70 user_input = input(\u0026#34;ENTER TEXT: \u0026#34;) 71 args = parse_user_input(user_input) 72 producer.send_to_kafka(**args) By default, the message creation timestamp is not shifted. We can do so by appending shift details (digit and scale) (e.g. 1 minute or 10 sec). Below shows some usage examples.\n1python utils/interactive_gen.py 2# ENTER TEXT: By default, message creation timestamp is not shifted! 3# \u0026gt;\u0026gt; text: By default, message creation timestamp is not shifted! 4# ENTER TEXT: -10 sec;Add time digit and scale to shift back. should be separated by semi-colon 5# \u0026gt;\u0026gt; text: Add time digit and scale to shift back. should be separated by semi-colon | current: 1719891195495 | shift: -10000 | shifted: 1719891185495 6# ENTER TEXT: -10s;We don\u0026#39;t have to keep a blank. 7# \u0026gt;\u0026gt; text: We don\u0026#39;t have to keep a blank. | current: 1719891203698 | shift: 1000 | shifted: 1719891204698 8# ENTER TEXT: 1 min;We can shift to the future but is it realistic? 9# \u0026gt;\u0026gt; text: We can shift to the future but is it realistic? | current: 1719891214039 | shift: 60000 | shifted: 1719891274039 Both the producer apps send text messages using the following Kafka producer class.\n1# utils/producer.py 2from kafka import KafkaProducer 3 4 5class TextProducer: 6 def __init__(self, bootstrap_servers: list, topic_name: str) -\u0026gt; None: 7 self.bootstrap_servers = bootstrap_servers 8 self.topic_name = topic_name 9 self.kafka_producer = self.create_producer() 10 11 def create_producer(self): 12 \u0026#34;\u0026#34;\u0026#34; 13 Returns a KafkaProducer instance 14 \u0026#34;\u0026#34;\u0026#34; 15 return KafkaProducer( 16 bootstrap_servers=self.bootstrap_servers, 17 value_serializer=lambda v: v.encode(\u0026#34;utf-8\u0026#34;), 18 ) 19 20 def send_to_kafka(self, text: str, timestamp_ms: int = None): 21 \u0026#34;\u0026#34;\u0026#34; 22 Sends text to a Kafka topic. 23 \u0026#34;\u0026#34;\u0026#34; 24 try: 25 args = {\u0026#34;topic\u0026#34;: self.topic_name, \u0026#34;value\u0026#34;: text} 26 if timestamp_ms is not None: 27 args = {**args, **{\u0026#34;timestamp_ms\u0026#34;: timestamp_ms}} 28 self.kafka_producer.send(**args) 29 self.kafka_producer.flush() 30 except Exception as e: 31 raise RuntimeError(\u0026#34;fails to send a message\u0026#34;) from e Beam Pipelines We develop two pipelines that obtain top K most frequent words and the word that has the longest word length.\nShared Transforms Both the pipelines read text messages from an input Kafka topic and write outputs to an output topic. Therefore, the data source and sink transforms are refactored into a utility module as shown below.\n1# chapter3/word_process_utils.py 2import re 3import typing 4 5import apache_beam as beam 6from apache_beam import pvalue 7from apache_beam.io import kafka 8 9 10def decode_message(kafka_kv: tuple): 11 print(kafka_kv) 12 return kafka_kv[1].decode(\u0026#34;utf-8\u0026#34;) 13 14 15def tokenize(element: str): 16 return re.findall(r\u0026#34;[A-Za-z\\\u0026#39;]+\u0026#34;, element) 17 18 19class ReadWordsFromKafka(beam.PTransform): 20 def __init__( 21 self, 22 bootstrap_servers: str, 23 topics: typing.List[str], 24 group_id: str, 25 verbose: bool = False, 26 expansion_service: typing.Any = None, 27 label: str | None = None, 28 ) -\u0026gt; None: 29 super().__init__(label) 30 self.boostrap_servers = bootstrap_servers 31 self.topics = topics 32 self.group_id = group_id 33 self.verbose = verbose 34 self.expansion_service = expansion_service 35 36 def expand(self, input: pvalue.PBegin): 37 return ( 38 input 39 | \u0026#34;ReadFromKafka\u0026#34; 40 \u0026gt;\u0026gt; kafka.ReadFromKafka( 41 consumer_config={ 42 \u0026#34;bootstrap.servers\u0026#34;: self.boostrap_servers, 43 \u0026#34;auto.offset.reset\u0026#34;: \u0026#34;latest\u0026#34;, 44 # \u0026#34;enable.auto.commit\u0026#34;: \u0026#34;true\u0026#34;, 45 \u0026#34;group.id\u0026#34;: self.group_id, 46 }, 47 topics=self.topics, 48 timestamp_policy=kafka.ReadFromKafka.create_time_policy, 49 commit_offset_in_finalize=True, 50 expansion_service=self.expansion_service, 51 ) 52 | \u0026#34;DecodeMessage\u0026#34; \u0026gt;\u0026gt; beam.Map(decode_message) 53 ) 54 55 56class WriteProcessOutputsToKafka(beam.PTransform): 57 def __init__( 58 self, 59 bootstrap_servers: str, 60 topic: str, 61 expansion_service: typing.Any = None, 62 label: str | None = None, 63 ) -\u0026gt; None: 64 super().__init__(label) 65 self.boostrap_servers = bootstrap_servers 66 self.topic = topic 67 self.expansion_service = expansion_service 68 69 def expand(self, pcoll: pvalue.PCollection): 70 return pcoll | \u0026#34;WriteToKafka\u0026#34; \u0026gt;\u0026gt; kafka.WriteToKafka( 71 producer_config={\u0026#34;bootstrap.servers\u0026#34;: self.boostrap_servers}, 72 topic=self.topic, 73 expansion_service=self.expansion_service, 74 ) Calculate the K most frequent words The main transform of this pipeline (CalculateTopKWords) performs\nWindowing: assigns input text messages into a fixed time window of configurable length - 10 seconds by default Tokenize: tokenizes (or converts) text into words CountPerWord: counts occurrence of each word TopKWords: selects top k words - 3 by default Flatten: flattens a list of words into individual words Also, it adds the window start and end timestamps when creating output messages (CreateMessags).\n1# chapter3/top_k_words.py 2import os 3import argparse 4import json 5import re 6import typing 7import logging 8 9import apache_beam as beam 10from apache_beam import pvalue 11from apache_beam.transforms.window import FixedWindows 12from apache_beam.options.pipeline_options import PipelineOptions 13from apache_beam.options.pipeline_options import SetupOptions 14 15from word_process_utils import tokenize, ReadWordsFromKafka, WriteProcessOutputsToKafka 16 17 18class CalculateTopKWords(beam.PTransform): 19 def __init__( 20 self, window_length: int, top_k: int, label: str | None = None 21 ) -\u0026gt; None: 22 self.window_length = window_length 23 self.top_k = top_k 24 super().__init__(label) 25 26 def expand(self, pcoll: pvalue.PCollection): 27 return ( 28 pcoll 29 | \u0026#34;Windowing\u0026#34; \u0026gt;\u0026gt; beam.WindowInto(FixedWindows(size=self.window_length)) 30 | \u0026#34;Tokenize\u0026#34; \u0026gt;\u0026gt; beam.FlatMap(tokenize) 31 | \u0026#34;CountPerWord\u0026#34; \u0026gt;\u0026gt; beam.combiners.Count.PerElement() 32 | \u0026#34;TopKWords\u0026#34; 33 \u0026gt;\u0026gt; beam.combiners.Top.Of(self.top_k, lambda e: e[1]).without_defaults() 34 | \u0026#34;Flatten\u0026#34; \u0026gt;\u0026gt; beam.FlatMap(lambda e: e) 35 ) 36 37 38def create_message(element: typing.Tuple[str, str, str, int]): 39 msg = json.dumps(dict(zip([\u0026#34;window_start\u0026#34;, \u0026#34;window_end\u0026#34;, \u0026#34;word\u0026#34;, \u0026#34;freq\u0026#34;], element))) 40 print(msg) 41 return \u0026#34;\u0026#34;.encode(\u0026#34;utf-8\u0026#34;), msg.encode(\u0026#34;utf-8\u0026#34;) 42 43 44class AddWindowTS(beam.DoFn): 45 def process(self, top_k: typing.Tuple[str, int], win_param=beam.DoFn.WindowParam): 46 yield ( 47 win_param.start.to_rfc3339(), 48 win_param.end.to_rfc3339(), 49 top_k[0], 50 top_k[1], 51 ) 52 53 54class CreateMessags(beam.PTransform): 55 def expand(self, pcoll: pvalue.PCollection): 56 return ( 57 pcoll 58 | \u0026#34;AddWindowTS\u0026#34; \u0026gt;\u0026gt; beam.ParDo(AddWindowTS()) 59 | \u0026#34;CreateMessages\u0026#34; 60 \u0026gt;\u0026gt; beam.Map(create_message).with_output_types(typing.Tuple[bytes, bytes]) 61 ) 62 63 64def run(argv=None, save_main_session=True): 65 parser = argparse.ArgumentParser(description=\u0026#34;Beam pipeline arguments\u0026#34;) 66 parser.add_argument( 67 \u0026#34;--bootstrap_servers\u0026#34;, 68 default=\u0026#34;host.docker.internal:29092\u0026#34;, 69 help=\u0026#34;Kafka bootstrap server addresses\u0026#34;, 70 ) 71 parser.add_argument(\u0026#34;--input_topic\u0026#34;, default=\u0026#34;input-topic\u0026#34;, help=\u0026#34;Input topic\u0026#34;) 72 parser.add_argument( 73 \u0026#34;--output_topic\u0026#34;, 74 default=re.sub(\u0026#34;_\u0026#34;, \u0026#34;-\u0026#34;, re.sub(\u0026#34;.py$\u0026#34;, \u0026#34;\u0026#34;, os.path.basename(__file__))), 75 help=\u0026#34;Output topic\u0026#34;, 76 ) 77 parser.add_argument(\u0026#34;--window_length\u0026#34;, default=\u0026#34;10\u0026#34;, type=int, help=\u0026#34;Window length\u0026#34;) 78 parser.add_argument(\u0026#34;--top_k\u0026#34;, default=\u0026#34;3\u0026#34;, type=int, help=\u0026#34;Top k\u0026#34;) 79 80 known_args, pipeline_args = parser.parse_known_args(argv) 81 print(f\u0026#34;known args - {known_args}\u0026#34;) 82 print(f\u0026#34;pipeline args - {pipeline_args}\u0026#34;) 83 84 # We use the save_main_session option because one or more DoFn\u0026#39;s in this 85 # workflow rely on global context (e.g., a module imported at module level). 86 pipeline_options = PipelineOptions(pipeline_args) 87 pipeline_options.view_as(SetupOptions).save_main_session = save_main_session 88 89 with beam.Pipeline(options=pipeline_options) as p: 90 ( 91 p 92 | \u0026#34;ReadInputsFromKafka\u0026#34; 93 \u0026gt;\u0026gt; ReadWordsFromKafka( 94 bootstrap_servers=known_args.bootstrap_servers, 95 topics=[known_args.input_topic], 96 group_id=f\u0026#34;{known_args.output_topic}-group\u0026#34;, 97 ) 98 | \u0026#34;CalculateTopKWords\u0026#34; 99 \u0026gt;\u0026gt; CalculateTopKWords( 100 window_length=known_args.window_length, 101 top_k=known_args.top_k, 102 ) 103 | \u0026#34;CreateMessags\u0026#34; \u0026gt;\u0026gt; CreateMessags() 104 | \u0026#34;WriteOutputsToKafka\u0026#34; 105 \u0026gt;\u0026gt; WriteProcessOutputsToKafka( 106 bootstrap_servers=known_args.bootstrap_servers, 107 topic=known_args.output_topic, 108 ) 109 ) 110 111 logging.getLogger().setLevel(logging.WARN) 112 logging.info(\u0026#34;Building pipeline ...\u0026#34;) 113 114 115if __name__ == \u0026#34;__main__\u0026#34;: 116 run() Pipeline Test As described in this documentation, we can test a Beam pipeline as following.\nCreate a TestPipeline. Create some static, known test input data. Create a PCollection of input data using the Create transform (if bounded source) or a TestStream (if unbounded source) Apply the transform to the input PCollection and save the resulting output PCollection. Use PAssert and its subclasses (or testing utils in Python) to verify that the output PCollection contains the elements that you expect. We configure the first three lines to be delivered in 2 seconds and the remaining two lines after 10 seconds. Therefore, the whole elements are split into two fixed time windows, given the window length of 10 seconds. We can check the top 3 words are line, first and the in the first window while line, in and window are the top 3 words in the second window. Then, we can create the expected output as a list of tuples of word and number of occurrences and compare it with the pipeline output.\n1# chapter3/top_k_words_test.py 2import unittest 3 4from apache_beam.coders import coders 5from apache_beam.utils.timestamp import Timestamp 6from apache_beam.testing.test_pipeline import TestPipeline 7from apache_beam.testing.util import assert_that, equal_to 8from apache_beam.testing.test_stream import TestStream 9from apache_beam.transforms.window import TimestampedValue 10from apache_beam.options.pipeline_options import PipelineOptions, StandardOptions 11 12from top_k_words import CalculateTopKWords 13 14 15class TopKWordsTest(unittest.TestCase): 16 def test_windowing_behaviour(self): 17 options = PipelineOptions() 18 options.view_as(StandardOptions).streaming = True 19 with TestPipeline(options=options) as p: 20 test_stream = ( 21 TestStream(coder=coders.StrUtf8Coder()) 22 .with_output_types(str) 23 .add_elements( 24 [ 25 TimestampedValue(\u0026#34;This is the first line.\u0026#34;, Timestamp.of(0)), 26 TimestampedValue( 27 \u0026#34;This is second line in the first window\u0026#34;, Timestamp.of(1) 28 ), 29 TimestampedValue( 30 \u0026#34;Last line in the first window\u0026#34;, Timestamp.of(2) 31 ), 32 TimestampedValue( 33 \u0026#34;This is another line, but in different window.\u0026#34;, 34 Timestamp.of(10), 35 ), 36 TimestampedValue( 37 \u0026#34;Last line, in the same window as previous line.\u0026#34;, 38 Timestamp.of(11), 39 ), 40 ] 41 ) 42 .advance_watermark_to_infinity() 43 ) 44 45 output = ( 46 p 47 | test_stream 48 | \u0026#34;CalculateTopKWords\u0026#34; 49 \u0026gt;\u0026gt; CalculateTopKWords( 50 window_length=10, 51 top_k=3, 52 ) 53 ) 54 55 EXPECTED_OUTPUT = [ 56 (\u0026#34;line\u0026#34;, 3), 57 (\u0026#34;first\u0026#34;, 3), 58 (\u0026#34;the\u0026#34;, 3), 59 (\u0026#34;line\u0026#34;, 3), 60 (\u0026#34;in\u0026#34;, 2), 61 (\u0026#34;window\u0026#34;, 2), 62 ] 63 64 assert_that(output, equal_to(EXPECTED_OUTPUT)) 65 66 67if __name__ == \u0026#34;__main__\u0026#34;: 68 unittest.main() We can execute the pipeline test as shown below.\n1python chapter2/top_k_words_test.py -v 2test_windowing_behaviour (__main__.TopKWordsTest) ... ok 3 4---------------------------------------------------------------------- 5Ran 1 test in 0.354s 6 7OK Pipeline Execution Below shows an example of executing the pipeline by specifying pipeline arguments only while accepting default values of the known arguments (bootstrap_servers, input_topic \u0026hellip;). Note that, by specifying the flink_master value to localhost:8081, it deployed the pipeline on a local Flink cluster. Alternatively, we can use an embedded Flink cluster if we exclude that argument.\n1## start the beam pipeline 2## exclude --flink_master if using an embedded cluster 3python chapter2/top_k_words.py --job_name=top-k-words \\ 4\t--runner FlinkRunner --flink_master=localhost:8081 \\ 5\t--streaming --environment_type=LOOPBACK --parallelism=3 \\ 6\t--checkpointing_interval=10000 --experiment=use_deprecated_read On Flink UI, we see the pipeline polls messages and performs the main transform in multiple tasks while keeping Kafka offset commit as a separate task. Note that, although I added a flag (use_deprecated_read) to use the legacy read (ReadFromKafkaViaUnbounded), the splittable DoFn based read (ReadFromKafkaViaSDF) is used. It didn\u0026rsquo;t happen when I used Flink 1.16.3, and I\u0026rsquo;m looking into it. It looks okay to go through the example pipelines, but check this issue before deciding which read to use in production.\nOn Kafka UI, we can check the output messages include the frequent word details as well as window start/end timestamps.\nCalculate the maximal word length The main transform of this pipeline (CalculateMaxWordLength) performs\nWindowing: assigns input text messages into a global window and emits (or triggers) the result for every new input message with the following configuration Disallows Late data (allowed_lateness=0) Assigns the output timestamp from the latest input timestamp (timestamp_combiner=TimestampCombiner.OUTPUT_AT_LATEST) Keeps the previous elements that were already fired (accumulation_mode=AccumulationMode.ACCUMULATING) Tokenize: tokenizes (or converts) text into words GetLongestWord: selects top 1 word that has the longest length Flatten: flattens a list of words into a word Also, the output timestamp is added when creating output messages (CreateMessags).\n1# chapter3/max_word_length_with_ts.py 2import os 3import argparse 4import json 5import re 6import logging 7import typing 8 9import apache_beam as beam 10from apache_beam import pvalue 11from apache_beam.transforms.window import GlobalWindows, TimestampCombiner 12from apache_beam.transforms.trigger import AfterCount, AccumulationMode, AfterWatermark 13from apache_beam.transforms.util import Reify 14from apache_beam.options.pipeline_options import PipelineOptions 15from apache_beam.options.pipeline_options import SetupOptions 16 17 18from word_process_utils import tokenize, ReadWordsFromKafka, WriteProcessOutputsToKafka 19 20 21class CalculateMaxWordLength(beam.PTransform): 22 def expand(self, pcoll: pvalue.PCollection): 23 return ( 24 pcoll 25 | \u0026#34;Windowing\u0026#34; 26 \u0026gt;\u0026gt; beam.WindowInto( 27 GlobalWindows(), 28 trigger=AfterWatermark(early=AfterCount(1)), 29 allowed_lateness=0, 30 timestamp_combiner=TimestampCombiner.OUTPUT_AT_LATEST, 31 accumulation_mode=AccumulationMode.ACCUMULATING, 32 ) 33 | \u0026#34;Tokenize\u0026#34; \u0026gt;\u0026gt; beam.FlatMap(tokenize) 34 | \u0026#34;GetLongestWord\u0026#34; \u0026gt;\u0026gt; beam.combiners.Top.Of(1, key=len).without_defaults() 35 | \u0026#34;Flatten\u0026#34; \u0026gt;\u0026gt; beam.FlatMap(lambda e: e) 36 ) 37 38 39def create_message(element: typing.Tuple[str, str]): 40 msg = json.dumps(dict(zip([\u0026#34;created_at\u0026#34;, \u0026#34;word\u0026#34;], element))) 41 print(msg) 42 return \u0026#34;\u0026#34;.encode(\u0026#34;utf-8\u0026#34;), msg.encode(\u0026#34;utf-8\u0026#34;) 43 44 45class AddTS(beam.DoFn): 46 def process(self, word: str, ts_param=beam.DoFn.TimestampParam): 47 yield ts_param.to_rfc3339(), word 48 49 50class CreateMessags(beam.PTransform): 51 def expand(self, pcoll: pvalue.PCollection): 52 return ( 53 pcoll 54 | \u0026#34;ReifyTimestamp\u0026#34; \u0026gt;\u0026gt; Reify.Timestamp() 55 | \u0026#34;AddTimestamp\u0026#34; \u0026gt;\u0026gt; beam.ParDo(AddTS()) 56 | \u0026#34;CreateMessages\u0026#34; 57 \u0026gt;\u0026gt; beam.Map(create_message).with_output_types(typing.Tuple[bytes, bytes]) 58 ) 59 60 61def run(argv=None, save_main_session=True): 62 parser = argparse.ArgumentParser(description=\u0026#34;Beam pipeline arguments\u0026#34;) 63 parser.add_argument( 64 \u0026#34;--bootstrap_servers\u0026#34;, 65 default=\u0026#34;host.docker.internal:29092\u0026#34;, 66 help=\u0026#34;Kafka bootstrap server addresses\u0026#34;, 67 ) 68 parser.add_argument(\u0026#34;--input_topic\u0026#34;, default=\u0026#34;input-topic\u0026#34;, help=\u0026#34;Input topic\u0026#34;) 69 parser.add_argument( 70 \u0026#34;--output_topic\u0026#34;, 71 default=re.sub(\u0026#34;_\u0026#34;, \u0026#34;-\u0026#34;, re.sub(\u0026#34;.py$\u0026#34;, \u0026#34;\u0026#34;, os.path.basename(__file__))), 72 help=\u0026#34;Output topic\u0026#34;, 73 ) 74 75 known_args, pipeline_args = parser.parse_known_args(argv) 76 print(f\u0026#34;known args - {known_args}\u0026#34;) 77 print(f\u0026#34;pipeline args - {pipeline_args}\u0026#34;) 78 79 # We use the save_main_session option because one or more DoFn\u0026#39;s in this 80 # workflow rely on global context (e.g., a module imported at module level). 81 pipeline_options = PipelineOptions(pipeline_args) 82 pipeline_options.view_as(SetupOptions).save_main_session = save_main_session 83 84 with beam.Pipeline(options=pipeline_options) as p: 85 ( 86 p 87 | \u0026#34;ReadInputsFromKafka\u0026#34; 88 \u0026gt;\u0026gt; ReadWordsFromKafka( 89 bootstrap_servers=known_args.bootstrap_servers, 90 topics=[known_args.input_topic], 91 group_id=f\u0026#34;{known_args.output_topic}-group\u0026#34;, 92 ) 93 | \u0026#34;CalculateMaxWordLength\u0026#34; \u0026gt;\u0026gt; CalculateMaxWordLength() 94 | \u0026#34;CreateMessags\u0026#34; \u0026gt;\u0026gt; CreateMessags() 95 | \u0026#34;WriteOutputsToKafka\u0026#34; 96 \u0026gt;\u0026gt; WriteProcessOutputsToKafka( 97 bootstrap_servers=known_args.bootstrap_servers, 98 topic=known_args.output_topic, 99 ) 100 ) 101 102 logging.getLogger().setLevel(logging.WARN) 103 logging.info(\u0026#34;Building pipeline ...\u0026#34;) 104 105 106if __name__ == \u0026#34;__main__\u0026#34;: 107 run() Pipeline Test The elements are configured so that the lengths of words increase for the first three elements (a, bb, ccc), and the output changes each time. The length of the fourth element (d) is smaller than the previous one, and the output remains the same. Note that the last output is emitted additionally because the watermark is advanced into the end of the global window, and it fires the after watermark trigger. We can create the expected output as a list of TestWindowedValues and compare it with the pipeline output.\n1# chapter3/max_word_length_with_ts_test.py 2import unittest 3 4from apache_beam.coders import coders 5from apache_beam.utils.timestamp import Timestamp 6from apache_beam.testing.test_pipeline import TestPipeline 7from apache_beam.testing.util import assert_that, equal_to, TestWindowedValue 8from apache_beam.testing.test_stream import TestStream 9from apache_beam.transforms.window import GlobalWindow, TimestampedValue 10from apache_beam.transforms.util import Reify 11from apache_beam.options.pipeline_options import PipelineOptions, StandardOptions 12 13from max_word_length_with_ts import CalculateMaxWordLength 14 15 16class MaxWordLengthTest(unittest.TestCase): 17 def test_windowing_behaviour(self): 18 options = PipelineOptions() 19 options.view_as(StandardOptions).streaming = True 20 with TestPipeline(options=options) as p: 21 \u0026#34;\u0026#34;\u0026#34; 22 We should put each element separately. The reason we do this is to ensure 23 that our trigger will be invoked in between each element. Because the trigger 24 invocation is optional, a runner might skip a particular firing. Putting each element 25 separately into addElements makes DirectRunner (our testing runner) invokes a trigger 26 for each input element. 27 \u0026#34;\u0026#34;\u0026#34; 28 test_stream = ( 29 TestStream(coder=coders.StrUtf8Coder()) 30 .with_output_types(str) 31 .add_elements([TimestampedValue(\u0026#34;a\u0026#34;, Timestamp.of(0))]) 32 .add_elements([TimestampedValue(\u0026#34;bb\u0026#34;, Timestamp.of(10))]) 33 .add_elements([TimestampedValue(\u0026#34;ccc\u0026#34;, Timestamp.of(20))]) 34 .add_elements([TimestampedValue(\u0026#34;d\u0026#34;, Timestamp.of(30))]) 35 .advance_watermark_to_infinity() 36 ) 37 38 output = ( 39 p 40 | test_stream 41 | \u0026#34;CalculateMaxWordLength\u0026#34; \u0026gt;\u0026gt; CalculateMaxWordLength() 42 | \u0026#34;ReifyTimestamp\u0026#34; \u0026gt;\u0026gt; Reify.Timestamp() 43 ) 44 45 EXPECTED_OUTPUT = [ 46 TestWindowedValue( 47 value=\u0026#34;a\u0026#34;, timestamp=Timestamp(0), windows=[GlobalWindow()] 48 ), 49 TestWindowedValue( 50 value=\u0026#34;bb\u0026#34;, timestamp=Timestamp(10), windows=[GlobalWindow()] 51 ), 52 TestWindowedValue( 53 value=\u0026#34;ccc\u0026#34;, timestamp=Timestamp(20), windows=[GlobalWindow()] 54 ), 55 TestWindowedValue( 56 value=\u0026#34;ccc\u0026#34;, timestamp=Timestamp(30), windows=[GlobalWindow()] 57 ), 58 TestWindowedValue( 59 value=\u0026#34;ccc\u0026#34;, timestamp=Timestamp(30), windows=[GlobalWindow()] 60 ), 61 ] 62 63 assert_that(output, equal_to(EXPECTED_OUTPUT), reify_windows=True) 64 65 66if __name__ == \u0026#34;__main__\u0026#34;: 67 unittest.main() The pipeline can be tested as shown below.\n1python chapter2/max_word_length_test.py -v 2test_windowing_behaviour (__main__.MaxWordLengthTest) ... ok 3 4---------------------------------------------------------------------- 5Ran 1 test in 0.446s 6 7OK Pipeline Execution Similar to the previous example, we can execute the pipeline by specifying pipeline arguments only while accepting default values of the known arguments (bootstrap_servers, input_topic \u0026hellip;).\n1## start the beam pipeline 2## exclude --flink_master if using an embedded cluster 3python chapter2/max_word_length_with_ts.py --job_name=max-word-len \\ 4\t--runner FlinkRunner --flink_master=localhost:8081 \\ 5\t--streaming --environment_type=LOOPBACK --parallelism=3 \\ 6\t--checkpointing_interval=10000 --experiment=use_deprecated_read On Flink UI, we see the pipeline polls messages and performs the main transform in multiple tasks while keeping Kafka offset commit as a separate task.\nOn Kafka UI, we can check the output messages include the longest word as well as its timestamp. Note that, as the input text message has multiple words, we can have multiple output messages that have the same timestamp - recall the accumulation mode is accumulating.\n","date":"July 4, 2024","img":"/blog/2024-07-04-beam-examples-1/featured.png","lang":"en","langName":"English","largeImg":"/blog/2024-07-04-beam-examples-1/featured_hud3ce9831fcc98de0424cfc6e53706491_96881_500x0_resize_box_3.png","permalink":"/blog/2024-07-04-beam-examples-1/","series":[{"title":"Apache Beam Python Examples","url":"/series/apache-beam-python-examples/"}],"smallImg":"/blog/2024-07-04-beam-examples-1/featured_hud3ce9831fcc98de0424cfc6e53706491_96881_180x0_resize_box_3.png","tags":[{"title":"Apache Beam","url":"/tags/apache-beam/"},{"title":"Apache Flink","url":"/tags/apache-flink/"},{"title":"Python","url":"/tags/python/"},{"title":"Docker","url":"/tags/docker/"},{"title":"Docker Compose","url":"/tags/docker-compose/"}],"timestamp":1720051200,"title":"Apache Beam Python Examples - Part 1 Calculate K Most Frequent Words and Max Word Length"},{"categories":[{"title":"Data Streaming","url":"/categories/data-streaming/"}],"content":"In this post, we develop an Apache Beam pipeline using the Python SDK and deploy it on an Apache Flink cluster via the Apache Flink Runner. Same as Part I, we deploy a Kafka cluster using the Strimzi Operator on a minikube cluster as the pipeline uses Apache Kafka topics for its data source and sink. Then, we develop the pipeline as a Python package and add the package to a custom Docker image so that Python user code can be executed externally. For deployment, we create a Flink session cluster via the Flink Kubernetes Operator, and deploy the pipeline using a Kubernetes job. Finally, we check the output of the application by sending messages to the input Kafka topic using a Python producer application.\nPart 1 PyFlink Application Part 2 Beam Pipeline on Flink Runner (this post) Resources to run a Python Beam pipeline on Flink We develop an Apache Beam pipeline using the Python SDK and deploy it on an Apache Flink cluster via the Apache Flink Runner. While the Flink cluster is created by the Flink Kubernetes Operator, we need two components (Job Service and SDK Harness) to run the pipeline on the Flink Runner. Roughly speaking, the former converts details about a Python pipeline into a format that the Flink runner can understand while the Python user code is executed by the latter - see this post for more details. Note that the Python SDK provides convenience wrappers to manage those components, and it can be utilised by specifying FlinkRunner in the pipeline option (i.e. --runner=FlinkRunner). We let the Job Service be managed automatically while relying on our own SDK Harness as a sidecar container for simplicity. Also, we need the Java IO Expansion Service as an extra because the pipeline uses Apache Kafka topics for its data source and sink, and the Kafka Connector I/O is developed in Java. Simply put, the expansion service is used to serialise data for the Java SDK.\nSetup Kafka Cluster Same as Part I, we deploy a Kafka cluster using the Strimzi Operator on a minikube cluster. Also, we create UI for Apache Kafka (kafka-ui) to facilitate development. See Part I for details about how to create them. The source of this post can be found in this GitHub repository.\nWhen a Kafka cluster is created successfully, we can see the following resources.\n1kubectl get po,strimzipodsets.core.strimzi.io,svc -l app.kubernetes.io/instance=demo-cluster 2NAME READY STATUS RESTARTS AGE 3pod/demo-cluster-kafka-0 1/1 Running 0 4m16s 4pod/demo-cluster-zookeeper-0 1/1 Running 0 4m44s 5 6NAME PODS READY PODS CURRENT PODS AGE 7strimzipodset.core.strimzi.io/demo-cluster-kafka 1 1 1 4m17s 8strimzipodset.core.strimzi.io/demo-cluster-zookeeper 1 1 1 4m45s 9 10NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE 11service/demo-cluster-kafka-bootstrap ClusterIP 10.100.17.151 \u0026lt;none\u0026gt; 9091/TCP,9092/TCP 4m17s 12service/demo-cluster-kafka-brokers ClusterIP None \u0026lt;none\u0026gt; 9090/TCP,9091/TCP,8443/TCP,9092/TCP 4m17s 13service/demo-cluster-kafka-external-0 NodePort 10.104.252.159 \u0026lt;none\u0026gt; 29092:30867/TCP 4m17s 14service/demo-cluster-kafka-external-bootstrap NodePort 10.98.229.176 \u0026lt;none\u0026gt; 29092:31488/TCP 4m17s 15service/demo-cluster-zookeeper-client ClusterIP 10.109.80.70 \u0026lt;none\u0026gt; 2181/TCP 4m46s 16service/demo-cluster-zookeeper-nodes ClusterIP None \u0026lt;none\u0026gt; 2181/TCP,2888/TCP,3888/TCP 4m46s The Kafka management app (kafka-ui) is deployed as a Kubernetes deployment and exposed by a single service.\n1kubectl get all -l app=kafka-ui 2NAME READY STATUS RESTARTS AGE 3pod/kafka-ui-65dbbc98dc-xjnvh 1/1 Running 0 10m 4 5NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE 6service/kafka-ui ClusterIP 10.106.116.129 \u0026lt;none\u0026gt; 8080/TCP 10m 7 8NAME READY UP-TO-DATE AVAILABLE AGE 9deployment.apps/kafka-ui 1/1 1 1 10m 10 11NAME DESIRED CURRENT READY AGE 12replicaset.apps/kafka-ui-65dbbc98dc 1 1 1 10m We can use kubectl port-forward to connect to the kafka-ui server running in the minikube cluster on port 8080.\n1kubectl port-forward svc/kafka-ui 8080 Develop Stream Processing App We develop an Apache Beam pipeline as a Python package and add it to a custom Docker image, which is used to execute Python user code (SDK Harness). We also build another custom Docker image, which adds the Java SDK of Apache Beam to the official Flink base image. This image is used for deploying a Flink cluster as well as for executing Java user code of the Kafka Connector I/O.\nBeam Pipeline Code The application begins with reading text messages from an input Kafka topic, followed by extracting words by splitting the messages (ReadWordsFromKafka). Next, the elements (words) are added to a fixed time window of 5 seconds and their average length is calculated (CalculateAvgWordLen). Finally, we include the window start/end timestamps and send the updated element to an output Kafka topic (WriteWordLenToKafka).\nNote that we create a custom Java IO Expansion Service (get_expansion_service) and add it to the ReadFromKafka and WriteToKafka transforms of the Kafka Connector I/O. Although the Kafka I/O provides a function to create that service, it did not work for me (or I do not understand how to make use of it yet). Instead, I ended up creating a custom service as illustrated in Building Big Data Pipelines with Apache Beam by Jan Lukavský. Note further that the expansion service Jar file (beam-sdks-java-io-expansion-service.jar) should exist in the Kubernetes job that executes the pipeline while the Java SDK (/opt/apache/beam/boot) should exist in the runner worker.\n1# beam/word_len/word_len.py 2import json 3import argparse 4import re 5import logging 6import typing 7 8import apache_beam as beam 9from apache_beam import pvalue 10from apache_beam.io import kafka 11from apache_beam.transforms.window import FixedWindows 12from apache_beam.options.pipeline_options import PipelineOptions 13from apache_beam.options.pipeline_options import SetupOptions 14 15from apache_beam.transforms.external import JavaJarExpansionService 16 17 18def get_expansion_service( 19 jar=\u0026#34;/opt/apache/beam/jars/beam-sdks-java-io-expansion-service.jar\u0026#34;, args=None 20): 21 if args == None: 22 args = [ 23 \u0026#34;--defaultEnvironmentType=PROCESS\u0026#34;, 24 \u0026#39;--defaultEnvironmentConfig={\u0026#34;command\u0026#34;: \u0026#34;/opt/apache/beam/boot\u0026#34;}\u0026#39;, 25 \u0026#34;--experiments=use_deprecated_read\u0026#34;, 26 ] 27 return JavaJarExpansionService(jar, [\u0026#34;{{PORT}}\u0026#34;] + args) 28 29 30class WordAccum(typing.NamedTuple): 31 length: int 32 count: int 33 34 35beam.coders.registry.register_coder(WordAccum, beam.coders.RowCoder) 36 37 38def decode_message(kafka_kv: tuple, verbose: bool = False): 39 if verbose: 40 print(kafka_kv) 41 return kafka_kv[1].decode(\u0026#34;utf-8\u0026#34;) 42 43 44def tokenize(element: str): 45 return re.findall(r\u0026#34;[A-Za-z\\\u0026#39;]+\u0026#34;, element) 46 47 48def create_message(element: typing.Tuple[str, str, float]): 49 msg = json.dumps(dict(zip([\u0026#34;window_start\u0026#34;, \u0026#34;window_end\u0026#34;, \u0026#34;avg_len\u0026#34;], element))) 50 print(msg) 51 return \u0026#34;\u0026#34;.encode(\u0026#34;utf-8\u0026#34;), msg.encode(\u0026#34;utf-8\u0026#34;) 52 53 54class AverageFn(beam.CombineFn): 55 def create_accumulator(self): 56 return WordAccum(length=0, count=0) 57 58 def add_input(self, mutable_accumulator: WordAccum, element: str): 59 length, count = tuple(mutable_accumulator) 60 return WordAccum(length=length + len(element), count=count + 1) 61 62 def merge_accumulators(self, accumulators: typing.List[WordAccum]): 63 lengths, counts = zip(*accumulators) 64 return WordAccum(length=sum(lengths), count=sum(counts)) 65 66 def extract_output(self, accumulator: WordAccum): 67 length, count = tuple(accumulator) 68 return length / count if count else float(\u0026#34;NaN\u0026#34;) 69 70 def get_accumulator_coder(self): 71 return beam.coders.registry.get_coder(WordAccum) 72 73 74class AddWindowTS(beam.DoFn): 75 def process(self, avg_len: float, win_param=beam.DoFn.WindowParam): 76 yield ( 77 win_param.start.to_rfc3339(), 78 win_param.end.to_rfc3339(), 79 avg_len, 80 ) 81 82 83class ReadWordsFromKafka(beam.PTransform): 84 def __init__( 85 self, 86 bootstrap_servers: str, 87 topics: typing.List[str], 88 group_id: str, 89 verbose: bool = False, 90 expansion_service: typing.Any = None, 91 label: str | None = None, 92 ) -\u0026gt; None: 93 super().__init__(label) 94 self.boostrap_servers = bootstrap_servers 95 self.topics = topics 96 self.group_id = group_id 97 self.verbose = verbose 98 self.expansion_service = expansion_service 99 100 def expand(self, input: pvalue.PBegin): 101 return ( 102 input 103 | \u0026#34;ReadFromKafka\u0026#34; 104 \u0026gt;\u0026gt; kafka.ReadFromKafka( 105 consumer_config={ 106 \u0026#34;bootstrap.servers\u0026#34;: self.boostrap_servers, 107 \u0026#34;auto.offset.reset\u0026#34;: \u0026#34;latest\u0026#34;, 108 # \u0026#34;enable.auto.commit\u0026#34;: \u0026#34;true\u0026#34;, 109 \u0026#34;group.id\u0026#34;: self.group_id, 110 }, 111 topics=self.topics, 112 timestamp_policy=kafka.ReadFromKafka.create_time_policy, 113 commit_offset_in_finalize=True, 114 expansion_service=self.expansion_service, 115 ) 116 | \u0026#34;DecodeMessage\u0026#34; \u0026gt;\u0026gt; beam.Map(decode_message) 117 | \u0026#34;Tokenize\u0026#34; \u0026gt;\u0026gt; beam.FlatMap(tokenize) 118 ) 119 120 121class CalculateAvgWordLen(beam.PTransform): 122 def expand(self, input: pvalue.PCollection): 123 return ( 124 input 125 | \u0026#34;Windowing\u0026#34; \u0026gt;\u0026gt; beam.WindowInto(FixedWindows(size=5)) 126 | \u0026#34;GetAvgWordLength\u0026#34; \u0026gt;\u0026gt; beam.CombineGlobally(AverageFn()).without_defaults() 127 ) 128 129 130class WriteWordLenToKafka(beam.PTransform): 131 def __init__( 132 self, 133 bootstrap_servers: str, 134 topic: str, 135 expansion_service: typing.Any = None, 136 label: str | None = None, 137 ) -\u0026gt; None: 138 super().__init__(label) 139 self.boostrap_servers = bootstrap_servers 140 self.topic = topic 141 self.expansion_service = expansion_service 142 143 def expand(self, input: pvalue.PCollection): 144 return ( 145 input 146 | \u0026#34;AddWindowTS\u0026#34; \u0026gt;\u0026gt; beam.ParDo(AddWindowTS()) 147 | \u0026#34;CreateMessages\u0026#34; 148 \u0026gt;\u0026gt; beam.Map(create_message).with_output_types(typing.Tuple[bytes, bytes]) 149 | \u0026#34;WriteToKafka\u0026#34; 150 \u0026gt;\u0026gt; kafka.WriteToKafka( 151 producer_config={\u0026#34;bootstrap.servers\u0026#34;: self.boostrap_servers}, 152 topic=self.topic, 153 expansion_service=self.expansion_service, 154 ) 155 ) 156 157 158def run(argv=None, save_main_session=True): 159 parser = argparse.ArgumentParser(description=\u0026#34;Beam pipeline arguments\u0026#34;) 160 parser.add_argument( 161 \u0026#34;--deploy\u0026#34;, 162 dest=\u0026#34;deploy\u0026#34;, 163 action=\u0026#34;store_true\u0026#34;, 164 default=\u0026#34;Flag to indicate whether to deploy to a cluster\u0026#34;, 165 ) 166 parser.add_argument( 167 \u0026#34;--bootstrap_servers\u0026#34;, 168 dest=\u0026#34;bootstrap\u0026#34;, 169 default=\u0026#34;host.docker.internal:29092\u0026#34;, 170 help=\u0026#34;Kafka bootstrap server addresses\u0026#34;, 171 ) 172 parser.add_argument( 173 \u0026#34;--input_topic\u0026#34;, 174 dest=\u0026#34;input\u0026#34;, 175 default=\u0026#34;input-topic\u0026#34;, 176 help=\u0026#34;Kafka input topic name\u0026#34;, 177 ) 178 parser.add_argument( 179 \u0026#34;--output_topic\u0026#34;, 180 dest=\u0026#34;output\u0026#34;, 181 default=\u0026#34;output-topic-beam\u0026#34;, 182 help=\u0026#34;Kafka output topic name\u0026#34;, 183 ) 184 parser.add_argument( 185 \u0026#34;--group_id\u0026#34;, 186 dest=\u0026#34;group\u0026#34;, 187 default=\u0026#34;beam-word-len\u0026#34;, 188 help=\u0026#34;Kafka output group ID\u0026#34;, 189 ) 190 191 known_args, pipeline_args = parser.parse_known_args(argv) 192 193 print(known_args) 194 print(pipeline_args) 195 196 # We use the save_main_session option because one or more DoFn\u0026#39;s in this 197 # workflow rely on global context (e.g., a module imported at module level). 198 pipeline_options = PipelineOptions(pipeline_args) 199 pipeline_options.view_as(SetupOptions).save_main_session = save_main_session 200 201 expansion_service = None 202 if known_args.deploy is True: 203 expansion_service = get_expansion_service() 204 205 with beam.Pipeline(options=pipeline_options) as p: 206 ( 207 p 208 | \u0026#34;ReadWordsFromKafka\u0026#34; 209 \u0026gt;\u0026gt; ReadWordsFromKafka( 210 bootstrap_servers=known_args.bootstrap, 211 topics=[known_args.input], 212 group_id=known_args.group, 213 expansion_service=expansion_service, 214 ) 215 | \u0026#34;CalculateAvgWordLen\u0026#34; \u0026gt;\u0026gt; CalculateAvgWordLen() 216 | \u0026#34;WriteWordLenToKafka\u0026#34; 217 \u0026gt;\u0026gt; WriteWordLenToKafka( 218 bootstrap_servers=known_args.bootstrap, 219 topic=known_args.output, 220 expansion_service=expansion_service, 221 ) 222 ) 223 224 logging.getLogger().setLevel(logging.DEBUG) 225 logging.info(\u0026#34;Building pipeline ...\u0026#34;) 226 227 228if __name__ == \u0026#34;__main__\u0026#34;: 229 run() The pipeline script is added to a Python package under a folder named word_len, and a simple module named run is created as it should be executed as a module (i.e. python -m ...). (I had an error if I execute the pipeline as a script.) Note that this way of packaging is for demonstration only and see this document for a recommended way of packaging a pipeline.\n1# beam/word_len/run.py 2from . import * 3 4run() Overall, the pipeline package is structured as shown below.\n1tree beam/word_len 2 3beam/word_len 4├── __init__.py 5├── run.py 6└── word_len.py Build Docker Images As mentioned earlier, we build a custom Docker image (beam-python-example:1.16) that is used for deploying a Flink cluster as well as for executing Java user code of the Kafka Connector I/O.\n1# beam/Dockerfile 2FROM flink:1.16 3 4COPY --from=apache/beam_java11_sdk:2.56.0 /opt/apache/beam/ /opt/apache/beam/ We also build another custom Docker image (beam-python-harness:2.56.0) to run Python user code (SDK Harness). From the Python SDK Docker image, it first installs JDK Development Kit (JDK) and downloads the Java IO Expansion Service Jar file. Then, Beam pipeline packages are copied to the /app folder, and it ends up adding the app folder into the PYTHONPATH environment variable, which makes the packages to be searchable.\n1# beam/Dockerfile-python-harness 2FROM apache/beam_python3.10_sdk:2.56.0 3 4ARG BEAM_VERSION 5ENV BEAM_VERSION=${BEAM_VERSION:-2.56.0} 6ENV REPO_BASE_URL=https://repo1.maven.org/maven2/org/apache/beam 7 8RUN apt-get update \u0026amp;\u0026amp; apt-get install -y default-jdk 9 10RUN mkdir -p /opt/apache/beam/jars \\ 11 \u0026amp;\u0026amp; wget ${REPO_BASE_URL}/beam-sdks-java-io-expansion-service/${BEAM_VERSION}/beam-sdks-java-io-expansion-service-${BEAM_VERSION}.jar \\ 12 --progress=bar:force:noscroll -O /opt/apache/beam/jars/beam-sdks-java-io-expansion-service.jar 13 14COPY word_len /app/word_len 15COPY word_count /app/word_count 16 17ENV PYTHONPATH=\u0026#34;$PYTHONPATH:/app\u0026#34; As the custom images should be accessible in the minikube cluster, we should point the terminal\u0026rsquo;s docker-cli to the minikube\u0026rsquo;s Docker engine. Then, the images can be built as usual using docker build.\n1eval $(minikube docker-env) 2docker build -t beam-python-example:1.16 beam/ 3docker build -t beam-python-harness:2.56.0 -f beam/Dockerfile-python-harness beam/ Deploy Stream Processing App The Beam pipeline is executed on a Flink session cluster, which is deployed via the Flink Kubernetes Operator. Note that the application deployment mode where the Beam pipeline is deployed as a Flink job doesn\u0026rsquo;t seem to work (or I don\u0026rsquo;t understand how to do so yet) due to either job submission timeout error or failing to upload the job artifact. After the pipeline is deployed, we check the output of the application by sending text messages to the input Kafka topic.\nDeploy Flink Kubernetes Operator We first need to install the certificate manager on the minikube cluster to enable adding the webhook component. Then, the operator can be installed using a Helm chart, and the version 1.8.0 is installed in the post.\n1kubectl create -f https://github.com/jetstack/cert-manager/releases/download/v1.8.2/cert-manager.yaml 2helm repo add flink-operator-repo https://downloads.apache.org/flink/flink-kubernetes-operator-1.8.0/ 3helm install flink-kubernetes-operator flink-operator-repo/flink-kubernetes-operator 4# NAME: flink-kubernetes-operator 5# LAST DEPLOYED: Mon Jun 03 21:37:45 2024 6# NAMESPACE: default 7# STATUS: deployed 8# REVISION: 1 9# TEST SUITE: None 10 11helm list 12# NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION 13# flink-kubernetes-operator default 1 2024-06-03 21:37:45.579302452 +1000 AEST deployed flink-kubernetes-operator-1.8.0 1.8.0 Deploy Beam Pipeline First, we create a Flink session cluster. In the manifest file, we configure common properties such as the Docker image, Flink version, cluster configuration and pod template. These properties are applied to the Flink job manager and task manager, and only the replica and resource are specified additionally to them. Note that we add a sidecar container to the task manager and this SDK Harness container is configured to execute Python user code - see the job configuration below.\n1# beam/word_len_cluster.yml 2apiVersion: flink.apache.org/v1beta1 3kind: FlinkDeployment 4metadata: 5 name: word-len-cluster 6spec: 7 image: beam-python-example:1.16 8 imagePullPolicy: Never 9 flinkVersion: v1_16 10 flinkConfiguration: 11 taskmanager.numberOfTaskSlots: \u0026#34;10\u0026#34; 12 serviceAccount: flink 13 podTemplate: 14 spec: 15 containers: 16 - name: flink-main-container 17 volumeMounts: 18 - mountPath: /opt/flink/log 19 name: flink-logs 20 volumes: 21 - name: flink-logs 22 emptyDir: {} 23 jobManager: 24 resource: 25 memory: \u0026#34;2048Mi\u0026#34; 26 cpu: 2 27 taskManager: 28 replicas: 1 29 resource: 30 memory: \u0026#34;2048Mi\u0026#34; 31 cpu: 2 32 podTemplate: 33 spec: 34 containers: 35 - name: python-harness 36 image: beam-python-harness:2.56.0 37 args: [\u0026#34;-worker_pool\u0026#34;] 38 ports: 39 - containerPort: 50000 40 name: harness-port The pipeline is deployed using a Kubernetes job, and the custom SDK Harness image is used to execute the pipeline as a module. The first two arguments are application specific arguments and the rest are arguments for pipeline options. The pipeline arguments are self-explanatory, or you can check available options in the pipeline options source and Flink Runner document. Note that, to execute Python user code in the sidecar container, we set the environment type to EXTERNAL and environment config to localhost:50000.\n1# beam/word_len_job.yml 2apiVersion: batch/v1 3kind: Job 4metadata: 5 name: word-len-job 6spec: 7 template: 8 metadata: 9 labels: 10 app: word-len-job 11 spec: 12 containers: 13 - name: beam-word-len-job 14 image: beam-python-harness:2.56.0 15 command: [\u0026#34;python\u0026#34;] 16 args: 17 - \u0026#34;-m\u0026#34; 18 - \u0026#34;word_len.run\u0026#34; 19 - \u0026#34;--deploy\u0026#34; 20 - \u0026#34;--bootstrap_servers=demo-cluster-kafka-bootstrap:9092\u0026#34; 21 - \u0026#34;--runner=FlinkRunner\u0026#34; 22 - \u0026#34;--flink_master=word-len-cluster-rest:8081\u0026#34; 23 - \u0026#34;--job_name=beam-word-len\u0026#34; 24 - \u0026#34;--streaming\u0026#34; 25 - \u0026#34;--parallelism=3\u0026#34; 26 - \u0026#34;--flink_submit_uber_jar\u0026#34; 27 - \u0026#34;--environment_type=EXTERNAL\u0026#34; 28 - \u0026#34;--environment_config=localhost:50000\u0026#34; 29 - \u0026#34;--checkpointing_interval=10000\u0026#34; 30 restartPolicy: Never The session cluster and job can be deployed using kubectl create, and the former creates the FlinkDeployment custom resource, which manages the job manager deployment, task manager pod and associated services. When we check the log of the job\u0026rsquo;s pod, we see that it starts the Job Service after downloading the Jar file, uploads the pipeline artifact, submit the pipeline as a Flink job and continuously monitors the job status.\n1kubectl create -f beam/word_len_cluster.yml 2# flinkdeployment.flink.apache.org/word-len-cluster created 3kubectl create -f beam/word_len_job.yml 4# job.batch/word-len-job created 5 6kubectl logs word-len-job-p5rph -f 7# WARNING:apache_beam.options.pipeline_options:Unknown pipeline options received: --checkpointing_interval=10000. Ignore if flags are used for internal purposes. 8# WARNING:apache_beam.options.pipeline_options:Unknown pipeline options received: --checkpointing_interval=10000. Ignore if flags are used for internal purposes. 9# INFO:root:Building pipeline ... 10# INFO:apache_beam.runners.portability.flink_runner:Adding HTTP protocol scheme to flink_master parameter: http://word-len-cluster-rest:8081 11# WARNING:apache_beam.options.pipeline_options:Unknown pipeline options received: --checkpointing_interval=10000. Ignore if flags are used for internal purposes. 12# DEBUG:apache_beam.runners.portability.abstract_job_service:Got Prepare request. 13# DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): word-len-cluster-rest:8081 14# DEBUG:urllib3.connectionpool:http://word-len-cluster-rest:8081 \u0026#34;GET /v1/config HTTP/1.1\u0026#34; 200 240 15# INFO:apache_beam.utils.subprocess_server:Downloading job server jar from https://repo.maven.apache.org/maven2/org/apache/beam/beam-runners-flink-1.16-job-server/2.56.0/beam-runners-flink-1.16-job-server-2.56.0.jar 16# INFO:apache_beam.runners.portability.abstract_job_service:Artifact server started on port 43287 17# DEBUG:apache_beam.runners.portability.abstract_job_service:Prepared job \u0026#39;job\u0026#39; as \u0026#39;job-edc1c2f1-80ef-48b7-af14-7e6fc86f338a\u0026#39; 18# INFO:apache_beam.runners.portability.abstract_job_service:Running job \u0026#39;job-edc1c2f1-80ef-48b7-af14-7e6fc86f338a\u0026#39; 19# DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): word-len-cluster-rest:8081 20# DEBUG:urllib3.connectionpool:http://word-len-cluster-rest:8081 \u0026#34;POST /v1/jars/upload HTTP/1.1\u0026#34; 200 148 21# DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): word-len-cluster-rest:8081 22# DEBUG:urllib3.connectionpool:http://word-len-cluster-rest:8081 \u0026#34;POST /v1/jars/e1984c45-d8bc-4aa1-9b66-369a23826921_beam.jar/run HTTP/1.1\u0026#34; 200 44 23# INFO:apache_beam.runners.portability.flink_uber_jar_job_server:Started Flink job as a403cb2f92fecee65b8fd7cc8ac6e68a 24# INFO:apache_beam.runners.portability.portable_runner:Job state changed to STOPPED 25# DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): word-len-cluster-rest:8081 26# DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): word-len-cluster-rest:8081 27# DEBUG:urllib3.connectionpool:http://word-len-cluster-rest:8081 \u0026#34;GET /v1/jobs/a403cb2f92fecee65b8fd7cc8ac6e68a/execution-result HTTP/1.1\u0026#34; 200 31 28# DEBUG:urllib3.connectionpool:http://word-len-cluster-rest:8081 \u0026#34;GET /v1/jobs/a403cb2f92fecee65b8fd7cc8ac6e68a/execution-result HTTP/1.1\u0026#34; 200 31 29# INFO:apache_beam.runners.portability.portable_runner:Job state changed to RUNNING 30# DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): word-len-cluster-rest:8081 31# DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): word-len-cluster-rest:8081 32# DEBUG:urllib3.connectionpool:http://word-len-cluster-rest:8081 \u0026#34;GET /v1/jobs/a403cb2f92fecee65b8fd7cc8ac6e68a/execution-result HTTP/1.1\u0026#34; 200 31 33# DEBUG:urllib3.connectionpool:http://word-len-cluster-rest:8081 \u0026#34;GET /v1/jobs/a403cb2f92fecee65b8fd7cc8ac6e68a/execution-result HTTP/1.1\u0026#34; 200 31 34# ... After deployment is complete, we can see the following Flink session cluster and job related resources.\n1kubectl get all -l app=word-len-cluster 2# NAME READY STATUS RESTARTS AGE 3# pod/word-len-cluster-7c98f6f868-d4hbx 1/1 Running 0 5m32s 4# pod/word-len-cluster-taskmanager-1-1 2/2 Running 0 4m3s 5 6# NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE 7# service/word-len-cluster ClusterIP None \u0026lt;none\u0026gt; 6123/TCP,6124/TCP 5m32s 8# service/word-len-cluster-rest ClusterIP 10.104.23.28 \u0026lt;none\u0026gt; 8081/TCP 5m32s 9 10# NAME READY UP-TO-DATE AVAILABLE AGE 11# deployment.apps/word-len-cluster 1/1 1 1 5m32s 12 13# NAME DESIRED CURRENT READY AGE 14# replicaset.apps/word-len-cluster-7c98f6f868 1 1 1 5m32s 15 16kubectl get all -l app=word-len-job 17# NAME READY STATUS RESTARTS AGE 18# pod/word-len-job-24r6q 1/1 Running 0 5m24s 19 20# NAME COMPLETIONS DURATION AGE 21# job.batch/word-len-job 0/1 5m24s 5m24s The Flink web UI can be accessed using kubectl port-forward on port 8081. In the job graph, we see there are two tasks where the first task is performed until adding word elements into a fixed time window and the second one is up to sending the average word length records to the output topic.\n1kubectl port-forward svc/flink-word-len-rest 8081 The Kafka I/O automatically creates a topic if it doesn\u0026rsquo;t exist, and we can see the input topic is created on kafka-ui.\nKafka Producer A simple Python Kafka producer is created to check the output of the application. The producer app sends random text from the Faker package to the input Kafka topic every 1 second by default.\n1# kafka/client/producer.py 2import os 3import time 4 5from faker import Faker 6from kafka import KafkaProducer 7 8 9class TextProducer: 10 def __init__(self, bootstrap_servers: list, topic_name: str) -\u0026gt; None: 11 self.bootstrap_servers = bootstrap_servers 12 self.topic_name = topic_name 13 self.kafka_producer = self.create_producer() 14 15 def create_producer(self): 16 \u0026#34;\u0026#34;\u0026#34; 17 Returns a KafkaProducer instance 18 \u0026#34;\u0026#34;\u0026#34; 19 return KafkaProducer( 20 bootstrap_servers=self.bootstrap_servers, 21 value_serializer=lambda v: v.encode(\u0026#34;utf-8\u0026#34;), 22 ) 23 24 def send_to_kafka(self, text: str, timestamp_ms: int = None): 25 \u0026#34;\u0026#34;\u0026#34; 26 Sends text to a Kafka topic. 27 \u0026#34;\u0026#34;\u0026#34; 28 try: 29 args = {\u0026#34;topic\u0026#34;: self.topic_name, \u0026#34;value\u0026#34;: text} 30 if timestamp_ms is not None: 31 args = {**args, **{\u0026#34;timestamp_ms\u0026#34;: timestamp_ms}} 32 self.kafka_producer.send(**args) 33 self.kafka_producer.flush() 34 except Exception as e: 35 raise RuntimeError(\u0026#34;fails to send a message\u0026#34;) from e 36 37 38if __name__ == \u0026#34;__main__\u0026#34;: 39 producer = TextProducer( 40 os.getenv(\u0026#34;BOOTSTRAP_SERVERS\u0026#34;, \u0026#34;localhost:29092\u0026#34;), 41 os.getenv(\u0026#34;TOPIC_NAME\u0026#34;, \u0026#34;input-topic\u0026#34;), 42 ) 43 fake = Faker() 44 45 num_events = 0 46 while True: 47 num_events += 1 48 text = fake.text() 49 producer.send_to_kafka(text) 50 if num_events % 5 == 0: 51 print(f\u0026#34;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;{num_events} text sent... current\u0026gt;\u0026gt;\u0026gt;\u0026gt;\\n{text}\u0026#34;) 52 time.sleep(int(os.getenv(\u0026#34;DELAY_SECONDS\u0026#34;, \u0026#34;1\u0026#34;))) The Kafka bootstrap server can be exposed on port 29092 using kubectl port-forward and the producer app can be started by executing the Python script.\n1kubectl port-forward svc/demo-cluster-kafka-external-bootstrap 29092 2 3python kafka/client/producer.py We can see the output topic (output-topic-flink) is created on kafka-ui.\nAlso, we can check the output messages are created as expected in the Topics tab.\nDelete Resources The Kubernetes resources and minikube cluster can be deleted as shown below.\n1## delete flink operator and related resoruces 2kubectl delete -f beam/word_len_cluster.yml 3kubectl delete -f beam/word_len_job.yml 4helm uninstall flink-kubernetes-operator 5helm repo remove flink-operator-repo 6kubectl delete -f https://github.com/jetstack/cert-manager/releases/download/v1.8.2/cert-manager.yaml 7 8## delete kafka cluster and related resources 9STRIMZI_VERSION=\u0026#34;0.39.0\u0026#34; 10kubectl delete -f kafka/manifests/kafka-cluster.yaml 11kubectl delete -f kafka/manifests/kafka-ui.yaml 12kubectl delete -f kafka/manifests/strimzi-cluster-operator-$STRIMZI_VERSION.yaml 13 14## delete minikube 15minikube delete Summary In this series, we discuss how to deploy Python stream processing applications on Kubernetes, and we developed a PyFlink application in the previous post. In this post, we developed an Apache Beam pipeline using the Python SDK and deployed it on an Apache Flink cluster via the Apache Flink Runner. Same as the previous post, we deployed a Kafka cluster using the Strimzi Operator on a minikube cluster as the pipeline uses Apache Kafka topics for its data source and sink. Then, we developed the pipeline as a Python package and added the package to a custom Docker image so that Python user code can be executed externally. For deployment, we created a Flink session cluster via the Flink Kubernetes Operator, and deployed the pipeline using a Kubernetes job. Finally, we checked the output of the application by sending messages to the input Kafka topic using a Python producer application.\n","date":"June 6, 2024","img":"/blog/2024-06-06-beam-deploy-2/featured.png","lang":"en","langName":"English","largeImg":"/blog/2024-06-06-beam-deploy-2/featured_hu6895fa92b308fae81533c434dc37b86f_58020_500x0_resize_box_3.png","permalink":"/blog/2024-06-06-beam-deploy-2/","series":[{"title":"Deploy Python Stream Processing App on Kubernetes","url":"/series/deploy-python-stream-processing-app-on-kubernetes/"}],"smallImg":"/blog/2024-06-06-beam-deploy-2/featured_hu6895fa92b308fae81533c434dc37b86f_58020_180x0_resize_box_3.png","tags":[{"title":"Apache Beam","url":"/tags/apache-beam/"},{"title":"Apache Flink","url":"/tags/apache-flink/"},{"title":"Apache Kafka","url":"/tags/apache-kafka/"},{"title":"Kubernetes","url":"/tags/kubernetes/"},{"title":"Minikube","url":"/tags/minikube/"},{"title":"Python","url":"/tags/python/"},{"title":"Docker","url":"/tags/docker/"}],"timestamp":1717632000,"title":"Deploy Python Stream Processing App on Kubernetes - Part 2 Beam Pipeline on Flink Runner"},{"categories":[{"title":"Data Streaming","url":"/categories/data-streaming/"}],"content":"Flink Kubernetes Operator acts as a control plane to manage the complete deployment lifecycle of Apache Flink applications. With the operator, we can simplify deployment and management of Python stream processing applications. In this series, we discuss how to deploy a PyFlink application and Python Apache Beam pipeline on the Flink Runner on Kubernetes.\nIn Part 1, we first deploy a Kafka cluster on a minikube cluster as the source and sink of the PyFlink application are Kafka topics. Then, the application source is packaged in a custom Docker image and deployed on the minikube cluster using the Flink Kubernetes Operator. Finally, the output of the application is checked by sending messages to the input Kafka topic using a Python producer application.\nPart 1 PyFlink Application (this post) Part 2 Beam Pipeline on Flink Runner Setup Kafka Cluster As the source and sink of the stream processing application are Kafka topics, a Kafka cluster is deployed using the Strimzi Operator on a minikube cluster. We install Strimzi version 0.39.0 and Kubernetes version 1.25.3. Once the minikube CLI and Docker are installed, a minikube cluster can be created by specifying the desired Kubernetes version.\n1minikube start --cpus=\u0026#39;max\u0026#39; --memory=20480 --addons=metrics-server --kubernetes-version=v1.25.3 Deploy Strimzi Operator The project repository keeps manifest files that can be used to deploy the Strimzi Operator, Kafka cluster and Kafka management app. If you want to download a different version of the operator, you can download the relevant manifest file by specifying the desired version. By default, the manifest file assumes that the resources are deployed in the myproject namespace. As we deploy them in the default namespace, however, we need to change the resource namespace using sed. The operator can be deployed using kubectl create.\n1## download and deploy strimzi oeprator 2STRIMZI_VERSION=\u0026#34;0.39.0\u0026#34; 3 4## (optional) if downloading a different version 5DOWNLOAD_URL=https://github.com/strimzi/strimzi-kafka-operator/releases/download/$STRIMZI_VERSION/strimzi-cluster-operator-$STRIMZI_VERSION.yaml 6curl -L -o kafka/manifests/strimzi-cluster-operator-$STRIMZI_VERSION.yaml ${DOWNLOAD_URL} 7# update namespace from myproject to default 8sed -i \u0026#39;s/namespace: .*/namespace: default/\u0026#39; kafka/manifests/strimzi-cluster-operator-$STRIMZI_VERSION.yaml 9 10## deploy strimzi cluster operator 11kubectl create -f kafka/manifests/strimzi-cluster-operator-$STRIMZI_VERSION.yaml We can check the Strimzi Operator runs as a Deployment.\n1kubectl get deploy,rs,po 2# NAME READY UP-TO-DATE AVAILABLE AGE 3# deployment.apps/strimzi-cluster-operator 1/1 1 1 2m50s 4 5# NAME DESIRED CURRENT READY AGE 6# replicaset.apps/strimzi-cluster-operator-8d6d4795c 1 1 1 2m50s 7 8# NAME READY STATUS RESTARTS AGE 9# pod/strimzi-cluster-operator-8d6d4795c-94t8c 1/1 Running 0 2m49s Deploy Kafka Cluster We deploy a Kafka cluster with a single broker and Zookeeper node. It has both internal and external listeners on port 9092 and 29092 respectively. Note that the external listener will be used to access the Kafka cluster outside the minikube cluster. Also, the cluster is configured to allow automatic creation of topics (auto.create.topics.enable: \u0026ldquo;true\u0026rdquo;) and the default number of partitions is set to 3 (num.partitions: 3).\n1# kafka/manifests/kafka-cluster.yaml 2apiVersion: kafka.strimzi.io/v1beta2 3kind: Kafka 4metadata: 5 name: demo-cluster 6spec: 7 kafka: 8 version: 3.5.2 9 replicas: 1 10 resources: 11 requests: 12 memory: 256Mi 13 cpu: 250m 14 limits: 15 memory: 512Mi 16 cpu: 500m 17 listeners: 18 - name: plain 19 port: 9092 20 type: internal 21 tls: false 22 - name: external 23 port: 29092 24 type: nodeport 25 tls: false 26 storage: 27 type: jbod 28 volumes: 29 - id: 0 30 type: persistent-claim 31 size: 20Gi 32 deleteClaim: true 33 config: 34 offsets.topic.replication.factor: 1 35 transaction.state.log.replication.factor: 1 36 transaction.state.log.min.isr: 1 37 default.replication.factor: 1 38 min.insync.replicas: 1 39 inter.broker.protocol.version: \u0026#34;3.5\u0026#34; 40 auto.create.topics.enable: \u0026#34;true\u0026#34; 41 num.partitions: 3 42 zookeeper: 43 replicas: 1 44 resources: 45 requests: 46 memory: 256Mi 47 cpu: 250m 48 limits: 49 memory: 512Mi 50 cpu: 500m 51 storage: 52 type: persistent-claim 53 size: 10Gi 54 deleteClaim: true The Kafka cluster can be deployed using kubectl create.\n1kubectl create -f kafka/manifests/kafka-cluster.yaml The Kafka and Zookeeper nodes are managed by the StrimziPodSet custom resource. It also creates multiple services, and we use the following services in this series.\ncommunication within the Kubernetes cluster demo-cluster-kafka-bootstrap - to access Kafka brokers from the client and management apps demo-cluster-zookeeper-client - to access Zookeeper node from the management app communication from the host demo-cluster-kafka-external-bootstrap - to access Kafka brokers from the producer app 1kubectl get po,strimzipodsets.core.strimzi.io,svc -l app.kubernetes.io/instance=demo-cluster 2# NAME READY STATUS RESTARTS AGE 3# pod/demo-cluster-kafka-0 1/1 Running 0 115s 4# pod/demo-cluster-zookeeper-0 1/1 Running 0 2m20s 5 6# NAME PODS READY PODS CURRENT PODS AGE 7# strimzipodset.core.strimzi.io/demo-cluster-kafka 1 1 1 115s 8# strimzipodset.core.strimzi.io/demo-cluster-zookeeper 1 1 1 2m20s 9 10# NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE 11# service/demo-cluster-kafka-bootstrap ClusterIP 10.101.175.64 \u0026lt;none\u0026gt; 9091/TCP,9092/TCP 115s 12# service/demo-cluster-kafka-brokers ClusterIP None \u0026lt;none\u0026gt; 9090/TCP,9091/TCP,8443/TCP,9092/TCP 115s 13# service/demo-cluster-kafka-external-0 NodePort 10.106.155.20 \u0026lt;none\u0026gt; 29092:32475/TCP 115s 14# service/demo-cluster-kafka-external-bootstrap NodePort 10.111.244.128 \u0026lt;none\u0026gt; 29092:32674/TCP 115s 15# service/demo-cluster-zookeeper-client ClusterIP 10.100.215.29 \u0026lt;none\u0026gt; 2181/TCP 2m20s 16# service/demo-cluster-zookeeper-nodes ClusterIP None \u0026lt;none\u0026gt; 2181/TCP,2888/TCP,3888/TCP 2m20s Deploy Kafka UI UI for Apache Kafka (kafka-ui) is a free and open-source Kafka management application, and it is deployed as a Kubernetes Deployment. The Deployment is configured to have a single instance, and the Kafka cluster access details are specified as environment variables.\n1# kafka/manifests/kafka-ui.yaml 2apiVersion: v1 3kind: Service 4metadata: 5 labels: 6 app: kafka-ui 7 name: kafka-ui 8spec: 9 type: ClusterIP 10 ports: 11 - port: 8080 12 targetPort: 8080 13 selector: 14 app: kafka-ui 15--- 16apiVersion: apps/v1 17kind: Deployment 18metadata: 19 labels: 20 app: kafka-ui 21 name: kafka-ui 22spec: 23 replicas: 1 24 selector: 25 matchLabels: 26 app: kafka-ui 27 template: 28 metadata: 29 labels: 30 app: kafka-ui 31 spec: 32 containers: 33 - image: provectuslabs/kafka-ui:v0.7.1 34 name: kafka-ui-container 35 ports: 36 - containerPort: 8080 37 env: 38 - name: KAFKA_CLUSTERS_0_NAME 39 value: demo-cluster 40 - name: KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS 41 value: demo-cluster-kafka-bootstrap:9092 42 - name: KAFKA_CLUSTERS_0_ZOOKEEPER 43 value: demo-cluster-zookeeper-client:2181 44 resources: 45 requests: 46 memory: 256Mi 47 cpu: 250m 48 limits: 49 memory: 512Mi 50 cpu: 500m The Kafka management app (kafka-ui) can be deployed using kubectl create.\n1kubectl create -f kafka/manifests/kafka-ui.yaml 2 3kubectl get all -l app=kafka-ui 4# NAME READY STATUS RESTARTS AGE 5# pod/kafka-ui-65dbbc98dc-zl5gv 1/1 Running 0 35s 6 7# NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE 8# service/kafka-ui ClusterIP 10.109.14.33 \u0026lt;none\u0026gt; 8080/TCP 36s 9 10# NAME READY UP-TO-DATE AVAILABLE AGE 11# deployment.apps/kafka-ui 1/1 1 1 35s 12 13# NAME DESIRED CURRENT READY AGE 14# replicaset.apps/kafka-ui-65dbbc98dc 1 1 1 35s We can use kubectl port-forward to connect to the kafka-ui server running in the minikube cluster on port 8080.\n1kubectl port-forward svc/kafka-ui 8080 Develop Stream Processing App A stream processing application is developed using PyFlink, and it is packaged in a custom Docker image for deployment.\nPyFlink Code The application begins with reading text messages from a Kafka topic named input-topic, followed by extracting words by splitting the messages. Next, as we are going to calculate the average lengths of all words, all of them are added to a Tumbling window of 5 seconds - we use a processing time window for simplicity. After that, it calculates the average length of words within a window. Note that, as we are going to include the window start and end timestamps, the ProcessAllWindowFunction is used instead of the AggregateFunction. Finally, the output reocrds are sent to a Kafka topic named output-topic-flink.\n1# flink/word_len.py 2import os 3import re 4import json 5import datetime 6import logging 7import typing 8 9from pyflink.common import WatermarkStrategy 10from pyflink.datastream import ( 11 DataStream, 12 StreamExecutionEnvironment, 13 RuntimeExecutionMode, 14) 15from pyflink.common.typeinfo import Types 16from pyflink.datastream.functions import ProcessAllWindowFunction 17from pyflink.datastream.window import TumblingProcessingTimeWindows, Time, TimeWindow 18from pyflink.datastream.connectors.kafka import ( 19 KafkaSource, 20 KafkaOffsetsInitializer, 21 KafkaSink, 22 KafkaRecordSerializationSchema, 23 DeliveryGuarantee, 24) 25from pyflink.common.serialization import SimpleStringSchema 26 27 28def tokenize(element: str): 29 for word in re.findall(r\u0026#34;[A-Za-z\\\u0026#39;]+\u0026#34;, element): 30 yield word 31 32 33def create_message(element: typing.Tuple[str, str, float]): 34 return json.dumps(dict(zip([\u0026#34;window_start\u0026#34;, \u0026#34;window_end\u0026#34;, \u0026#34;avg_len\u0026#34;], element))) 35 36 37class AverageWindowFunction(ProcessAllWindowFunction): 38 def process( 39 self, context: ProcessAllWindowFunction.Context, elements: typing.Iterable[str] 40 ) -\u0026gt; typing.Iterable[typing.Tuple[str, str, float]]: 41 window: TimeWindow = context.window() 42 window_start = datetime.datetime.fromtimestamp(window.start // 1000).isoformat( 43 timespec=\u0026#34;seconds\u0026#34; 44 ) 45 window_end = datetime.datetime.fromtimestamp(window.end // 1000).isoformat( 46 timespec=\u0026#34;seconds\u0026#34; 47 ) 48 length, count = 0, 0 49 for e in elements: 50 length += len(e) 51 count += 1 52 result = window_start, window_end, length / count if count else float(\u0026#34;NaN\u0026#34;) 53 logging.info(f\u0026#34;AverageWindowFunction: result - {result}\u0026#34;) 54 yield result 55 56 57def define_workflow(source_system: DataStream): 58 return ( 59 source_system.flat_map(tokenize) 60 .window_all(TumblingProcessingTimeWindows.of(Time.seconds(5))) 61 .process(AverageWindowFunction()) 62 ) 63 64 65if __name__ == \u0026#34;__main__\u0026#34;: 66 env = StreamExecutionEnvironment.get_execution_environment() 67 env.set_runtime_mode(RuntimeExecutionMode.STREAMING) 68 env.enable_checkpointing(5000) 69 env.set_parallelism(3) 70 71 input_source = ( 72 KafkaSource.builder() 73 .set_bootstrap_servers(os.getenv(\u0026#34;BOOTSTRAP_SERVERS\u0026#34;, \u0026#34;localhost:29092\u0026#34;)) 74 .set_topics(os.getenv(\u0026#34;INPUT_TOPIC\u0026#34;, \u0026#34;input-topic\u0026#34;)) 75 .set_group_id(os.getenv(\u0026#34;GROUP_ID\u0026#34;, \u0026#34;flink-word-len\u0026#34;)) 76 .set_starting_offsets(KafkaOffsetsInitializer.latest()) 77 .set_value_only_deserializer(SimpleStringSchema()) 78 .build() 79 ) 80 81 input_stream = env.from_source( 82 input_source, WatermarkStrategy.no_watermarks(), \u0026#34;input_source\u0026#34; 83 ) 84 85 output_sink = ( 86 KafkaSink.builder() 87 .set_bootstrap_servers(os.getenv(\u0026#34;BOOTSTRAP_SERVERS\u0026#34;, \u0026#34;localhost:29092\u0026#34;)) 88 .set_record_serializer( 89 KafkaRecordSerializationSchema.builder() 90 .set_topic(os.getenv(\u0026#34;OUTPUT_TOPIC\u0026#34;, \u0026#34;output-topic-flink\u0026#34;)) 91 .set_value_serialization_schema(SimpleStringSchema()) 92 .build() 93 ) 94 .set_delivery_guarantee(DeliveryGuarantee.AT_LEAST_ONCE) 95 .build() 96 ) 97 98 define_workflow(input_stream).map( 99 create_message, output_type=Types.STRING() 100 ).sink_to(output_sink).name(\u0026#34;output_sink\u0026#34;) 101 102 env.execute(\u0026#34;avg-word-length-flink\u0026#34;) Build Docker Image A custom Docker image named flink-python-example:1.17 is created for deployment. Based on the official flink:1.17 image, it downloads dependnent Jar files, installs Python and the apache-flink package, and adds the application source.\n1# flink/Dockerfile 2FROM flink:1.17 3 4ARG PYTHON_VERSION 5ENV PYTHON_VERSION=${PYTHON_VERSION:-3.10.13} 6ARG FLINK_VERSION 7ENV FLINK_VERSION=${FLINK_VERSION:-1.17.2} 8 9## download connector libs 10RUN wget -P /opt/flink/lib/ https://repo.maven.apache.org/maven2/org/apache/kafka/kafka-clients/3.2.3/kafka-clients-3.2.3.jar \\ 11 \u0026amp;\u0026amp; wget -P /opt/flink/lib/ https://repo.maven.apache.org/maven2/org/apache/flink/flink-sql-connector-kafka/$FLINK_VERSION/flink-sql-connector-kafka-$FLINK_VERSION.jar 12 13## install python 14RUN apt-get update -y \u0026amp;\u0026amp; \\ 15 apt-get install -y build-essential libssl-dev zlib1g-dev libbz2-dev libffi-dev liblzma-dev \u0026amp;\u0026amp; \\ 16 wget https://www.python.org/ftp/python/${PYTHON_VERSION}/Python-${PYTHON_VERSION}.tgz \u0026amp;\u0026amp; \\ 17 tar -xvf Python-${PYTHON_VERSION}.tgz \u0026amp;\u0026amp; \\ 18 cd Python-${PYTHON_VERSION} \u0026amp;\u0026amp; \\ 19 ./configure --without-tests --enable-shared \u0026amp;\u0026amp; \\ 20 make -j6 \u0026amp;\u0026amp; \\ 21 make install \u0026amp;\u0026amp; \\ 22 ldconfig /usr/local/lib \u0026amp;\u0026amp; \\ 23 cd .. \u0026amp;\u0026amp; rm -f Python-${PYTHON_VERSION}.tgz \u0026amp;\u0026amp; rm -rf Python-${PYTHON_VERSION} \u0026amp;\u0026amp; \\ 24 ln -s /usr/local/bin/python3 /usr/local/bin/python \u0026amp;\u0026amp; \\ 25 apt-get clean \u0026amp;\u0026amp; \\ 26 rm -rf /var/lib/apt/lists/* 27 28## install pip packages 29RUN pip3 install apache-flink==${FLINK_VERSION} 30 31## add python script 32USER flink 33RUN mkdir /opt/flink/usrlib 34ADD word_len.py /opt/flink/usrlib/word_len.py As the custom image should be accessible in the minikube cluster, we should point the terminal\u0026rsquo;s docker-cli to the minikube\u0026rsquo;s Docker engine. Then, the image can be built as usual using docker build.\n1# point the docker-cli to the minikube\u0026#39;s Docker engine 2eval $(minikube docker-env) 3# build image 4docker build -t flink-python-example:1.17 flink/ Deploy Stream Processing App The Pyflink application is deployed as a single job of a Flink cluster using the Flink Kubernetes Operator. Then, we check the output of the application by sending text messages to the input Kafka topic.\nDeploy Flink Kubernetes Operator We first need to install the certificate manager on the minikube cluster to enable adding the webhook component. Then, the operator can be installed using a Helm chart, and the version 1.8.0 is installed in the post.\n1kubectl create -f https://github.com/jetstack/cert-manager/releases/download/v1.8.2/cert-manager.yaml 2helm repo add flink-operator-repo https://downloads.apache.org/flink/flink-kubernetes-operator-1.8.0/ 3helm install flink-kubernetes-operator flink-operator-repo/flink-kubernetes-operator 4# NAME: flink-kubernetes-operator 5# LAST DEPLOYED: Wed May 29 05:05:08 2024 6# NAMESPACE: default 7# STATUS: deployed 8# REVISION: 1 9# TEST SUITE: None 10 11helm list 12# NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION 13# flink-kubernetes-operator default 1 2024-05-29 05:05:08.241071054 +1000 AEST deployed flink-kubernetes-operator-1.8.0 1.8.0 Deploy PyFlink App The PyFlink app is deployed as a single job of a Flink cluster using the FlinkDeployment custom resource. In the manifest file, we configure common properties such as the Docker image, Flink version, cluster configuration and pod template. These properties are applied to the Flink job manager and task manager, and only the replica and resource are specified additionally to them. Finally, the PyFlink app is added as a job where the main PythonDriver entry class requires the paths of the Python executable and application source script. See this page for details about the FlinkDeployment resource.\n1# flink/word_len.yml 2apiVersion: flink.apache.org/v1beta1 3kind: FlinkDeployment 4metadata: 5 name: flink-word-len 6spec: 7 image: flink-python-example:1.17 8 imagePullPolicy: Never 9 flinkVersion: v1_17 10 flinkConfiguration: 11 taskmanager.numberOfTaskSlots: \u0026#34;5\u0026#34; 12 serviceAccount: flink 13 podTemplate: 14 spec: 15 containers: 16 - name: flink-main-container 17 env: 18 - name: BOOTSTRAP_SERVERS 19 value: demo-cluster-kafka-bootstrap:9092 20 - name: INPUT_TOPIC 21 value: input-topic 22 - name: GROUP_ID 23 value: flink-word-len 24 - name: OUTPUT_TOPIC 25 value: output-topic-flink 26 volumeMounts: 27 - mountPath: /opt/flink/log 28 name: flink-logs 29 - mountPath: /tmp/flink-artifact-staging 30 name: flink-staging 31 volumes: 32 - name: flink-logs 33 emptyDir: {} 34 - name: flink-staging 35 emptyDir: {} 36 jobManager: 37 resource: 38 memory: \u0026#34;2048m\u0026#34; 39 cpu: 1 40 taskManager: 41 replicas: 2 42 resource: 43 memory: \u0026#34;2048m\u0026#34; 44 cpu: 1 45 job: 46 jarURI: local:///opt/flink/opt/flink-python-1.17.2.jar 47 entryClass: \u0026#34;org.apache.flink.client.python.PythonDriver\u0026#34; 48 args: 49 [ 50 \u0026#34;-pyclientexec\u0026#34;, 51 \u0026#34;/usr/local/bin/python3\u0026#34;, 52 \u0026#34;-py\u0026#34;, 53 \u0026#34;/opt/flink/usrlib/word_len.py\u0026#34;, 54 ] 55 parallelism: 3 56 upgradeMode: stateless Before we deploy the PyFlink app, make sure the input topic is created. We can create it using kafka-ui easily.\nThe app can be deployed using kubectl create, and it creates the FlinkDeployment custom resource, which manages the job manager deployment, task manager pod and associated services.\n1kubectl create -f flink/word_len.yml 2# flinkdeployment.flink.apache.org/flink-word-len created 3 4kubectl get flinkdeployments.flink.apache.org 5# NAME JOB STATUS LIFECYCLE STATE 6# flink-word-len RUNNING STABLE 7 8kubectl get all -l app=flink-word-len 9# NAME READY STATUS RESTARTS AGE 10# pod/flink-word-len-854cf856d8-w8cjf 1/1 Running 0 78s 11# pod/flink-word-len-taskmanager-1-1 1/1 Running 0 66s 12 13# NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE 14# service/flink-word-len ClusterIP None \u0026lt;none\u0026gt; 6123/TCP,6124/TCP 78s 15# service/flink-word-len-rest ClusterIP 10.107.62.132 \u0026lt;none\u0026gt; 8081/TCP 78s 16 17# NAME READY UP-TO-DATE AVAILABLE AGE 18# deployment.apps/flink-word-len 1/1 1 1 78s 19 20# NAME DESIRED CURRENT READY AGE 21# replicaset.apps/flink-word-len-854cf856d8 1 1 1 78s The Flink web UI can be accessed using kubectl port-forward on port 8081. In the job graph, we see there are two tasks where the first task is performed until tokenizing input messages and the second one is up to sending the average word length records to the output topic.\n1kubectl port-forward svc/flink-word-len-rest 8081 Kafka Producer A simple Python Kafka producer is created to check the output of the application. The producer app sends random text from the Faker package to the input Kafka topic every 1 second by default.\n1# kafka/client/producer.py 2import os 3import time 4 5from faker import Faker 6from kafka import KafkaProducer 7 8 9class TextProducer: 10 def __init__(self, bootstrap_servers: list, topic_name: str) -\u0026gt; None: 11 self.bootstrap_servers = bootstrap_servers 12 self.topic_name = topic_name 13 self.kafka_producer = self.create_producer() 14 15 def create_producer(self): 16 \u0026#34;\u0026#34;\u0026#34; 17 Returns a KafkaProducer instance 18 \u0026#34;\u0026#34;\u0026#34; 19 return KafkaProducer( 20 bootstrap_servers=self.bootstrap_servers, 21 value_serializer=lambda v: v.encode(\u0026#34;utf-8\u0026#34;), 22 ) 23 24 def send_to_kafka(self, text: str, timestamp_ms: int = None): 25 \u0026#34;\u0026#34;\u0026#34; 26 Sends text to a Kafka topic. 27 \u0026#34;\u0026#34;\u0026#34; 28 try: 29 args = {\u0026#34;topic\u0026#34;: self.topic_name, \u0026#34;value\u0026#34;: text} 30 if timestamp_ms is not None: 31 args = {**args, **{\u0026#34;timestamp_ms\u0026#34;: timestamp_ms}} 32 self.kafka_producer.send(**args) 33 self.kafka_producer.flush() 34 except Exception as e: 35 raise RuntimeError(\u0026#34;fails to send a message\u0026#34;) from e 36 37 38if __name__ == \u0026#34;__main__\u0026#34;: 39 producer = TextProducer( 40 os.getenv(\u0026#34;BOOTSTRAP_SERVERS\u0026#34;, \u0026#34;localhost:29092\u0026#34;), 41 os.getenv(\u0026#34;TOPIC_NAME\u0026#34;, \u0026#34;input-topic\u0026#34;), 42 ) 43 fake = Faker() 44 45 num_events = 0 46 while True: 47 num_events += 1 48 text = fake.text() 49 producer.send_to_kafka(text) 50 if num_events % 5 == 0: 51 print(f\u0026#34;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;{num_events} text sent... current\u0026gt;\u0026gt;\u0026gt;\u0026gt;\\n{text}\u0026#34;) 52 time.sleep(int(os.getenv(\u0026#34;DELAY_SECONDS\u0026#34;, \u0026#34;1\u0026#34;))) The Kafka bootstrap server can be exposed on port 29092 using kubectl port-forward and the producer app can be started by executing the Python script.\n1kubectl port-forward svc/demo-cluster-kafka-external-bootstrap 29092 2 3python kafka/client/producer.py We can see the output topic (output-topic-flink) is created on kafka-ui.\nAlso, we can check the output messages are created as expected in the Topics tab.\nDelete Resources The Kubernetes resources and minikube cluster can be deleted as shown below.\n1## delete flink operator and related resoruces 2kubectl delete flinkdeployment/flink-word-len 3helm uninstall flink-kubernetes-operator 4helm repo remove flink-operator-repo 5kubectl delete -f https://github.com/jetstack/cert-manager/releases/download/v1.8.2/cert-manager.yaml 6 7## delete kafka cluster and related resources 8STRIMZI_VERSION=\u0026#34;0.39.0\u0026#34; 9kubectl delete -f kafka/manifests/kafka-cluster.yaml 10kubectl delete -f kafka/manifests/kafka-ui.yaml 11kubectl delete -f kafka/manifests/strimzi-cluster-operator-$STRIMZI_VERSION.yaml 12 13## delete minikube 14minikube delete Summary Flink Kubernetes Operator acts as a control plane to manage the complete deployment lifecycle of Apache Flink applications. With the operator, we can simplify deployment and management of Python stream processing applications on Kubernetes. In this post, we discussed how to deploy a PyFlink application on Kubernetes. We first deployed a Kafka cluster on a minikube cluster as the source and sink of the PyFlink application are Kafka topics. Then, the application source is packaged in a custom Docker image and deployed on the minikube cluster using the Flink Kubernetes Operator. Finally, the output of the application is checked by sending messages to the input Kafka topic using a Python producer application.\n","date":"May 30, 2024","img":"/blog/2024-05-30-beam-deploy-1/featured.png","lang":"en","langName":"English","largeImg":"/blog/2024-05-30-beam-deploy-1/featured_hu716373a174df6c1bd689346398ab2e3c_64457_500x0_resize_box_3.png","permalink":"/blog/2024-05-30-beam-deploy-1/","series":[{"title":"Deploy Python Stream Processing App on Kubernetes","url":"/series/deploy-python-stream-processing-app-on-kubernetes/"}],"smallImg":"/blog/2024-05-30-beam-deploy-1/featured_hu716373a174df6c1bd689346398ab2e3c_64457_180x0_resize_box_3.png","tags":[{"title":"Apache Flink","url":"/tags/apache-flink/"},{"title":"Apache Kafka","url":"/tags/apache-kafka/"},{"title":"Kubernetes","url":"/tags/kubernetes/"},{"title":"Minikube","url":"/tags/minikube/"},{"title":"Python","url":"/tags/python/"},{"title":"Docker","url":"/tags/docker/"}],"timestamp":1717027200,"title":"Deploy Python Stream Processing App on Kubernetes - Part 1 PyFlink Application"},{"categories":[{"title":"Apache Beam","url":"/categories/apache-beam/"}],"content":"We developed batch and streaming pipelines in Part 2 and Part 4. Often it is faster and simpler to identify and fix bugs on the pipeline code by performing local unit testing. Moreover, especially when it comes to creating a streaming pipeline, unit testing cases can facilitate development further by using TestStream as it allows us to advance watermarks or processing time according to different scenarios. In this post, we discuss how to perform unit testing of the batch and streaming pipelines that we developed earlier.\nPart 1 Pipeline, Notebook, SQL and DataFrame Part 2 Batch Pipelines Part 3 Flink Runner Part 4 Streaming Pipelines Part 5 Testing Pipelines (this post) Batch Pipeline Testing Pipeline Code The pipeline begins with reading data from a folder named inputs and parses the Json lines. Then it creates a key-value pair where the key is the user ID (id) and the value is the file size bytes (file_size_bytes). After that, the records are grouped by the key and aggregated to obtain website visit count and traffic consumption distribution using a ParDo transform. Finally, the output records are written to a folder named outputs after being converted into dictionary.\n1# section4/user_traffic.py 2import os 3import datetime 4import argparse 5import json 6import logging 7import typing 8 9import apache_beam as beam 10from apache_beam.options.pipeline_options import PipelineOptions 11from apache_beam.options.pipeline_options import StandardOptions 12 13 14class EventLog(typing.NamedTuple): 15 ip: str 16 id: str 17 lat: float 18 lng: float 19 user_agent: str 20 age_bracket: str 21 opted_into_marketing: bool 22 http_request: str 23 http_response: int 24 file_size_bytes: int 25 event_datetime: str 26 event_ts: int 27 28 29class UserTraffic(typing.NamedTuple): 30 id: str 31 page_views: int 32 total_bytes: int 33 max_bytes: int 34 min_bytes: int 35 36 37def parse_json(element: str): 38 row = json.loads(element) 39 # lat/lng sometimes empty string 40 if not row[\u0026#34;lat\u0026#34;] or not row[\u0026#34;lng\u0026#34;]: 41 row = {**row, **{\u0026#34;lat\u0026#34;: -1, \u0026#34;lng\u0026#34;: -1}} 42 return EventLog(**row) 43 44 45class Aggregate(beam.DoFn): 46 def process(self, element: typing.Tuple[str, typing.Iterable[int]]): 47 key, values = element 48 yield UserTraffic( 49 id=key, 50 page_views=len(values), 51 total_bytes=sum(values), 52 max_bytes=max(values), 53 min_bytes=min(values), 54 ) 55 56 57def run(): 58 parser = argparse.ArgumentParser(description=\u0026#34;Beam pipeline arguments\u0026#34;) 59 parser.add_argument( 60 \u0026#34;--inputs\u0026#34;, 61 default=\u0026#34;inputs\u0026#34;, 62 help=\u0026#34;Specify folder name that event records are saved\u0026#34;, 63 ) 64 parser.add_argument( 65 \u0026#34;--runner\u0026#34;, default=\u0026#34;DirectRunner\u0026#34;, help=\u0026#34;Specify Apache Beam Runner\u0026#34; 66 ) 67 opts = parser.parse_args() 68 PARENT_DIR = os.path.dirname(os.path.dirname(os.path.realpath(__file__))) 69 70 options = PipelineOptions() 71 options.view_as(StandardOptions).runner = opts.runner 72 73 p = beam.Pipeline(options=options) 74 ( 75 p 76 | \u0026#34;Read from files\u0026#34; 77 \u0026gt;\u0026gt; beam.io.ReadFromText( 78 file_pattern=os.path.join(PARENT_DIR, opts.inputs, \u0026#34;*.out\u0026#34;) 79 ) 80 | \u0026#34;Parse elements\u0026#34; \u0026gt;\u0026gt; beam.Map(parse_json).with_output_types(EventLog) 81 | \u0026#34;Form key value pair\u0026#34; \u0026gt;\u0026gt; beam.Map(lambda e: (e.id, e.file_size_bytes)) 82 | \u0026#34;Group by key\u0026#34; \u0026gt;\u0026gt; beam.GroupByKey() 83 | \u0026#34;Aggregate by id\u0026#34; \u0026gt;\u0026gt; beam.ParDo(Aggregate()).with_output_types(UserTraffic) 84 | \u0026#34;To dict\u0026#34; \u0026gt;\u0026gt; beam.Map(lambda e: e._asdict()) 85 | \u0026#34;Write to file\u0026#34; 86 \u0026gt;\u0026gt; beam.io.WriteToText( 87 file_path_prefix=os.path.join( 88 PARENT_DIR, 89 \u0026#34;outputs\u0026#34;, 90 f\u0026#34;{int(datetime.datetime.now().timestamp() * 1000)}\u0026#34;, 91 ), 92 file_name_suffix=\u0026#34;.out\u0026#34;, 93 ) 94 ) 95 96 logging.getLogger().setLevel(logging.INFO) 97 logging.info(\u0026#34;Building pipeline ...\u0026#34;) 98 99 p.run().wait_until_finish() 100 101 102if __name__ == \u0026#34;__main__\u0026#34;: 103 run() Test Pipeline As illustrated in the Beam documentation, we can use the following pattern to test a Beam pipeline.\nCreate a TestPipeline. Create some static, known test input data. Use the Create transform to create a PCollection of your input data. Apply your transform to the input PCollection and save the resulting output PCollection. Use PAssert and its subclasses (or testing functions in Python) to verify that the output PCollection contains the elements that you expect. We have three unit testing cases for this batch pipeline. The first two cases checks whether the input elements are parsed into the custom EventLog type as expected while the last case is used to test if the pipeline aggregates the elements by user correctly. In all cases, pipeline outputs are compared to expected outputs for verification.\n1# section4/user_traffic_test.py 2import sys 3import unittest 4 5import apache_beam as beam 6from apache_beam.testing.test_pipeline import TestPipeline 7from apache_beam.testing.util import assert_that, equal_to 8 9from user_traffic import EventLog, UserTraffic, parse_json, Aggregate 10 11 12def main(out=sys.stderr, verbosity=2): 13 loader = unittest.TestLoader() 14 15 suite = loader.loadTestsFromModule(sys.modules[__name__]) 16 unittest.TextTestRunner(out, verbosity=verbosity).run(suite) 17 18 19class ParseJsonTest(unittest.TestCase): 20 def test_parse_json(self): 21 with TestPipeline() as p: 22 LINES = [ 23 \u0026#39;{\u0026#34;ip\u0026#34;: \u0026#34;138.201.212.70\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;462520009613048791\u0026#34;, \u0026#34;lat\u0026#34;: 50.4779, \u0026#34;lng\u0026#34;: 12.3713, \u0026#34;user_agent\u0026#34;: \u0026#34;Mozilla/5.0 (iPod; U; CPU iPhone OS 3_1 like Mac OS X; ks-IN) AppleWebKit/532.30.7 (KHTML, like Gecko) Version/3.0.5 Mobile/8B115 Safari/6532.30.7\u0026#34;, \u0026#34;age_bracket\u0026#34;: \u0026#34;18-25\u0026#34;, \u0026#34;opted_into_marketing\u0026#34;: false, \u0026#34;http_request\u0026#34;: \u0026#34;GET eucharya.html HTTP/1.0\u0026#34;, \u0026#34;http_response\u0026#34;: 200, \u0026#34;file_size_bytes\u0026#34;: 207, \u0026#34;event_datetime\u0026#34;: \u0026#34;2024-03-01T05:51:22.083\u0026#34;, \u0026#34;event_ts\u0026#34;: 1709232682083}\u0026#39;, 24 \u0026#39;{\u0026#34;ip\u0026#34;: \u0026#34;105.100.237.193\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;5135574965990269004\u0026#34;, \u0026#34;lat\u0026#34;: 36.7323, \u0026#34;lng\u0026#34;: 3.0875, \u0026#34;user_agent\u0026#34;: \u0026#34;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_9 rv:2.0; wo-SN) AppleWebKit/531.18.2 (KHTML, like Gecko) Version/4.0.4 Safari/531.18.2\u0026#34;, \u0026#34;age_bracket\u0026#34;: \u0026#34;26-40\u0026#34;, \u0026#34;opted_into_marketing\u0026#34;: false, \u0026#34;http_request\u0026#34;: \u0026#34;GET coniferophyta.html HTTP/1.0\u0026#34;, \u0026#34;http_response\u0026#34;: 200, \u0026#34;file_size_bytes\u0026#34;: 427, \u0026#34;event_datetime\u0026#34;: \u0026#34;2024-03-01T05:48:52.985\u0026#34;, \u0026#34;event_ts\u0026#34;: 1709232532985}\u0026#39;, 25 ] 26 27 EXPECTED_OUTPUT = [ 28 EventLog( 29 ip=\u0026#34;138.201.212.70\u0026#34;, 30 id=\u0026#34;462520009613048791\u0026#34;, 31 lat=50.4779, 32 lng=12.3713, 33 user_agent=\u0026#34;Mozilla/5.0 (iPod; U; CPU iPhone OS 3_1 like Mac OS X; ks-IN) AppleWebKit/532.30.7 (KHTML, like Gecko) Version/3.0.5 Mobile/8B115 Safari/6532.30.7\u0026#34;, 34 age_bracket=\u0026#34;18-25\u0026#34;, 35 opted_into_marketing=False, 36 http_request=\u0026#34;GET eucharya.html HTTP/1.0\u0026#34;, 37 http_response=200, 38 file_size_bytes=207, 39 event_datetime=\u0026#34;2024-03-01T05:51:22.083\u0026#34;, 40 event_ts=1709232682083, 41 ), 42 EventLog( 43 ip=\u0026#34;105.100.237.193\u0026#34;, 44 id=\u0026#34;5135574965990269004\u0026#34;, 45 lat=36.7323, 46 lng=3.0875, 47 user_agent=\u0026#34;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_9 rv:2.0; wo-SN) AppleWebKit/531.18.2 (KHTML, like Gecko) Version/4.0.4 Safari/531.18.2\u0026#34;, 48 age_bracket=\u0026#34;26-40\u0026#34;, 49 opted_into_marketing=False, 50 http_request=\u0026#34;GET coniferophyta.html HTTP/1.0\u0026#34;, 51 http_response=200, 52 file_size_bytes=427, 53 event_datetime=\u0026#34;2024-03-01T05:48:52.985\u0026#34;, 54 event_ts=1709232532985, 55 ), 56 ] 57 58 output = ( 59 p 60 | beam.Create(LINES) 61 | beam.Map(parse_json).with_output_types(EventLog) 62 ) 63 64 assert_that(output, equal_to(EXPECTED_OUTPUT)) 65 66 def test_parse_null_lat_lng(self): 67 with TestPipeline() as p: 68 LINES = [ 69 \u0026#39;{\u0026#34;ip\u0026#34;: \u0026#34;138.201.212.70\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;462520009613048791\u0026#34;, \u0026#34;lat\u0026#34;: null, \u0026#34;lng\u0026#34;: null, \u0026#34;user_agent\u0026#34;: \u0026#34;Mozilla/5.0 (iPod; U; CPU iPhone OS 3_1 like Mac OS X; ks-IN) AppleWebKit/532.30.7 (KHTML, like Gecko) Version/3.0.5 Mobile/8B115 Safari/6532.30.7\u0026#34;, \u0026#34;age_bracket\u0026#34;: \u0026#34;18-25\u0026#34;, \u0026#34;opted_into_marketing\u0026#34;: false, \u0026#34;http_request\u0026#34;: \u0026#34;GET eucharya.html HTTP/1.0\u0026#34;, \u0026#34;http_response\u0026#34;: 200, \u0026#34;file_size_bytes\u0026#34;: 207, \u0026#34;event_datetime\u0026#34;: \u0026#34;2024-03-01T05:51:22.083\u0026#34;, \u0026#34;event_ts\u0026#34;: 1709232682083}\u0026#39;, 70 ] 71 72 EXPECTED_OUTPUT = [ 73 EventLog( 74 ip=\u0026#34;138.201.212.70\u0026#34;, 75 id=\u0026#34;462520009613048791\u0026#34;, 76 lat=-1, 77 lng=-1, 78 user_agent=\u0026#34;Mozilla/5.0 (iPod; U; CPU iPhone OS 3_1 like Mac OS X; ks-IN) AppleWebKit/532.30.7 (KHTML, like Gecko) Version/3.0.5 Mobile/8B115 Safari/6532.30.7\u0026#34;, 79 age_bracket=\u0026#34;18-25\u0026#34;, 80 opted_into_marketing=False, 81 http_request=\u0026#34;GET eucharya.html HTTP/1.0\u0026#34;, 82 http_response=200, 83 file_size_bytes=207, 84 event_datetime=\u0026#34;2024-03-01T05:51:22.083\u0026#34;, 85 event_ts=1709232682083, 86 ), 87 ] 88 89 output = ( 90 p 91 | beam.Create(LINES) 92 | beam.Map(parse_json).with_output_types(EventLog) 93 ) 94 95 assert_that(output, equal_to(EXPECTED_OUTPUT)) 96 97 98class AggregateTest(unittest.TestCase): 99 def test_aggregate(self): 100 with TestPipeline() as p: 101 LINES = [ 102 \u0026#39;{\u0026#34;ip\u0026#34;: \u0026#34;138.201.212.70\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;462520009613048791\u0026#34;, \u0026#34;lat\u0026#34;: 50.4779, \u0026#34;lng\u0026#34;: 12.3713, \u0026#34;user_agent\u0026#34;: \u0026#34;Mozilla/5.0 (iPod; U; CPU iPhone OS 3_1 like Mac OS X; ks-IN) AppleWebKit/532.30.7 (KHTML, like Gecko) Version/3.0.5 Mobile/8B115 Safari/6532.30.7\u0026#34;, \u0026#34;age_bracket\u0026#34;: \u0026#34;18-25\u0026#34;, \u0026#34;opted_into_marketing\u0026#34;: false, \u0026#34;http_request\u0026#34;: \u0026#34;GET eucharya.html HTTP/1.0\u0026#34;, \u0026#34;http_response\u0026#34;: 200, \u0026#34;file_size_bytes\u0026#34;: 207, \u0026#34;event_datetime\u0026#34;: \u0026#34;2024-03-01T05:51:22.083\u0026#34;, \u0026#34;event_ts\u0026#34;: 1709232682083}\u0026#39;, 103 \u0026#39;{\u0026#34;ip\u0026#34;: \u0026#34;138.201.212.70\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;462520009613048791\u0026#34;, \u0026#34;lat\u0026#34;: 50.4779, \u0026#34;lng\u0026#34;: 12.3713, \u0026#34;user_agent\u0026#34;: \u0026#34;Mozilla/5.0 (iPod; U; CPU iPhone OS 3_1 like Mac OS X; ks-IN) AppleWebKit/532.30.7 (KHTML, like Gecko) Version/3.0.5 Mobile/8B115 Safari/6532.30.7\u0026#34;, \u0026#34;age_bracket\u0026#34;: \u0026#34;18-25\u0026#34;, \u0026#34;opted_into_marketing\u0026#34;: false, \u0026#34;http_request\u0026#34;: \u0026#34;GET blastocladiomycota.html HTTP/1.0\u0026#34;, \u0026#34;http_response\u0026#34;: 200, \u0026#34;file_size_bytes\u0026#34;: 446, \u0026#34;event_datetime\u0026#34;: \u0026#34;2024-03-01T05:51:48.719\u0026#34;, \u0026#34;event_ts\u0026#34;: 1709232708719}\u0026#39;, 104 \u0026#39;{\u0026#34;ip\u0026#34;: \u0026#34;138.201.212.70\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;462520009613048791\u0026#34;, \u0026#34;lat\u0026#34;: 50.4779, \u0026#34;lng\u0026#34;: 12.3713, \u0026#34;user_agent\u0026#34;: \u0026#34;Mozilla/5.0 (iPod; U; CPU iPhone OS 3_1 like Mac OS X; ks-IN) AppleWebKit/532.30.7 (KHTML, like Gecko) Version/3.0.5 Mobile/8B115 Safari/6532.30.7\u0026#34;, \u0026#34;age_bracket\u0026#34;: \u0026#34;18-25\u0026#34;, \u0026#34;opted_into_marketing\u0026#34;: false, \u0026#34;http_request\u0026#34;: \u0026#34;GET home.html HTTP/1.0\u0026#34;, \u0026#34;http_response\u0026#34;: 200, \u0026#34;file_size_bytes\u0026#34;: 318, \u0026#34;event_datetime\u0026#34;: \u0026#34;2024-03-01T05:51:35.181\u0026#34;, \u0026#34;event_ts\u0026#34;: 1709232695181}\u0026#39;, 105 ] 106 107 EXPECTED_OUTPUT = [ 108 UserTraffic( 109 id=\u0026#34;462520009613048791\u0026#34;, 110 page_views=3, 111 total_bytes=971, 112 max_bytes=446, 113 min_bytes=207, 114 ) 115 ] 116 117 output = ( 118 p 119 | beam.Create(LINES) 120 | beam.Map(parse_json).with_output_types(EventLog) 121 | beam.Map(lambda e: (e.id, e.file_size_bytes)) 122 | beam.GroupByKey() 123 | beam.ParDo(Aggregate()).with_output_types(UserTraffic) 124 ) 125 126 assert_that(output, equal_to(EXPECTED_OUTPUT)) 127 128 129if __name__ == \u0026#34;__main__\u0026#34;: 130 main(out=None) We can perform unit testing of the batch pipeline simply by executing the test script.\n1$ python section4/user_traffic_test.py 2test_aggregate (__main__.AggregateTest) ... ok 3test_parse_json (__main__.ParseJsonTest) ... ok 4test_parse_null_lat_lng (__main__.ParseJsonTest) ... ok 5 6---------------------------------------------------------------------- 7Ran 3 tests in 1.278s 8 9OK Streaming Pipeline Testing Pipeline Code It begins with reading and decoding messages from a Kafka topic named website-visit, followed by parsing the decoded Json string into a custom type named EventLog. After that, timestamp is re-assigned based on the event_datetime attribute and the element is converted into a key-value pair where user ID is taken as the key and 1 is given as the value. Finally, the tuple elements are aggregated in a fixed time window of 20 seconds and written to a Kafka topic named traffic-agg. Note that the output messages include two additional attributes (window_start and window_end) to clarify in which window they belong to.\n1# section4/traffic_agg.py 2import os 3import datetime 4import argparse 5import json 6import logging 7import typing 8 9import apache_beam as beam 10from apache_beam.io import kafka 11from apache_beam.options.pipeline_options import PipelineOptions 12from apache_beam.options.pipeline_options import SetupOptions 13 14 15class EventLog(typing.NamedTuple): 16 ip: str 17 id: str 18 lat: float 19 lng: float 20 user_agent: str 21 age_bracket: str 22 opted_into_marketing: bool 23 http_request: str 24 http_response: int 25 file_size_bytes: int 26 event_datetime: str 27 event_ts: int 28 29 30def decode_message(kafka_kv: tuple): 31 return kafka_kv[1].decode(\u0026#34;utf-8\u0026#34;) 32 33 34def create_message(element: dict): 35 key = {\u0026#34;event_id\u0026#34;: element[\u0026#34;id\u0026#34;], \u0026#34;window_start\u0026#34;: element[\u0026#34;window_start\u0026#34;]} 36 print(element) 37 return json.dumps(key).encode(\u0026#34;utf-8\u0026#34;), json.dumps(element).encode(\u0026#34;utf-8\u0026#34;) 38 39 40def parse_json(element: str): 41 row = json.loads(element) 42 # lat/lng sometimes empty string 43 if not row[\u0026#34;lat\u0026#34;] or not row[\u0026#34;lng\u0026#34;]: 44 row = {**row, **{\u0026#34;lat\u0026#34;: -1, \u0026#34;lng\u0026#34;: -1}} 45 return EventLog(**row) 46 47 48def assign_timestamp(element: EventLog): 49 ts = datetime.datetime.strptime( 50 element.event_datetime, \u0026#34;%Y-%m-%dT%H:%M:%S.%f\u0026#34; 51 ).timestamp() 52 return beam.window.TimestampedValue(element, ts) 53 54 55class AddWindowTS(beam.DoFn): 56 def process(self, element: tuple, window=beam.DoFn.WindowParam): 57 window_start = window.start.to_utc_datetime().isoformat(timespec=\u0026#34;seconds\u0026#34;) 58 window_end = window.end.to_utc_datetime().isoformat(timespec=\u0026#34;seconds\u0026#34;) 59 output = { 60 \u0026#34;id\u0026#34;: element[0], 61 \u0026#34;window_start\u0026#34;: window_start, 62 \u0026#34;window_end\u0026#34;: window_end, 63 \u0026#34;page_views\u0026#34;: element[1], 64 } 65 yield output 66 67 68def run(): 69 parser = argparse.ArgumentParser(description=\u0026#34;Beam pipeline arguments\u0026#34;) 70 parser.add_argument( 71 \u0026#34;--runner\u0026#34;, default=\u0026#34;FlinkRunner\u0026#34;, help=\u0026#34;Specify Apache Beam Runner\u0026#34; 72 ) 73 parser.add_argument( 74 \u0026#34;--use_own\u0026#34;, 75 action=\u0026#34;store_true\u0026#34;, 76 default=\u0026#34;Flag to indicate whether to use a own local cluster\u0026#34;, 77 ) 78 opts = parser.parse_args() 79 80 pipeline_opts = { 81 \u0026#34;runner\u0026#34;: opts.runner, 82 \u0026#34;job_name\u0026#34;: \u0026#34;traffic-agg\u0026#34;, 83 \u0026#34;environment_type\u0026#34;: \u0026#34;LOOPBACK\u0026#34;, 84 \u0026#34;streaming\u0026#34;: True, 85 \u0026#34;parallelism\u0026#34;: 3, 86 \u0026#34;experiments\u0026#34;: [ 87 \u0026#34;use_deprecated_read\u0026#34; 88 ], ## https://github.com/apache/beam/issues/20979 89 \u0026#34;checkpointing_interval\u0026#34;: \u0026#34;60000\u0026#34;, 90 } 91 if opts.use_own is True: 92 pipeline_opts = {**pipeline_opts, **{\u0026#34;flink_master\u0026#34;: \u0026#34;localhost:8081\u0026#34;}} 93 print(pipeline_opts) 94 options = PipelineOptions([], **pipeline_opts) 95 # Required, else it will complain that when importing worker functions 96 options.view_as(SetupOptions).save_main_session = True 97 98 p = beam.Pipeline(options=options) 99 ( 100 p 101 | \u0026#34;Read from Kafka\u0026#34; 102 \u0026gt;\u0026gt; kafka.ReadFromKafka( 103 consumer_config={ 104 \u0026#34;bootstrap.servers\u0026#34;: os.getenv( 105 \u0026#34;BOOTSTRAP_SERVERS\u0026#34;, 106 \u0026#34;host.docker.internal:29092\u0026#34;, 107 ), 108 \u0026#34;auto.offset.reset\u0026#34;: \u0026#34;earliest\u0026#34;, 109 # \u0026#34;enable.auto.commit\u0026#34;: \u0026#34;true\u0026#34;, 110 \u0026#34;group.id\u0026#34;: \u0026#34;traffic-agg\u0026#34;, 111 }, 112 topics=[\u0026#34;website-visit\u0026#34;], 113 ) 114 | \u0026#34;Decode messages\u0026#34; \u0026gt;\u0026gt; beam.Map(decode_message) 115 | \u0026#34;Parse elements\u0026#34; \u0026gt;\u0026gt; beam.Map(parse_json).with_output_types(EventLog) 116 | \u0026#34;Assign timestamp\u0026#34; \u0026gt;\u0026gt; beam.Map(assign_timestamp) 117 | \u0026#34;Form key value pair\u0026#34; \u0026gt;\u0026gt; beam.Map(lambda e: (e.id, 1)) 118 | \u0026#34;Tumble window per minute\u0026#34; \u0026gt;\u0026gt; beam.WindowInto(beam.window.FixedWindows(20)) 119 | \u0026#34;Sum by key\u0026#34; \u0026gt;\u0026gt; beam.CombinePerKey(sum) 120 | \u0026#34;Add window timestamp\u0026#34; \u0026gt;\u0026gt; beam.ParDo(AddWindowTS()) 121 | \u0026#34;Create messages\u0026#34; 122 \u0026gt;\u0026gt; beam.Map(create_message).with_output_types(typing.Tuple[bytes, bytes]) 123 | \u0026#34;Write to Kafka\u0026#34; 124 \u0026gt;\u0026gt; kafka.WriteToKafka( 125 producer_config={ 126 \u0026#34;bootstrap.servers\u0026#34;: os.getenv( 127 \u0026#34;BOOTSTRAP_SERVERS\u0026#34;, 128 \u0026#34;host.docker.internal:29092\u0026#34;, 129 ) 130 }, 131 topic=\u0026#34;traffic-agg\u0026#34;, 132 ) 133 ) 134 135 logging.getLogger().setLevel(logging.INFO) 136 logging.info(\u0026#34;Building pipeline ...\u0026#34;) 137 138 p.run().wait_until_finish() 139 140 141if __name__ == \u0026#34;__main__\u0026#34;: 142 run() Test Pipeline For testing the streaming pipeline, we use a TestStream, which is used to generate events on an unbounded PCollection of elements. The stream has three elements of a single user with the following timestamp values.\n2024-03-01T05:51:22.083 2024-03-01T05:51:32.083 2024-03-01T05:51:52.083 Therefore, if we aggregate the elements in a fixed time window of 20 seconds, we can expect the first two elements are grouped together while the last element is grouped on its own. The expected output can be created accordingly and compared to the pipeline output for verification.\n1# section4/traffic_agg_test.py 2import datetime 3import sys 4import unittest 5 6import apache_beam as beam 7from apache_beam.testing.test_pipeline import TestPipeline 8from apache_beam.testing.util import assert_that, equal_to 9from apache_beam.testing.test_stream import TestStream 10from apache_beam.options.pipeline_options import PipelineOptions, StandardOptions 11 12from traffic_agg import EventLog, parse_json, assign_timestamp 13 14 15def main(out=sys.stderr, verbosity=2): 16 loader = unittest.TestLoader() 17 18 suite = loader.loadTestsFromModule(sys.modules[__name__]) 19 unittest.TextTestRunner(out, verbosity=verbosity).run(suite) 20 21 22class TrafficWindowingTest(unittest.TestCase): 23 def test_windowing_behaviour(self): 24 options = PipelineOptions() 25 options.view_as(StandardOptions).streaming = True 26 with TestPipeline(options=options) as p: 27 EVENTS = [ 28 \u0026#39;{\u0026#34;ip\u0026#34;: \u0026#34;138.201.212.70\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;462520009613048791\u0026#34;, \u0026#34;lat\u0026#34;: 50.4779, \u0026#34;lng\u0026#34;: 12.3713, \u0026#34;user_agent\u0026#34;: \u0026#34;Mozilla/5.0 (iPod; U; CPU iPhone OS 3_1 like Mac OS X; ks-IN) AppleWebKit/532.30.7 (KHTML, like Gecko) Version/3.0.5 Mobile/8B115 Safari/6532.30.7\u0026#34;, \u0026#34;age_bracket\u0026#34;: \u0026#34;18-25\u0026#34;, \u0026#34;opted_into_marketing\u0026#34;: false, \u0026#34;http_request\u0026#34;: \u0026#34;GET eucharya.html HTTP/1.0\u0026#34;, \u0026#34;http_response\u0026#34;: 200, \u0026#34;file_size_bytes\u0026#34;: 207, \u0026#34;event_datetime\u0026#34;: \u0026#34;2024-03-01T05:51:22.083\u0026#34;, \u0026#34;event_ts\u0026#34;: 1709232682083}\u0026#39;, 29 \u0026#39;{\u0026#34;ip\u0026#34;: \u0026#34;138.201.212.70\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;462520009613048791\u0026#34;, \u0026#34;lat\u0026#34;: 50.4779, \u0026#34;lng\u0026#34;: 12.3713, \u0026#34;user_agent\u0026#34;: \u0026#34;Mozilla/5.0 (iPod; U; CPU iPhone OS 3_1 like Mac OS X; ks-IN) AppleWebKit/532.30.7 (KHTML, like Gecko) Version/3.0.5 Mobile/8B115 Safari/6532.30.7\u0026#34;, \u0026#34;age_bracket\u0026#34;: \u0026#34;18-25\u0026#34;, \u0026#34;opted_into_marketing\u0026#34;: false, \u0026#34;http_request\u0026#34;: \u0026#34;GET eucharya.html HTTP/1.0\u0026#34;, \u0026#34;http_response\u0026#34;: 200, \u0026#34;file_size_bytes\u0026#34;: 207, \u0026#34;event_datetime\u0026#34;: \u0026#34;2024-03-01T05:51:32.083\u0026#34;, \u0026#34;event_ts\u0026#34;: 1709232682083}\u0026#39;, 30 \u0026#39;{\u0026#34;ip\u0026#34;: \u0026#34;138.201.212.70\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;462520009613048791\u0026#34;, \u0026#34;lat\u0026#34;: 50.4779, \u0026#34;lng\u0026#34;: 12.3713, \u0026#34;user_agent\u0026#34;: \u0026#34;Mozilla/5.0 (iPod; U; CPU iPhone OS 3_1 like Mac OS X; ks-IN) AppleWebKit/532.30.7 (KHTML, like Gecko) Version/3.0.5 Mobile/8B115 Safari/6532.30.7\u0026#34;, \u0026#34;age_bracket\u0026#34;: \u0026#34;18-25\u0026#34;, \u0026#34;opted_into_marketing\u0026#34;: false, \u0026#34;http_request\u0026#34;: \u0026#34;GET eucharya.html HTTP/1.0\u0026#34;, \u0026#34;http_response\u0026#34;: 200, \u0026#34;file_size_bytes\u0026#34;: 207, \u0026#34;event_datetime\u0026#34;: \u0026#34;2024-03-01T05:51:52.083\u0026#34;, \u0026#34;event_ts\u0026#34;: 1709232682083}\u0026#39;, 31 ] 32 33 test_stream = ( 34 TestStream() 35 .advance_watermark_to(0) 36 .add_elements([EVENTS[0], EVENTS[1], EVENTS[2]]) 37 .advance_watermark_to_infinity() 38 ) 39 40 output = ( 41 p 42 | test_stream 43 | beam.Map(parse_json).with_output_types(EventLog) 44 | beam.Map(assign_timestamp) 45 | beam.Map(lambda e: (e.id, 1)) 46 | beam.WindowInto(beam.window.FixedWindows(20)) 47 | beam.CombinePerKey(sum) 48 ) 49 50 EXPECTED_OUTPUT = [(\u0026#34;462520009613048791\u0026#34;, 2), (\u0026#34;462520009613048791\u0026#34;, 1)] 51 52 assert_that(output, equal_to(EXPECTED_OUTPUT)) 53 54 55if __name__ == \u0026#34;__main__\u0026#34;: 56 main(out=None) Similar to the previous testing, we can execute the test script for performing unit testing of the streaming pipeline.\n1$ python section4/traffic_agg_test.py 2test_windowing_behaviour (__main__.TrafficWindowingTest) ... ok 3 4---------------------------------------------------------------------- 5Ran 1 test in 0.527s 6 7OK Summary In this series of posts, we discussed local development of Apache Beam pipelines using Python. In Part 1, a basic Beam pipeline was introduced, followed by demonstrating how to utilise Jupyter notebooks for interactive development. Several notebook examples were covered including Beam SQL and Beam DataFrames. Batch pipelines were developed in Part 2, and we used pipelines from GCP Python DataFlow Quest while modifying them to access local resources only. Each batch pipeline has two versions with/without SQL. In Part 3, we discussed how to set up local Flink and Kafka clusters for deploying streaming pipelines on the Flink Runner. A streaming pipeline with/without Beam SQL was built in Part 4, and this series concludes with illustrating unit testing of existing pipelines in this post.\n","date":"May 9, 2024","img":"/blog/2024-05-09-beam-local-dev-5/featured.png","lang":"en","langName":"English","largeImg":"/blog/2024-05-09-beam-local-dev-5/featured_huc6e6335fd74f53db2ce0e3219c776146_53603_500x0_resize_box_3.png","permalink":"/blog/2024-05-09-beam-local-dev-5/","series":[{"title":"Apache Beam Local Development With Python","url":"/series/apache-beam-local-development-with-python/"}],"smallImg":"/blog/2024-05-09-beam-local-dev-5/featured_huc6e6335fd74f53db2ce0e3219c776146_53603_180x0_resize_box_3.png","tags":[{"title":"Apache Beam","url":"/tags/apache-beam/"},{"title":"Apache Flink","url":"/tags/apache-flink/"},{"title":"Python","url":"/tags/python/"}],"timestamp":1715212800,"title":"Apache Beam Local Development With Python - Part 5 Testing Pipelines"},{"categories":[{"title":"Apache Beam","url":"/categories/apache-beam/"}],"content":"In Part 3, we discussed the portability layer of Apache Beam as it helps understand (1) how Python pipelines run on the Flink Runner and (2) how multiple SDKs can be used in a single pipeline, followed by demonstrating local Flink and Kafka cluster creation for developing streaming pipelines. In this post, we build a streaming pipeline that aggregates page visits by user in a fixed time window of 20 seconds. Two versions of the pipeline are created with/without relying on Beam SQL.\nPart 1 Pipeline, Notebook, SQL and DataFrame Part 2 Batch Pipelines Part 3 Flink Runner Part 4 Streaming Pipelines (this post) Part 5 Testing Pipelines Streaming Pipeline The streaming pipeline we discuss in this post aggregates website visits by user ID in a fixed time window of 20 seconds. Two versions of the pipeline are created with/without relying on Beam SQL, and they run on a Flink cluster at the end. The source of this post can be found in this GitHub repository.\nTraffic Aggregation It begins with reading and decoding messages from a Kafka topic named website-visit, followed by parsing the decoded Json string into a custom type named EventLog. Note the coder for this custom type is registered, but it is not required because we don\u0026rsquo;t have a cross-language transformation that deals with it. On the other hand, the coder has to be registered for the SQL version because it is used by the SQL transformation, which is performed using the Java SDK.\nAfter that, timestamp is re-assigned based on the event_datetime attribute and the element is converted into a key-value pair where user ID is taken as the key and 1 is given as the value. By default, the Kafka reader assigns processing time (wall clock) as the element timestamp. If record timestamp is different from wall clock, we would have more relevant outcomes by re-assigning based on record timestamp.\nThe tuple elements are aggregated in a fixed time window of 20 seconds and written to a Kafka topic named traffic-agg. The output messages include two additional attributes (window_start and window_end) to clarify in which window they belong to.\n1# section3/traffic_agg.py 2import os 3import datetime 4import argparse 5import json 6import logging 7import typing 8 9import apache_beam as beam 10from apache_beam.io import kafka 11from apache_beam.options.pipeline_options import PipelineOptions 12from apache_beam.options.pipeline_options import SetupOptions 13 14 15class EventLog(typing.NamedTuple): 16 ip: str 17 id: str 18 lat: float 19 lng: float 20 user_agent: str 21 age_bracket: str 22 opted_into_marketing: bool 23 http_request: str 24 http_response: int 25 file_size_bytes: int 26 event_datetime: str 27 event_ts: int 28 29 30beam.coders.registry.register_coder(EventLog, beam.coders.RowCoder) 31 32 33def decode_message(kafka_kv: tuple): 34 return kafka_kv[1].decode(\u0026#34;utf-8\u0026#34;) 35 36 37def create_message(element: dict): 38 key = {\u0026#34;event_id\u0026#34;: element[\u0026#34;id\u0026#34;], \u0026#34;window_start\u0026#34;: element[\u0026#34;window_start\u0026#34;]} 39 print(element) 40 return json.dumps(key).encode(\u0026#34;utf-8\u0026#34;), json.dumps(element).encode(\u0026#34;utf-8\u0026#34;) 41 42 43def parse_json(element: str): 44 row = json.loads(element) 45 # lat/lng sometimes empty string 46 if not row[\u0026#34;lat\u0026#34;] or not row[\u0026#34;lng\u0026#34;]: 47 row = {**row, **{\u0026#34;lat\u0026#34;: -1, \u0026#34;lng\u0026#34;: -1}} 48 return EventLog(**row) 49 50 51def assign_timestamp(element: EventLog): 52 ts = datetime.datetime.strptime( 53 element.event_datetime, \u0026#34;%Y-%m-%dT%H:%M:%S.%f\u0026#34; 54 ).timestamp() 55 return beam.window.TimestampedValue(element, ts) 56 57 58class AddWindowTS(beam.DoFn): 59 def process(self, element: tuple, window=beam.DoFn.WindowParam): 60 window_start = window.start.to_utc_datetime().isoformat(timespec=\u0026#34;seconds\u0026#34;) 61 window_end = window.end.to_utc_datetime().isoformat(timespec=\u0026#34;seconds\u0026#34;) 62 output = { 63 \u0026#34;id\u0026#34;: element[0], 64 \u0026#34;window_start\u0026#34;: window_start, 65 \u0026#34;window_end\u0026#34;: window_end, 66 \u0026#34;page_views\u0026#34;: element[1], 67 } 68 yield output 69 70 71def run(): 72 parser = argparse.ArgumentParser(description=\u0026#34;Beam pipeline arguments\u0026#34;) 73 parser.add_argument( 74 \u0026#34;--runner\u0026#34;, default=\u0026#34;FlinkRunner\u0026#34;, help=\u0026#34;Specify Apache Beam Runner\u0026#34; 75 ) 76 parser.add_argument( 77 \u0026#34;--use_own\u0026#34;, 78 action=\u0026#34;store_true\u0026#34;, 79 default=\u0026#34;Flag to indicate whether to use an own local cluster\u0026#34;, 80 ) 81 opts = parser.parse_args() 82 83 pipeline_opts = { 84 \u0026#34;runner\u0026#34;: opts.runner, 85 \u0026#34;job_name\u0026#34;: \u0026#34;traffic-agg\u0026#34;, 86 \u0026#34;environment_type\u0026#34;: \u0026#34;LOOPBACK\u0026#34;, 87 \u0026#34;streaming\u0026#34;: True, 88 \u0026#34;parallelism\u0026#34;: 3, 89 \u0026#34;experiments\u0026#34;: [ 90 \u0026#34;use_deprecated_read\u0026#34; 91 ], ## https://github.com/apache/beam/issues/20979 92 \u0026#34;checkpointing_interval\u0026#34;: \u0026#34;60000\u0026#34;, 93 } 94 if opts.use_own is True: 95 pipeline_opts = {**pipeline_opts, **{\u0026#34;flink_master\u0026#34;: \u0026#34;localhost:8081\u0026#34;}} 96 print(pipeline_opts) 97 options = PipelineOptions([], **pipeline_opts) 98 # Required, else it will complain that when importing worker functions 99 options.view_as(SetupOptions).save_main_session = True 100 101 p = beam.Pipeline(options=options) 102 ( 103 p 104 | \u0026#34;Read from Kafka\u0026#34; 105 \u0026gt;\u0026gt; kafka.ReadFromKafka( 106 consumer_config={ 107 \u0026#34;bootstrap.servers\u0026#34;: os.getenv( 108 \u0026#34;BOOTSTRAP_SERVERS\u0026#34;, 109 \u0026#34;host.docker.internal:29092\u0026#34;, 110 ), 111 \u0026#34;auto.offset.reset\u0026#34;: \u0026#34;earliest\u0026#34;, 112 # \u0026#34;enable.auto.commit\u0026#34;: \u0026#34;true\u0026#34;, 113 \u0026#34;group.id\u0026#34;: \u0026#34;traffic-agg\u0026#34;, 114 }, 115 topics=[\u0026#34;website-visit\u0026#34;], 116 ) 117 | \u0026#34;Decode messages\u0026#34; \u0026gt;\u0026gt; beam.Map(decode_message) 118 | \u0026#34;Parse elements\u0026#34; \u0026gt;\u0026gt; beam.Map(parse_json).with_output_types(EventLog) 119 | \u0026#34;Assign timestamp\u0026#34; \u0026gt;\u0026gt; beam.Map(assign_timestamp) 120 | \u0026#34;Form key value pair\u0026#34; \u0026gt;\u0026gt; beam.Map(lambda e: (e.id, 1)) 121 | \u0026#34;Tumble window per minute\u0026#34; \u0026gt;\u0026gt; beam.WindowInto(beam.window.FixedWindows(20)) 122 | \u0026#34;Sum by key\u0026#34; \u0026gt;\u0026gt; beam.CombinePerKey(sum) 123 | \u0026#34;Add window timestamp\u0026#34; \u0026gt;\u0026gt; beam.ParDo(AddWindowTS()) 124 | \u0026#34;Create messages\u0026#34; 125 \u0026gt;\u0026gt; beam.Map(create_message).with_output_types(typing.Tuple[bytes, bytes]) 126 | \u0026#34;Write to Kafka\u0026#34; 127 \u0026gt;\u0026gt; kafka.WriteToKafka( 128 producer_config={ 129 \u0026#34;bootstrap.servers\u0026#34;: os.getenv( 130 \u0026#34;BOOTSTRAP_SERVERS\u0026#34;, 131 \u0026#34;host.docker.internal:29092\u0026#34;, 132 ) 133 }, 134 topic=\u0026#34;traffic-agg\u0026#34;, 135 ) 136 ) 137 138 logging.getLogger().setLevel(logging.INFO) 139 logging.info(\u0026#34;Building pipeline ...\u0026#34;) 140 141 p.run().wait_until_finish() 142 143 144if __name__ == \u0026#34;__main__\u0026#34;: 145 run() SQL Traffic Aggregation The main difference of this version is that multiple transformations are performed by a single SQL transformation. Specifically it aggregates the number of page views by user in a fixed time window of 20 seconds. The SQL transformation performed in a separate Docker container using the Java SDK and thus the output type has to be specified before it. Otherwise, an error is thrown because the Java SDK doesn\u0026rsquo;t know how to encode/decode the elements.\n1# section3/traffic_agg_sql.py 2import os 3import datetime 4import argparse 5import json 6import logging 7import typing 8 9import apache_beam as beam 10from apache_beam.io import kafka 11from apache_beam.transforms.sql import SqlTransform 12from apache_beam.options.pipeline_options import PipelineOptions 13from apache_beam.options.pipeline_options import SetupOptions 14 15 16class EventLog(typing.NamedTuple): 17 ip: str 18 id: str 19 lat: float 20 lng: float 21 user_agent: str 22 age_bracket: str 23 opted_into_marketing: bool 24 http_request: str 25 http_response: int 26 file_size_bytes: int 27 event_datetime: str 28 event_ts: int 29 30 31beam.coders.registry.register_coder(EventLog, beam.coders.RowCoder) 32 33 34def decode_message(kafka_kv: tuple): 35 return kafka_kv[1].decode(\u0026#34;utf-8\u0026#34;) 36 37 38def create_message(element: dict): 39 key = {\u0026#34;event_id\u0026#34;: element[\u0026#34;event_id\u0026#34;], \u0026#34;window_start\u0026#34;: element[\u0026#34;window_start\u0026#34;]} 40 print(element) 41 return json.dumps(key).encode(\u0026#34;utf-8\u0026#34;), json.dumps(element).encode(\u0026#34;utf-8\u0026#34;) 42 43 44def parse_json(element: str): 45 row = json.loads(element) 46 # lat/lng sometimes empty string 47 if not row[\u0026#34;lat\u0026#34;] or not row[\u0026#34;lng\u0026#34;]: 48 row = {**row, **{\u0026#34;lat\u0026#34;: -1, \u0026#34;lng\u0026#34;: -1}} 49 return EventLog(**row) 50 51 52def format_timestamp(element: EventLog): 53 event_ts = datetime.datetime.fromisoformat(element.event_datetime) 54 temp_dict = element._asdict() 55 temp_dict[\u0026#34;event_datetime\u0026#34;] = datetime.datetime.strftime( 56 event_ts, \u0026#34;%Y-%m-%d %H:%M:%S\u0026#34; 57 ) 58 return EventLog(**temp_dict) 59 60 61def run(): 62 parser = argparse.ArgumentParser(description=\u0026#34;Beam pipeline arguments\u0026#34;) 63 parser.add_argument( 64 \u0026#34;--runner\u0026#34;, default=\u0026#34;FlinkRunner\u0026#34;, help=\u0026#34;Specify Apache Beam Runner\u0026#34; 65 ) 66 parser.add_argument( 67 \u0026#34;--use_own\u0026#34;, 68 action=\u0026#34;store_true\u0026#34;, 69 default=\u0026#34;Flag to indicate whether to use an own local cluster\u0026#34;, 70 ) 71 opts = parser.parse_args() 72 73 options = PipelineOptions() 74 pipeline_opts = { 75 \u0026#34;runner\u0026#34;: opts.runner, 76 \u0026#34;job_name\u0026#34;: \u0026#34;traffic-agg-sql\u0026#34;, 77 \u0026#34;environment_type\u0026#34;: \u0026#34;LOOPBACK\u0026#34;, 78 \u0026#34;streaming\u0026#34;: True, 79 \u0026#34;parallelism\u0026#34;: 3, 80 \u0026#34;experiments\u0026#34;: [ 81 \u0026#34;use_deprecated_read\u0026#34; 82 ], ## https://github.com/apache/beam/issues/20979 83 \u0026#34;checkpointing_interval\u0026#34;: \u0026#34;60000\u0026#34;, 84 } 85 if opts.use_own is True: 86 pipeline_opts = {**pipeline_opts, **{\u0026#34;flink_master\u0026#34;: \u0026#34;localhost:8081\u0026#34;}} 87 print(pipeline_opts) 88 options = PipelineOptions([], **pipeline_opts) 89 # Required, else it will complain that when importing worker functions 90 options.view_as(SetupOptions).save_main_session = True 91 92 query = \u0026#34;\u0026#34;\u0026#34; 93 WITH cte AS ( 94 SELECT 95 id, 96 CAST(event_datetime AS TIMESTAMP) AS ts 97 FROM PCOLLECTION 98 ) 99 SELECT 100 id AS event_id, 101 CAST(TUMBLE_START(ts, INTERVAL \u0026#39;20\u0026#39; SECOND) AS VARCHAR) AS window_start, 102 CAST(TUMBLE_END(ts, INTERVAL \u0026#39;20\u0026#39; SECOND) AS VARCHAR) AS window_end, 103 COUNT(*) AS page_view 104 FROM cte 105 GROUP BY 106 TUMBLE(ts, INTERVAL \u0026#39;20\u0026#39; SECOND), id 107 \u0026#34;\u0026#34;\u0026#34; 108 109 p = beam.Pipeline(options=options) 110 ( 111 p 112 | \u0026#34;Read from Kafka\u0026#34; 113 \u0026gt;\u0026gt; kafka.ReadFromKafka( 114 consumer_config={ 115 \u0026#34;bootstrap.servers\u0026#34;: os.getenv( 116 \u0026#34;BOOTSTRAP_SERVERS\u0026#34;, 117 \u0026#34;host.docker.internal:29092\u0026#34;, 118 ), 119 \u0026#34;auto.offset.reset\u0026#34;: \u0026#34;earliest\u0026#34;, 120 # \u0026#34;enable.auto.commit\u0026#34;: \u0026#34;true\u0026#34;, 121 \u0026#34;group.id\u0026#34;: \u0026#34;traffic-agg-sql\u0026#34;, 122 }, 123 topics=[\u0026#34;website-visit\u0026#34;], 124 ) 125 | \u0026#34;Decode messages\u0026#34; \u0026gt;\u0026gt; beam.Map(decode_message) 126 | \u0026#34;Parse elements\u0026#34; \u0026gt;\u0026gt; beam.Map(parse_json) 127 | \u0026#34;Format timestamp\u0026#34; \u0026gt;\u0026gt; beam.Map(format_timestamp).with_output_types(EventLog) 128 | \u0026#34;Count per minute\u0026#34; \u0026gt;\u0026gt; SqlTransform(query) 129 | \u0026#34;To dictionary\u0026#34; \u0026gt;\u0026gt; beam.Map(lambda e: e._asdict()) 130 | \u0026#34;Create messages\u0026#34; 131 \u0026gt;\u0026gt; beam.Map(create_message).with_output_types(typing.Tuple[bytes, bytes]) 132 | \u0026#34;Write to Kafka\u0026#34; 133 \u0026gt;\u0026gt; kafka.WriteToKafka( 134 producer_config={ 135 \u0026#34;bootstrap.servers\u0026#34;: os.getenv( 136 \u0026#34;BOOTSTRAP_SERVERS\u0026#34;, 137 \u0026#34;host.docker.internal:29092\u0026#34;, 138 ) 139 }, 140 topic=\u0026#34;traffic-agg-sql\u0026#34;, 141 ) 142 ) 143 144 logging.getLogger().setLevel(logging.INFO) 145 logging.info(\u0026#34;Building pipeline ...\u0026#34;) 146 147 p.run().wait_until_finish() 148 149 150if __name__ == \u0026#34;__main__\u0026#34;: 151 run() Run Pipeline We can use local Flink and Kafka clusters as discussed in Part 3. The Flink cluster is optional as Beam runs a pipeline on an embedded Flink cluster if we do not specify a cluster URL.\nStart Flink/Kafka Clusters As shown later, I have an issue to run the SQL version of the pipeline on a local cluster, and it has to be deployed on an embedded cluster instead. With the -a option, we can deploy local Flink and Kafka clusters, and they are used for the pipeline without SQL while only a local Kafka cluster is launched for the SQL version with the -k option.\n1# start both flink and kafka cluster for traffic aggregation 2$ ./setup/start-flink-env.sh -a 3 4# start only kafka cluster for sql traffic aggregation 5$ ./setup/start-flink-env.sh -k Data Generation For streaming data generation, we can use the website visit log generator that was introduced in Part 1. We can execute the script while specifying the source argument to streaming. Below shows an example of generating Kafka messages for the streaming pipeline.\n1$ python datagen/generate_data.py --source streaming --num_users 5 --delay_seconds 0.5 2... 310 events created so far... 4{\u0026#39;ip\u0026#39;: \u0026#39;151.21.93.137\u0026#39;, \u0026#39;id\u0026#39;: \u0026#39;2142139324490406578\u0026#39;, \u0026#39;lat\u0026#39;: 45.5253, \u0026#39;lng\u0026#39;: 9.333, \u0026#39;user_agent\u0026#39;: \u0026#39;Mozilla/5.0 (iPad; CPU iPad OS 14_2_1 like Mac OS X) AppleWebKit/536.0 (KHTML, like Gecko) FxiOS/16.3w0588.0 Mobile/66I206 Safari/536.0\u0026#39;, \u0026#39;age_bracket\u0026#39;: \u0026#39;26-40\u0026#39;, \u0026#39;opted_into_marketing\u0026#39;: True, \u0026#39;http_request\u0026#39;: \u0026#39;GET amoebozoa.html HTTP/1.0\u0026#39;, \u0026#39;http_response\u0026#39;: 200, \u0026#39;file_size_bytes\u0026#39;: 453, \u0026#39;event_datetime\u0026#39;: \u0026#39;2024-04-28T23:12:50.484\u0026#39;, \u0026#39;event_ts\u0026#39;: 1714309970484} 520 events created so far... 6{\u0026#39;ip\u0026#39;: \u0026#39;146.13.4.138\u0026#39;, \u0026#39;id\u0026#39;: \u0026#39;5642783739616136718\u0026#39;, \u0026#39;lat\u0026#39;: 39.0437, \u0026#39;lng\u0026#39;: -77.4875, \u0026#39;user_agent\u0026#39;: \u0026#39;Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10_7_5 rv:4.0; bg-BG) AppleWebKit/532.16.6 (KHTML, like Gecko) Version/4.0.5 Safari/532.16.6\u0026#39;, \u0026#39;age_bracket\u0026#39;: \u0026#39;41-55\u0026#39;, \u0026#39;opted_into_marketing\u0026#39;: False, \u0026#39;http_request\u0026#39;: \u0026#39;GET archaea.html HTTP/1.0\u0026#39;, \u0026#39;http_response\u0026#39;: 200, \u0026#39;file_size_bytes\u0026#39;: 207, \u0026#39;event_datetime\u0026#39;: \u0026#39;2024-04-28T23:12:55.526\u0026#39;, \u0026#39;event_ts\u0026#39;: 1714309975526} 730 events created so far... 8{\u0026#39;ip\u0026#39;: \u0026#39;36.255.131.188\u0026#39;, \u0026#39;id\u0026#39;: \u0026#39;676397447776623774\u0026#39;, \u0026#39;lat\u0026#39;: 31.2222, \u0026#39;lng\u0026#39;: 121.4581, \u0026#39;user_agent\u0026#39;: \u0026#39;Mozilla/5.0 (compatible; MSIE 7.0; Windows 98; Win 9x 4.90; Trident/4.0)\u0026#39;, \u0026#39;age_bracket\u0026#39;: \u0026#39;26-40\u0026#39;, \u0026#39;opted_into_marketing\u0026#39;: False, \u0026#39;http_request\u0026#39;: \u0026#39;GET fungi.html HTTP/1.0\u0026#39;, \u0026#39;http_response\u0026#39;: 200, \u0026#39;file_size_bytes\u0026#39;: 440, \u0026#39;event_datetime\u0026#39;: \u0026#39;2024-04-28T23:13:00.564\u0026#39;, \u0026#39;event_ts\u0026#39;: 1714309980564} Execute Pipeline Script Traffic Aggregation The traffic aggregation pipeline can be executed using the local Flink cluster by specifying the use_own argument.\n1$ python section3/traffic_agg.py --use_own After a while, we can check both the input and output topics in the Topics section of kafka-ui. It can be accessed on localhost:8080.\nWe can use the Flink web UI to monitor the pipeline as a Flink job. When we click the traffic-agg job in the Running Jobs section, we see 4 operations are linked in the Overview tab. The first two operations are polling and reading Kafka source description. All the transformations up to windowing the keyed elements are performed in the third operation, and the elements are aggregated and written to the Kafka output topic in the last operation.\nSQL Traffic Aggregation I see the following error when I execute the SQL version of the pipeline with the use_own option. It seems that the Java SDK container for SQL transformation fails to download its expansion service and does not complete initialisation steps - see Part 3 for details about how multiple SDKs can be used in a single pipeline. Therefore, the Flink job fails to access the SDK container, and it keeps recreate a new container.\nWe can see lots of containers are stopped and get recreated.\n1$ docker ps -a --format \u0026#34;table {{.ID}}\\t{{.Image}}\\t{{.Status}}\u0026#34; | grep apache/beam_java11_sdk 246c51d89e966 apache/beam_java11_sdk:2.53.0 Up 7 seconds 32ad755fc66df apache/beam_java11_sdk:2.53.0 Up 7 seconds 4cf023d9bf39f apache/beam_java11_sdk:2.53.0 Exited (1) 13 seconds ago 5a549729318e3 apache/beam_java11_sdk:2.53.0 Exited (1) 38 seconds ago 695626f645252 apache/beam_java11_sdk:2.53.0 Exited (1) 57 seconds ago 738b56216e29a apache/beam_java11_sdk:2.53.0 Exited (1) About a minute ago 83aee486b472f apache/beam_java11_sdk:2.53.0 Exited (1) About a minute ago Instead, we can run the pipeline on an embedded Flink cluster without adding the use_own option. Note that we need to stop the existing clusters, start only a Kafka cluster with the -k option and re-generate data before executing this pipeline script.\n1$ python section3/traffic_agg_sql.py Similar to the earlier version, we can check the input and output topics on localhost:8080 as well.\nSummary In this post, we developed a streaming pipeline that aggregates website visits by user in a fixed time window of 20 seconds. Two versions of the pipeline were created with/without relying on Beam SQL. The first version that doesn\u0026rsquo;t rely on SQL was deployed on a local Flink cluster, and how it is deployed as a Flink job is checked on the Flink web UI. The second version, however, had an issue to deploy on a local Flink cluster, and it was deployed on an embedded Flink cluster.\n","date":"May 2, 2024","img":"/blog/2024-05-02-beam-local-dev-4/featured.png","lang":"en","langName":"English","largeImg":"/blog/2024-05-02-beam-local-dev-4/featured_hu03c27d6b9ebae8125b322f8d9fec30d2_54556_500x0_resize_box_3.png","permalink":"/blog/2024-05-02-beam-local-dev-4/","series":[{"title":"Apache Beam Local Development With Python","url":"/series/apache-beam-local-development-with-python/"}],"smallImg":"/blog/2024-05-02-beam-local-dev-4/featured_hu03c27d6b9ebae8125b322f8d9fec30d2_54556_180x0_resize_box_3.png","tags":[{"title":"Apache Beam","url":"/tags/apache-beam/"},{"title":"Apache Flink","url":"/tags/apache-flink/"},{"title":"Python","url":"/tags/python/"}],"timestamp":1714608000,"title":"Apache Beam Local Development With Python - Part 4 Streaming Pipelines"},{"categories":[{"title":"Apache Beam","url":"/categories/apache-beam/"}],"content":"In this series, we discuss local development of Apache Beam pipelines using Python. In the previous posts, we mainly talked about Batch pipelines with/without Beam SQL. Beam pipelines are portable between batch and streaming semantics, and we will discuss streaming pipeline development in this and the next posts. While there are multiple Beam Runners, not every Runner supports Python or some Runners have too limited features in streaming semantics - see Beam Capability Matrix for details. So far, the Apache Flink and Google Cloud Dataflow Runners are the best options, and we will use the Flink Runner in this series. This post begins with demonstrating the portability layer of Apache Beam as it helps understand (1) how a pipeline developed by the Python SDK can be executed in the Flink Runner that only understands Java JAR and (2) how multiple SDKs can be used in a single pipeline. Then we discuss how to start up/tear down local Flink and Kafka clusters using bash scripts. Finally, we end up demonstrating a simple streaming pipeline, which reads and writes website visit logs from and to Kafka topics.\nPart 1 Pipeline, Notebook, SQL and DataFrame Part 2 Batch Pipelines Part 3 Flink Runner (this post) Part 4 Streaming Pipelines Part 5 Testing Pipelines Portability Layer Apache Beam Pipelines are portable on several layers between (1) Beam Runners, (2) batch/streaming semantics and (3) programming languages.\nThe portability between programming languages are achieved by the portability layer, and it has two components - Apache Beam components and Runner components. Essentially the Beam Runners (Apache Flink, Apache Spark, Google Cloud Datafolow \u0026hellip;) don\u0026rsquo;t have to understand a Beam SDK but are able to execute pipelines built by it regardless.\nEach Runner typically has a coordinator that needs to receive a job submission and creates tasks for worker nodes according to the submission. For example, the coordinator of the Flink Runner is the Flink JobManager, and it receives a Java JAR file for job execution along with the Directed Acyclic Graph (DAG) of transforms, serialized user code and so on.\nThen there are two problems to solve.\nHow can a non-Java SDK converts a Beam pipeline into a Java JAR that the Flink Runner understands? How can the Runner\u0026rsquo;s worker nodes execute non-Java user code? These problems are tackled down by (1) portable pipeline representation, (2) Job Service, and (3) SDK harness.\nPortable pipeline representation A non-Java SDK converts a Beam pipeline into a portable representation, which mainly includes the Directed Acyclic Graph (DAG) of transforms and serialized user-defined functions (UDFs)/user code - eg beam.Map(\u0026hellip;). Protocol buffers are used for this representation, and it is submitted to Job Service once created.\nJob Service This component receives a portable representation of a pipeline and converts it into a format that a Runner can understand. Note that, when it creates a job, it replaces calls to UDFs with calls to the SDK harness process in which the UDFs are actually executed for the Runner. Also, it instructs the Runner coordinator to create a SDK harness process for each worker.\nSDK harness As mentioned, calls to UDFs on a Runner worker are delegated to calls in the SDK harness process, and the UDFs are executed using the same SDK that they are created. Note that communication between the Runner worker and the SDK harness process is made by gRPC - an HTTP/2 based protocol that relies on protocol buffers as its serialization mechanism. The harness is specific to SDK and, for the Python SDK, there are multiple options.\nDOCKER (default): User code is executed within a container started on each worker node. PROCESS: User code is executed by processes that are automatically started by the runner on each worker node. EXTERNAL: User code is dispatched to an external service. LOOPBACK: User code is executed within the same process that submitted the pipeline and is useful for local testing. Cross-language Pipelines The portability layer can be extended to cross-language pipelines where transforms are mixed from multiple SDKs. A typical example is the Kafka Connector I/O for the Python SDK where the ReadFromKafka and WriteToKafka transforms are made by the Java SDK. Also, the SQL transform (SqlTransform) of Beam SQL is performed by the Java SDK.\nHere the challenge is how to make a non-Java SDK to be able to serialize data for a Java SDK so that its portable pipeline representation can be created! This challenge is handled by the expansion service. Simply put, when a source SDK wants to submit a pipeline to a Runner, it creates its portable pipeline representation. During this process, if it sees an external (cross-language) transform, it sends a request to the expansion service, asking it to expand the transform into a portable representation. Then, the expansion service creates/returns the portable representation, and it is inserted into the complete pipeline representation. For the Python SDK, the expansion service gets started automatically, or we can customize it, for example, to change the SDK harness from DOCKER to PROCESS.\nNote this section is based on Building Big Data Pipelines with Apache Beam by Jan Lukavský and you can check more details in the book!\nManage Streaming Environment The streaming development environment requires local Apache Flink and Apache Kafka clusters. Initially I was going to create a Flink cluster on Docker, but I had an issue that the Kafka Connect I/O fails to resolve Kafka bootstrap addresses. Specifically, for the Kafka I/O, a docker container is launched by the Flink TaskManager with the host network (--network host) - remind that the default SDK harness option is DOCKER. Then the SDK harness container looks for Kafka bootstrap addresses in its host, which is the Flink TaskManager container, not the Docker host machine. Therefore, the address resolution fails because the Kafka cluster doesn\u0026rsquo;t run there. It would work with other SDH harness options, but I thought it requires too much setup for local development. On the other hand, the issue no longer applies if we launch a Flink cluster locally, and we will use this approach instead. The source of this post can be found in this GitHub repository.\nFlink Cluster The latest supported version of Apache Flink is 1.16 as of writing this post, and we can download and unpack it in a dedicated folder named setup as shown below. Note that some scripts in the bin folder should be executable, and their permission is changed at the end.\n1$ mkdir setup \u0026amp;\u0026amp; cd setup 2$ wget https://dlcdn.apache.org/flink/flink-1.16.3/flink-1.16.3-bin-scala_2.12.tgz 3$ tar -zxf flink-1.16.3-bin-scala_2.12.tgz 4$ chmod -R +x flink-1.16.3/bin/ Next, Flink configuration is updated so that the Flink UI is accessible and the number of tasks slots is increased to 10.\n1# setup/flink-1.16.3/config/flink-conf.yaml 2 rest.port: 8081 # uncommented 3 rest.address: localhost # kept as is 4 rest.bind-address: 0.0.0.0 # changed from localhost 5 taskmanager.numberOfTaskSlots: 10 # updated from 1 Kafka Cluster A Kafka cluster with 1 broker and 1 Zookeeper node is used for this post together with a Kafka management app (kafka-ui). The details of setting up the resources can be found in my Kafka Development with Docker series.\nPart 1 Cluster Setup Part 2 Management App Those resources are deployed using Docker Compose with the following configuration file.\n1# setup/docker-compose.yml 2version: \u0026#34;3.5\u0026#34; 3 4services: 5 zookeeper: 6 image: bitnami/zookeeper:3.5 7 container_name: zookeeper 8 expose: 9 - 2181 10 networks: 11 - appnet 12 environment: 13 - ALLOW_ANONYMOUS_LOGIN=yes 14 volumes: 15 - zookeeper_data:/bitnami/zookeeper 16 kafka-0: 17 image: bitnami/kafka:2.8.1 18 container_name: kafka-0 19 expose: 20 - 9092 21 ports: 22 - \u0026#34;29092:29092\u0026#34; 23 networks: 24 - appnet 25 environment: 26 - ALLOW_PLAINTEXT_LISTENER=yes 27 - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181 28 - KAFKA_CFG_BROKER_ID=0 29 - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT 30 - KAFKA_CFG_LISTENERS=INTERNAL://:9092,EXTERNAL://:29092 31 - KAFKA_CFG_ADVERTISED_LISTENERS=INTERNAL://kafka-0:9092,EXTERNAL://localhost:29092 32 - KAFKA_CFG_INTER_BROKER_LISTENER_NAME=INTERNAL 33 - KAFKA_CFG_NUM_PARTITIONS=3 34 - KAFKA_CFG_DEFAULT_REPLICATION_FACTOR=1 35 volumes: 36 - kafka_0_data:/bitnami/kafka 37 depends_on: 38 - zookeeper 39 kafka-ui: 40 image: provectuslabs/kafka-ui:v0.7.1 41 container_name: kafka-ui 42 ports: 43 - \u0026#34;8080:8080\u0026#34; 44 networks: 45 - appnet 46 environment: 47 KAFKA_CLUSTERS_0_NAME: local 48 KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka-0:9092 49 KAFKA_CLUSTERS_0_ZOOKEEPER: zookeeper:2181 50 depends_on: 51 - zookeeper 52 - kafka-0 53 54networks: 55 appnet: 56 name: app-network 57 58volumes: 59 zookeeper_data: 60 driver: local 61 name: zookeeper_data 62 kafka_0_data: 63 driver: local 64 name: kafka_0_data Note that the bootstrap server is exposed on port 29092, and it can be accessed via localhost:29092 from the Docker host machine and host.docker.internal:29092 from a Docker container that is launched with the host network. Note further that, for the latter to work, we have to update the /etc/hosts file by adding an entry for host.docker.internal as shown below.\n1$ cat /etc/hosts | grep host.docker.internal 2# 127.0.0.1 host.docker.internal Manage Flink/Kafka Clusters The Flink and Kafka clusters are managed by bash scripts. They accept three arguments: -k or -f to launch a Kafka or Flink cluster individually or -a to launch both of them. Below shows the startup script, and it creates a Kafka cluster on Docker followed by starting a Flink cluster if conditions are met.\n1# setup/start-flink-env.sh 2#!/usr/bin/env bash 3 4while [[ \u0026#34;$#\u0026#34; -gt 0 ]]; do 5 case $1 in 6 -k|--kafka) start_kafka=true;; 7 -f|--flink) start_flink=true;; 8 -a|--all) start_all=true;; 9 *) echo \u0026#34;Unknown parameter passed: $1\u0026#34;; exit 1 ;; 10 esac 11 shift 12done 13 14if [ ! -z $start_all ] \u0026amp;\u0026amp; [ $start_all = true ]; then 15 start_kafka=true 16 start_flink=true 17fi 18 19SCRIPT_DIR=$(dirname \u0026#34;$(readlink -f \u0026#34;$0\u0026#34;)\u0026#34;) 20 21#### start kafka cluster on docker 22if [ ! -z $start_kafka ] \u0026amp;\u0026amp; [ $start_kafka = true ]; then 23 docker-compose -f ${SCRIPT_DIR}/docker-compose.yml up -d 24fi 25 26#### start local flink cluster 27if [ ! -z $start_flink ] \u0026amp;\u0026amp; [ $start_flink = true ]; then 28 ${SCRIPT_DIR}/flink-1.16.3/bin/start-cluster.sh 29fi The teardown script is structured to stop/remove the Kafka-related containers and Docker volumes, stop the Flink cluster and remove unused containers. The last pruning is for cleaning up containers that are created by Java SDK harness processes.\n1# setup/stop-flink-env.sh 2#!/usr/bin/env bash 3 4while [[ \u0026#34;$#\u0026#34; -gt 0 ]]; do 5 case $1 in 6 -k|--kafka) stop_kafka=true;; 7 -f|--flink) stop_flink=true;; 8 -a|--all) stop_all=true;; 9 *) echo \u0026#34;Unknown parameter passed: $1\u0026#34;; exit 1 ;; 10 esac 11 shift 12done 13 14if [ ! -z $stop_all ] \u0026amp;\u0026amp; [ $stop_all = true ]; then 15 stop_kafka=true 16 stop_flink=true 17fi 18 19SCRIPT_DIR=$(dirname \u0026#34;$(readlink -f \u0026#34;$0\u0026#34;)\u0026#34;) 20 21#### stop kafka cluster in docker 22if [ ! -z $stop_kafka ] \u0026amp;\u0026amp; [ $stop_kafka = true ]; then 23 docker-compose -f ${SCRIPT_DIR}/docker-compose.yml down -v 24fi 25 26#### stop local flink cluster 27if [ ! -z $stop_flink ] \u0026amp;\u0026amp; [ $stop_flink = true ]; then 28 ${SCRIPT_DIR}/flink-1.16.3/bin/stop-cluster.sh 29fi 30 31#### remove all stopped containers 32docker container prune -f Steaming Pipeline The streaming pipeline is really simple in this post that it just (1) reads/decodes messages from an input Kafka topic named website-visit, (2) parses the elements into a pre-defined type of EventLog and (3) encodes/sends them into an output Kafka topic named website-out.\nThe pipeline has a number of notable options as shown below. Those that are specific to the Flink Runner are marked in bold, and they can be checked further in the Flink Runner document.\nrunner - The name of the Beam Runner, default to FlinkRunner. job_name - The pipeline job name that can be checked eg on the Flink UI. environment_type - The SDK harness environment type. LOOPBACK is selected for development, so that user code is executed within the same process that submitted the pipeline. streaming - The flag whether to enforce streaming mode or not. parallelism - The degree of parallelism to be used when distributing operations onto workers. experiments \u0026gt; use_deprecated_read - Use the depreciated read mode for the Kafka IO to work. See BEAM-11998 for details. checkpointing_interval - The interval in milliseconds at which to trigger checkpoints of the running pipeline. flink_master - The address of the Flink Master where the pipeline should be executed. It is automatically set as localhost:8081 if the use_own argument is included. Otherwise, the pipeline runs with an embedded Flink cluster. 1# section3/kafka_io.py 2import os 3import argparse 4import json 5import logging 6import typing 7 8import apache_beam as beam 9from apache_beam.io import kafka 10from apache_beam.options.pipeline_options import PipelineOptions 11from apache_beam.options.pipeline_options import SetupOptions 12 13 14class EventLog(typing.NamedTuple): 15 ip: str 16 id: str 17 lat: float 18 lng: float 19 user_agent: str 20 age_bracket: str 21 opted_into_marketing: bool 22 http_request: str 23 http_response: int 24 file_size_bytes: int 25 event_datetime: str 26 event_ts: int 27 28 29beam.coders.registry.register_coder(EventLog, beam.coders.RowCoder) 30 31 32def decode_message(kafka_kv: tuple): 33 return kafka_kv[1].decode(\u0026#34;utf-8\u0026#34;) 34 35 36def create_message(element: EventLog): 37 key = {\u0026#34;event_id\u0026#34;: element.id, \u0026#34;event_ts\u0026#34;: element.event_ts} 38 value = element._asdict() 39 print(key) 40 return json.dumps(key).encode(\u0026#34;utf-8\u0026#34;), json.dumps(value).encode(\u0026#34;utf-8\u0026#34;) 41 42 43def parse_json(element: str): 44 row = json.loads(element) 45 # lat/lng sometimes empty string 46 if not row[\u0026#34;lat\u0026#34;] or not row[\u0026#34;lng\u0026#34;]: 47 row = {**row, **{\u0026#34;lat\u0026#34;: -1, \u0026#34;lng\u0026#34;: -1}} 48 return EventLog(**row) 49 50 51def run(): 52 parser = argparse.ArgumentParser(description=\u0026#34;Beam pipeline arguments\u0026#34;) 53 parser.add_argument( 54 \u0026#34;--runner\u0026#34;, default=\u0026#34;FlinkRunner\u0026#34;, help=\u0026#34;Specify Apache Beam Runner\u0026#34; 55 ) 56 parser.add_argument( 57 \u0026#34;--use_own\u0026#34;, 58 action=\u0026#34;store_true\u0026#34;, 59 default=\u0026#34;Flag to indicate whether to use an own local cluster\u0026#34;, 60 ) 61 opts = parser.parse_args() 62 63 pipeline_opts = { 64 \u0026#34;runner\u0026#34;: opts.runner, 65 \u0026#34;job_name\u0026#34;: \u0026#34;kafka-io\u0026#34;, 66 \u0026#34;environment_type\u0026#34;: \u0026#34;LOOPBACK\u0026#34;, 67 \u0026#34;streaming\u0026#34;: True, 68 \u0026#34;parallelism\u0026#34;: 3, 69 \u0026#34;experiments\u0026#34;: [ 70 \u0026#34;use_deprecated_read\u0026#34; 71 ], ## https://github.com/apache/beam/issues/20979 72 \u0026#34;checkpointing_interval\u0026#34;: \u0026#34;60000\u0026#34;, 73 } 74 if opts.use_own is True: 75 pipeline_opts = {**pipeline_opts, **{\u0026#34;flink_master\u0026#34;: \u0026#34;localhost:8081\u0026#34;}} 76 print(pipeline_opts) 77 options = PipelineOptions([], **pipeline_opts) 78 # Required, else it will complain that when importing worker functions 79 options.view_as(SetupOptions).save_main_session = True 80 81 p = beam.Pipeline(options=options) 82 ( 83 p 84 | \u0026#34;Read from Kafka\u0026#34; 85 \u0026gt;\u0026gt; kafka.ReadFromKafka( 86 consumer_config={ 87 \u0026#34;bootstrap.servers\u0026#34;: os.getenv( 88 \u0026#34;BOOTSTRAP_SERVERS\u0026#34;, 89 \u0026#34;host.docker.internal:29092\u0026#34;, 90 ), 91 \u0026#34;auto.offset.reset\u0026#34;: \u0026#34;earliest\u0026#34;, 92 # \u0026#34;enable.auto.commit\u0026#34;: \u0026#34;true\u0026#34;, 93 \u0026#34;group.id\u0026#34;: \u0026#34;kafka-io\u0026#34;, 94 }, 95 topics=[\u0026#34;website-visit\u0026#34;], 96 ) 97 | \u0026#34;Decode messages\u0026#34; \u0026gt;\u0026gt; beam.Map(decode_message) 98 | \u0026#34;Parse elements\u0026#34; \u0026gt;\u0026gt; beam.Map(parse_json).with_output_types(EventLog) 99 | \u0026#34;Create messages\u0026#34; 100 \u0026gt;\u0026gt; beam.Map(create_message).with_output_types(typing.Tuple[bytes, bytes]) 101 | \u0026#34;Write to Kafka\u0026#34; 102 \u0026gt;\u0026gt; kafka.WriteToKafka( 103 producer_config={ 104 \u0026#34;bootstrap.servers\u0026#34;: os.getenv( 105 \u0026#34;BOOTSTRAP_SERVERS\u0026#34;, 106 \u0026#34;host.docker.internal:29092\u0026#34;, 107 ) 108 }, 109 topic=\u0026#34;website-out\u0026#34;, 110 ) 111 ) 112 113 logging.getLogger().setLevel(logging.INFO) 114 logging.info(\u0026#34;Building pipeline ...\u0026#34;) 115 116 p.run().wait_until_finish() 117 118 119if __name__ == \u0026#34;__main__\u0026#34;: 120 run() Start Flink/Kafka Clusters To run the pipeline, we need to launch a Kafka cluster and optionally a Flink cluster. They can be created using the startup script with the -a or -k option. We create both the clusters for the example pipeline of this post.\n1# start both flink and kafka cluster 2$ ./setup/start-flink-env.sh -a 3 4# start only kafka cluster 5# ./setup/start-flink-env.sh -k Once the clusters are launched, we can check the Kafka resources on localhost:8080.\nAnd the Flink web UI is accessible on localhost:8081.\nGenerate Data For streaming data generation, we can use the website visit log generator that was introduced in Part 1. We can execute the script while specifying the source argument to streaming. Below shows an example of generating Kafka messages for the streaming pipeline.\n1$ python datagen/generate_data.py --source streaming --num_users 5 --delay_seconds 0.5 2# 10 events created so far... 3# {\u0026#39;ip\u0026#39;: \u0026#39;126.1.20.79\u0026#39;, \u0026#39;id\u0026#39;: \u0026#39;-2901668335848977108\u0026#39;, \u0026#39;lat\u0026#39;: 35.6895, \u0026#39;lng\u0026#39;: 139.6917, \u0026#39;user_agent\u0026#39;: \u0026#39;Mozilla/5.0 (Windows; U; Windows NT 5.1) AppleWebKit/533.44.1 (KHTML, like Gecko) Version/4.0.2 Safari/533.44.1\u0026#39;, \u0026#39;age_bracket\u0026#39;: \u0026#39;26-40\u0026#39;, \u0026#39;opted_into_marketing\u0026#39;: True, \u0026#39;http_request\u0026#39;: \u0026#39;GET chromista.html HTTP/1.0\u0026#39;, \u0026#39;http_response\u0026#39;: 200, \u0026#39;file_size_bytes\u0026#39;: 316, \u0026#39;event_datetime\u0026#39;: \u0026#39;2024-04-14T21:51:33.042\u0026#39;, \u0026#39;event_ts\u0026#39;: 1713095493042} 4# 20 events created so far... 5# {\u0026#39;ip\u0026#39;: \u0026#39;126.1.20.79\u0026#39;, \u0026#39;id\u0026#39;: \u0026#39;-2901668335848977108\u0026#39;, \u0026#39;lat\u0026#39;: 35.6895, \u0026#39;lng\u0026#39;: 139.6917, \u0026#39;user_agent\u0026#39;: \u0026#39;Mozilla/5.0 (Windows; U; Windows NT 5.1) AppleWebKit/533.44.1 (KHTML, like Gecko) Version/4.0.2 Safari/533.44.1\u0026#39;, \u0026#39;age_bracket\u0026#39;: \u0026#39;26-40\u0026#39;, \u0026#39;opted_into_marketing\u0026#39;: True, \u0026#39;http_request\u0026#39;: \u0026#39;GET archaea.html HTTP/1.0\u0026#39;, \u0026#39;http_response\u0026#39;: 200, \u0026#39;file_size_bytes\u0026#39;: 119, \u0026#39;event_datetime\u0026#39;: \u0026#39;2024-04-14T21:51:38.090\u0026#39;, \u0026#39;event_ts\u0026#39;: 1713095498090} 6# ... Run Pipeline The streaming pipeline can be executed as shown below. As mentioned, we use the local Flink cluster by specifying the use_own argument.\n1$ python section3/kafka_io.py --use_own After a while, we can check both the input and output topics in the Topics section of kafka-ui.\nWe can use the Flink web UI to monitor the pipeline as a Flink job. When we click the kafka-io job in the Running Jobs section, we see 3 operations are linked in the Overview tab. The first two operations are polling and reading Kafka source description while the actual pipeline runs in the last operation.\nNote that, although the main pipeline\u0026rsquo;s SDK harness is set to LOOPBACK, the Kafka I/O runs on the Java SDK, and it associates with its own SDK harness, which defaults to DOCKER. We can check the Kafka I/O\u0026rsquo;s SDK harness process is launched in a container as following.\n1$ docker ps -a --format \u0026#34;table {{.ID}}\\t{{.Image}}\\t{{.Command}}\\t{{.Status}}\u0026#34; 2CONTAINER ID IMAGE COMMAND STATUS 33cd5a628a970 apache/beam_java11_sdk:2.53.0 \u0026#34;/opt/apache/beam/bo…\u0026#34; Up 4 minutes Stop Flink/Kafka Clusters After we stop the pipeline and data generation scripts, we can tear down the Flink and Kafka clusters using the bash script that was explained earlier with -a or -k arguments.\n1# stop both flink and kafka cluster 2$ ./setup/stop-flink-env.sh -a 3 4# stop only kafka cluster 5# ./setup/stop-flink-env.sh -k Summary The Apache Flink Runner supports Python, and it has good features that allow us to develop streaming pipelines effectively. We first discussed the portability layer of Apache Beam as it helps understand (1) how a pipeline developed by the Python SDK can be executed in the Flink Runner that only understands Java JAR and (2) how multiple SDKs can be used in a single pipeline. Then we moved on to how to manage local Flink and Kafka clusters using bash scripts. Finally, we ended up illustrating a simple streaming pipeline, which reads and writes website visit logs from and to Kafka topics.\n","date":"April 18, 2024","img":"/blog/2024-04-18-beam-local-dev-3/featured.png","lang":"en","langName":"English","largeImg":"/blog/2024-04-18-beam-local-dev-3/featured_hua9988bbf060f67942c8c24ced892e1f1_262307_500x0_resize_box_3.png","permalink":"/blog/2024-04-18-beam-local-dev-3/","series":[{"title":"Apache Beam Local Development With Python","url":"/series/apache-beam-local-development-with-python/"}],"smallImg":"/blog/2024-04-18-beam-local-dev-3/featured_hua9988bbf060f67942c8c24ced892e1f1_262307_180x0_resize_box_3.png","tags":[{"title":"Apache Beam","url":"/tags/apache-beam/"},{"title":"Apache Flink","url":"/tags/apache-flink/"},{"title":"Python","url":"/tags/python/"},{"title":"Jupyter Notebook","url":"/tags/jupyter-notebook/"}],"timestamp":1713398400,"title":"Apache Beam Local Development With Python - Part 3 Flink Runner"},{"categories":[{"title":"Apache Beam","url":"/categories/apache-beam/"}],"content":"In this series, we discuss local development of Apache Beam pipelines using Python. A basic Beam pipeline was introduced in Part 1, followed by demonstrating how to utilise Jupyter notebooks, Beam SQL and Beam DataFrames. In this post, we discuss Batch pipelines that aggregate website visit log by user and time. The pipelines are developed with and without Beam SQL. Additionally, each pipeline is implemented on a Jupyter notebook for demonstration.\nPart 1 Pipeline, Notebook, SQL and DataFrame Part 2 Batch Pipelines (this post) Part 3 Flink Runner Part 4 Streaming Pipelines Part 5 Testing Pipelines Data Generation We first need to generate website visit log data. As the second pipeline aggregates data by time, the max lag seconds (--max_lag_seconds) is set to 300 so that records are spread over 5 minutes period. See Part 1 for details about the data generation script and the source of this post can be found in the GitHub repository.\n1$ python datagen/generate_data.py --source batch --num_users 20 --num_events 10000 --max_lag_seconds 300 2... 3$ head -n 3 inputs/d919cc9e-dade-44be-872e-86598a4d0e47.out 4{\u0026#34;ip\u0026#34;: \u0026#34;81.112.193.168\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;-7450326752843155888\u0026#34;, \u0026#34;lat\u0026#34;: 41.8919, \u0026#34;lng\u0026#34;: 12.5113, \u0026#34;user_agent\u0026#34;: \u0026#34;Opera/8.29.(Windows CE; hi-IN) Presto/2.9.160 Version/10.00\u0026#34;, \u0026#34;age_bracket\u0026#34;: \u0026#34;18-25\u0026#34;, \u0026#34;opted_into_marketing\u0026#34;: false, \u0026#34;http_request\u0026#34;: \u0026#34;GET eucharya.html HTTP/1.0\u0026#34;, \u0026#34;http_response\u0026#34;: 200, \u0026#34;file_size_bytes\u0026#34;: 131, \u0026#34;event_datetime\u0026#34;: \u0026#34;2024-04-02T04:30:18.999\u0026#34;, \u0026#34;event_ts\u0026#34;: 1711992618999} 5{\u0026#34;ip\u0026#34;: \u0026#34;204.14.50.181\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;3385356383147784679\u0026#34;, \u0026#34;lat\u0026#34;: 47.7918, \u0026#34;lng\u0026#34;: -122.2243, \u0026#34;user_agent\u0026#34;: \u0026#34;Opera/9.77.(Windows NT 5.01; ber-DZ) Presto/2.9.166 Version/12.00\u0026#34;, \u0026#34;age_bracket\u0026#34;: \u0026#34;55+\u0026#34;, \u0026#34;opted_into_marketing\u0026#34;: true, \u0026#34;http_request\u0026#34;: \u0026#34;GET coniferophyta.html HTTP/1.0\u0026#34;, \u0026#34;http_response\u0026#34;: 200, \u0026#34;file_size_bytes\u0026#34;: 229, \u0026#34;event_datetime\u0026#34;: \u0026#34;2024-04-02T04:28:13.144\u0026#34;, \u0026#34;event_ts\u0026#34;: 1711992493144} 6{\u0026#34;ip\u0026#34;: \u0026#34;11.32.249.163\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;8764514706569354597\u0026#34;, \u0026#34;lat\u0026#34;: 37.2242, \u0026#34;lng\u0026#34;: -95.7083, \u0026#34;user_agent\u0026#34;: \u0026#34;Opera/8.45.(Windows NT 4.0; xh-ZA) Presto/2.9.177 Version/10.00\u0026#34;, \u0026#34;age_bracket\u0026#34;: \u0026#34;26-40\u0026#34;, \u0026#34;opted_into_marketing\u0026#34;: false, \u0026#34;http_request\u0026#34;: \u0026#34;GET protozoa.html HTTP/1.0\u0026#34;, \u0026#34;http_response\u0026#34;: 200, \u0026#34;file_size_bytes\u0026#34;: 373, \u0026#34;event_datetime\u0026#34;: \u0026#34;2024-04-02T04:30:12.612\u0026#34;, \u0026#34;event_ts\u0026#34;: 1711992612612} User Traffic This pipeline basically calculates the number of website visits and distribution of traffic consumption by user.\nBeam Pipeline The pipeline begins with reading data from a folder named inputs and parses the Json lines. Then it creates a key-value pair where the key is the user ID (id) and the value is the file size bytes (file_size_bytes). After that, the records are grouped by the key and aggregated to obtain website visit count and traffic consumption distribution using a ParDo transform. Finally, the output records are written to a folder named outputs after being converted into dictionary.\nNote that custom types are created for the input and output elements using element schema (EventLog and UserTraffic), and transformations become more expressive using them. Also, the custom types are registered to the coder registry so that they are encoded/decoded appropriately - see the transformations that specify the output types via with_output_types.\n[Update 2024-04-30] Note the coders for the custom types are registered, but it is not required because we don\u0026rsquo;t have a cross-language transformation that deals with them.\n1# section2/user_traffic.py 2import os 3import datetime 4import argparse 5import json 6import logging 7import typing 8 9import apache_beam as beam 10from apache_beam.options.pipeline_options import PipelineOptions 11from apache_beam.options.pipeline_options import StandardOptions 12 13 14class EventLog(typing.NamedTuple): 15 ip: str 16 id: str 17 lat: float 18 lng: float 19 user_agent: str 20 age_bracket: str 21 opted_into_marketing: bool 22 http_request: str 23 http_response: int 24 file_size_bytes: int 25 event_datetime: str 26 event_ts: int 27 28 29class UserTraffic(typing.NamedTuple): 30 id: str 31 page_views: int 32 total_bytes: int 33 max_bytes: int 34 min_bytes: int 35 36 37beam.coders.registry.register_coder(EventLog, beam.coders.RowCoder) 38beam.coders.registry.register_coder(UserTraffic, beam.coders.RowCoder) 39 40 41def parse_json(element: str): 42 row = json.loads(element) 43 # lat/lng sometimes empty string 44 if not row[\u0026#34;lat\u0026#34;] or not row[\u0026#34;lng\u0026#34;]: 45 row = {**row, **{\u0026#34;lat\u0026#34;: -1, \u0026#34;lng\u0026#34;: -1}} 46 return EventLog(**row) 47 48 49class Aggregate(beam.DoFn): 50 def process(self, element: typing.Tuple[str, typing.Iterable[int]]): 51 key, values = element 52 yield UserTraffic( 53 id=key, 54 page_views=len(values), 55 total_bytes=sum(values), 56 max_bytes=max(values), 57 min_bytes=min(values), 58 ) 59 60 61def run(): 62 parser = argparse.ArgumentParser(description=\u0026#34;Beam pipeline arguments\u0026#34;) 63 parser.add_argument( 64 \u0026#34;--inputs\u0026#34;, 65 default=\u0026#34;inputs\u0026#34;, 66 help=\u0026#34;Specify folder name that event records are saved\u0026#34;, 67 ) 68 parser.add_argument( 69 \u0026#34;--runner\u0026#34;, default=\u0026#34;DirectRunner\u0026#34;, help=\u0026#34;Specify Apache Beam Runner\u0026#34; 70 ) 71 opts = parser.parse_args() 72 PARENT_DIR = os.path.dirname(os.path.dirname(os.path.realpath(__file__))) 73 74 options = PipelineOptions() 75 options.view_as(StandardOptions).runner = opts.runner 76 77 p = beam.Pipeline(options=options) 78 ( 79 p 80 | \u0026#34;Read from files\u0026#34; 81 \u0026gt;\u0026gt; beam.io.ReadFromText( 82 file_pattern=os.path.join(PARENT_DIR, opts.inputs, \u0026#34;*.out\u0026#34;) 83 ) 84 | \u0026#34;Parse elements\u0026#34; \u0026gt;\u0026gt; beam.Map(parse_json).with_output_types(EventLog) 85 | \u0026#34;Form key value pair\u0026#34; \u0026gt;\u0026gt; beam.Map(lambda e: (e.id, e.file_size_bytes)) 86 | \u0026#34;Group by key\u0026#34; \u0026gt;\u0026gt; beam.GroupByKey() 87 | \u0026#34;Aggregate by id\u0026#34; \u0026gt;\u0026gt; beam.ParDo(Aggregate()).with_output_types(UserTraffic) 88 | \u0026#34;To dict\u0026#34; \u0026gt;\u0026gt; beam.Map(lambda e: e._asdict()) 89 | \u0026#34;Write to file\u0026#34; 90 \u0026gt;\u0026gt; beam.io.WriteToText( 91 file_path_prefix=os.path.join( 92 PARENT_DIR, 93 \u0026#34;outputs\u0026#34;, 94 f\u0026#34;{int(datetime.datetime.now().timestamp() * 1000)}\u0026#34;, 95 ), 96 file_name_suffix=\u0026#34;.out\u0026#34;, 97 ) 98 ) 99 100 logging.getLogger().setLevel(logging.INFO) 101 logging.info(\u0026#34;Building pipeline ...\u0026#34;) 102 103 p.run().wait_until_finish() 104 105 106if __name__ == \u0026#34;__main__\u0026#34;: 107 run() Once we execute the pipeline, we can check the output records by reading the output file. By default, the pipeline runs via the Direct Runner.\n1$ python section2/user_traffic.py 2... 3$ cat outputs/1712032400886-00000-of-00001.out 4{\u0026#39;id\u0026#39;: \u0026#39;-7450326752843155888\u0026#39;, \u0026#39;page_views\u0026#39;: 466, \u0026#39;total_bytes\u0026#39;: 139811, \u0026#39;max_bytes\u0026#39;: 498, \u0026#39;min_bytes\u0026#39;: 100} 5{\u0026#39;id\u0026#39;: \u0026#39;3385356383147784679\u0026#39;, \u0026#39;page_views\u0026#39;: 471, \u0026#39;total_bytes\u0026#39;: 142846, \u0026#39;max_bytes\u0026#39;: 499, \u0026#39;min_bytes\u0026#39;: 103} 6{\u0026#39;id\u0026#39;: \u0026#39;8764514706569354597\u0026#39;, \u0026#39;page_views\u0026#39;: 498, \u0026#39;total_bytes\u0026#39;: 152005, \u0026#39;max_bytes\u0026#39;: 499, \u0026#39;min_bytes\u0026#39;: 101} 7{\u0026#39;id\u0026#39;: \u0026#39;1097462159655745840\u0026#39;, \u0026#39;page_views\u0026#39;: 483, \u0026#39;total_bytes\u0026#39;: 145655, \u0026#39;max_bytes\u0026#39;: 499, \u0026#39;min_bytes\u0026#39;: 101} 8{\u0026#39;id\u0026#39;: \u0026#39;5107076440238203196\u0026#39;, \u0026#39;page_views\u0026#39;: 520, \u0026#39;total_bytes\u0026#39;: 155475, \u0026#39;max_bytes\u0026#39;: 499, \u0026#39;min_bytes\u0026#39;: 100} 9{\u0026#39;id\u0026#39;: \u0026#39;3817299155409964875\u0026#39;, \u0026#39;page_views\u0026#39;: 515, \u0026#39;total_bytes\u0026#39;: 155516, \u0026#39;max_bytes\u0026#39;: 498, \u0026#39;min_bytes\u0026#39;: 100} 10{\u0026#39;id\u0026#39;: \u0026#39;4396740364429657096\u0026#39;, \u0026#39;page_views\u0026#39;: 534, \u0026#39;total_bytes\u0026#39;: 159351, \u0026#39;max_bytes\u0026#39;: 499, \u0026#39;min_bytes\u0026#39;: 101} 11{\u0026#39;id\u0026#39;: \u0026#39;323358690592146285\u0026#39;, \u0026#39;page_views\u0026#39;: 503, \u0026#39;total_bytes\u0026#39;: 150204, \u0026#39;max_bytes\u0026#39;: 497, \u0026#39;min_bytes\u0026#39;: 100} 12{\u0026#39;id\u0026#39;: \u0026#39;-297761604717604766\u0026#39;, \u0026#39;page_views\u0026#39;: 519, \u0026#39;total_bytes\u0026#39;: 157246, \u0026#39;max_bytes\u0026#39;: 499, \u0026#39;min_bytes\u0026#39;: 103} 13{\u0026#39;id\u0026#39;: \u0026#39;-8832654768096800604\u0026#39;, \u0026#39;page_views\u0026#39;: 489, \u0026#39;total_bytes\u0026#39;: 145166, \u0026#39;max_bytes\u0026#39;: 499, \u0026#39;min_bytes\u0026#39;: 100} 14{\u0026#39;id\u0026#39;: \u0026#39;-7508437511513814045\u0026#39;, \u0026#39;page_views\u0026#39;: 492, \u0026#39;total_bytes\u0026#39;: 146561, \u0026#39;max_bytes\u0026#39;: 499, \u0026#39;min_bytes\u0026#39;: 100} 15{\u0026#39;id\u0026#39;: \u0026#39;-4225319382884577471\u0026#39;, \u0026#39;page_views\u0026#39;: 518, \u0026#39;total_bytes\u0026#39;: 158242, \u0026#39;max_bytes\u0026#39;: 499, \u0026#39;min_bytes\u0026#39;: 101} 16{\u0026#39;id\u0026#39;: \u0026#39;-6246779037351548961\u0026#39;, \u0026#39;page_views\u0026#39;: 432, \u0026#39;total_bytes\u0026#39;: 127013, \u0026#39;max_bytes\u0026#39;: 499, \u0026#39;min_bytes\u0026#39;: 101} 17{\u0026#39;id\u0026#39;: \u0026#39;7514213899672341122\u0026#39;, \u0026#39;page_views\u0026#39;: 515, \u0026#39;total_bytes\u0026#39;: 154753, \u0026#39;max_bytes\u0026#39;: 498, \u0026#39;min_bytes\u0026#39;: 100} 18{\u0026#39;id\u0026#39;: \u0026#39;8063196327933870504\u0026#39;, \u0026#39;page_views\u0026#39;: 526, \u0026#39;total_bytes\u0026#39;: 159395, \u0026#39;max_bytes\u0026#39;: 499, \u0026#39;min_bytes\u0026#39;: 101} 19{\u0026#39;id\u0026#39;: \u0026#39;4927182384805166657\u0026#39;, \u0026#39;page_views\u0026#39;: 501, \u0026#39;total_bytes\u0026#39;: 151023, \u0026#39;max_bytes\u0026#39;: 498, \u0026#39;min_bytes\u0026#39;: 100} 20{\u0026#39;id\u0026#39;: \u0026#39;134630243715938340\u0026#39;, \u0026#39;page_views\u0026#39;: 506, \u0026#39;total_bytes\u0026#39;: 153509, \u0026#39;max_bytes\u0026#39;: 498, \u0026#39;min_bytes\u0026#39;: 101} 21{\u0026#39;id\u0026#39;: \u0026#39;-5889211929143180249\u0026#39;, \u0026#39;page_views\u0026#39;: 491, \u0026#39;total_bytes\u0026#39;: 146755, \u0026#39;max_bytes\u0026#39;: 499, \u0026#39;min_bytes\u0026#39;: 100} 22{\u0026#39;id\u0026#39;: \u0026#39;3809491485105813594\u0026#39;, \u0026#39;page_views\u0026#39;: 518, \u0026#39;total_bytes\u0026#39;: 155992, \u0026#39;max_bytes\u0026#39;: 499, \u0026#39;min_bytes\u0026#39;: 100} 23{\u0026#39;id\u0026#39;: \u0026#39;4086706052291208999\u0026#39;, \u0026#39;page_views\u0026#39;: 503, \u0026#39;total_bytes\u0026#39;: 154038, \u0026#39;max_bytes\u0026#39;: 499, \u0026#39;min_bytes\u0026#39;: 100} We can run the pipeline using a different runner e.g. by specifying the Flink Runner in the runner argument. Interestingly it completes the pipeline by multiple tasks.\n1$ python section2/user_traffic.py --runner FlinkRunner 2... 3$ ls outputs/ | grep 1712032503940 41712032503940-00000-of-00005.out 51712032503940-00003-of-00005.out 61712032503940-00002-of-00005.out 71712032503940-00004-of-00005.out 81712032503940-00001-of-00005.out Beam SQL The pipeline that calculates user statistic can also be developed using SqlTransform, which translates a SQL query into PTransforms. The following pipeline creates the same output to the previous pipeline.\n1# section2/user_traffic_sql.py 2import os 3import datetime 4import argparse 5import json 6import logging 7import typing 8 9import apache_beam as beam 10from apache_beam.transforms.sql import SqlTransform 11from apache_beam.options.pipeline_options import PipelineOptions 12from apache_beam.options.pipeline_options import StandardOptions 13 14 15class EventLog(typing.NamedTuple): 16 ip: str 17 id: str 18 lat: float 19 lng: float 20 user_agent: str 21 age_bracket: str 22 opted_into_marketing: bool 23 http_request: str 24 http_response: int 25 file_size_bytes: int 26 event_datetime: str 27 event_ts: int 28 29 30beam.coders.registry.register_coder(EventLog, beam.coders.RowCoder) 31 32 33def parse_json(element: str): 34 row = json.loads(element) 35 # lat/lng sometimes empty string 36 if not row[\u0026#34;lat\u0026#34;] or not row[\u0026#34;lng\u0026#34;]: 37 row = {**row, **{\u0026#34;lat\u0026#34;: -1, \u0026#34;lng\u0026#34;: -1}} 38 return EventLog(**row) 39 40 41def run(): 42 parser = argparse.ArgumentParser(description=\u0026#34;Beam pipeline arguments\u0026#34;) 43 parser.add_argument( 44 \u0026#34;--inputs\u0026#34;, 45 default=\u0026#34;inputs\u0026#34;, 46 help=\u0026#34;Specify folder name that event records are saved\u0026#34;, 47 ) 48 parser.add_argument( 49 \u0026#34;--runner\u0026#34;, default=\u0026#34;DirectRunner\u0026#34;, help=\u0026#34;Specify Apache Beam Runner\u0026#34; 50 ) 51 opts = parser.parse_args() 52 PARENT_DIR = os.path.dirname(os.path.dirname(os.path.realpath(__file__))) 53 54 options = PipelineOptions() 55 options.view_as(StandardOptions).runner = opts.runner 56 57 query = \u0026#34;\u0026#34;\u0026#34; 58 SELECT 59 id, 60 COUNT(*) AS page_views, 61 SUM(file_size_bytes) AS total_bytes, 62 MAX(file_size_bytes) AS max_bytes, 63 MIN(file_size_bytes) AS min_bytes 64 FROM PCOLLECTION 65 GROUP BY id 66 \u0026#34;\u0026#34;\u0026#34; 67 68 p = beam.Pipeline(options=options) 69 ( 70 p 71 | \u0026#34;Read from files\u0026#34; 72 \u0026gt;\u0026gt; beam.io.ReadFromText( 73 file_pattern=os.path.join(PARENT_DIR, opts.inputs, \u0026#34;*.out\u0026#34;) 74 ) 75 | \u0026#34;Parse elements\u0026#34; \u0026gt;\u0026gt; beam.Map(parse_json).with_output_types(EventLog) 76 | \u0026#34;Aggregate by user\u0026#34; \u0026gt;\u0026gt; SqlTransform(query) 77 | \u0026#34;To Dict\u0026#34; \u0026gt;\u0026gt; beam.Map(lambda e: e._asdict()) 78 | \u0026#34;Write to file\u0026#34; 79 \u0026gt;\u0026gt; beam.io.WriteToText( 80 file_path_prefix=os.path.join( 81 PARENT_DIR, 82 \u0026#34;outputs\u0026#34;, 83 f\u0026#34;{int(datetime.datetime.now().timestamp() * 1000)}\u0026#34;, 84 ), 85 file_name_suffix=\u0026#34;.out\u0026#34;, 86 ) 87 ) 88 89 logging.getLogger().setLevel(logging.INFO) 90 logging.info(\u0026#34;Building pipeline ...\u0026#34;) 91 92 p.run().wait_until_finish() 93 94 95if __name__ == \u0026#34;__main__\u0026#34;: 96 run() When we execute the pipeline script, we see that a Docker container is launched and actual transformations are performed within it via the Java SDK. The container exits when the pipeline completes.\n1$ python section2/user_traffic_sql.py 2... 3 4$ docker ps -a --format \u0026#34;table {{.ID}}\\t{{.Image}}\\t{{.Status}}\u0026#34; 5CONTAINER ID IMAGE STATUS 6c7d7bad6b1e9 apache/beam_java11_sdk:2.53.0 Exited (137) 5 minutes ago 7 8$ cat outputs/1712032675637-00000-of-00001.out 9{\u0026#39;id\u0026#39;: \u0026#39;-297761604717604766\u0026#39;, \u0026#39;page_views\u0026#39;: 519, \u0026#39;total_bytes\u0026#39;: 157246, \u0026#39;max_bytes\u0026#39;: 499, \u0026#39;min_bytes\u0026#39;: 103} 10{\u0026#39;id\u0026#39;: \u0026#39;3817299155409964875\u0026#39;, \u0026#39;page_views\u0026#39;: 515, \u0026#39;total_bytes\u0026#39;: 155516, \u0026#39;max_bytes\u0026#39;: 498, \u0026#39;min_bytes\u0026#39;: 100} 11{\u0026#39;id\u0026#39;: \u0026#39;-8832654768096800604\u0026#39;, \u0026#39;page_views\u0026#39;: 489, \u0026#39;total_bytes\u0026#39;: 145166, \u0026#39;max_bytes\u0026#39;: 499, \u0026#39;min_bytes\u0026#39;: 100} 12{\u0026#39;id\u0026#39;: \u0026#39;4086706052291208999\u0026#39;, \u0026#39;page_views\u0026#39;: 503, \u0026#39;total_bytes\u0026#39;: 154038, \u0026#39;max_bytes\u0026#39;: 499, \u0026#39;min_bytes\u0026#39;: 100} 13{\u0026#39;id\u0026#39;: \u0026#39;3809491485105813594\u0026#39;, \u0026#39;page_views\u0026#39;: 518, \u0026#39;total_bytes\u0026#39;: 155992, \u0026#39;max_bytes\u0026#39;: 499, \u0026#39;min_bytes\u0026#39;: 100} 14{\u0026#39;id\u0026#39;: \u0026#39;323358690592146285\u0026#39;, \u0026#39;page_views\u0026#39;: 503, \u0026#39;total_bytes\u0026#39;: 150204, \u0026#39;max_bytes\u0026#39;: 497, \u0026#39;min_bytes\u0026#39;: 100} 15{\u0026#39;id\u0026#39;: \u0026#39;1097462159655745840\u0026#39;, \u0026#39;page_views\u0026#39;: 483, \u0026#39;total_bytes\u0026#39;: 145655, \u0026#39;max_bytes\u0026#39;: 499, \u0026#39;min_bytes\u0026#39;: 101} 16{\u0026#39;id\u0026#39;: \u0026#39;-7508437511513814045\u0026#39;, \u0026#39;page_views\u0026#39;: 492, \u0026#39;total_bytes\u0026#39;: 146561, \u0026#39;max_bytes\u0026#39;: 499, \u0026#39;min_bytes\u0026#39;: 100} 17{\u0026#39;id\u0026#39;: \u0026#39;7514213899672341122\u0026#39;, \u0026#39;page_views\u0026#39;: 515, \u0026#39;total_bytes\u0026#39;: 154753, \u0026#39;max_bytes\u0026#39;: 498, \u0026#39;min_bytes\u0026#39;: 100} 18{\u0026#39;id\u0026#39;: \u0026#39;8764514706569354597\u0026#39;, \u0026#39;page_views\u0026#39;: 498, \u0026#39;total_bytes\u0026#39;: 152005, \u0026#39;max_bytes\u0026#39;: 499, \u0026#39;min_bytes\u0026#39;: 101} 19{\u0026#39;id\u0026#39;: \u0026#39;5107076440238203196\u0026#39;, \u0026#39;page_views\u0026#39;: 520, \u0026#39;total_bytes\u0026#39;: 155475, \u0026#39;max_bytes\u0026#39;: 499, \u0026#39;min_bytes\u0026#39;: 100} 20{\u0026#39;id\u0026#39;: \u0026#39;134630243715938340\u0026#39;, \u0026#39;page_views\u0026#39;: 506, \u0026#39;total_bytes\u0026#39;: 153509, \u0026#39;max_bytes\u0026#39;: 498, \u0026#39;min_bytes\u0026#39;: 101} 21{\u0026#39;id\u0026#39;: \u0026#39;-6246779037351548961\u0026#39;, \u0026#39;page_views\u0026#39;: 432, \u0026#39;total_bytes\u0026#39;: 127013, \u0026#39;max_bytes\u0026#39;: 499, \u0026#39;min_bytes\u0026#39;: 101} 22{\u0026#39;id\u0026#39;: \u0026#39;-7450326752843155888\u0026#39;, \u0026#39;page_views\u0026#39;: 466, \u0026#39;total_bytes\u0026#39;: 139811, \u0026#39;max_bytes\u0026#39;: 498, \u0026#39;min_bytes\u0026#39;: 100} 23{\u0026#39;id\u0026#39;: \u0026#39;4927182384805166657\u0026#39;, \u0026#39;page_views\u0026#39;: 501, \u0026#39;total_bytes\u0026#39;: 151023, \u0026#39;max_bytes\u0026#39;: 498, \u0026#39;min_bytes\u0026#39;: 100} 24{\u0026#39;id\u0026#39;: \u0026#39;-5889211929143180249\u0026#39;, \u0026#39;page_views\u0026#39;: 491, \u0026#39;total_bytes\u0026#39;: 146755, \u0026#39;max_bytes\u0026#39;: 499, \u0026#39;min_bytes\u0026#39;: 100} 25{\u0026#39;id\u0026#39;: \u0026#39;8063196327933870504\u0026#39;, \u0026#39;page_views\u0026#39;: 526, \u0026#39;total_bytes\u0026#39;: 159395, \u0026#39;max_bytes\u0026#39;: 499, \u0026#39;min_bytes\u0026#39;: 101} 26{\u0026#39;id\u0026#39;: \u0026#39;3385356383147784679\u0026#39;, \u0026#39;page_views\u0026#39;: 471, \u0026#39;total_bytes\u0026#39;: 142846, \u0026#39;max_bytes\u0026#39;: 499, \u0026#39;min_bytes\u0026#39;: 103} 27{\u0026#39;id\u0026#39;: \u0026#39;4396740364429657096\u0026#39;, \u0026#39;page_views\u0026#39;: 534, \u0026#39;total_bytes\u0026#39;: 159351, \u0026#39;max_bytes\u0026#39;: 499, \u0026#39;min_bytes\u0026#39;: 101} 28{\u0026#39;id\u0026#39;: \u0026#39;-4225319382884577471\u0026#39;, \u0026#39;page_views\u0026#39;: 518, \u0026#39;total_bytes\u0026#39;: 158242, \u0026#39;max_bytes\u0026#39;: 499, \u0026#39;min_bytes\u0026#39;: 101} When we develop a pipeline, Interactive Beam on a Jupyter notebook can be convenient. The user traffic pipelines are implemented using notebooks, and they can be found in section2/user_traffic.ipynb and section2/user_traffic_sql.ipynb respectively. We can start a Jupyter server while enabling Jupyter Lab and ignoring authentication as shown below. Once started, it can be accessed on http://localhost:8888.\n1$ JUPYTER_ENABLE_LAB=yes jupyter lab --ServerApp.token=\u0026#39;\u0026#39; --ServerApp.password=\u0026#39;\u0026#39; Minute Traffic This pipeline aggregates the number of website visits in fixed time windows over 60 seconds.\nBeam Pipeline As the user traffic pipeline, it begins with reading data from a folder named inputs and parses the Json lines. Then it adds timestamp to elements by parsing the event_datetime attribute, defines fixed time windows over 60 seconds, and counts the number of records within the windows. Finally, it writes the aggregated records into a folder named outputs after adding window_start and window_end timestamp attributes.\n[Update 2024-04-30] Note the coder for the custom type is registered, but it is not required because we don\u0026rsquo;t have a cross-language transformation that deals with it.\n1# section2/minute_traffic.py 2import os 3import datetime 4import argparse 5import json 6import logging 7import typing 8 9import apache_beam as beam 10from apache_beam.transforms.combiners import CountCombineFn 11from apache_beam.options.pipeline_options import PipelineOptions 12from apache_beam.options.pipeline_options import StandardOptions 13 14 15class EventLog(typing.NamedTuple): 16 ip: str 17 id: str 18 lat: float 19 lng: float 20 user_agent: str 21 age_bracket: str 22 opted_into_marketing: bool 23 http_request: str 24 http_response: int 25 file_size_bytes: int 26 event_datetime: str 27 event_ts: int 28 29 30beam.coders.registry.register_coder(EventLog, beam.coders.RowCoder) 31 32 33def parse_json(element: str): 34 row = json.loads(element) 35 # lat/lng sometimes empty string 36 if not row[\u0026#34;lat\u0026#34;] or not row[\u0026#34;lng\u0026#34;]: 37 row = {**row, **{\u0026#34;lat\u0026#34;: -1, \u0026#34;lng\u0026#34;: -1}} 38 return EventLog(**row) 39 40 41def add_timestamp(element: EventLog): 42 ts = datetime.datetime.strptime( 43 element.event_datetime, \u0026#34;%Y-%m-%dT%H:%M:%S.%f\u0026#34; 44 ).timestamp() 45 return beam.window.TimestampedValue(element, ts) 46 47 48class AddWindowTS(beam.DoFn): 49 def process(self, element: int, window=beam.DoFn.WindowParam): 50 window_start = window.start.to_utc_datetime().isoformat(timespec=\u0026#34;seconds\u0026#34;) 51 window_end = window.end.to_utc_datetime().isoformat(timespec=\u0026#34;seconds\u0026#34;) 52 output = { 53 \u0026#34;window_start\u0026#34;: window_start, 54 \u0026#34;window_end\u0026#34;: window_end, 55 \u0026#34;page_views\u0026#34;: element, 56 } 57 yield output 58 59 60def run(): 61 parser = argparse.ArgumentParser(description=\u0026#34;Beam pipeline arguments\u0026#34;) 62 parser.add_argument( 63 \u0026#34;--inputs\u0026#34;, 64 default=\u0026#34;inputs\u0026#34;, 65 help=\u0026#34;Specify folder name that event records are saved\u0026#34;, 66 ) 67 parser.add_argument( 68 \u0026#34;--runner\u0026#34;, default=\u0026#34;DirectRunner\u0026#34;, help=\u0026#34;Specify Apache Beam Runner\u0026#34; 69 ) 70 opts = parser.parse_args() 71 PARENT_DIR = os.path.dirname(os.path.dirname(os.path.realpath(__file__))) 72 73 options = PipelineOptions() 74 options.view_as(StandardOptions).runner = opts.runner 75 76 p = beam.Pipeline(options=options) 77 ( 78 p 79 | \u0026#34;Read from files\u0026#34; 80 \u0026gt;\u0026gt; beam.io.ReadFromText( 81 file_pattern=os.path.join(os.path.join(PARENT_DIR, \u0026#34;inputs\u0026#34;, \u0026#34;*.out\u0026#34;)) 82 ) 83 | \u0026#34;Parse elements\u0026#34; \u0026gt;\u0026gt; beam.Map(parse_json).with_output_types(EventLog) 84 | \u0026#34;Add event timestamp\u0026#34; \u0026gt;\u0026gt; beam.Map(add_timestamp) 85 | \u0026#34;Tumble window per minute\u0026#34; \u0026gt;\u0026gt; beam.WindowInto(beam.window.FixedWindows(60)) 86 | \u0026#34;Count per minute\u0026#34; 87 \u0026gt;\u0026gt; beam.CombineGlobally(CountCombineFn()).without_defaults() 88 | \u0026#34;Add window timestamp\u0026#34; \u0026gt;\u0026gt; beam.ParDo(AddWindowTS()) 89 | \u0026#34;Write to file\u0026#34; 90 \u0026gt;\u0026gt; beam.io.WriteToText( 91 file_path_prefix=os.path.join( 92 PARENT_DIR, 93 \u0026#34;outputs\u0026#34;, 94 f\u0026#34;{int(datetime.datetime.now().timestamp() * 1000)}\u0026#34;, 95 ), 96 file_name_suffix=\u0026#34;.out\u0026#34;, 97 ) 98 ) 99 100 logging.getLogger().setLevel(logging.INFO) 101 logging.info(\u0026#34;Building pipeline ...\u0026#34;) 102 103 p.run().wait_until_finish() 104 105 106if __name__ == \u0026#34;__main__\u0026#34;: 107 run() As mentioned earlier, we set the max lag seconds (--max_lag_seconds) to 300 so that records are spread over 5 minutes period. Therefore, we can see that website visit counts are found in multiple time windows.\n1$ python section2/minute_traffic.py 2... 3$ cat outputs/1712033031226-00000-of-00001.out 4{\u0026#39;window_start\u0026#39;: \u0026#39;2024-04-01T17:30:00\u0026#39;, \u0026#39;window_end\u0026#39;: \u0026#39;2024-04-01T17:31:00\u0026#39;, \u0026#39;page_views\u0026#39;: 1963} 5{\u0026#39;window_start\u0026#39;: \u0026#39;2024-04-01T17:28:00\u0026#39;, \u0026#39;window_end\u0026#39;: \u0026#39;2024-04-01T17:29:00\u0026#39;, \u0026#39;page_views\u0026#39;: 2023} 6{\u0026#39;window_start\u0026#39;: \u0026#39;2024-04-01T17:27:00\u0026#39;, \u0026#39;window_end\u0026#39;: \u0026#39;2024-04-01T17:28:00\u0026#39;, \u0026#39;page_views\u0026#39;: 1899} 7{\u0026#39;window_start\u0026#39;: \u0026#39;2024-04-01T17:29:00\u0026#39;, \u0026#39;window_end\u0026#39;: \u0026#39;2024-04-01T17:30:00\u0026#39;, \u0026#39;page_views\u0026#39;: 2050} 8{\u0026#39;window_start\u0026#39;: \u0026#39;2024-04-01T17:31:00\u0026#39;, \u0026#39;window_end\u0026#39;: \u0026#39;2024-04-01T17:32:00\u0026#39;, \u0026#39;page_views\u0026#39;: 1970} 9{\u0026#39;window_start\u0026#39;: \u0026#39;2024-04-01T17:32:00\u0026#39;, \u0026#39;window_end\u0026#39;: \u0026#39;2024-04-01T17:33:00\u0026#39;, \u0026#39;page_views\u0026#39;: 95} Beam SQL The traffic by fixed time window can also be obtained using SqlTransform. As mentioned in Part 1, Beam SQL supports two dialects - Calcite SQL and ZetaSQL. The following pipeline creates output files using both the dialects.\n1# section2/minute_traffic_sql.py 2import os 3import datetime 4import argparse 5import json 6import logging 7import typing 8 9import apache_beam as beam 10from apache_beam.transforms.sql import SqlTransform 11from apache_beam.options.pipeline_options import PipelineOptions 12from apache_beam.options.pipeline_options import StandardOptions 13 14 15class EventLog(typing.NamedTuple): 16 ip: str 17 id: str 18 lat: float 19 lng: float 20 user_agent: str 21 age_bracket: str 22 opted_into_marketing: bool 23 http_request: str 24 http_response: int 25 file_size_bytes: int 26 event_datetime: str 27 event_ts: int 28 29 30beam.coders.registry.register_coder(EventLog, beam.coders.RowCoder) 31 32 33def parse_json(element: str): 34 row = json.loads(element) 35 # lat/lng sometimes empty string 36 if not row[\u0026#34;lat\u0026#34;] or not row[\u0026#34;lng\u0026#34;]: 37 row = {**row, **{\u0026#34;lat\u0026#34;: -1, \u0026#34;lng\u0026#34;: -1}} 38 return EventLog(**row) 39 40 41def format_timestamp(element: EventLog): 42 event_ts = datetime.datetime.fromisoformat(element.event_datetime) 43 temp_dict = element._asdict() 44 temp_dict[\u0026#34;event_datetime\u0026#34;] = datetime.datetime.strftime( 45 event_ts, \u0026#34;%Y-%m-%d %H:%M:%S\u0026#34; 46 ) 47 return EventLog(**temp_dict) 48 49 50def run(): 51 parser = argparse.ArgumentParser(description=\u0026#34;Beam pipeline arguments\u0026#34;) 52 parser.add_argument( 53 \u0026#34;--inputs\u0026#34;, 54 default=\u0026#34;inputs\u0026#34;, 55 help=\u0026#34;Specify folder name that event records are saved\u0026#34;, 56 ) 57 parser.add_argument( 58 \u0026#34;--runner\u0026#34;, default=\u0026#34;DirectRunner\u0026#34;, help=\u0026#34;Specify Apache Beam Runner\u0026#34; 59 ) 60 opts = parser.parse_args() 61 PARENT_DIR = os.path.dirname(os.path.dirname(os.path.realpath(__file__))) 62 63 options = PipelineOptions() 64 options.view_as(StandardOptions).runner = opts.runner 65 66 calcite_query = \u0026#34;\u0026#34;\u0026#34; 67 WITH cte AS ( 68 SELECT CAST(event_datetime AS TIMESTAMP) AS ts 69 FROM PCOLLECTION 70 ) 71 SELECT 72 CAST(TUMBLE_START(ts, INTERVAL \u0026#39;1\u0026#39; MINUTE) AS VARCHAR) AS window_start, 73 CAST(TUMBLE_END(ts, INTERVAL \u0026#39;1\u0026#39; MINUTE) AS VARCHAR) AS window_end, 74 COUNT(*) AS page_view 75 FROM cte 76 GROUP BY 77 TUMBLE(ts, INTERVAL \u0026#39;1\u0026#39; MINUTE) 78 \u0026#34;\u0026#34;\u0026#34; 79 80 zeta_query = \u0026#34;\u0026#34;\u0026#34; 81 SELECT 82 STRING(window_start) AS start_time, 83 STRING(window_end) AS end_time, 84 COUNT(*) AS page_views 85 FROM 86 TUMBLE( 87 (SELECT TIMESTAMP(event_datetime) AS ts FROM PCOLLECTION), 88 DESCRIPTOR(ts), 89 \u0026#39;INTERVAL 1 MINUTE\u0026#39;) 90 GROUP BY 91 window_start, window_end 92 \u0026#34;\u0026#34;\u0026#34; 93 94 p = beam.Pipeline(options=options) 95 transformed = ( 96 p 97 | \u0026#34;Read from files\u0026#34; 98 \u0026gt;\u0026gt; beam.io.ReadFromText( 99 file_pattern=os.path.join(os.path.join(PARENT_DIR, \u0026#34;inputs\u0026#34;, \u0026#34;*.out\u0026#34;)) 100 ) 101 | \u0026#34;Parse elements\u0026#34; \u0026gt;\u0026gt; beam.Map(parse_json).with_output_types(EventLog) 102 | \u0026#34;Format timestamp\u0026#34; \u0026gt;\u0026gt; beam.Map(format_timestamp).with_output_types(EventLog) 103 ) 104 105 ## calcite sql output 106 ( 107 transformed 108 | \u0026#34;Count per minute via Caltice\u0026#34; \u0026gt;\u0026gt; SqlTransform(calcite_query) 109 | \u0026#34;To Dict via Caltice\u0026#34; \u0026gt;\u0026gt; beam.Map(lambda e: e._asdict()) 110 | \u0026#34;Write to file via Caltice\u0026#34; 111 \u0026gt;\u0026gt; beam.io.WriteToText( 112 file_path_prefix=os.path.join( 113 PARENT_DIR, 114 \u0026#34;outputs\u0026#34;, 115 f\u0026#34;{int(datetime.datetime.now().timestamp() * 1000)}-calcite\u0026#34;, 116 ), 117 file_name_suffix=\u0026#34;.out\u0026#34;, 118 ) 119 ) 120 121 ## zeta sql output 122 ( 123 transformed 124 | \u0026#34;Count per minute via Zeta\u0026#34; \u0026gt;\u0026gt; SqlTransform(zeta_query, dialect=\u0026#34;zetasql\u0026#34;) 125 | \u0026#34;To Dict via Zeta\u0026#34; \u0026gt;\u0026gt; beam.Map(lambda e: e._asdict()) 126 | \u0026#34;Write to file via Zeta\u0026#34; 127 \u0026gt;\u0026gt; beam.io.WriteToText( 128 file_path_prefix=os.path.join( 129 PARENT_DIR, 130 \u0026#34;outputs\u0026#34;, 131 f\u0026#34;{int(datetime.datetime.now().timestamp() * 1000)}-zeta\u0026#34;, 132 ), 133 file_name_suffix=\u0026#34;.out\u0026#34;, 134 ) 135 ) 136 137 logging.getLogger().setLevel(logging.INFO) 138 logging.info(\u0026#34;Building pipeline ...\u0026#34;) 139 140 p.run().wait_until_finish() 141 142 143if __name__ == \u0026#34;__main__\u0026#34;: 144 run() As expected, both the SQL dialects produce the same output.\n1$ python section2/minute_traffic_sql.py 2... 3$ cat outputs/1712033090583-calcite-00000-of-00001.out 4{\u0026#39;window_start\u0026#39;: \u0026#39;2024-04-02 04:32:00\u0026#39;, \u0026#39;window_end\u0026#39;: \u0026#39;2024-04-02 04:33:00\u0026#39;, \u0026#39;page_view\u0026#39;: 95} 5{\u0026#39;window_start\u0026#39;: \u0026#39;2024-04-02 04:28:00\u0026#39;, \u0026#39;window_end\u0026#39;: \u0026#39;2024-04-02 04:29:00\u0026#39;, \u0026#39;page_view\u0026#39;: 2023} 6{\u0026#39;window_start\u0026#39;: \u0026#39;2024-04-02 04:30:00\u0026#39;, \u0026#39;window_end\u0026#39;: \u0026#39;2024-04-02 04:31:00\u0026#39;, \u0026#39;page_view\u0026#39;: 1963} 7{\u0026#39;window_start\u0026#39;: \u0026#39;2024-04-02 04:29:00\u0026#39;, \u0026#39;window_end\u0026#39;: \u0026#39;2024-04-02 04:30:00\u0026#39;, \u0026#39;page_view\u0026#39;: 2050} 8{\u0026#39;window_start\u0026#39;: \u0026#39;2024-04-02 04:27:00\u0026#39;, \u0026#39;window_end\u0026#39;: \u0026#39;2024-04-02 04:28:00\u0026#39;, \u0026#39;page_view\u0026#39;: 1899} 9{\u0026#39;window_start\u0026#39;: \u0026#39;2024-04-02 04:31:00\u0026#39;, \u0026#39;window_end\u0026#39;: \u0026#39;2024-04-02 04:32:00\u0026#39;, \u0026#39;page_view\u0026#39;: 1970} 10 11$ cat outputs/1712033101760-zeta-00000-of-00001.out 12{\u0026#39;start_time\u0026#39;: \u0026#39;2024-04-02 04:30:00+00\u0026#39;, \u0026#39;end_time\u0026#39;: \u0026#39;2024-04-02 04:31:00+00\u0026#39;, \u0026#39;page_views\u0026#39;: 1963} 13{\u0026#39;start_time\u0026#39;: \u0026#39;2024-04-02 04:29:00+00\u0026#39;, \u0026#39;end_time\u0026#39;: \u0026#39;2024-04-02 04:30:00+00\u0026#39;, \u0026#39;page_views\u0026#39;: 2050} 14{\u0026#39;start_time\u0026#39;: \u0026#39;2024-04-02 04:31:00+00\u0026#39;, \u0026#39;end_time\u0026#39;: \u0026#39;2024-04-02 04:32:00+00\u0026#39;, \u0026#39;page_views\u0026#39;: 1970} 15{\u0026#39;start_time\u0026#39;: \u0026#39;2024-04-02 04:32:00+00\u0026#39;, \u0026#39;end_time\u0026#39;: \u0026#39;2024-04-02 04:33:00+00\u0026#39;, \u0026#39;page_views\u0026#39;: 95} 16{\u0026#39;start_time\u0026#39;: \u0026#39;2024-04-02 04:28:00+00\u0026#39;, \u0026#39;end_time\u0026#39;: \u0026#39;2024-04-02 04:29:00+00\u0026#39;, \u0026#39;page_views\u0026#39;: 2023} 17{\u0026#39;start_time\u0026#39;: \u0026#39;2024-04-02 04:27:00+00\u0026#39;, \u0026#39;end_time\u0026#39;: \u0026#39;2024-04-02 04:28:00+00\u0026#39;, \u0026#39;page_views\u0026#39;: 1899} Jupyter notebooks are created for the minute traffic pipelines, and they can be found in section2/minute_traffic.ipynb and section2/minute_traffic_sql.ipynb respectively.\nSummary As part of discussing local development of Apache Beam pipelines using Python, we developed Batch pipelines that aggregate website visit log by user and time in this post. The pipelines were developed with and without Beam SQL. Additionally, each pipeline was implemented on a Jupyter notebook for demonstration.\n","date":"April 4, 2024","img":"/blog/2024-04-04-beam-local-dev-2/featured.png","lang":"en","langName":"English","largeImg":"/blog/2024-04-04-beam-local-dev-2/featured_huc60febc0cd72955054b95c3bf20650a7_55405_500x0_resize_box_3.png","permalink":"/blog/2024-04-04-beam-local-dev-2/","series":[{"title":"Apache Beam Local Development With Python","url":"/series/apache-beam-local-development-with-python/"}],"smallImg":"/blog/2024-04-04-beam-local-dev-2/featured_huc60febc0cd72955054b95c3bf20650a7_55405_180x0_resize_box_3.png","tags":[{"title":"Apache Beam","url":"/tags/apache-beam/"},{"title":"Apache Flink","url":"/tags/apache-flink/"},{"title":"Python","url":"/tags/python/"},{"title":"Jupyter Notebook","url":"/tags/jupyter-notebook/"}],"timestamp":1712188800,"title":"Apache Beam Local Development With Python - Part 2 Batch Pipelines"},{"categories":[{"title":"Apache Beam","url":"/categories/apache-beam/"}],"content":"Apache Beam and Apache Flink are open-source frameworks for parallel, distributed data processing at scale. Flink has DataStream and Table/SQL APIs and the former has more capacity to develop sophisticated data streaming applications. The DataStream API of PyFlink, Flink\u0026rsquo;s Python API, however, is not as complete as its Java counterpart, and it doesn\u0026rsquo;t provide enough capability to extend when there are missing features in Python. Recently I had a chance to look through Apache Beam and found it supports more possibility to extend and/or customise its features.\nIn this series of posts, we discuss local development of Apache Beam pipelines using Python. In Part 1, a basic Beam pipeline is introduced, followed by demonstrating how to utilise Jupyter notebooks for interactive development. Several notebook examples are covered including Beam SQL and Beam DataFrames. Batch pipelines will be developed in Part 2, and we use pipelines from GCP Python DataFlow Quest while modifying them to access local resources only. Each batch pipeline has two versions with/without SQL. Beam doesn\u0026rsquo;t have its own processing engine and Beam pipelines are executed on a runner such as Apache Flink, Apache Spark, or Google Cloud Dataflow instead. We will use the Flink Runner for deploying streaming pipelines as it supports a wide range of features especially in streaming context. In Part 3, we will discuss how to set up a local Flink cluster as well as a local Kafka cluster for data source and sink. A streaming pipeline with/without Beam SQL will be built in Part 4, and this series concludes with illustrating unit testing of existing pipelines in Part 5.\nPart 1 Pipeline, Notebook, SQL and DataFrame (this post) Part 2 Batch Pipelines Part 3 Flink Runner Part 4 Streaming Pipelines Part 5 Testing Pipelines Prerequisites We need to install Java, Docker and GraphViz.\nJava 11 to launch a local Flink cluster Docker for Kafka connection/executing Beam SQL as well as deploying a Kafka cluster GraphViz to visualize pipeline DAGs 1$ java --version 2openjdk 11.0.22 2024-01-16 3OpenJDK Runtime Environment (build 11.0.22+7-post-Ubuntu-0ubuntu220.04.1) 4OpenJDK 64-Bit Server VM (build 11.0.22+7-post-Ubuntu-0ubuntu220.04.1, mixed mode, sharing) 5 6$ docker --version 7Docker version 24.0.6, build ed223bc 8 9$ dot -V 10dot - graphviz version 2.43.0 (0) Also, I use Python 3.10.13 and Beam 2.53.0 - supported Python versions are 3.8, 3.9, 3.10, 3.11. The following list shows key dependent packages.\napache-beam[gcp,aws,azure,test,docs,interactive]==2.53.0 jupyterlab==4.1.2 kafka-python faker geocoder The source of this post can be found in the GitHub repository.\nData Generator Website visit log is used as source data for both batch and streaming data pipelines. It begins with creating a configurable number of users and simulates their website visit history. When the source argument is batch, it writes the records into a folder named inputs while those are sent to a Kafka topic named website-visit if the source is streaming - we will talk further about streaming source generation in Part 3. Below shows how to execute the script for batch and streaming sources respectively.\n1# Batch example: 2python datagen/generate_data.py --source batch --num_users 20 --num_events 10000 --max_lag_seconds 60 3 4# Streaming example: 5python datagen/generate_data.py --source streaming --num_users 5 --delay_seconds 0.5 1# datagen/generate_data.py 2import os 3import json 4import uuid 5import argparse 6import datetime 7import time 8import math 9from faker import Faker 10import geocoder 11import random 12from kafka import KafkaProducer 13 14 15class EventGenerator: 16 def __init__( 17 self, 18 source: str, 19 num_users: int, 20 num_events: int, 21 max_lag_seconds: int, 22 delay_seconds: float, 23 bootstrap_servers: list, 24 topic_name: str, 25 file_name: str = str(uuid.uuid4()), 26 ): 27 self.source = source 28 self.num_users = num_users 29 self.num_events = num_events 30 self.max_lag_seconds = max_lag_seconds 31 self.delay_seconds = delay_seconds 32 self.bootstrap_servers = bootstrap_servers 33 self.topic_name = topic_name 34 self.file_name = file_name 35 self.user_pool = self.create_user_pool() 36 if self.source == \u0026#34;streaming\u0026#34;: 37 self.kafka_producer = self.create_producer() 38 39 def create_user_pool(self): 40 \u0026#34;\u0026#34;\u0026#34; 41 Returns a list of user instances given the max number of users. 42 Each user instances is a dictionary that has the following attributes: 43 ip, id, lat, lng, user_agent, age_bracket, opted_into_marketing 44 \u0026#34;\u0026#34;\u0026#34; 45 init_fields = [ 46 \u0026#34;ip\u0026#34;, 47 \u0026#34;id\u0026#34;, 48 \u0026#34;lat\u0026#34;, 49 \u0026#34;lng\u0026#34;, 50 \u0026#34;user_agent\u0026#34;, 51 \u0026#34;age_bracket\u0026#34;, 52 \u0026#34;opted_into_marketing\u0026#34;, 53 ] 54 user_pool = [] 55 for _ in range(self.num_users): 56 user_pool.append(dict(zip(init_fields, self.set_initial_values()))) 57 return user_pool 58 59 def create_producer(self): 60 \u0026#34;\u0026#34;\u0026#34; 61 Returns a KafkaProducer instance 62 \u0026#34;\u0026#34;\u0026#34; 63 return KafkaProducer( 64 bootstrap_servers=self.bootstrap_servers, 65 value_serializer=lambda v: json.dumps(v).encode(\u0026#34;utf-8\u0026#34;), 66 key_serializer=lambda v: json.dumps(v).encode(\u0026#34;utf-8\u0026#34;), 67 ) 68 69 def set_initial_values(self, faker=Faker()): 70 \u0026#34;\u0026#34;\u0026#34; 71 Returns initial user attribute values using Faker 72 \u0026#34;\u0026#34;\u0026#34; 73 ip = faker.ipv4() 74 lookup = geocoder.ip(ip) 75 try: 76 lat, lng = lookup.latlng 77 except Exception: 78 lat, lng = \u0026#34;\u0026#34;, \u0026#34;\u0026#34; 79 id = str(hash(f\u0026#34;{ip}{lat}{lng}\u0026#34;)) 80 user_agent = random.choice( 81 [ 82 faker.firefox, 83 faker.chrome, 84 faker.safari, 85 faker.internet_explorer, 86 faker.opera, 87 ] 88 )() 89 age_bracket = random.choice([\u0026#34;18-25\u0026#34;, \u0026#34;26-40\u0026#34;, \u0026#34;41-55\u0026#34;, \u0026#34;55+\u0026#34;]) 90 opted_into_marketing = random.choice([True, False]) 91 return ip, id, lat, lng, user_agent, age_bracket, opted_into_marketing 92 93 def set_req_info(self): 94 \u0026#34;\u0026#34;\u0026#34; 95 Returns a tuple of HTTP request information - http_request, http_response, file_size_bytes 96 \u0026#34;\u0026#34;\u0026#34; 97 uri = random.choice( 98 [ 99 \u0026#34;home.html\u0026#34;, 100 \u0026#34;archea.html\u0026#34;, 101 \u0026#34;archaea.html\u0026#34;, 102 \u0026#34;bacteria.html\u0026#34;, 103 \u0026#34;eucharya.html\u0026#34;, 104 \u0026#34;protozoa.html\u0026#34;, 105 \u0026#34;amoebozoa.html\u0026#34;, 106 \u0026#34;chromista.html\u0026#34;, 107 \u0026#34;cryptista.html\u0026#34;, 108 \u0026#34;plantae.html\u0026#34;, 109 \u0026#34;coniferophyta.html\u0026#34;, 110 \u0026#34;fungi.html\u0026#34;, 111 \u0026#34;blastocladiomycota.html\u0026#34;, 112 \u0026#34;animalia.html\u0026#34;, 113 \u0026#34;acanthocephala.html\u0026#34;, 114 ] 115 ) 116 file_size_bytes = random.choice(range(100, 500)) 117 http_request = f\u0026#34;{random.choice([\u0026#39;GET\u0026#39;])} {uri} HTTP/1.0\u0026#34; 118 http_response = random.choice([200]) 119 return http_request, http_response, file_size_bytes 120 121 def append_to_file(self, event: dict): 122 \u0026#34;\u0026#34;\u0026#34; 123 Appends a website visit event record into an event output file. 124 \u0026#34;\u0026#34;\u0026#34; 125 parent_dir = os.path.dirname(os.path.dirname(os.path.realpath(__file__))) 126 with open( 127 os.path.join(parent_dir, \u0026#34;inputs\u0026#34;, f\u0026#34;{self.file_name}.out\u0026#34;), \u0026#34;a\u0026#34; 128 ) as fp: 129 fp.write(f\u0026#34;{json.dumps(event)}\\n\u0026#34;) 130 131 def send_to_kafka(self, event: dict): 132 \u0026#34;\u0026#34;\u0026#34; 133 Sends a website visit event record into a Kafka topic. 134 \u0026#34;\u0026#34;\u0026#34; 135 try: 136 self.kafka_producer.send( 137 self.topic_name, 138 key={\u0026#34;event_id\u0026#34;: event[\u0026#34;id\u0026#34;], \u0026#34;event_ts\u0026#34;: event[\u0026#34;event_ts\u0026#34;]}, 139 value=event, 140 ) 141 self.kafka_producer.flush() 142 except Exception as e: 143 raise RuntimeError(\u0026#34;fails to send a message\u0026#34;) from e 144 145 def generate_events(self): 146 \u0026#34;\u0026#34;\u0026#34; 147 Generate webstie visit events as per the max number events. 148 Events are either saved to an output file (batch) or sent to a Kafka topic (streaming). 149 \u0026#34;\u0026#34;\u0026#34; 150 num_events = 0 151 while True: 152 num_events += 1 153 if num_events \u0026gt; self.num_events: 154 break 155 event_ts = datetime.datetime.utcnow() + datetime.timedelta( 156 seconds=random.uniform(0, self.max_lag_seconds) 157 ) 158 req_info = dict( 159 zip( 160 [\u0026#34;http_request\u0026#34;, \u0026#34;http_response\u0026#34;, \u0026#34;file_size_bytes\u0026#34;], 161 self.set_req_info(), 162 ) 163 ) 164 event = { 165 **random.choice(self.user_pool), 166 **req_info, 167 **{ 168 \u0026#34;event_datetime\u0026#34;: event_ts.isoformat(timespec=\u0026#34;milliseconds\u0026#34;), 169 \u0026#34;event_ts\u0026#34;: int(event_ts.timestamp() * 1000), 170 }, 171 } 172 divide_by = 100 if self.source == \u0026#34;batch\u0026#34; else 10 173 if num_events % divide_by == 0: 174 print(f\u0026#34;{num_events} events created so far...\u0026#34;) 175 print(event) 176 if self.source == \u0026#34;batch\u0026#34;: 177 self.append_to_file(event) 178 else: 179 self.send_to_kafka(event) 180 time.sleep(self.delay_seconds or 0) 181 182 183if __name__ == \u0026#34;__main__\u0026#34;: 184 \u0026#34;\u0026#34;\u0026#34; 185 Batch example: 186 python datagen/generate_data.py --source batch --num_users 20 --num_events 10000 --max_lag_seconds 60 187 Streaming example: 188 python datagen/generate_data.py --source streaming --num_users 5 --delay_seconds 0.5 189 \u0026#34;\u0026#34;\u0026#34; 190 parser = argparse.ArgumentParser(__file__, description=\u0026#34;Web Server Data Generator\u0026#34;) 191 parser.add_argument( 192 \u0026#34;--source\u0026#34;, 193 \u0026#34;-s\u0026#34;, 194 type=str, 195 default=\u0026#34;batch\u0026#34;, 196 choices=[\u0026#34;batch\u0026#34;, \u0026#34;streaming\u0026#34;], 197 help=\u0026#34;The data source - batch or streaming\u0026#34;, 198 ) 199 parser.add_argument( 200 \u0026#34;--num_users\u0026#34;, 201 \u0026#34;-u\u0026#34;, 202 type=int, 203 default=50, 204 help=\u0026#34;The number of users to create\u0026#34;, 205 ) 206 parser.add_argument( 207 \u0026#34;--num_events\u0026#34;, 208 \u0026#34;-e\u0026#34;, 209 type=int, 210 default=math.inf, 211 help=\u0026#34;The number of events to create.\u0026#34;, 212 ) 213 parser.add_argument( 214 \u0026#34;--max_lag_seconds\u0026#34;, 215 \u0026#34;-l\u0026#34;, 216 type=int, 217 default=0, 218 help=\u0026#34;The maximum seconds that a record can be lagged.\u0026#34;, 219 ) 220 parser.add_argument( 221 \u0026#34;--delay_seconds\u0026#34;, 222 \u0026#34;-d\u0026#34;, 223 type=float, 224 default=None, 225 help=\u0026#34;The amount of time that a record should be delayed. Only applicable to streaming.\u0026#34;, 226 ) 227 228 args = parser.parse_args() 229 source = args.source 230 num_users = args.num_users 231 num_events = args.num_events 232 max_lag_seconds = args.max_lag_seconds 233 delay_seconds = args.delay_seconds 234 235 gen = EventGenerator( 236 source, 237 num_users, 238 num_events, 239 max_lag_seconds, 240 delay_seconds, 241 os.getenv(\u0026#34;BOOTSTRAP_SERVERS\u0026#34;, \u0026#34;localhost:29092\u0026#34;), 242 os.getenv(\u0026#34;TOPIC_NAME\u0026#34;, \u0026#34;website-visit\u0026#34;), 243 ) 244 gen.generate_events() Once we execute the script for generating batch pipeline data, we see a new file is created in the inputs folder as shown below.\n1$ python datagen/generate_data.py --source batch --num_users 20 --num_events 10000 --max_lag_seconds 60 2... 3$ head -n 3 inputs/4b1dba74-d970-4c67-a631-e0d7f52ad00e.out 4{\u0026#34;ip\u0026#34;: \u0026#34;74.236.125.208\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;-5227372761963790049\u0026#34;, \u0026#34;lat\u0026#34;: 26.3587, \u0026#34;lng\u0026#34;: -80.0831, \u0026#34;user_agent\u0026#34;: \u0026#34;Mozilla/5.0 (Windows; U; Windows NT 5.1) AppleWebKit/534.23.4 (KHTML, like Gecko) Version/4.0.5 Safari/534.23.4\u0026#34;, \u0026#34;age_bracket\u0026#34;: \u0026#34;26-40\u0026#34;, \u0026#34;opted_into_marketing\u0026#34;: true, \u0026#34;http_request\u0026#34;: \u0026#34;GET eucharya.html HTTP/1.0\u0026#34;, \u0026#34;http_response\u0026#34;: 200, \u0026#34;file_size_bytes\u0026#34;: 115, \u0026#34;event_datetime\u0026#34;: \u0026#34;2024-03-25T04:26:55.473\u0026#34;, \u0026#34;event_ts\u0026#34;: 1711301215473} 5{\u0026#34;ip\u0026#34;: \u0026#34;75.153.216.235\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;5836835583895516006\u0026#34;, \u0026#34;lat\u0026#34;: 49.2302, \u0026#34;lng\u0026#34;: -122.9952, \u0026#34;user_agent\u0026#34;: \u0026#34;Mozilla/5.0 (Android 6.0.1; Mobile; rv:11.0) Gecko/11.0 Firefox/11.0\u0026#34;, \u0026#34;age_bracket\u0026#34;: \u0026#34;41-55\u0026#34;, \u0026#34;opted_into_marketing\u0026#34;: true, \u0026#34;http_request\u0026#34;: \u0026#34;GET acanthocephala.html HTTP/1.0\u0026#34;, \u0026#34;http_response\u0026#34;: 200, \u0026#34;file_size_bytes\u0026#34;: 358, \u0026#34;event_datetime\u0026#34;: \u0026#34;2024-03-25T04:27:01.930\u0026#34;, \u0026#34;event_ts\u0026#34;: 1711301221930} 6{\u0026#34;ip\u0026#34;: \u0026#34;134.157.0.190\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;-5123638115214052647\u0026#34;, \u0026#34;lat\u0026#34;: 48.8534, \u0026#34;lng\u0026#34;: 2.3488, \u0026#34;user_agent\u0026#34;: \u0026#34;Mozilla/5.0 (X11; Linux i686) AppleWebKit/534.2 (KHTML, like Gecko) Chrome/23.0.825.0 Safari/534.2\u0026#34;, \u0026#34;age_bracket\u0026#34;: \u0026#34;55+\u0026#34;, \u0026#34;opted_into_marketing\u0026#34;: true, \u0026#34;http_request\u0026#34;: \u0026#34;GET bacteria.html HTTP/1.0\u0026#34;, \u0026#34;http_response\u0026#34;: 200, \u0026#34;file_size_bytes\u0026#34;: 402, \u0026#34;event_datetime\u0026#34;: \u0026#34;2024-03-25T04:27:16.037\u0026#34;, \u0026#34;event_ts\u0026#34;: 1711301236037} Basic Pipeline Below shows a basic Beam pipeline. It (1) reads one or more files that match a file name pattern, (2) parses lines of Json string into Python dictionaries, (3) filters records where opted_into_marketing is TRUE, (4) selects a subset of attributes and finally (5) writes the updated records into a folder named outputs. It uses the Direct Runner by default, and we can also try a different runner by specifying the runner name (eg python section1/basic.py --runner FlinkRunner).\n1# section1/basic.py 2import os 3import datetime 4import argparse 5import json 6import logging 7 8import apache_beam as beam 9from apache_beam.options.pipeline_options import PipelineOptions 10from apache_beam.options.pipeline_options import StandardOptions 11 12 13def run(): 14 parser = argparse.ArgumentParser(description=\u0026#34;Beam pipeline arguments\u0026#34;) 15 parser.add_argument( 16 \u0026#34;--inputs\u0026#34;, 17 default=\u0026#34;inputs\u0026#34;, 18 help=\u0026#34;Specify folder name that event records are saved\u0026#34;, 19 ) 20 parser.add_argument( 21 \u0026#34;--runner\u0026#34;, default=\u0026#34;DirectRunner\u0026#34;, help=\u0026#34;Specify Apache Beam Runner\u0026#34; 22 ) 23 opts = parser.parse_args() 24 PARENT_DIR = os.path.dirname(os.path.dirname(os.path.realpath(__file__))) 25 26 options = PipelineOptions() 27 options.view_as(StandardOptions).runner = opts.runner 28 29 p = beam.Pipeline(options=options) 30 ( 31 p 32 | \u0026#34;Read from files\u0026#34; 33 \u0026gt;\u0026gt; beam.io.ReadFromText( 34 file_pattern=os.path.join(PARENT_DIR, opts.inputs, \u0026#34;*.out\u0026#34;) 35 ) 36 | \u0026#34;Parse Json\u0026#34; \u0026gt;\u0026gt; beam.Map(lambda line: json.loads(line)) 37 | \u0026#34;Filter status\u0026#34; \u0026gt;\u0026gt; beam.Filter(lambda d: d[\u0026#34;opted_into_marketing\u0026#34;] is True) 38 | \u0026#34;Select columns\u0026#34; 39 \u0026gt;\u0026gt; beam.Map( 40 lambda d: { 41 k: v 42 for k, v in d.items() 43 if k in [\u0026#34;ip\u0026#34;, \u0026#34;id\u0026#34;, \u0026#34;lat\u0026#34;, \u0026#34;lng\u0026#34;, \u0026#34;age_bracket\u0026#34;] 44 } 45 ) 46 | \u0026#34;Write to file\u0026#34; 47 \u0026gt;\u0026gt; beam.io.WriteToText( 48 file_path_prefix=os.path.join( 49 PARENT_DIR, 50 \u0026#34;outputs\u0026#34;, 51 f\u0026#34;{int(datetime.datetime.now().timestamp() * 1000)}\u0026#34;, 52 ), 53 file_name_suffix=\u0026#34;.out\u0026#34;, 54 ) 55 ) 56 57 logging.getLogger().setLevel(logging.INFO) 58 logging.info(\u0026#34;Building pipeline ...\u0026#34;) 59 60 p.run().wait_until_finish() 61 62 63if __name__ == \u0026#34;__main__\u0026#34;: 64 run() Once executed, we can check the output records in the outputs folder.\n1$ python section1/basic.py 2INFO:root:Building pipeline ... 3INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== \u0026lt;function annotate_downstream_side_inputs at 0x7f1d24906050\u0026gt; ==================== 4INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== \u0026lt;function fix_side_input_pcoll_coders at 0x7f1d24906170\u0026gt; ==================== 5INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== \u0026lt;function pack_combiners at 0x7f1d24906680\u0026gt; ==================== 6INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== \u0026lt;function lift_combiners at 0x7f1d24906710\u0026gt; ==================== 7INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== \u0026lt;function expand_sdf at 0x7f1d249068c0\u0026gt; ==================== 8INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== \u0026lt;function expand_gbk at 0x7f1d24906950\u0026gt; ==================== 9INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== \u0026lt;function sink_flattens at 0x7f1d24906a70\u0026gt; ==================== 10INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== \u0026lt;function greedily_fuse at 0x7f1d24906b00\u0026gt; ==================== 11INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== \u0026lt;function read_to_impulse at 0x7f1d24906b90\u0026gt; ==================== 12INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== \u0026lt;function impulse_to_input at 0x7f1d24906c20\u0026gt; ==================== 13INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== \u0026lt;function sort_stages at 0x7f1d24906e60\u0026gt; ==================== 14INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== \u0026lt;function add_impulse_to_dangling_transforms at 0x7f1d24906f80\u0026gt; ==================== 15INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== \u0026lt;function setup_timer_mapping at 0x7f1d24906dd0\u0026gt; ==================== 16INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== \u0026lt;function populate_data_channel_coders at 0x7f1d24906ef0\u0026gt; ==================== 17INFO:apache_beam.runners.worker.statecache:Creating state cache with size 104857600 18INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Created Worker handler \u0026lt;apache_beam.runners.portability.fn_api_runner.worker_handlers.EmbeddedWorkerHandler object at 0x7f1d2c548550\u0026gt; for environment ref_Environment_default_environment_1 (beam:env:embedded_python:v1, b\u0026#39;\u0026#39;) 19INFO:apache_beam.io.filebasedsink:Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1 20INFO:apache_beam.io.filebasedsink:Renamed 1 shards in 0.01 seconds. 1$ head -n 3 outputs/1711341025220-00000-of-00001.out 2{\u0026#39;ip\u0026#39;: \u0026#39;74.236.125.208\u0026#39;, \u0026#39;id\u0026#39;: \u0026#39;-5227372761963790049\u0026#39;, \u0026#39;lat\u0026#39;: 26.3587, \u0026#39;lng\u0026#39;: -80.0831, \u0026#39;age_bracket\u0026#39;: \u0026#39;26-40\u0026#39;} 3{\u0026#39;ip\u0026#39;: \u0026#39;75.153.216.235\u0026#39;, \u0026#39;id\u0026#39;: \u0026#39;5836835583895516006\u0026#39;, \u0026#39;lat\u0026#39;: 49.2302, \u0026#39;lng\u0026#39;: -122.9952, \u0026#39;age_bracket\u0026#39;: \u0026#39;41-55\u0026#39;} 4{\u0026#39;ip\u0026#39;: \u0026#39;134.157.0.190\u0026#39;, \u0026#39;id\u0026#39;: \u0026#39;-5123638115214052647\u0026#39;, \u0026#39;lat\u0026#39;: 48.8534, \u0026#39;lng\u0026#39;: 2.3488, \u0026#39;age_bracket\u0026#39;: \u0026#39;55+\u0026#39;} Interactive Beam Interactive Beam is aimed at integrating Apache Beam with Jupyter notebook to make pipeline prototyping and data exploration much faster and easier. It provides nice features such as graphical representation of pipeline DAGs and PCollection elements, fetching PCollections as pandas DataFrame and faster execution/re-execution of pipelines.\nWe can start a Jupyter server while enabling Jupyter Lab and ignoring authentication as shown below. Once started, it can be accessed on http://localhost:8888.\n1$ JUPYTER_ENABLE_LAB=yes jupyter lab --ServerApp.token=\u0026#39;\u0026#39; --ServerApp.password=\u0026#39;\u0026#39; The basic pipeline is recreated in section1/basic.ipynb. The InteractiveRunner is used for the pipeline and, by default, the Python Direct Runner is taken as the underlying runner. When we run the first two cells, we can show the output records in a data table.\nThe pipeline DAG can be visualized using the show_graph method as shown below. It helps identify or share how a pipeline is executed more effectively.\nBeam SQL Beam SQL allows a Beam user (currently only available in Beam Java and Python) to query bounded and unbounded PCollections with SQL statements. An SQL query is translated to a PTransform, and it can be particularly useful for joining multiple PCollections.\nIn section1/sql.ipynb, we first create a PCollection of 3 elements that can be used as source data.\nBeam SQL is executed as an IPython extension, and it should be loaded before being used. The magic function requires query, and we can optionally specify the output name (OUTPUT_NAME) and runner (RUNNER).\nAfter the extension is loaded, we execute a SQL query, optionally specifying the output PCollection name (filtered). We can use the existing PCollection named items as the source.\nThere are several notes about Beam SQL on a notebook.\nThe SQL query is executed in a separate Docker container and data is processed via the Java SDK. Currently it only supports the Direct Runner and Dataflow Runner. The output PCollection is accessible in the entire notebook, and we can use it in another cell. While Beam SQL supports both Calcite SQL and ZetaSQL, the magic function doesn\u0026rsquo;t allow us to select which dialect to choose. Only the default Calcite SQL will be used on a notebook. Beam DataFrames The Apache Beam Python SDK provides a DataFrame API for working with pandas-like DataFrame objects. This feature lets you convert a PCollection to a DataFrame and then interact with the DataFrame using the standard methods available on the pandas DataFrame API.\nIn section1/dataframe.ipynb, we also create a PCollection of 3 elements as the Beam SQL example.\nSubsequently we convert the source PCollection into a pandas DataFrame using the to_dataframe method, process data via pandas API and return to PCollection using the to_pcollection method.\nSummary Apache Beam and Apache Flink are open-source frameworks for parallel, distributed data processing at scale. While PyFlink, Flink\u0026rsquo;s Python API, is limited to build sophisticated data streaming applications, Apache Beam\u0026rsquo;s Python SDK has potential as it supports more capacity to extend and/or customise its features. In this series of posts, we discuss local development of Apache Beam pipelines using Python. In Part 1, a basic Beam pipeline was introduced, followed by demonstrating how to utilise Jupyter notebooks for interactive development. We also covered Beam SQL and Beam DataFrames examples on notebooks.\n","date":"March 28, 2024","img":"/blog/2024-03-28-beam-local-dev-1/featured.png","lang":"en","langName":"English","largeImg":"/blog/2024-03-28-beam-local-dev-1/featured_hu8df3c1065468b257d73fe4ccb72b4c47_88260_500x0_resize_box_3.png","permalink":"/blog/2024-03-28-beam-local-dev-1/","series":[{"title":"Apache Beam Local Development With Python","url":"/series/apache-beam-local-development-with-python/"}],"smallImg":"/blog/2024-03-28-beam-local-dev-1/featured_hu8df3c1065468b257d73fe4ccb72b4c47_88260_180x0_resize_box_3.png","tags":[{"title":"Apache Beam","url":"/tags/apache-beam/"},{"title":"Apache Flink","url":"/tags/apache-flink/"},{"title":"Python","url":"/tags/python/"},{"title":"Jupyter Notebook","url":"/tags/jupyter-notebook/"}],"timestamp":1711584000,"title":"Apache Beam Local Development With Python - Part 1 Pipeline, Notebook, SQL and DataFrame"},{"categories":[{"title":"Data Engineering","url":"/categories/data-engineering/"}],"content":"In Part 5, we developed a dbt project that that targets Apache Iceberg where transformations are performed on Amazon Athena. Two dimension tables that keep product and user records are created as Type 2 slowly changing dimension (SCD Type 2) tables, and one transactional fact table is built to keep pizza orders. To improve query performance, the fact table is denormalized to pre-join records from the dimension tables using the array and struct data types. In this post, we discuss how to set up an ETL process on the project using Apache Airflow.\nPart 1 Modelling on PostgreSQL Part 2 ETL on PostgreSQL via Airflow Part 3 Modelling on BigQuery Part 4 ETL on BigQuery via Airflow Part 5 Modelling on Amazon Athena Part 6 ETL on Amazon Athena via Airflow (this post) Infrastructure Apache Airflow and Amazon Athena are used in this post, and the former is deployed locally using Docker Compose. The source can be found in the GitHub repository of this post.\nAirflow Airflow is simplified by using the Local Executor where both scheduling and task execution are handled by the airflow scheduler service - i.e. AIRFLOW__CORE__EXECUTOR: LocalExecutor. Also, it is configured to be able to run the dbt project (see Part 5 for details) within the scheduler service by\ninstalling the dbt-athena-community and awswrangler packages as additional pip packages, volume-mapping folders that keep the dbt project and dbt project profile, and specifying environment variables for accessing AWS services (AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY) 1# docker-compose.yml 2version: \u0026#34;3\u0026#34; 3x-airflow-common: \u0026amp;airflow-common 4 image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:2.8.0} 5 networks: 6 - appnet 7 environment: \u0026amp;airflow-common-env 8 AIRFLOW__CORE__EXECUTOR: LocalExecutor 9 AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow 10 # For backward compatibility, with Airflow \u0026lt;2.3 11 AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow 12 AIRFLOW__CORE__FERNET_KEY: \u0026#34;\u0026#34; 13 AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: \u0026#34;true\u0026#34; 14 AIRFLOW__CORE__LOAD_EXAMPLES: \u0026#34;false\u0026#34; 15 AIRFLOW__API__AUTH_BACKENDS: \u0026#34;airflow.api.auth.backend.basic_auth\u0026#34; 16 _PIP_ADDITIONAL_REQUIREMENTS: ${_PIP_ADDITIONAL_REQUIREMENTS:- dbt-athena-community==1.7.1 awswrangler pendulum} 17 AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID} 18 AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY} 19 TZ: Australia/Sydney 20 volumes: 21 - ./airflow/dags:/opt/airflow/dags 22 - ./airflow/plugins:/opt/airflow/plugins 23 - ./airflow/logs:/opt/airflow/logs 24 - ./pizza_shop:/tmp/pizza_shop # dbt project 25 - ./airflow/dbt-profiles:/opt/airflow/dbt-profiles # dbt profiles 26 user: \u0026#34;${AIRFLOW_UID:-50000}:0\u0026#34; 27 depends_on: \u0026amp;airflow-common-depends-on 28 postgres: 29 condition: service_healthy 30 31services: 32 postgres: 33 image: postgres:13 34 container_name: postgres 35 ports: 36 - 5432:5432 37 networks: 38 - appnet 39 environment: 40 POSTGRES_USER: airflow 41 POSTGRES_PASSWORD: airflow 42 POSTGRES_DB: airflow 43 TZ: Australia/Sydney 44 volumes: 45 - postgres_data:/var/lib/postgresql/data 46 healthcheck: 47 test: [\u0026#34;CMD\u0026#34;, \u0026#34;pg_isready\u0026#34;, \u0026#34;-U\u0026#34;, \u0026#34;airflow\u0026#34;] 48 interval: 5s 49 retries: 5 50 51 airflow-webserver: 52 \u0026lt;\u0026lt;: *airflow-common 53 container_name: webserver 54 command: webserver 55 ports: 56 - 8080:8080 57 depends_on: 58 \u0026lt;\u0026lt;: *airflow-common-depends-on 59 airflow-init: 60 condition: service_completed_successfully 61 62 airflow-scheduler: 63 \u0026lt;\u0026lt;: *airflow-common 64 command: scheduler 65 container_name: scheduler 66 environment: 67 \u0026lt;\u0026lt;: *airflow-common-env 68 depends_on: 69 \u0026lt;\u0026lt;: *airflow-common-depends-on 70 airflow-init: 71 condition: service_completed_successfully 72 73 airflow-init: 74 \u0026lt;\u0026lt;: *airflow-common 75 container_name: init 76 entrypoint: /bin/bash 77 command: 78 - -c 79 - | 80 mkdir -p /sources/logs /sources/dags /sources/plugins /sources/scheduler 81 chown -R \u0026#34;${AIRFLOW_UID}:0\u0026#34; /sources/{logs,dags,plugins,scheduler} 82 exec /entrypoint airflow version 83 environment: 84 \u0026lt;\u0026lt;: *airflow-common-env 85 _AIRFLOW_DB_UPGRADE: \u0026#34;true\u0026#34; 86 _AIRFLOW_WWW_USER_CREATE: \u0026#34;true\u0026#34; 87 _AIRFLOW_WWW_USER_USERNAME: ${_AIRFLOW_WWW_USER_USERNAME:-airflow} 88 _AIRFLOW_WWW_USER_PASSWORD: ${_AIRFLOW_WWW_USER_PASSWORD:-airflow} 89 _PIP_ADDITIONAL_REQUIREMENTS: \u0026#34;\u0026#34; 90 user: \u0026#34;0:0\u0026#34; 91 volumes: 92 - ./airflow:/sources 93 94volumes: 95 airflow_log_volume: 96 driver: local 97 name: airflow_log_volume 98 postgres_data: 99 driver: local 100 name: postgres_data 101 102networks: 103 appnet: 104 name: app-network Before we deploy the Airflow services, we need to create staging tables and insert initial records. It can be achieved by executing a Python script (insert_records.py) - see Part 5 for details about the prerequisite step. Then the services can be started using the docker-compose up command. Note that it is recommended to specify the host user\u0026rsquo;s ID as the AIRFLOW_UID value. Otherwise, Airflow can fail to launch due to insufficient permission to write logs.\n1## prerequisite 2## create staging tables and insert records - python setup/insert_records.py 3 4## start airflow services 5$ AIRFLOW_UID=$(id -u) docker-compose up -d Once started, we can visit the Airflow web server on http://localhost:8080.\nETL Job A simple ETL job (demo_etl) is created, which updates source records (update_records) followed by running and testing the dbt project. Note that the dbt project and profile are accessible as they are volume-mapped to the Airflow scheduler container.\n1# airflow/dags/operators.py 2import pendulum 3from airflow.models.dag import DAG 4from airflow.operators.bash import BashOperator 5from airflow.operators.python import PythonOperator 6 7import update_records 8 9with DAG( 10 dag_id=\u0026#34;demo_etl\u0026#34;, 11 schedule=None, 12 start_date=pendulum.datetime(2024, 1, 1, tz=\u0026#34;Australia/Sydney\u0026#34;), 13 catchup=False, 14 tags=[\u0026#34;pizza\u0026#34;], 15): 16 task_records_update = PythonOperator( 17 task_id=\u0026#34;update_records\u0026#34;, python_callable=update_records.main 18 ) 19 20 task_dbt_run = BashOperator( 21 task_id=\u0026#34;dbt_run\u0026#34;, 22 bash_command=\u0026#34;dbt run --profiles-dir /opt/airflow/dbt-profiles --project-dir /tmp/pizza_shop\u0026#34;, 23 ) 24 25 task_dbt_test = BashOperator( 26 task_id=\u0026#34;dbt_test\u0026#34;, 27 bash_command=\u0026#34;dbt test --profiles-dir /opt/airflow/dbt-profiles --project-dir /tmp/pizza_shop\u0026#34;, 28 ) 29 30 task_records_update \u0026gt;\u0026gt; task_dbt_run \u0026gt;\u0026gt; task_dbt_test Update Records As dbt takes care of data transformation only, we should create a task that updates source records. As shown below, the task updates as many as a half of records of the dimension tables (products and users) and appends 5,000 order records in a single run.\n1# airflow/dags/update_records.py 2import os 3import datetime 4import dataclasses 5import json 6import random 7import string 8 9import boto3 10import pandas as pd 11import awswrangler as wr 12 13 14class QueryHelper: 15 def __init__(self, db_name: str, bucket_name: str): 16 self.db_name = db_name 17 self.bucket_name = bucket_name 18 19 def read_sql_query(self, stmt: str): 20 return wr.athena.read_sql_query( 21 stmt, 22 database=self.db_name, 23 boto3_session=boto3.Session( 24 region_name=os.getenv(\u0026#34;AWS_REGION\u0026#34;, \u0026#34;ap-southeast-2\u0026#34;) 25 ), 26 ) 27 28 def load_source(self, df: pd.DataFrame, obj_name: str): 29 if obj_name not in [\u0026#34;users\u0026#34;, \u0026#34;products\u0026#34;, \u0026#34;orders\u0026#34;]: 30 raise ValueError(\u0026#34;object name should be one of users, products, orders\u0026#34;) 31 wr.s3.to_parquet( 32 df=df, 33 path=f\u0026#34;s3://{self.bucket_name}/staging/{obj_name}/\u0026#34;, 34 dataset=True, 35 database=self.db_name, 36 table=f\u0026#34;staging_{obj_name}\u0026#34;, 37 boto3_session=boto3.Session( 38 region_name=os.getenv(\u0026#34;AWS_REGION\u0026#34;, \u0026#34;ap-southeast-2\u0026#34;) 39 ), 40 ) 41 42 43def update_products( 44 query_helper: QueryHelper, 45 percent: float = 0.5, 46 created_at: datetime.datetime = datetime.datetime.now(), 47): 48 stmt = \u0026#34;\u0026#34;\u0026#34; 49 WITH windowed AS ( 50 SELECT 51 *, 52 ROW_NUMBER() OVER (PARTITION BY id ORDER BY created_at DESC) AS rn 53 FROM pizza_shop.staging_products 54 ) 55 SELECT id, name, description, price, category, image 56 FROM windowed 57 WHERE rn = 1; 58 \u0026#34;\u0026#34;\u0026#34; 59 products = query_helper.read_sql_query(stmt) 60 products.insert(products.shape[1], \u0026#34;created_at\u0026#34;, created_at) 61 records = products.sample(n=int(products.shape[0] * percent)) 62 records[\u0026#34;price\u0026#34;] = records[\u0026#34;price\u0026#34;] + 10 63 query_helper.load_source(records, \u0026#34;products\u0026#34;) 64 return records 65 66 67def update_users( 68 query_helper: QueryHelper, 69 percent: float = 0.5, 70 created_at: datetime.datetime = datetime.datetime.now(), 71): 72 stmt = \u0026#34;\u0026#34;\u0026#34; 73 WITH windowed AS ( 74 SELECT 75 *, 76 ROW_NUMBER() OVER (PARTITION BY id ORDER BY created_at DESC) AS rn 77 FROM pizza_shop.staging_users 78 ) 79 SELECT id, first_name, last_name, email, residence, lat, lon 80 FROM windowed 81 WHERE rn = 1; 82 \u0026#34;\u0026#34;\u0026#34; 83 users = query_helper.read_sql_query(stmt) 84 users.insert(users.shape[1], \u0026#34;created_at\u0026#34;, created_at) 85 records = users.sample(n=int(users.shape[0] * percent)) 86 records[ 87 \u0026#34;email\u0026#34; 88 ] = f\u0026#34;{\u0026#39;\u0026#39;.join(random.choices(string.ascii_letters, k=5)).lower()}@email.com\u0026#34; 89 query_helper.load_source(records, \u0026#34;users\u0026#34;) 90 return records 91 92 93@dataclasses.dataclass 94class Order: 95 id: int 96 user_id: int 97 items: str 98 created_at: datetime.datetime 99 100 def to_json(self): 101 return dataclasses.asdict(self) 102 103 @classmethod 104 def create(cls, id: int, created_at: datetime.datetime): 105 order_items = [ 106 {\u0026#34;product_id\u0026#34;: id, \u0026#34;quantity\u0026#34;: random.randint(1, 5)} 107 for id in set(random.choices(range(1, 82), k=random.randint(1, 10))) 108 ] 109 return cls( 110 id=id, 111 user_id=random.randint(1, 10000), 112 items=json.dumps([item for item in order_items]), 113 created_at=created_at, 114 ) 115 116 @staticmethod 117 def insert( 118 query_helper: QueryHelper, 119 max_id: int, 120 num_orders: int = 5000, 121 created_at: datetime.datetime = datetime.datetime.now(), 122 ): 123 records = [] 124 for _ in range(num_orders): 125 records.append(Order.create(max_id + 1, created_at).to_json()) 126 max_id += 1 127 query_helper.load_source(pd.DataFrame.from_records(records), \u0026#34;orders\u0026#34;) 128 return records 129 130 @staticmethod 131 def get_max_id(query_helper: QueryHelper): 132 stmt = \u0026#34;SELECT max(id) AS mx FROM pizza_shop.staging_orders\u0026#34; 133 df = query_helper.read_sql_query(stmt) 134 return next(iter(df.mx)) 135 136 137def main(): 138 query_helper = QueryHelper(db_name=\u0026#34;pizza_shop\u0026#34;, bucket_name=\u0026#34;dbt-pizza-shop-demo\u0026#34;) 139 created_at = datetime.datetime.now() 140 # update product and user records 141 updated_products = update_products(query_helper, created_at=created_at) 142 print(f\u0026#34;{len(updated_products)} product records updated\u0026#34;) 143 updated_users = update_users(query_helper, created_at=created_at) 144 print(f\u0026#34;{len(updated_users)} user records updated\u0026#34;) 145 # create order records 146 max_order_id = Order.get_max_id(query_helper) 147 new_orders = Order.insert(query_helper, max_order_id, created_at=created_at) 148 print(f\u0026#34;{len(new_orders)} order records created\u0026#34;) The details of the ETL job can be found on the Airflow web server as shown below.\nRun ETL Below shows example product dimension records after the ETL job is completed. It is shown that the product 2 is updated, and a new surrogate key is assigned to the new record as well as the valid_from and valid_to column values are updated accordingly.\n1SELECT product_key, product_id AS id, price, valid_from, valid_to 2FROM pizza_shop.dim_products 3WHERE product_id IN (1, 2) 4ORDER BY product_id, valid_from; 5 6# product_key id price valid_from valid_to 71 b8c187845db8b7e55626659cfbb8aea1 1 335.0 2024-03-01 10:16:36.481000 2199-12-31 00:00:00.000000 82 * 8311b52111a924582c0fe5cb566cfa9a 2 60.0 2024-03-01 10:16:36.481000 2024-03-01 10:16:50.932000 93 * 0f4df52917ddff1bcf618b798c8aff43 2 70.0 2024-03-01 10:16:50.932000 2199-12-31 00:00:00.000000 When we query the fact table for orders with the same product ID, we can check correct product key values are mapped - note value_to is not inclusive.\n1SELECT o.order_id, p.key, p.id, p.price, p.quantity, o.created_at 2FROM pizza_shop.fct_orders AS o 3CROSS JOIN UNNEST(product) AS t(p) 4WHERE o.order_id IN (11146, 20398) AND p.id IN (1, 2) 5ORDER BY o.order_id, p.id; 6 7# order_id key id price quantity created_at 81 11146 b8c187845db8b7e55626659cfbb8aea1 1 335.0 1 2024-03-01 10:16:37.981000 92 11146 * 8311b52111a924582c0fe5cb566cfa9a 2 60.0 2 2024-03-01 10:16:37.981000 103 20398 b8c187845db8b7e55626659cfbb8aea1 1 335.0 5 2024-03-01 10:16:50.932000 114 20398 * 0f4df52917ddff1bcf618b798c8aff43 2 70.0 3 2024-03-01 10:16:50.932000 Summary In this series of posts, we discuss data warehouse/lakehouse examples using data build tool (dbt) including ETL orchestration with Apache Airflow. In this post, we discussed how to set up an ETL process on the dbt project developed in Part 5 using Airflow. A demo ETL job was created that updates records followed by running and testing the dbt project. Finally, the result of ETL job was validated by checking sample records.\n","date":"March 14, 2024","img":"/blog/2024-03-14-dbt-pizza-shop-6/featured.png","lang":"en","langName":"English","largeImg":"/blog/2024-03-14-dbt-pizza-shop-6/featured_hu72fe52c30196d9a511f344ff548f2036_82921_500x0_resize_box_3.png","permalink":"/blog/2024-03-14-dbt-pizza-shop-6/","series":[{"title":"DBT Pizza Shop Demo","url":"/series/dbt-pizza-shop-demo/"}],"smallImg":"/blog/2024-03-14-dbt-pizza-shop-6/featured_hu72fe52c30196d9a511f344ff548f2036_82921_180x0_resize_box_3.png","tags":[{"title":"Data Build Tool (DBT)","url":"/tags/data-build-tool-dbt/"},{"title":"AWS","url":"/tags/aws/"},{"title":"Amazon Athena","url":"/tags/amazon-athena/"},{"title":"Apache Iceberg","url":"/tags/apache-iceberg/"},{"title":"Docker","url":"/tags/docker/"},{"title":"Docker Compose","url":"/tags/docker-compose/"},{"title":"Apache Airflow","url":"/tags/apache-airflow/"},{"title":"Python","url":"/tags/python/"}],"timestamp":1710374400,"title":"Data Build Tool (Dbt) Pizza Shop Demo - Part 6 ETL on Amazon Athena via Airflow"},{"categories":[{"title":"Data Engineering","url":"/categories/data-engineering/"}],"content":"In Part 1 and Part 3, we developed data build tool (dbt) projects that target PostgreSQL and BigQuery using fictional pizza shop data. The data is modelled by SCD type 2 dimension tables and one transactional fact table. While the order records should be joined with dimension tables to get complete details for PostgreSQL, the fact table is denormalized using nested and repeated fields to improve query performance for BigQuery.\nOpen Table Formats such as Apache Iceberg bring a new opportunity that implements data warehousing features in a data lake (i.e. data lakehouse) and Amazon Athena is probably the easiest way to perform such tasks on AWS. In this post, we create a new dbt project that targets Apache Iceberg where transformations are performed on Amazon Athena. Data modelling is similar to the BigQuery project where the dimension tables are modelled by the SCD type 2 approach and the fact table is denormalized using the array and struct data types.\nPart 1 Modelling on PostgreSQL Part 2 ETL on PostgreSQL via Airflow Part 3 Modelling on BigQuery Part 4 ETL on BigQuery via Airflow Part 5 Modelling on Amazon Athena (this post) Part 6 ETL on Amazon Athena via Airflow Setup Amazon Athena Amazon Athena is used in the post, and the source can be found in the GitHub repository of this post.\nPrepare Data Fictional pizza shop data from Building Real-Time Analytics Systems is used in this post. There are three data sets - products, users and orders. The first two data sets are copied from the book\u0026rsquo;s GitHub repository and saved into the setup/data folder. The last order data set is generated by the following Python script.\n1# setup/generate_orders.py 2import os 3import csv 4import dataclasses 5import random 6import json 7 8 9@dataclasses.dataclass 10class Order: 11 user_id: int 12 items: str 13 14 @classmethod 15 def create(self): 16 order_items = [ 17 {\u0026#34;product_id\u0026#34;: id, \u0026#34;quantity\u0026#34;: random.randint(1, 5)} 18 for id in set(random.choices(range(1, 82), k=random.randint(1, 10))) 19 ] 20 return Order( 21 user_id=random.randint(1, 10000), 22 items=json.dumps([item for item in order_items]), 23 ) 24 25 26if __name__ == \u0026#34;__main__\u0026#34;: 27 \u0026#34;\u0026#34;\u0026#34; 28 Generate random orders given by the NUM_ORDERS environment variable. 29 - orders.csv will be written to ./data folder 30 31 Example: 32 python generate_orders.py 33 NUM_ORDERS=10000 python generate_orders.py 34 \u0026#34;\u0026#34;\u0026#34; 35 NUM_ORDERS = int(os.getenv(\u0026#34;NUM_ORDERS\u0026#34;, \u0026#34;20000\u0026#34;)) 36 CURRENT_DIR = os.path.dirname(os.path.realpath(__file__)) 37 orders = [Order.create() for _ in range(NUM_ORDERS)] 38 39 filepath = os.path.join(CURRENT_DIR, \u0026#34;data\u0026#34;, \u0026#34;orders.csv\u0026#34;) 40 if os.path.exists(filepath): 41 os.remove(filepath) 42 43 with open(os.path.join(CURRENT_DIR, \u0026#34;data\u0026#34;, \u0026#34;orders.csv\u0026#34;), \u0026#34;w\u0026#34;) as f: 44 writer = csv.writer(f) 45 writer.writerow([\u0026#34;user_id\u0026#34;, \u0026#34;items\u0026#34;]) 46 for order in orders: 47 writer.writerow(dataclasses.asdict(order).values()) Below shows sample order records generated by the script. It includes user ID and order items.\n1user_id,items 22911,\u0026#34;[{\u0026#34;\u0026#34;product_id\u0026#34;\u0026#34;: 57, \u0026#34;\u0026#34;quantity\u0026#34;\u0026#34;: 3}, {\u0026#34;\u0026#34;product_id\u0026#34;\u0026#34;: 66, \u0026#34;\u0026#34;quantity\u0026#34;\u0026#34;: 4}, {\u0026#34;\u0026#34;product_id\u0026#34;\u0026#34;: 27, \u0026#34;\u0026#34;quantity\u0026#34;\u0026#34;: 4}]\u0026#34; 34483,\u0026#34;[{\u0026#34;\u0026#34;product_id\u0026#34;\u0026#34;: 13, \u0026#34;\u0026#34;quantity\u0026#34;\u0026#34;: 3}]\u0026#34; 4894,\u0026#34;[{\u0026#34;\u0026#34;product_id\u0026#34;\u0026#34;: 59, \u0026#34;\u0026#34;quantity\u0026#34;\u0026#34;: 1}, {\u0026#34;\u0026#34;product_id\u0026#34;\u0026#34;: 22, \u0026#34;\u0026#34;quantity\u0026#34;\u0026#34;: 5}]\u0026#34; The folder structure of source data sets and order generation script can be found below.\n1$ tree setup/ -P \u0026#34;*.csv|generate*\u0026#34; 2setup/ 3├── data 4│ ├── orders.csv 5│ ├── products.csv 6│ └── users.csv 7└── generate_orders.py Insert Source Data The source data is inserted using the AWS SDK for pandas (awswrangler) package. It is a simple process that reads records from the source data files as Pandas DataFrame, adds incremental ID/creation datetime and inserts to S3. Upon successful insertion, we can check the source records are saved into S3 and their table metadata is registered in a Glue database named pizza_shop.\n1# setup/insert_records.py 2import os 3import datetime 4 5import boto3 6import botocore 7import pandas as pd 8import awswrangler as wr 9 10 11class QueryHelper: 12 def __init__(self, db_name: str, bucket_name: str, current_dir: str = None): 13 self.db_name = db_name 14 self.bucket_name = bucket_name 15 self.current_dir = current_dir or os.path.dirname(os.path.realpath(__file__)) 16 self.glue_client = boto3.client( 17 \u0026#34;glue\u0026#34;, region_name=os.getenv(\u0026#34;AWS_REGION\u0026#34;, \u0026#34;ap-southeast-2\u0026#34;) 18 ) 19 20 def check_db(self): 21 try: 22 self.glue_client.get_database(Name=self.db_name) 23 return True 24 except botocore.exceptions.ClientError as err: 25 if err.response[\u0026#34;Error\u0026#34;][\u0026#34;Code\u0026#34;] == \u0026#34;EntityNotFoundException\u0026#34;: 26 return False 27 else: 28 raise err 29 30 def create_db(self): 31 try: 32 self.glue_client.create_database( 33 DatabaseInput={ 34 \u0026#34;Name\u0026#34;: self.db_name, 35 } 36 ) 37 return True 38 except botocore.exceptions.ClientError as err: 39 if err.response[\u0026#34;Error\u0026#34;][\u0026#34;Code\u0026#34;] == \u0026#34;AlreadyExistsException\u0026#34;: 40 return True 41 else: 42 raise err 43 44 def read_source(self, file_name: str): 45 df = pd.read_csv(os.path.join(self.current_dir, \u0026#34;data\u0026#34;, file_name)) 46 df.insert(0, \u0026#34;id\u0026#34;, range(1, len(df) + 1)) 47 df.insert( 48 df.shape[1], 49 \u0026#34;created_at\u0026#34;, 50 datetime.datetime.now(), 51 ) 52 return df 53 54 def load_source(self, df: pd.DataFrame, obj_name: str): 55 if obj_name not in [\u0026#34;users\u0026#34;, \u0026#34;products\u0026#34;, \u0026#34;orders\u0026#34;]: 56 raise ValueError(\u0026#34;object name should be one of users, products, orders\u0026#34;) 57 wr.s3.to_parquet( 58 df=df, 59 path=f\u0026#34;s3://{self.bucket_name}/staging/{obj_name}/\u0026#34;, 60 dataset=True, 61 database=self.db_name, 62 table=f\u0026#34;staging_{obj_name}\u0026#34;, 63 boto3_session=boto3.Session( 64 region_name=os.getenv(\u0026#34;AWS_REGION\u0026#34;, \u0026#34;ap-southeast-2\u0026#34;) 65 ), 66 ) 67 68 69if __name__ == \u0026#34;__main__\u0026#34;: 70 query_helper = QueryHelper(db_name=\u0026#34;pizza_shop\u0026#34;, bucket_name=\u0026#34;dbt-pizza-shop-demo\u0026#34;) 71 if not query_helper.check_db(): 72 query_helper.create_db() 73 print(\u0026#34;inserting products...\u0026#34;) 74 products = query_helper.read_source(\u0026#34;products.csv\u0026#34;) 75 query_helper.load_source(products, \u0026#34;products\u0026#34;) 76 print(\u0026#34;inserting users...\u0026#34;) 77 users = query_helper.read_source(\u0026#34;users.csv\u0026#34;) 78 query_helper.load_source(users, \u0026#34;users\u0026#34;) 79 print(\u0026#34;inserting orders...\u0026#34;) 80 orders = query_helper.read_source(\u0026#34;orders.csv\u0026#34;) 81 query_helper.load_source(orders, \u0026#34;orders\u0026#34;) Setup DBT Project A dbt project named pizza_shop is created using the dbt-athena-community package (dbt-athena-community==1.7.1). Specifically, it is created using the dbt init command, and it bootstraps the project in the pizza_shop folder as well as adds the project profile to the dbt profiles file. See this page for details about how to set up Amazon Athena for a dbt project.\n1# $HOME/.dbt/profiles.yml 2pizza_shop: 3 outputs: 4 dev: 5 type: athena 6 database: awsdatacatalog 7 schema: pizza_shop 8 region_name: ap-southeast-2 9 s3_data_dir: s3://dbt-pizza-shop-demo/dbt-data/ 10 s3_staging_dir: s3://dbt-pizza-shop-demo/dbt-staging/ 11 threads: 4 12 target: dev Project Sources Recall that three staging tables are created earlier, and they are used as sources of the project. Their details are kept in sources.yml to be referred easily in other models.\n1# pizza_shop/models/sources.yml 2version: 2 3 4sources: 5 - name: raw 6 schema: pizza_shop 7 tables: 8 - name: users 9 identifier: staging_users 10 - name: products 11 identifier: staging_products 12 - name: orders 13 identifier: staging_orders Using the raw sources, three models are created by performing simple transformations such as adding surrogate keys using the dbt_utils package and changing column names. Note that, as the products and users dimension tables are kept by Type 2 slowly changing dimension (SCD type 2), the surrogate keys are used to uniquely identify relevant dimension records.\n1-- pizza_shop/models/src/src_products.sql 2WITH raw_products AS ( 3 SELECT * FROM {{ source(\u0026#39;raw\u0026#39;, \u0026#39;products\u0026#39;) }} 4) 5SELECT 6 {{ dbt_utils.generate_surrogate_key([\u0026#39;name\u0026#39;, \u0026#39;description\u0026#39;, \u0026#39;price\u0026#39;, \u0026#39;category\u0026#39;, \u0026#39;image\u0026#39;]) }} as product_key, 7 id AS product_id, 8 name, 9 description, 10 price, 11 category, 12 image, 13 created_at 14FROM raw_products 1-- pizza_shop/models/src/src_users.sql 2WITH raw_users AS ( 3 SELECT * FROM {{ source(\u0026#39;raw\u0026#39;, \u0026#39;users\u0026#39;) }} 4) 5SELECT 6 {{ dbt_utils.generate_surrogate_key([\u0026#39;first_name\u0026#39;, \u0026#39;last_name\u0026#39;, \u0026#39;email\u0026#39;, \u0026#39;residence\u0026#39;, \u0026#39;lat\u0026#39;, \u0026#39;lon\u0026#39;]) }} as user_key, 7 id AS user_id, 8 first_name, 9 last_name, 10 email, 11 residence, 12 lat AS latitude, 13 lon AS longitude, 14 created_at 15FROM raw_users 1-- pizza_shop/models/src/src_orders.sql 2WITH raw_orders AS ( 3 SELECT * FROM {{ source(\u0026#39;raw\u0026#39;, \u0026#39;orders\u0026#39;) }} 4) 5SELECT 6 id AS order_id, 7 user_id, 8 items, 9 created_at 10FROM raw_orders Data Modelling The dimension tables are materialized as table where the table type is chosen as iceberg. Also, for SCD type 2, two additional columns are created - valid_from and valid_to. The extra columns are for setting up a time range where a record is applicable, and they are used to map a relevant record in the fact table when there are multiple dimension records, having the same natural key. Note that SCD type 2 tables can also be maintained by dbt snapshots.\n1-- pizza_shop/models/dim/dim_products.sql 2{{ 3 config( 4 materialized = \u0026#39;table\u0026#39;, 5 table_type=\u0026#39;iceberg\u0026#39;, 6 format=\u0026#39;parquet\u0026#39;, 7 table_properties={ 8 \u0026#39;optimize_rewrite_delete_file_threshold\u0026#39;: \u0026#39;2\u0026#39; 9 }) 10}} 11WITH src_products AS ( 12 SELECT 13 product_key, 14 product_id, 15 name, 16 description, 17 price, 18 category, 19 image, 20 CAST(created_at AS TIMESTAMP(6)) AS created_at 21 FROM {{ ref(\u0026#39;src_products\u0026#39;) }} 22) 23SELECT 24 *, 25 created_at AS valid_from, 26 COALESCE( 27 LEAD(created_at, 1) OVER (PARTITION BY product_id ORDER BY created_at), 28 CAST(\u0026#39;2199-12-31\u0026#39; AS TIMESTAMP(6)) 29 ) AS valid_to 30FROM src_products 1-- pizza_shop/models/dim/dim_users.sql 2{{ 3 config( 4 materialized = \u0026#39;table\u0026#39;, 5 table_type=\u0026#39;iceberg\u0026#39;, 6 format=\u0026#39;parquet\u0026#39;, 7 table_properties={ 8 \u0026#39;optimize_rewrite_delete_file_threshold\u0026#39;: \u0026#39;2\u0026#39; 9 }) 10}} 11WITH src_users AS ( 12 SELECT 13 user_key, 14 user_id, 15 first_name, 16 last_name, 17 email, 18 residence, 19 latitude, 20 longitude, 21 CAST(created_at AS TIMESTAMP(6)) AS created_at 22 FROM {{ ref(\u0026#39;src_users\u0026#39;) }} 23) 24SELECT 25 *, 26 created_at AS valid_from, 27 COALESCE( 28 LEAD(created_at, 1) OVER (PARTITION BY user_id ORDER BY created_at), 29 CAST(\u0026#39;2199-12-31\u0026#39; AS TIMESTAMP(6)) 30 ) AS valid_to 31FROM src_users When it comes to the transactional fact table, its materialization and incremental strategy are chosen to be incremental and append respectively as only new records need to be added to it. Also, it is created as a partitioned table for improving query performance by applying the partition transform of day on the created_at column. Finally, the user and order items records are pre-joined from the relevant dimension tables. As can be seen later, the order items (product) and user fields are converted into an array of struct and struct data types.\n1-- pizza_shop/models/fct/fct_orders.sql 2{{ 3 config( 4 materialized = \u0026#39;incremental\u0026#39;, 5 table_type=\u0026#39;iceberg\u0026#39;, 6 format=\u0026#39;parquet\u0026#39;, 7 partitioned_by=[\u0026#39;day(created_at)\u0026#39;], 8 incremental_strategy=\u0026#39;append\u0026#39;, 9 unique_key=\u0026#39;order_id\u0026#39;, 10 table_properties={ 11 \u0026#39;optimize_rewrite_delete_file_threshold\u0026#39;: \u0026#39;2\u0026#39; 12 }) 13}} 14WITH dim_products AS ( 15 SELECT * FROM {{ ref(\u0026#39;dim_products\u0026#39;) }} 16), dim_users AS ( 17 SELECT * FROM {{ ref(\u0026#39;dim_users\u0026#39;) }} 18), expanded_orders AS ( 19 SELECT 20 o.order_id, 21 o.user_id, 22 row.product_id, 23 row.quantity, 24 CAST(created_at AS TIMESTAMP(6)) AS created_at 25 FROM {{ ref(\u0026#39;src_orders\u0026#39;) }} AS o 26 CROSS JOIN UNNEST(CAST(JSON_PARSE(o.items) as ARRAY(ROW(product_id INTEGER, quantity INTEGER)))) as t(row) 27) 28SELECT 29 o.order_id, 30 ARRAY_AGG( 31 CAST( 32 ROW(p.product_key, o.product_id, p.name, p.price, o.quantity, p.description, p.category, p.image) 33 AS ROW(key VARCHAR, id BIGINT, name VARCHAR, price DOUBLE, quantity INTEGER, description VARCHAR, category VARCHAR, image VARCHAR)) 34 ) AS product, 35 CAST( 36 ROW(u.user_key, o.user_id, u.first_name, u.last_name, u.email, u.residence, u.latitude, u.longitude) 37 AS ROW(key VARCHAR, id BIGINT, first_name VARCHAR, last_name VARCHAR, email VARCHAR, residence VARCHAR, latitude DOUBLE, longitude DOUBLE)) AS user, 38 o.created_at 39FROM expanded_orders o 40JOIN dim_products p 41 ON o.product_id = p.product_id 42 AND o.created_at \u0026gt;= p.valid_from 43 AND o.created_at \u0026lt; p.valid_to 44JOIN dim_users u 45 ON o.user_id = u.user_id 46 AND o.created_at \u0026gt;= u.valid_from 47 AND o.created_at \u0026lt; u.valid_to 48{% if is_incremental() %} 49 WHERE o.created_at \u0026gt; (SELECT max(created_at) from {{ this }}) 50{% endif %} 51GROUP BY 52 o.order_id, 53 u.user_key, 54 o.user_id, 55 u.first_name, 56 u.last_name, 57 u.email, 58 u.residence, 59 u.latitude, 60 u.longitude, 61 o.created_at We can keep the final models in a separate YAML file for testing and enhanced documentation.\n1# pizza_shop/models/schema.yml 2version: 2 3 4models: 5 - name: dim_products 6 description: Products table, which is converted into SCD Type 2 7 columns: 8 - name: product_key 9 description: | 10 Primary key of the table 11 Surrogate key, which is generated by md5 hash using the following columns 12 - name, description, price, category, image 13 tests: 14 - not_null 15 - unique 16 - name: product_id 17 description: Natural key of products 18 - name: name 19 description: Porduct name 20 - name: description 21 description: Product description 22 - name: price 23 description: Product price 24 - name: category 25 description: Product category 26 - name: image 27 description: Product image 28 - name: created_at 29 description: Timestamp when the record is loaded 30 - name: valid_from 31 description: Effective start timestamp of the corresponding record (inclusive) 32 - name: valid_to 33 description: Effective end timestamp of the corresponding record (exclusive) 34 - name: dim_users 35 description: Users table, which is converted into SCD Type 2 36 columns: 37 - name: user_key 38 description: | 39 Primary key of the table 40 Surrogate key, which is generated by md5 hash using the following columns 41 - first_name, last_name, email, residence, lat, lon 42 tests: 43 - not_null 44 - unique 45 - name: user_id 46 description: Natural key of users 47 - name: first_name 48 description: First name 49 - name: last_name 50 description: Last name 51 - name: email 52 description: Email address 53 - name: residence 54 description: User address 55 - name: latitude 56 description: Latitude of user address 57 - name: longitude 58 description: Longitude of user address 59 - name: created_at 60 description: Timestamp when the record is loaded 61 - name: valid_from 62 description: Effective start timestamp of the corresponding record (inclusive) 63 - name: valid_to 64 description: Effective end timestamp of the corresponding record (exclusive) 65 - name: fct_orders 66 description: Orders fact table. Order items are exploded into rows 67 columns: 68 - name: order_id 69 description: Natural key of orders 70 - name: product 71 description: | 72 Array of products in an order. 73 A product is an array of struct where the following attributes are pre-joined from the dim_products table: 74 key (product_key), id (product_id), name, price, quantity, description, category, and image 75 - name: user 76 description: | 77 A struct where the following attributes are pre-joined from the dim_users table: 78 key (user_key), id (user_id), first_name, last_name, email, residence, latitude, and longitude 79 - name: created_at 80 description: Timestamp when the record is loaded The project can be executed using the dbt run command as shown below.\n1$ dbt run 208:55:00 Running with dbt=1.7.9 308:55:00 Registered adapter: athena=1.7.1 408:55:00 Found 6 models, 4 tests, 3 sources, 0 exposures, 0 metrics, 537 macros, 0 groups, 0 semantic models 508:55:00 608:55:02 Concurrency: 4 threads (target=\u0026#39;dev\u0026#39;) 708:55:02 808:55:02 1 of 6 START sql view model pizza_shop.src_orders .............................. [RUN] 908:55:02 2 of 6 START sql view model pizza_shop.src_products ............................ [RUN] 1008:55:02 3 of 6 START sql view model pizza_shop.src_users ............................... [RUN] 1108:55:05 1 of 6 OK created sql view model pizza_shop.src_orders ......................... [OK -1 in 3.28s] 1208:55:05 3 of 6 OK created sql view model pizza_shop.src_users .......................... [OK -1 in 3.27s] 1308:55:05 2 of 6 OK created sql view model pizza_shop.src_products ....................... [OK -1 in 3.28s] 1408:55:05 4 of 6 START sql table model pizza_shop.dim_users .............................. [RUN] 1508:55:05 5 of 6 START sql table model pizza_shop.dim_products ........................... [RUN] 1608:55:10 5 of 6 OK created sql table model pizza_shop.dim_products ...................... [OK 81 in 5.18s] 1708:55:11 4 of 6 OK created sql table model pizza_shop.dim_users ......................... [OK 10000 in 6.16s] 1808:55:11 6 of 6 START sql incremental model pizza_shop.fct_orders ....................... [RUN] 1908:55:22 6 of 6 OK created sql incremental model pizza_shop.fct_orders .................. [OK 20000 in 10.23s] 2008:55:22 2108:55:22 Finished running 3 view models, 2 table models, 1 incremental model in 0 hours 0 minutes and 21.41 seconds (21.41s). 2208:55:22 2308:55:22 Completed successfully 2408:55:22 2508:55:22 Done. PASS=6 WARN=0 ERROR=0 SKIP=0 TOTAL=6 Also, the project can be tested using the dbt test command.\n1$ dbt test 208:55:29 Running with dbt=1.7.9 308:55:30 Registered adapter: athena=1.7.1 408:55:30 Found 6 models, 4 tests, 3 sources, 0 exposures, 0 metrics, 537 macros, 0 groups, 0 semantic models 508:55:30 608:55:31 Concurrency: 4 threads (target=\u0026#39;dev\u0026#39;) 708:55:31 808:55:31 1 of 4 START test not_null_dim_products_product_key ............................ [RUN] 908:55:31 2 of 4 START test not_null_dim_users_user_key .................................. [RUN] 1008:55:31 3 of 4 START test unique_dim_products_product_key .............................. [RUN] 1108:55:31 4 of 4 START test unique_dim_users_user_key .................................... [RUN] 1208:55:33 3 of 4 PASS unique_dim_products_product_key .................................... [PASS in 2.17s] 1308:55:33 2 of 4 PASS not_null_dim_users_user_key ........................................ [PASS in 2.23s] 1408:55:34 4 of 4 PASS unique_dim_users_user_key .......................................... [PASS in 3.16s] 1508:55:34 1 of 4 PASS not_null_dim_products_product_key .................................. [PASS in 3.20s] 1608:55:34 1708:55:34 Finished running 4 tests in 0 hours 0 minutes and 4.37 seconds (4.37s). 1808:55:34 1908:55:34 Completed successfully 2008:55:34 2108:55:34 Done. PASS=4 WARN=0 ERROR=0 SKIP=0 TOTAL=4 Fact Table Structure The schema of the fact table can be found below. The product and user are marked as the array and struct type respectively.\nWhen we click an individual link, its detailed schema appears in a pop-up window as shown below.\nIn Athena, we can flatten the product array into multiple rows by using CROSS JOIN in conjunction with the UNNEST operator.\nUpdate Records Although we will discuss ETL orchestration with Apache Airflow in the next post, here I illustrate how the dimension and fact tables change when records are updated.\nProduct First, a new record is inserted into the staging_products table, and the price is set to increase by 10.\n1-- // update a product record 2INSERT INTO pizza_shop.staging_products (id, name, description, price, category, image, created_at) 3 SELECT 1, name, description, price + 10, category, image, CAST(date_add(\u0026#39;hour\u0026#39;, 11, CURRENT_TIMESTAMP) AS TIMESTAMP) 4 FROM pizza_shop.staging_products 5 WHERE id = 1; 6 7SELECT id, name, price, category, created_at 8FROM pizza_shop.staging_products 9WHERE id = 1 10ORDER BY created_at; 11 12#\tid name price category created_at 131\t1 Moroccan Spice Pasta Pizza - Veg\t335.0 veg pizzas 2024-02-29 19:54:03.999 142\t1 Moroccan Spice Pasta Pizza - Veg\t345.0 veg pizzas 2024-02-29 20:02:17.304 When we execute the dbt run command again, we see the corresponding dimension table reflects the change by adding a new record and updating valid_from and valid_to columns accordingly. With this change, any later order record that has this product should be mapped into the new product record.\n1SELECT product_key, price, created_at, valid_from, valid_to 2FROM pizza_shop.dim_products 3WHERE product_id = 1 4ORDER BY created_at; 5 6# product_key price created_at valid_from valid_to 71 b8c187845db8b7e55626659cfbb8aea1 335.0 2024-02-29 19:54:03.999000 2024-02-29 19:54:03.999000 2024-02-29 20:02:17.304000 82 590d4eb8831d78ae84f44b90e930d2f3 345.0 2024-02-29 20:02:17.304000 2024-02-29 20:02:17.304000 2199-12-31 00:00:00.000000 User Also, a new record is inserted into the staging_users table while modifying the email address.\n1-- // update a user record 2INSERT INTO pizza_shop.staging_users (id, first_name, last_name, email, residence, lat, lon, created_at) 3 SELECT 1, first_name, last_name, \u0026#39;john.doe@example.com\u0026#39;, residence, lat, lon, CAST(date_add(\u0026#39;hour\u0026#39;, 11, CURRENT_TIMESTAMP) AS TIMESTAMP) 4 FROM pizza_shop.staging_users 5 WHERE id = 1; 6 7SELECT id, first_name, last_name, email, created_at 8FROM pizza_shop.staging_users 9WHERE id = 1 10ORDER BY created_at; 11 12# id first_name last_name email created_at 131 1 Kismat Shroff drishyamallick@hotmail.com 2024-02-29 19:54:04.846 142 1 Kismat Shroff john.doe@example.com 2024-02-29 20:04:05.915 Again the corresponding dimension table reflects the change by adding a new record and updating valid_from and valid_to columns accordingly.\n1SELECT user_key, email, valid_from, valid_to 2FROM pizza_shop.dim_users 3WHERE user_id = 1 4ORDER BY created_at; 5 6# user_key email valid_from valid_to 71 48afd277176d7aed4e0d03ab15033f28 drishyamallick@hotmail.com 2024-02-29 19:54:04.846000 2024-02-29 20:04:05.915000 82 ee798a3c387582d3581a6f907c25af98 john.doe@example.com 2024-02-29 20:04:05.915000 2199-12-31 00:00:00.000000 Order We insert a new order record that has two items where the IDs of the first and second products are 1 and 2 respectively. In this example, we expect the first product maps to the updated product record while the record of the second product remains the same. We can check it by querying the new order record together with an existing record that has corresponding products. As expected, the query result shows the product record is updated only in the new order record.\n1-- // add an order record 2INSERT INTO pizza_shop.staging_orders(id, user_id, items, created_at) 3VALUES ( 4 20001, 5 1, 6 \u0026#39;[{\u0026#34;product_id\u0026#34;: 1, \u0026#34;quantity\u0026#34;: 2}, {\u0026#34;product_id\u0026#34;: 2, \u0026#34;quantity\u0026#34;: 3}]\u0026#39;, 7 CAST(date_add(\u0026#39;hour\u0026#39;, 11, CURRENT_TIMESTAMP) AS TIMESTAMP) 8); 9 10SELECT o.order_id, p.key, p.id, p.price, p.quantity, o.created_at 11FROM pizza_shop.fct_orders AS o 12CROSS JOIN UNNEST(o.product) as t(p) 13WHERE o.order_id in (11146, 20001) AND p.id IN (1, 2) 14ORDER BY o.order_id; 15 16# order_id key id price quantity created_at 171 11146 * b8c187845db8b7e55626659cfbb8aea1 1 335.0 1 2024-02-29 19:54:05.803000 182 11146 8311b52111a924582c0fe5cb566cfa9a 2 60.0 2 2024-02-29 19:54:05.803000 193 20001 * 590d4eb8831d78ae84f44b90e930d2f3 1 345.0 2 2024-02-29 20:08:32.756000 204 20001 8311b52111a924582c0fe5cb566cfa9a 2 60.0 3 2024-02-29 20:08:32.756000 Summary In this series, we discuss practical examples of data warehouse and lakehouse development where data transformation is performed by the data build tool (dbt) and ETL is managed by Apache Airflow. Open Table Formats such as Apache Iceberg bring a new opportunity that implements data warehousing features in a data lake (i.e. data lakehouse). In this post, we created a new dbt project that targets Apache Iceberg where transformations are performed on Amazon Athena. Data modelling was similar to the BigQuery project in Part 3 where the dimension tables were modelled by the SCD type 2 approach and the fact table was denormalized using the array and struct data types. Finally, impacts of record updates were discussed in detail.\n","date":"March 7, 2024","img":"/blog/2024-03-07-dbt-pizza-shop-5/featured.png","lang":"en","langName":"English","largeImg":"/blog/2024-03-07-dbt-pizza-shop-5/featured_hu5822ed54c6b6e6cff286a9a560a4b806_61499_500x0_resize_box_3.png","permalink":"/blog/2024-03-07-dbt-pizza-shop-5/","series":[{"title":"DBT Pizza Shop Demo","url":"/series/dbt-pizza-shop-demo/"}],"smallImg":"/blog/2024-03-07-dbt-pizza-shop-5/featured_hu5822ed54c6b6e6cff286a9a560a4b806_61499_180x0_resize_box_3.png","tags":[{"title":"Data Build Tool (DBT)","url":"/tags/data-build-tool-dbt/"},{"title":"AWS","url":"/tags/aws/"},{"title":"Amazon Athena","url":"/tags/amazon-athena/"},{"title":"Apache Iceberg","url":"/tags/apache-iceberg/"},{"title":"Python","url":"/tags/python/"}],"timestamp":1709769600,"title":"Data Build Tool (Dbt) Pizza Shop Demo - Part 5 Modelling on Amazon Athena"},{"categories":[{"title":"Data Engineering","url":"/categories/data-engineering/"}],"content":"In Part 3, we developed a dbt project that targets Google BigQuery with fictional pizza shop data. Two dimension tables that keep product and user records are created as Type 2 slowly changing dimension (SCD Type 2) tables, and one transactional fact table is built to keep pizza orders. The fact table is denormalized using nested and repeated fields for improving query performance. In this post, we discuss how to set up an ETL process on the project using Apache Airflow.\nPart 1 Modelling on PostgreSQL Part 2 ETL on PostgreSQL via Airflow Part 3 Modelling on BigQuery Part 4 ETL on BigQuery via Airflow (this post) Part 5 Modelling on Amazon Athena Part 6 ETL on Amazon Athena via Airflow Infrastructure Apache Airflow and Google BigQuery are used in this post, and the former is deployed locally using Docker Compose. The source can be found in the GitHub repository of this post.\nAirflow Airflow is simplified by using the Local Executor where both scheduling and task execution are handled by the airflow scheduler service - i.e. AIRFLOW__CORE__EXECUTOR: LocalExecutor. Also, it is configured to be able to run the dbt project (see Part 3 for details) within the scheduler service by\ninstalling the dbt-bigquery package as an additional pip package, volume-mapping folders that keep the dbt project and dbt project profile, and specifying environment variables for the dbt project profile (GCP_PROJECT and SA_KEYFILE) 1# docker-compose.yml 2version: \u0026#34;3\u0026#34; 3x-airflow-common: \u0026amp;airflow-common 4 image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:2.8.0} 5 networks: 6 - appnet 7 environment: \u0026amp;airflow-common-env 8 AIRFLOW__CORE__EXECUTOR: LocalExecutor 9 AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow 10 # For backward compatibility, with Airflow \u0026lt;2.3 11 AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow 12 AIRFLOW__CORE__FERNET_KEY: \u0026#34;\u0026#34; 13 AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: \u0026#34;true\u0026#34; 14 AIRFLOW__CORE__LOAD_EXAMPLES: \u0026#34;false\u0026#34; 15 AIRFLOW__API__AUTH_BACKENDS: \u0026#34;airflow.api.auth.backend.basic_auth\u0026#34; 16 _PIP_ADDITIONAL_REQUIREMENTS: ${_PIP_ADDITIONAL_REQUIREMENTS:- dbt-bigquery==1.7.4 pendulum} 17 SA_KEYFILE: /tmp/sa_key/key.json # service account key file, required for dbt profile and airflow python operator 18 GCP_PROJECT: ${GCP_PROJECT} # GCP project id, required for dbt profile 19 TZ: Australia/Sydney 20 volumes: 21 - ./airflow/dags:/opt/airflow/dags 22 - ./airflow/plugins:/opt/airflow/plugins 23 - ./airflow/logs:/opt/airflow/logs 24 - ./pizza_shop:/tmp/pizza_shop # dbt project 25 - ./sa_key:/tmp/sa_key # service account key file 26 - ./airflow/dbt-profiles:/opt/airflow/dbt-profiles # dbt profiles 27 user: \u0026#34;${AIRFLOW_UID:-50000}:0\u0026#34; 28 depends_on: \u0026amp;airflow-common-depends-on 29 postgres: 30 condition: service_healthy 31 32services: 33 postgres: 34 image: postgres:13 35 container_name: postgres 36 ports: 37 - 5432:5432 38 networks: 39 - appnet 40 environment: 41 POSTGRES_USER: airflow 42 POSTGRES_PASSWORD: airflow 43 POSTGRES_DB: airflow 44 TZ: Australia/Sydney 45 volumes: 46 - postgres_data:/var/lib/postgresql/data 47 healthcheck: 48 test: [\u0026#34;CMD\u0026#34;, \u0026#34;pg_isready\u0026#34;, \u0026#34;-U\u0026#34;, \u0026#34;airflow\u0026#34;] 49 interval: 5s 50 retries: 5 51 52 airflow-webserver: 53 \u0026lt;\u0026lt;: *airflow-common 54 container_name: webserver 55 command: webserver 56 ports: 57 - 8080:8080 58 depends_on: 59 \u0026lt;\u0026lt;: *airflow-common-depends-on 60 airflow-init: 61 condition: service_completed_successfully 62 63 airflow-scheduler: 64 \u0026lt;\u0026lt;: *airflow-common 65 command: scheduler 66 container_name: scheduler 67 environment: 68 \u0026lt;\u0026lt;: *airflow-common-env 69 depends_on: 70 \u0026lt;\u0026lt;: *airflow-common-depends-on 71 airflow-init: 72 condition: service_completed_successfully 73 74 airflow-init: 75 \u0026lt;\u0026lt;: *airflow-common 76 container_name: init 77 entrypoint: /bin/bash 78 command: 79 - -c 80 - | 81 mkdir -p /sources/logs /sources/dags /sources/plugins /sources/scheduler 82 chown -R \u0026#34;${AIRFLOW_UID}:0\u0026#34; /sources/{logs,dags,plugins,scheduler} 83 exec /entrypoint airflow version 84 environment: 85 \u0026lt;\u0026lt;: *airflow-common-env 86 _AIRFLOW_DB_UPGRADE: \u0026#34;true\u0026#34; 87 _AIRFLOW_WWW_USER_CREATE: \u0026#34;true\u0026#34; 88 _AIRFLOW_WWW_USER_USERNAME: ${_AIRFLOW_WWW_USER_USERNAME:-airflow} 89 _AIRFLOW_WWW_USER_PASSWORD: ${_AIRFLOW_WWW_USER_PASSWORD:-airflow} 90 _PIP_ADDITIONAL_REQUIREMENTS: \u0026#34;\u0026#34; 91 user: \u0026#34;0:0\u0026#34; 92 volumes: 93 - ./airflow:/sources 94 95volumes: 96 airflow_log_volume: 97 driver: local 98 name: airflow_log_volume 99 postgres_data: 100 driver: local 101 name: postgres_data 102 103networks: 104 appnet: 105 name: app-network Before we deploy the Airflow services, we need to create the BigQuery dataset and staging tables, followed by inserting initial records - see Part 3 for details about the prerequisite steps. Then the services can be started using the docker-compose up command. Note that it is recommended to specify the host user\u0026rsquo;s ID as the AIRFLOW_UID value. Otherwise, Airflow can fail to launch due to insufficient permission to write logs. Note also that the relevant GCP project ID should be included as it is read in the compose file.\n1## prerequisite 2## 1. create BigQuery dataset and staging tables 3## 2. insert initial records i.e. python setup/insert_records.py 4 5## start airflow services 6$ AIRFLOW_UID=$(id -u) GCP_PROJECT=\u0026lt;gcp-project-id\u0026gt; docker-compose up -d Once started, we can visit the Airflow web server on http://localhost:8080.\nETL Job A simple ETL job (demo_etl) is created, which updates source records (update_records) followed by running and testing the dbt project. Note that the dbt project and profile are accessible as they are volume-mapped to the Airflow scheduler container.\n1# airflow/dags/operators.py 2import pendulum 3from airflow.models.dag import DAG 4from airflow.operators.bash import BashOperator 5from airflow.operators.python import PythonOperator 6 7import update_records 8 9with DAG( 10 dag_id=\u0026#34;demo_etl\u0026#34;, 11 schedule=None, 12 start_date=pendulum.datetime(2024, 1, 1, tz=\u0026#34;Australia/Sydney\u0026#34;), 13 catchup=False, 14 tags=[\u0026#34;pizza\u0026#34;], 15): 16 task_records_update = PythonOperator( 17 task_id=\u0026#34;update_records\u0026#34;, python_callable=update_records.main 18 ) 19 20 task_dbt_run = BashOperator( 21 task_id=\u0026#34;dbt_run\u0026#34;, 22 bash_command=\u0026#34;dbt run --profiles-dir /opt/airflow/dbt-profiles --project-dir /tmp/pizza_shop\u0026#34;, 23 ) 24 25 task_dbt_test = BashOperator( 26 task_id=\u0026#34;dbt_test\u0026#34;, 27 bash_command=\u0026#34;dbt test --profiles-dir /opt/airflow/dbt-profiles --project-dir /tmp/pizza_shop\u0026#34;, 28 ) 29 30 task_records_update \u0026gt;\u0026gt; task_dbt_run \u0026gt;\u0026gt; task_dbt_test Update Records As dbt takes care of data transformation only, we should create a task that updates source records. As shown below, the task updates as many as a half of records of the dimension tables (products and users) and appends 5,000 order records in a single run.\n1# airflow/dags/update_records.py 2import os 3import datetime 4import dataclasses 5import json 6import random 7import string 8 9from google.cloud import bigquery 10from google.oauth2 import service_account 11 12 13class QueryHelper: 14 def __init__(self, sa_keyfile: str): 15 self.sa_keyfile = sa_keyfile 16 self.credentials = self.get_credentials() 17 self.client = bigquery.Client( 18 credentials=self.credentials, project=self.credentials.project_id 19 ) 20 21 def get_credentials(self): 22 # https://cloud.google.com/bigquery/docs/samples/bigquery-client-json-credentials#bigquery_client_json_credentials-python 23 return service_account.Credentials.from_service_account_file( 24 self.sa_keyfile, scopes=[\u0026#34;https://www.googleapis.com/auth/cloud-platform\u0026#34;] 25 ) 26 27 def get_table(self, dataset_name: str, table_name: str): 28 return self.client.get_table( 29 f\u0026#34;{self.credentials.project_id}.{dataset_name}.{table_name}\u0026#34; 30 ) 31 32 def fetch_rows(self, stmt: str): 33 query_job = self.client.query(stmt) 34 rows = query_job.result() 35 return rows 36 37 def insert_rows(self, dataset_name: str, table_name: str, records: list): 38 table = self.get_table(dataset_name, table_name) 39 errors = self.client.insert_rows_json(table, records) 40 if len(errors) \u0026gt; 0: 41 print(errors) 42 raise RuntimeError(\u0026#34;fails to insert records\u0026#34;) 43 44 45@dataclasses.dataclass 46class Product: 47 id: int 48 name: int 49 description: int 50 price: float 51 category: str 52 image: str 53 created_at: str = datetime.datetime.now().isoformat(timespec=\u0026#34;seconds\u0026#34;) 54 55 def __hash__(self) -\u0026gt; int: 56 return self.id 57 58 def to_json(self): 59 return dataclasses.asdict(self) 60 61 @classmethod 62 def from_row(cls, row: bigquery.Row): 63 return cls(**dict(row.items())) 64 65 @staticmethod 66 def fetch(query_helper: QueryHelper): 67 stmt = \u0026#34;\u0026#34;\u0026#34; 68 WITH windowed AS ( 69 SELECT 70 *, 71 ROW_NUMBER() OVER (PARTITION BY id ORDER BY created_at DESC) AS rn 72 FROM `pizza_shop.staging_products` 73 ) 74 SELECT id, name, description, price, category, image 75 FROM windowed 76 WHERE rn = 1; 77 \u0026#34;\u0026#34;\u0026#34; 78 return [Product.from_row(row) for row in query_helper.fetch_rows(stmt)] 79 80 @staticmethod 81 def insert(query_helper: QueryHelper, percent: float = 0.5): 82 products = Product.fetch(query_helper) 83 records = set(random.choices(products, k=int(len(products) * percent))) 84 for r in records: 85 r.price = r.price + 10 86 query_helper.insert_rows( 87 \u0026#34;pizza_shop\u0026#34;, 88 \u0026#34;staging_products\u0026#34;, 89 [r.to_json() for r in records], 90 ) 91 return records 92 93 94@dataclasses.dataclass 95class User: 96 id: int 97 first_name: str 98 last_name: str 99 email: str 100 residence: str 101 lat: float 102 lon: float 103 created_at: str = datetime.datetime.now().isoformat(timespec=\u0026#34;seconds\u0026#34;) 104 105 def __hash__(self) -\u0026gt; int: 106 return self.id 107 108 def to_json(self): 109 return { 110 k: v if k not in [\u0026#34;lat\u0026#34;, \u0026#34;lon\u0026#34;] else str(v) 111 for k, v in dataclasses.asdict(self).items() 112 } 113 114 @classmethod 115 def from_row(cls, row: bigquery.Row): 116 return cls(**dict(row.items())) 117 118 @staticmethod 119 def fetch(query_helper: QueryHelper): 120 stmt = \u0026#34;\u0026#34;\u0026#34; 121 WITH windowed AS ( 122 SELECT 123 *, 124 ROW_NUMBER() OVER (PARTITION BY id ORDER BY created_at DESC) AS rn 125 FROM `pizza_shop.staging_users` 126 ) 127 SELECT id, first_name, last_name, email, residence, lat, lon 128 FROM windowed 129 WHERE rn = 1; 130 \u0026#34;\u0026#34;\u0026#34; 131 return [User.from_row(row) for row in query_helper.fetch_rows(stmt)] 132 133 @staticmethod 134 def insert(query_helper: QueryHelper, percent: float = 0.5): 135 users = User.fetch(query_helper) 136 records = set(random.choices(users, k=int(len(users) * percent))) 137 for r in records: 138 r.email = f\u0026#34;{\u0026#39;\u0026#39;.join(random.choices(string.ascii_letters, k=5)).lower()}@email.com\u0026#34; 139 query_helper.insert_rows( 140 \u0026#34;pizza_shop\u0026#34;, 141 \u0026#34;staging_users\u0026#34;, 142 [r.to_json() for r in records], 143 ) 144 return records 145 146 147@dataclasses.dataclass 148class Order: 149 id: int 150 user_id: int 151 items: str 152 created_at: str = datetime.datetime.now().isoformat(timespec=\u0026#34;seconds\u0026#34;) 153 154 def to_json(self): 155 return dataclasses.asdict(self) 156 157 @classmethod 158 def create(cls, id: int): 159 order_items = [ 160 {\u0026#34;product_id\u0026#34;: id, \u0026#34;quantity\u0026#34;: random.randint(1, 5)} 161 for id in set(random.choices(range(1, 82), k=random.randint(1, 10))) 162 ] 163 return cls( 164 id=id, 165 user_id=random.randint(1, 10000), 166 items=json.dumps([item for item in order_items]), 167 ) 168 169 @staticmethod 170 def insert(query_helper: QueryHelper, max_id: int, num_orders: int = 5000): 171 records = [] 172 for _ in range(num_orders): 173 records.append(Order.create(max_id + 1)) 174 max_id += 1 175 query_helper.insert_rows( 176 \u0026#34;pizza_shop\u0026#34;, 177 \u0026#34;staging_orders\u0026#34;, 178 [r.to_json() for r in records], 179 ) 180 return records 181 182 @staticmethod 183 def get_max_id(query_helper: QueryHelper): 184 stmt = \u0026#34;SELECT max(id) AS max FROM `pizza_shop.staging_orders`\u0026#34; 185 return next(iter(query_helper.fetch_rows(stmt))).max 186 187 188def main(): 189 query_helper = QueryHelper(os.environ[\u0026#34;SA_KEYFILE\u0026#34;]) 190 ## update product and user records 191 updated_products = Product.insert(query_helper) 192 print(f\u0026#34;{len(updated_products)} product records updated\u0026#34;) 193 updated_users = User.insert(query_helper) 194 print(f\u0026#34;{len(updated_users)} user records updated\u0026#34;) 195 ## create order records 196 max_order_id = Order.get_max_id(query_helper) 197 new_orders = Order.insert(query_helper, max_order_id) 198 print(f\u0026#34;{len(new_orders)} order records created\u0026#34;) The details of the ETL job can be found on the Airflow web server as shown below.\nRun ETL Below shows example product dimension records after the ETL job is completed. It is shown that the product 1 is updated, and a new surrogate key is assigned to the new record as well as the valid_from and valid_to column values are updated accordingly.\n1SELECT product_key, product_id AS id, price, valid_from, valid_to 2FROM `pizza_shop.dim_products` 3WHERE product_id IN (1, 2) 4 5product_key id price valid_from valid_to 68dd51b3981692c787baa9d4335f15345 2 60.0 2024-02-16T22:07:19 2199-12-31T00:00:00 7* a8c5f8c082bcf52a164f2eccf2b493f6 1 335.0 2024-02-16T22:07:19 2024-02-16T22:09:37 8* c995d7e1ec035da116c0f37e6284d1d5 1 345.0 2024-02-16T22:09:37 2199-12-31T00:00:00 When we query the fact table for orders with the same product ID, we can check correct product key values are mapped - note value_to is not inclusive.\n1SELECT o.order_id, p.key, p.id, p.price, p.quantity, o.created_at 2FROM `pizza_shop.fct_orders` AS o, 3 UNNEST(o.product) AS p 4WHERE o.order_id IN (11146, 23296) AND p.id IN (1, 2) 5ORDER BY o.order_id, p.id 6 7order_id key id price quantity created_at 8 11146 * a8c5f8c082bcf52a164f2eccf2b493f6 1 335.0 1 2024-02-16T22:07:23 9 11146 8dd51b3981692c787baa9d4335f15345 2 60.0 2 2024-02-16T22:07:23 10 23296 * c995d7e1ec035da116c0f37e6284d1d5 1 345.0 4 2024-02-16T22:09:37 11 23296 8dd51b3981692c787baa9d4335f15345 2 60.0 4 2024-02-16T22:09:37 Summary In this series of posts, we discuss data warehouse/lakehouse examples using data build tool (dbt) including ETL orchestration with Apache Airflow. In this post, we discussed how to set up an ETL process on the dbt project developed in Part 3 using Airflow. A demo ETL job was created that updates records followed by running and testing the dbt project. Finally, the result of ETL job was validated by checking sample records.\n","date":"February 22, 2024","img":"/blog/2024-02-22-dbt-pizza-shop-4/featured.png","lang":"en","langName":"English","largeImg":"/blog/2024-02-22-dbt-pizza-shop-4/featured_hu487187112b89f2568f433199221f000a_89588_500x0_resize_box_3.png","permalink":"/blog/2024-02-22-dbt-pizza-shop-4/","series":[{"title":"DBT Pizza Shop Demo","url":"/series/dbt-pizza-shop-demo/"}],"smallImg":"/blog/2024-02-22-dbt-pizza-shop-4/featured_hu487187112b89f2568f433199221f000a_89588_180x0_resize_box_3.png","tags":[{"title":"Data Build Tool (DBT)","url":"/tags/data-build-tool-dbt/"},{"title":"GCP","url":"/tags/gcp/"},{"title":"BigQuery","url":"/tags/bigquery/"},{"title":"Docker","url":"/tags/docker/"},{"title":"Docker Compose","url":"/tags/docker-compose/"},{"title":"Apache Airflow","url":"/tags/apache-airflow/"},{"title":"Python","url":"/tags/python/"}],"timestamp":1708560000,"title":"Data Build Tool (Dbt) Pizza Shop Demo - Part 4 ETL on BigQuery via Airflow"},{"categories":[{"title":"Data Engineering","url":"/categories/data-engineering/"}],"content":"In this series, we discuss practical examples of data warehouse and lakehouse development where data transformation is performed by the data build tool (dbt) and ETL is managed by Apache Airflow. In Part 1, we developed a dbt project on PostgreSQL using fictional pizza shop data. At the end, the data sets are modelled by two SCD type 2 dimension tables and one transactional fact table. In this post, we create a new dbt project that targets Google BigQuery. While the dimension tables are kept by the same SCD type 2 approach, the fact table is denormalized using nested and repeated fields, which potentially can improve query performance by pre-joining corresponding dimension records.\nPart 1 Modelling on PostgreSQL Part 2 ETL on PostgreSQL via Airflow Part 3 Modelling on BigQuery (this post) Part 4 ETL on BigQuery via Airflow Part 5 Modelling on Amazon Athena Part 6 ETL on Amazon Athena via Airflow Setup BigQuery Google BigQuery is used in the post, and the source can be found in the GitHub repository of this post.\nPrepare Data Fictional pizza shop data from Building Real-Time Analytics Systems is used in this post. There are three data sets - products, users and orders. The first two data sets are copied from the book\u0026rsquo;s GitHub repository and saved into the setup/data folder. The last order data set is generated by the following Python script.\n1# setup/generate_orders.py 2import os 3import csv 4import dataclasses 5import random 6import json 7 8 9@dataclasses.dataclass 10class Order: 11 user_id: int 12 items: str 13 14 @staticmethod 15 def create(): 16 order_items = [ 17 {\u0026#34;product_id\u0026#34;: id, \u0026#34;quantity\u0026#34;: random.randint(1, 5)} 18 for id in set(random.choices(range(1, 82), k=random.randint(1, 10))) 19 ] 20 return Order( 21 user_id=random.randint(1, 10000), 22 items=json.dumps([item for item in order_items]), 23 ) 24 25 26if __name__ == \u0026#34;__main__\u0026#34;: 27 \u0026#34;\u0026#34;\u0026#34; 28 Generate random orders given by the NUM_ORDERS environment variable. 29 - orders.csv will be written to ./data folder 30 31 Example: 32 python generate_orders.py 33 NUM_ORDERS=10000 python generate_orders.py 34 \u0026#34;\u0026#34;\u0026#34; 35 NUM_ORDERS = int(os.getenv(\u0026#34;NUM_ORDERS\u0026#34;, \u0026#34;20000\u0026#34;)) 36 CURRENT_DIR = os.path.dirname(os.path.realpath(__file__)) 37 orders = [Order.create() for _ in range(NUM_ORDERS)] 38 39 filepath = os.path.join(CURRENT_DIR, \u0026#34;data\u0026#34;, \u0026#34;orders.csv\u0026#34;) 40 if os.path.exists(filepath): 41 os.remove(filepath) 42 43 with open(os.path.join(CURRENT_DIR, \u0026#34;data\u0026#34;, \u0026#34;orders.csv\u0026#34;), \u0026#34;w\u0026#34;) as f: 44 writer = csv.writer(f) 45 writer.writerow([\u0026#34;user_id\u0026#34;, \u0026#34;items\u0026#34;]) 46 for order in orders: 47 writer.writerow(dataclasses.asdict(order).values()) Below shows sample order records generated by the script. It includes user ID and order items.\n1user_id,items 26845,\u0026#34;[{\u0026#34;\u0026#34;product_id\u0026#34;\u0026#34;: 52, \u0026#34;\u0026#34;quantity\u0026#34;\u0026#34;: 4}, {\u0026#34;\u0026#34;product_id\u0026#34;\u0026#34;: 68, \u0026#34;\u0026#34;quantity\u0026#34;\u0026#34;: 5}]\u0026#34; 36164,\u0026#34;[{\u0026#34;\u0026#34;product_id\u0026#34;\u0026#34;: 77, \u0026#34;\u0026#34;quantity\u0026#34;\u0026#34;: 4}]\u0026#34; 49303,\u0026#34;[{\u0026#34;\u0026#34;product_id\u0026#34;\u0026#34;: 5, \u0026#34;\u0026#34;quantity\u0026#34;\u0026#34;: 2}, {\u0026#34;\u0026#34;product_id\u0026#34;\u0026#34;: 71, \u0026#34;\u0026#34;quantity\u0026#34;\u0026#34;: 3}, {\u0026#34;\u0026#34;product_id\u0026#34;\u0026#34;: 74, \u0026#34;\u0026#34;quantity\u0026#34;\u0026#34;: 2}, {\u0026#34;\u0026#34;product_id\u0026#34;\u0026#34;: 10, \u0026#34;\u0026#34;quantity\u0026#34;\u0026#34;: 5}, {\u0026#34;\u0026#34;product_id\u0026#34;\u0026#34;: 12, \u0026#34;\u0026#34;quantity\u0026#34;\u0026#34;: 2}]\u0026#34; The folder structure of source data sets and order generation script can be found below.\n1$ tree setup/ -P \u0026#34;*.csv|generate*\u0026#34; 2setup/ 3├── data 4│ ├── orders.csv 5│ ├── products.csv 6│ └── users.csv 7└── generate_orders.py Insert Source Data The source data sets are inserted into staging tables using a Python script.\nCreate Stage Tables A BigQuery dataset named pizza_shop is created, and three staging tables are created as shown below.\n1-- setup/create_stage_tables.sql 2DROP TABLE IF EXISTS pizza_shop.staging_users; 3DROP TABLE IF EXISTS pizza_shop.staging_products; 4DROP TABLE IF EXISTS pizza_shop.staging_orders; 5 6CREATE TABLE pizza_shop.staging_users 7( 8 id INTEGER, 9 first_name STRING, 10 last_name STRING, 11 email STRING, 12 residence STRING, 13 lat DECIMAL(10, 8), 14 lon DECIMAL(10, 8), 15 created_at DATETIME 16); 17 18CREATE TABLE pizza_shop.staging_products 19( 20 id INTEGER, 21 name STRING, 22 description STRING, 23 price FLOAT64, 24 category STRING, 25 image STRING, 26 created_at DATETIME 27); 28 29CREATE TABLE pizza_shop.staging_orders 30( 31 id INTEGER, 32 user_id INTEGER, 33 items JSON, 34 created_at DATETIME 35); Insert Records The source data is inserted using the Python Client for Google BigQuery. It is a simple process that reads records from the source data files as dictionary while adding incremental ID/creation datetime and inserts them using the client library. Note that a service account is used for authentication and its key file (key.json) is placed in the sa_key folder that exists in the same level of the script\u0026rsquo;s parent folder - see below for the required folder structure.\n1# setup/insert_records.py 2import os 3import csv 4import datetime 5 6from google.cloud import bigquery 7from google.oauth2 import service_account 8 9 10class QueryHelper: 11 def __init__(self, current_dir: str = None): 12 self.current_dir = current_dir or os.path.dirname(os.path.realpath(__file__)) 13 self.credentials = self.get_credentials() 14 self.client = bigquery.Client( 15 credentials=self.credentials, project=self.credentials.project_id 16 ) 17 18 def get_credentials(self): 19 # https://cloud.google.com/bigquery/docs/samples/bigquery-client-json-credentials#bigquery_client_json_credentials-python 20 sa_key_path = os.path.join( 21 os.path.dirname(self.current_dir), \u0026#34;sa_key\u0026#34;, \u0026#34;key.json\u0026#34; 22 ) 23 return service_account.Credentials.from_service_account_file( 24 sa_key_path, scopes=[\u0026#34;https://www.googleapis.com/auth/cloud-platform\u0026#34;] 25 ) 26 27 def get_table(self, dataset_name: str, table_name: str): 28 return self.client.get_table( 29 f\u0026#34;{self.credentials.project_id}.{dataset_name}.{table_name}\u0026#34; 30 ) 31 32 def insert_rows(self, dataset_name: str, table_name: str, records: list): 33 table = self.get_table(dataset_name, table_name) 34 errors = self.client.insert_rows_json(table, records) 35 if len(errors) \u0026gt; 0: 36 print(errors) 37 raise RuntimeError(\u0026#34;fails to insert records\u0026#34;) 38 39 40class DataHelper: 41 def __init__(self, current_dir: str = None): 42 self.current_dir = current_dir or os.path.dirname(os.path.realpath(__file__)) 43 44 def load_data(self, file_name: str): 45 created_at = datetime.datetime.now().isoformat(timespec=\u0026#34;seconds\u0026#34;) 46 records = [] 47 with open(os.path.join(self.current_dir, \u0026#34;data\u0026#34;, file_name), mode=\u0026#34;r\u0026#34;) as f: 48 rows = csv.DictReader(f) 49 for ind, row in enumerate(rows): 50 extras = {\u0026#34;id\u0026#34;: ind + 1, \u0026#34;created_at\u0026#34;: created_at} 51 records.append({**extras, **row}) 52 return records 53 54 55if __name__ == \u0026#34;__main__\u0026#34;: 56 dataset_name = os.getenv(\u0026#34;DATASET_NAME\u0026#34;, \u0026#34;pizza_shop\u0026#34;) 57 58 query_helper = QueryHelper() 59 data_helper = DataHelper() 60 print(\u0026#34;inserting products...\u0026#34;) 61 products = data_helper.load_data(\u0026#34;products.csv\u0026#34;) 62 query_helper.insert_rows(dataset_name, \u0026#34;staging_products\u0026#34;, products) 63 print(\u0026#34;inserting users...\u0026#34;) 64 users = data_helper.load_data(\u0026#34;users.csv\u0026#34;) 65 query_helper.insert_rows(dataset_name, \u0026#34;staging_users\u0026#34;, users) 66 print(\u0026#34;inserting orders...\u0026#34;) 67 orders = data_helper.load_data(\u0026#34;orders.csv\u0026#34;) 68 query_helper.insert_rows(dataset_name, \u0026#34;staging_orders\u0026#34;, orders) As mentioned, the key file (key.json) is located in the sa_key folder that exists in the same level of the script\u0026rsquo;s parent folder. It is kept separately as it can be shared by the dbt project and Airflow scheduler as well.\n1$ tree sa_key/ \u0026amp;\u0026amp; tree setup/ -P \u0026#34;insert_records.py|*.csv\u0026#34; 2sa_key/ 3└── key.json 4 5setup/ 6├── data 7│ ├── orders.csv 8│ ├── products.csv 9│ └── users.csv 10└── insert_records.py Setup DBT Project A dbt project named pizza_shop is created using the dbt-bigquery package (dbt-bigquery==1.7.4). Specifically, it is created using the dbt init command, and it bootstraps the project in the pizza_shop folder as well as adds the project profile to the dbt profiles file. Note that the service account is used for authentication and the path of its key file is specified in the keyfile attribute. See this page for details about how to set up BigQuery for a dbt project.\n1# $HOME/.dbt/profiles.yml 2pizza_shop: 3 outputs: 4 dev: 5 type: bigquery 6 method: service-account 7 project: \u0026lt;project-id\u0026gt; 8 dataset: pizza_shop 9 threads: 4 10 keyfile: \u0026lt;path-to-service-account-key-file\u0026gt; 11 job_execution_timeout_seconds: 300 12 job_retries: 1 13 location: US 14 priority: interactive 15 target: dev Project Sources Recall that three staging tables are created earlier, and they are used as sources of the project. Their details are kept in sources.yml to be referred easily in other models.\n1# pizza_shop/models/sources.yml 2version: 2 3 4sources: 5 - name: raw 6 schema: pizza_shop 7 tables: 8 - name: users 9 identifier: staging_users 10 - name: products 11 identifier: staging_products 12 - name: orders 13 identifier: staging_orders Using the raw sources, three models are created by performing simple transformations such as adding surrogate keys using the dbt_utils package and changing column names. Note that, as the products and users dimension tables are kept by Type 2 slowly changing dimension (SCD type 2), the surrogate keys are used to uniquely identify relevant dimension records.\n1-- pizza_shop/models/src/src_products.sql 2WITH raw_products AS ( 3 SELECT * FROM {{ source(\u0026#39;raw\u0026#39;, \u0026#39;products\u0026#39;) }} 4) 5SELECT 6 {{ dbt_utils.generate_surrogate_key([\u0026#39;name\u0026#39;, \u0026#39;description\u0026#39;, \u0026#39;price\u0026#39;, \u0026#39;category\u0026#39;, \u0026#39;image\u0026#39;]) }} as product_key, 7 id AS product_id, 8 name, 9 description, 10 price, 11 category, 12 image, 13 created_at 14FROM raw_products 1-- pizza_shop/models/src/src_users.sql 2WITH raw_users AS ( 3 SELECT * FROM {{ source(\u0026#39;raw\u0026#39;, \u0026#39;users\u0026#39;) }} 4) 5SELECT 6 {{ dbt_utils.generate_surrogate_key([\u0026#39;first_name\u0026#39;, \u0026#39;last_name\u0026#39;, \u0026#39;email\u0026#39;, \u0026#39;residence\u0026#39;, \u0026#39;lat\u0026#39;, \u0026#39;lon\u0026#39;]) }} as user_key, 7 id AS user_id, 8 first_name, 9 last_name, 10 email, 11 residence, 12 lat AS latitude, 13 lon AS longitude, 14 created_at 15FROM raw_users 1-- pizza_shop/models/src/src_orders.sql 2WITH raw_orders AS ( 3 SELECT * FROM {{ source(\u0026#39;raw\u0026#39;, \u0026#39;orders\u0026#39;) }} 4) 5SELECT 6 id AS order_id, 7 user_id, 8 items, 9 created_at 10FROM raw_orders Data Modelling For SCD type 2, the dimension tables are materialized as table and two additional columns are included - valid_from and valid_to. The extra columns are for setting up a time range where a record is applicable, and they are used to map a relevant record in the fact table when there are multiple dimension records according to the same natural key. Note that SCD type 2 tables can also be maintained by dbt snapshots.\n1-- pizza_shop/models/dim/dim_products.sql 2{{ 3 config( 4 materialized = \u0026#39;table\u0026#39;, 5 ) 6}} 7WITH src_products AS ( 8 SELECT * FROM {{ ref(\u0026#39;src_products\u0026#39;) }} 9) 10SELECT 11 *, 12 created_at AS valid_from, 13 COALESCE( 14 LEAD(created_at, 1) OVER (PARTITION BY product_id ORDER BY created_at), 15 CAST(\u0026#39;2199-12-31\u0026#39; AS DATETIME) 16 ) AS valid_to 17FROM src_products 1-- pizza_shop/models/dim/dim_users.sql 2{{ 3 config( 4 materialized = \u0026#39;table\u0026#39;, 5 ) 6}} 7WITH src_users AS ( 8 SELECT * FROM {{ ref(\u0026#39;src_users\u0026#39;) }} 9) 10SELECT 11 *, 12 created_at AS valid_from, 13 COALESCE( 14 LEAD(created_at, 1) OVER (PARTITION BY user_id ORDER BY created_at), 15 CAST(\u0026#39;2199-12-31\u0026#39; AS DATETIME) 16 ) AS valid_to 17FROM src_users The transactional fact table is materialized as incremental so that only new records are appended. Also, it is created as a partitioned table for improving query performance by adding date filter. Finally, the user and order items records are pre-joined from the relevant dimension tables. See below for details about how this fact table is structured in BigQuery.\n1-- pizza_shop/models/fct/fct_orders.sql 2{{ 3 config( 4 materialized = \u0026#39;incremental\u0026#39;, 5 partition_by = { 6 \u0026#39;field\u0026#39;: \u0026#39;order_date\u0026#39;, 7 \u0026#39;data_type\u0026#39;: \u0026#39;date\u0026#39;, 8 \u0026#39;granularity\u0026#39;: \u0026#39;day\u0026#39;, 9 \u0026#39;time_ingestion_partitioning\u0026#39;: true 10 }) 11}} 12WITH dim_products AS ( 13 SELECT * FROM {{ ref(\u0026#39;dim_products\u0026#39;) }} 14), dim_users AS ( 15 SELECT * FROM {{ ref(\u0026#39;dim_users\u0026#39;) }} 16), expanded_orders AS ( 17 SELECT 18 order_id, 19 user_id, 20 CAST(JSON_EXTRACT_SCALAR(json , \u0026#39;$.product_id\u0026#39;) AS INTEGER) AS product_id, 21 CAST(JSON_EXTRACT_SCALAR(json , \u0026#39;$.quantity\u0026#39;) AS INTEGER) AS quantity, 22 created_at 23 FROM {{ ref(\u0026#39;src_orders\u0026#39;) }} AS t, 24 UNNEST(JSON_EXTRACT_ARRAY(t.items , \u0026#39;$\u0026#39;)) AS json 25) 26SELECT 27 o.order_id, 28 ARRAY_AGG( 29 STRUCT(p.product_key AS key, o.product_id AS id, p.name, p.price, o.quantity, p.description, p.category, p.image) 30 ) as product, 31 STRUCT(u.user_key AS key, o.user_id AS id, u.first_name, u.last_name, u.email, u.residence, u.latitude, u.longitude) AS user, 32 o.created_at, 33 EXTRACT(DATE FROM o.created_at) AS order_date 34FROM expanded_orders o 35JOIN dim_products p 36 ON o.product_id = p.product_id 37 AND o.created_at \u0026gt;= p.valid_from 38 AND o.created_at \u0026lt; p.valid_to 39JOIN dim_users u 40 ON o.user_id = u.user_id 41 AND o.created_at \u0026gt;= u.valid_from 42 AND o.created_at \u0026lt; u.valid_to 43{% if is_incremental() %} 44 WHERE o.created_at \u0026gt; (SELECT max(created_at) from {{ this }}) 45{% endif %} 46GROUP BY 47 o.order_id, 48 u.user_key, 49 o.user_id, 50 u.first_name, 51 u.last_name, 52 u.email, 53 u.residence, 54 u.latitude, 55 u.longitude, 56 o.created_at We can keep the final models in a separate YAML file for testing and enhanced documentation.\n1# pizza_shop/models/schema.yml 2version: 2 3 4models: 5 - name: dim_products 6 description: Products table, which is converted into SCD type 2 7 columns: 8 - name: product_key 9 description: | 10 Primary key of the table 11 Surrogate key, which is generated by md5 hash using the following columns 12 - name, description, price, category, image 13 tests: 14 - not_null 15 - unique 16 - name: product_id 17 description: Natural key of products 18 - name: name 19 description: Porduct name 20 - name: description 21 description: Product description 22 - name: price 23 description: Product price 24 - name: category 25 description: Product category 26 - name: image 27 description: Product image 28 - name: created_at 29 description: Timestamp when the record is loaded 30 - name: valid_from 31 description: Effective start timestamp of the corresponding record (inclusive) 32 - name: valid_to 33 description: Effective end timestamp of the corresponding record (exclusive) 34 - name: dim_users 35 description: Users table, which is converted into SCD type 2 36 columns: 37 - name: user_key 38 description: | 39 Primary key of the table 40 Surrogate key, which is generated by md5 hash using the following columns 41 - first_name, last_name, email, residence, lat, lon 42 tests: 43 - not_null 44 - unique 45 - name: user_id 46 description: Natural key of users 47 - name: first_name 48 description: First name 49 - name: last_name 50 description: Last name 51 - name: email 52 description: Email address 53 - name: residence 54 description: User address 55 - name: latitude 56 description: Latitude of user address 57 - name: longitude 58 description: Longitude of user address 59 - name: created_at 60 description: Timestamp when the record is loaded 61 - name: valid_from 62 description: Effective start timestamp of the corresponding record (inclusive) 63 - name: valid_to 64 description: Effective end timestamp of the corresponding record (exclusive) 65 - name: fct_orders 66 description: Orders fact table. Order items are exploded into rows 67 columns: 68 - name: order_id 69 description: Natural key of orders 70 - name: product 71 description: | 72 Array of products in an order. 73 A product is an array of struct where the following attributes are pre-joined from the dim_products table: 74 key (product_key), id (product_id), name, price, quantity, description, category, and image 75 - name: user 76 description: | 77 A struct where the following attributes are pre-joined from the dim_users table: 78 key (user_key), id (user_id), first_name, last_name, email, residence, latitude, and longitude 79 - name: created_at 80 description: Timestamp when the record is loaded The project can be executed using the dbt run command as shown below.\n1$ dbt run 223:06:05 Running with dbt=1.7.7 323:06:05 Registered adapter: bigquery=1.7.4 423:06:05 Found 6 models, 4 tests, 3 sources, 0 exposures, 0 metrics, 568 macros, 0 groups, 0 semantic models 523:06:05 623:06:07 Concurrency: 4 threads (target=\u0026#39;dev\u0026#39;) 723:06:07 823:06:07 1 of 6 START sql view model pizza_shop.src_orders .............................. [RUN] 923:06:07 2 of 6 START sql view model pizza_shop.src_products ............................ [RUN] 1023:06:07 3 of 6 START sql view model pizza_shop.src_users ............................... [RUN] 1123:06:08 1 of 6 OK created sql view model pizza_shop.src_orders ......................... [CREATE VIEW (0 processed) in 1.65s] 1223:06:09 3 of 6 OK created sql view model pizza_shop.src_users .......................... [CREATE VIEW (0 processed) in 1.93s] 1323:06:09 2 of 6 OK created sql view model pizza_shop.src_products ....................... [CREATE VIEW (0 processed) in 1.93s] 1423:06:09 4 of 6 START sql table model pizza_shop.dim_users .............................. [RUN] 1523:06:09 5 of 6 START sql table model pizza_shop.dim_products ........................... [RUN] 1623:06:14 5 of 6 OK created sql table model pizza_shop.dim_products ...................... [CREATE TABLE (81.0 rows, 0 processed) in 5.21s] 1723:06:14 4 of 6 OK created sql table model pizza_shop.dim_users ......................... [CREATE TABLE (10.0k rows, 0 processed) in 5.21s] 1823:06:14 6 of 6 START sql incremental model pizza_shop.fct_orders ....................... [RUN] 1923:06:25 6 of 6 OK created sql incremental model pizza_shop.fct_orders .................. [INSERT (20.0k rows, 1.6 MiB processed) in 11.23s] 2023:06:25 2123:06:25 Finished running 3 view models, 2 table models, 1 incremental model in 0 hours 0 minutes and 19.92 seconds (19.92s). 2223:06:25 2323:06:25 Completed successfully 2423:06:25 2523:06:25 Done. PASS=6 WARN=0 ERROR=0 SKIP=0 TOTAL=6 Also, the project can be tested using the dbt test command.\n1$ dbt test 223:06:53 Running with dbt=1.7.7 323:06:54 Registered adapter: bigquery=1.7.4 423:06:54 Found 6 models, 4 tests, 3 sources, 0 exposures, 0 metrics, 568 macros, 0 groups, 0 semantic models 523:06:54 623:06:55 Concurrency: 4 threads (target=\u0026#39;dev\u0026#39;) 723:06:55 823:06:55 1 of 4 START test not_null_dim_products_product_key ............................ [RUN] 923:06:55 2 of 4 START test not_null_dim_users_user_key .................................. [RUN] 1023:06:55 3 of 4 START test unique_dim_products_product_key .............................. [RUN] 1123:06:55 4 of 4 START test unique_dim_users_user_key .................................... [RUN] 1223:06:57 2 of 4 PASS not_null_dim_users_user_key ........................................ [PASS in 2.27s] 1323:06:57 1 of 4 PASS not_null_dim_products_product_key .................................. [PASS in 2.40s] 1423:06:57 3 of 4 PASS unique_dim_products_product_key .................................... [PASS in 2.42s] 1523:06:57 4 of 4 PASS unique_dim_users_user_key .......................................... [PASS in 2.51s] 1623:06:57 1723:06:57 Finished running 4 tests in 0 hours 0 minutes and 3.23 seconds (3.23s). 1823:06:57 1923:06:57 Completed successfully 2023:06:57 2123:06:57 Done. PASS=4 WARN=0 ERROR=0 SKIP=0 TOTAL=4 Fact Table Structure The schema of the fact table can be found below. Both the product and user fields are marked as the RECORD type as they are structs (containers of fields). Also, the mode of the product is indicated as REPEATED, which means it is an array.\nIn the query result view, non-array fields are not repeated, and a row is split to fill each of the array items.\nWe can use the UNNEST operator if we need to convert the elements of an array into rows as shown below.\nUpdate Records Although we will discuss ETL orchestration with Apache Airflow in the next post, here I illustrate how the dimension and fact tables change when records are updated.\nProduct First, a new record is inserted into the staging_products table, and the price is set to increase by 10.\n1-- // update a product record 2INSERT INTO pizza_shop.staging_products (id, name, description, price, category, image, created_at) 3 SELECT 1, name, description, price + 10, category, image, CURRENT_DATETIME(\u0026#39;Australia/Sydney\u0026#39;) 4 FROM pizza_shop.staging_products 5 WHERE id = 1; 6 7SELECT id, name, price, category, created_at 8FROM pizza_shop.staging_products 9WHERE id = 1 10ORDER BY created_at; 11 12i dname price category created_at 131 Moroccan Spice Pasta Pizza - Veg 335.0 veg pizzas 2024-02-04T09:50:05 141 Moroccan Spice Pasta Pizza - Veg 345.0 veg pizzas 2024-02-04T10:15:30.227352 When we execute the dbt run command again, we see the corresponding dimension table reflects the change by adding a new record and updating valid_from and valid_to columns accordingly. With this change, any later order record that has this product should be mapped into the new product record.\n1SELECT product_key, price, created_at, valid_from, valid_to 2FROM pizza_shop.dim_products 3WHERE product_id = 1 4ORDER BY created_at; 5 6product_key price created_at valid_from valid_to 7a8c5f8c082bcf52a164f2eccf2b493f6 335.0 2024-02-04T09:50:05 2024-02-04T09:50:05 2024-02-04T10:15:30.227352 8c995d7e1ec035da116c0f37e6284d1d5 345.0 2024-02-04T10:15:30.227352 2024-02-04T10:15:30.227352 2199-12-31T00:00:00 User Also, a new record is inserted into the staging_users table while modifying the email address.\n1-- // update a user record 2INSERT INTO pizza_shop.staging_users (id, first_name, last_name, email, residence, lat, lon, created_at) 3 SELECT 1, first_name, last_name, \u0026#39;john.doe@example.com\u0026#39;, residence, lat, lon, CURRENT_DATETIME(\u0026#39;Australia/Sydney\u0026#39;) 4 FROM pizza_shop.staging_users 5 WHERE id = 1; 6 7SELECT id, first_name, last_name, email, created_at 8FROM pizza_shop.staging_users 9WHERE id = 1 10ORDER BY created_at; 11 12id first_name last_name email created_at 131 Kismat Shroff drishyamallick@hotmail.com 2024-02-04T09:50:07 141 Kismat Shroff john.doe@example.com 2024-02-04T10:17:48.340304 Again the corresponding dimension table reflects the change by adding a new record and updating valid_from and valid_to columns accordingly.\n1SELECT user_key, email, valid_from, valid_to 2FROM pizza_shop.dim_users 3WHERE user_id = 1 4ORDER BY created_at; 5 6user_key email valid_from valid_to 78adf084f4ea4b01d4863da22c61873de drishyamallick@hotmail.com 2024-02-04T09:50:07 2024-02-04T10:17:48.340304 865e019d6a1fa0ca03e520b49928ee95b john.doe@example.com 2024-02-04T10:17:48.340304 2199-12-31T00:00:00 Order We insert a new order record that has two items where the IDs of the first and second products are 1 and 2 respectively. In this example, we expect the first product maps to the updated product record while the record of the second product remains the same. We can check it by querying the new order record together with an existing record that has corresponding products. As expected, the query result shows the product record is updated only in the new order record.\n1-- // add an order record 2INSERT INTO pizza_shop.staging_orders(id, user_id, items, created_at) 3VALUES ( 4 20001, 5 1, 6 JSON_ARRAY(JSON \u0026#39;{\u0026#34;product_id\u0026#34;: 1, \u0026#34;quantity\u0026#34;: 2}\u0026#39;, JSON \u0026#39;{\u0026#34;product_id\u0026#34;: 2, \u0026#34;quantity\u0026#34;: 3}\u0026#39;), 7 CURRENT_DATETIME(\u0026#39;Australia/Sydney\u0026#39;) 8); 9 10SELECT o.order_id, p.key, p.id, p.price, p.quantity, o.created_at 11FROM `pizza_shop.fct_orders` AS o, 12 UNNEST(o.product) AS p 13WHERE o.order_id in (11146, 20001) AND p.id IN (1, 2) 14ORDER BY order_id; 15 16order_id key id price quantity created_at 1711146 * a8c5f8c082bcf52a164f2eccf2b493f6 1 335.0 1 2024-02-04T09:50:10 1811146 8dd51b3981692c787baa9d4335f15345 2 60.0 2 2024-02-04T09:50:10 1920001 * c995d7e1ec035da116c0f37e6284d1d5 1 345.0 2 2024-02-04T10:30:19.667473 2020001 8dd51b3981692c787baa9d4335f15345 2 60.0 3 2024-02-04T10:30:19.667473 Summary In this series, we discuss practical examples of data warehouse and lakehouse development where data transformation is performed by the data build tool (dbt) and ETL is managed by Apache Airflow. In this post, we developed a dbt project on Google BigQuery using fictional pizza shop data. Two SCD type 2 dimension tables and a single transaction tables were modelled on a dbt project. The transaction table was denormalized using nested and repeated fields, which potentially can improve query performance by pre-joining corresponding dimension records. Finally, impacts of record updates were discussed in detail.\n","date":"February 8, 2024","img":"/blog/2024-02-08-dbt-pizza-shop-3/featured.png","lang":"en","langName":"English","largeImg":"/blog/2024-02-08-dbt-pizza-shop-3/featured_hu5082aedceaa438015dc31206ac8f27c2_70297_500x0_resize_box_3.png","permalink":"/blog/2024-02-08-dbt-pizza-shop-3/","series":[{"title":"DBT Pizza Shop Demo","url":"/series/dbt-pizza-shop-demo/"}],"smallImg":"/blog/2024-02-08-dbt-pizza-shop-3/featured_hu5082aedceaa438015dc31206ac8f27c2_70297_180x0_resize_box_3.png","tags":[{"title":"Data Build Tool (DBT)","url":"/tags/data-build-tool-dbt/"},{"title":"GCP","url":"/tags/gcp/"},{"title":"BigQuery","url":"/tags/bigquery/"},{"title":"Python","url":"/tags/python/"}],"timestamp":1707350400,"title":"Data Build Tool (Dbt) Pizza Shop Demo - Part 3 Modelling on BigQuery"},{"categories":[{"title":"Data Engineering","url":"/categories/data-engineering/"}],"content":"In this series of posts, we discuss data warehouse/lakehouse examples using data build tool (dbt) including ETL orchestration with Apache Airflow. In Part 1, we developed a dbt project on PostgreSQL with fictional pizza shop data. Two dimension tables that keep product and user records are created as Type 2 slowly changing dimension (SCD Type 2) tables, and one transactional fact table is built to keep pizza orders. In this post, we discuss how to set up an ETL process on the project using Apache Airflow.\nPart 1 Modelling on PostgreSQL Part 2 ETL on PostgreSQL via Airflow (this post) Part 3 Modelling on BigQuery Part 4 ETL on BigQuery via Airflow Part 5 Modelling on Amazon Athena Part 6 ETL on Amazon Athena via Airflow Infrastructure Apache Airflow and PostgreSQL are used in this post, and they are deployed locally using Docker Compose. The source can be found in the GitHub repository of this post.\nDatabase As Part 1, a PostgreSQL server is deployed using Docker Compose. See the previous post for details about (1) how fictional pizza shop data sets are made available, and (2) how the database is bootstrapped using a script (bootstrap.sql), which creates necessary schemas/tables as well as loads initial records to the tables. Note that the database cluster is shared with Airflow and thus a database and role named airflow are created for it - see below for details.\n1# compose-orchestration.yml 2... 3services: 4 postgres: 5 image: postgres:13 6 container_name: postgres 7 ports: 8 - 5432:5432 9 networks: 10 - appnet 11 environment: 12 POSTGRES_USER: devuser 13 POSTGRES_PASSWORD: password 14 POSTGRES_DB: devdb 15 TZ: Australia/Sydney 16 volumes: 17 - ./initdb/scripts:/docker-entrypoint-initdb.d 18 - ./initdb/data:/tmp 19 - postgres_data:/var/lib/postgresql/data 20... 1-- initdb/scripts/bootstrap.sql 2 3... 4 5CREATE DATABASE airflow; 6CREATE ROLE airflow 7LOGIN 8PASSWORD \u0026#39;airflow\u0026#39;; 9GRANT ALL ON DATABASE airflow TO airflow; Airflow Airflow is simplified by using the Local Executor where both scheduling and task execution are handled by the airflow scheduler service - i.e. AIRFLOW__CORE__EXECUTOR: LocalExecutor. Also, it is configured to be able to run the dbt project (see Part 1 for details) within the scheduler service by\ninstalling the dbt-postgre package as an additional pip package, and volume-mapping folders that keep the dbt project and dbt project profile 1# compose-orchestration.yml 2version: \u0026#34;3\u0026#34; 3x-airflow-common: \u0026amp;airflow-common 4 image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:2.8.0} 5 networks: 6 - appnet 7 environment: \u0026amp;airflow-common-env 8 AIRFLOW__CORE__EXECUTOR: LocalExecutor 9 AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow 10 # For backward compatibility, with Airflow \u0026lt;2.3 11 AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow 12 AIRFLOW__CORE__FERNET_KEY: \u0026#34;\u0026#34; 13 AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: \u0026#34;true\u0026#34; 14 AIRFLOW__CORE__LOAD_EXAMPLES: \u0026#34;false\u0026#34; 15 AIRFLOW__API__AUTH_BACKENDS: \u0026#34;airflow.api.auth.backend.basic_auth\u0026#34; 16 # install dbt-postgres==1.7.4 17 _PIP_ADDITIONAL_REQUIREMENTS: ${_PIP_ADDITIONAL_REQUIREMENTS:- dbt-postgres==1.7.4 pendulum} 18 volumes: 19 - ./airflow/dags:/opt/airflow/dags 20 - ./airflow/plugins:/opt/airflow/plugins 21 - ./airflow/logs:/opt/airflow/logs 22 - ./pizza_shop:/tmp/pizza_shop # dbt project 23 - ./airflow/dbt-profiles:/opt/airflow/dbt-profiles # dbt profiles 24 user: \u0026#34;${AIRFLOW_UID:-50000}:0\u0026#34; 25 depends_on: \u0026amp;airflow-common-depends-on 26 postgres: 27 condition: service_healthy 28 29services: 30 postgres: 31 image: postgres:13 32 container_name: postgres 33 ports: 34 - 5432:5432 35 networks: 36 - appnet 37 environment: 38 POSTGRES_USER: devuser 39 POSTGRES_PASSWORD: password 40 POSTGRES_DB: devdb 41 TZ: Australia/Sydney 42 volumes: 43 - ./initdb/scripts:/docker-entrypoint-initdb.d 44 - ./initdb/data:/tmp 45 - postgres_data:/var/lib/postgresql/data 46 healthcheck: 47 test: [\u0026#34;CMD\u0026#34;, \u0026#34;pg_isready\u0026#34;, \u0026#34;-U\u0026#34;, \u0026#34;airflow\u0026#34;] 48 interval: 5s 49 retries: 5 50 51 airflow-webserver: 52 \u0026lt;\u0026lt;: *airflow-common 53 container_name: webserver 54 command: webserver 55 ports: 56 - 8080:8080 57 depends_on: 58 \u0026lt;\u0026lt;: *airflow-common-depends-on 59 airflow-init: 60 condition: service_completed_successfully 61 62 airflow-scheduler: 63 \u0026lt;\u0026lt;: *airflow-common 64 command: scheduler 65 container_name: scheduler 66 environment: 67 \u0026lt;\u0026lt;: *airflow-common-env 68 depends_on: 69 \u0026lt;\u0026lt;: *airflow-common-depends-on 70 airflow-init: 71 condition: service_completed_successfully 72 73 airflow-init: 74 \u0026lt;\u0026lt;: *airflow-common 75 container_name: init 76 entrypoint: /bin/bash 77 command: 78 - -c 79 - | 80 mkdir -p /sources/logs /sources/dags /sources/plugins 81 chown -R \u0026#34;${AIRFLOW_UID}:0\u0026#34; /sources/{logs,dags,plugins} 82 exec /entrypoint airflow version 83 environment: 84 \u0026lt;\u0026lt;: *airflow-common-env 85 _AIRFLOW_DB_UPGRADE: \u0026#34;true\u0026#34; 86 _AIRFLOW_WWW_USER_CREATE: \u0026#34;true\u0026#34; 87 _AIRFLOW_WWW_USER_USERNAME: ${_AIRFLOW_WWW_USER_USERNAME:-airflow} 88 _AIRFLOW_WWW_USER_PASSWORD: ${_AIRFLOW_WWW_USER_PASSWORD:-airflow} 89 _PIP_ADDITIONAL_REQUIREMENTS: \u0026#34;\u0026#34; 90 user: \u0026#34;0:0\u0026#34; 91 volumes: 92 - .:/sources 93 94volumes: 95 airflow_log_volume: 96 driver: local 97 name: airflow_log_volume 98 postgres_data: 99 driver: local 100 name: postgres_data 101 102networks: 103 appnet: 104 name: app-network The Airflow and PostgreSQL services can be deployed as shown below. Note that it is recommended to specify the host user\u0026rsquo;s ID as the AIRFLOW_UID value. Otherwise, Airflow can fail to launch due to insufficient permission to write logs.\n1$ AIRFLOW_UID=$(id -u) docker-compose -f compose-orchestration.yml up -d Once started, we can visit the Airflow web server on http://localhost:8080.\nETL Job A simple ETL job (demo_etl) is created, which updates source records (update_records) followed by running and testing the dbt project. Note that the dbt project and profile are accessible as they are volume-mapped to the Airflow scheduler container.\n1# airflow/dags/operators.py 2import pendulum 3from airflow.models.dag import DAG 4from airflow.operators.bash import BashOperator 5from airflow.operators.python import PythonOperator 6 7import update_records 8 9with DAG( 10 dag_id=\u0026#34;demo_etl\u0026#34;, 11 schedule=None, 12 start_date=pendulum.datetime(2024, 1, 1, tz=\u0026#34;Australia/Sydney\u0026#34;), 13 catchup=False, 14 tags=[\u0026#34;pizza\u0026#34;], 15): 16 task_records_update = PythonOperator( 17 task_id=\u0026#34;update_records\u0026#34;, python_callable=update_records.main 18 ) 19 20 task_dbt_run = BashOperator( 21 task_id=\u0026#34;dbt_run\u0026#34;, 22 bash_command=\u0026#34;dbt run --profiles-dir /opt/airflow/dbt-profiles --project-dir /tmp/pizza_shop\u0026#34;, 23 ) 24 25 task_dbt_test = BashOperator( 26 task_id=\u0026#34;dbt_test\u0026#34;, 27 bash_command=\u0026#34;dbt test --profiles-dir /opt/airflow/dbt-profiles --project-dir /tmp/pizza_shop\u0026#34;, 28 ) 29 30 task_records_update \u0026gt;\u0026gt; task_dbt_run \u0026gt;\u0026gt; task_dbt_test Update Records As dbt takes care of data transformation only, we should create a task that updates source records. As shown below, the task updates as many as a half of records of the dimension tables (products and users) and appends 5,000 order records in a single run.\n1# airflow/dags/update_records.py 2import os 3import json 4import dataclasses 5import random 6import string 7 8import psycopg2 9import psycopg2.extras 10 11 12class DbHelper: 13 def __init__(self) -\u0026gt; None: 14 self.conn = self.connect_db() 15 16 def connect_db(self): 17 conn = psycopg2.connect( 18 host=os.getenv(\u0026#34;DB_HOST\u0026#34;, \u0026#34;postgres\u0026#34;), 19 database=os.getenv(\u0026#34;DB_NAME\u0026#34;, \u0026#34;devdb\u0026#34;), 20 user=os.getenv(\u0026#34;DB_USER\u0026#34;, \u0026#34;devuser\u0026#34;), 21 password=os.getenv(\u0026#34;DB_PASSWORD\u0026#34;, \u0026#34;password\u0026#34;), 22 ) 23 conn.autocommit = False 24 return conn 25 26 def get_connection(self): 27 if (self.conn is None) or (self.conn.closed): 28 self.conn = self.connect_db() 29 30 def fetch_records(self, stmt: str): 31 self.get_connection() 32 with self.conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor) as cur: 33 cur.execute(stmt) 34 return cur.fetchall() 35 36 def update_records(self, stmt: str, records: list, to_fetch: bool = True): 37 self.get_connection() 38 with self.conn.cursor() as cur: 39 values = psycopg2.extras.execute_values(cur, stmt, records, fetch=to_fetch) 40 self.conn.commit() 41 if to_fetch: 42 return values 43 44 def commit(self): 45 if not self.conn.closed: 46 self.conn.commit() 47 48 def close(self): 49 if self.conn and (not self.conn.closed): 50 self.conn.close() 51 52 53@dataclasses.dataclass 54class Product: 55 id: int 56 name: int 57 description: int 58 price: float 59 category: str 60 image: str 61 62 def __hash__(self) -\u0026gt; int: 63 return self.id 64 65 @classmethod 66 def from_json(cls, r: dict): 67 return cls(**r) 68 69 @staticmethod 70 def load(db: DbHelper): 71 stmt = \u0026#34;\u0026#34;\u0026#34; 72 WITH windowed AS ( 73 SELECT 74 *, 75 ROW_NUMBER() OVER (PARTITION BY id ORDER BY created_at DESC) AS rn 76 FROM staging.products 77 ) 78 SELECT id, name, description, price, category, image 79 FROM windowed 80 WHERE rn = 1; 81 \u0026#34;\u0026#34;\u0026#34; 82 return [Product.from_json(r) for r in db.fetch_records(stmt)] 83 84 @staticmethod 85 def update(db: DbHelper, percent: float = 0.5): 86 stmt = \u0026#34;INSERT INTO staging.products(id, name, description, price, category, image) VALUES %s RETURNING id, price\u0026#34; 87 products = Product.load(db) 88 records = set(random.choices(products, k=int(len(products) * percent))) 89 for r in records: 90 r.price = r.price + 10 91 values = db.update_records( 92 stmt, [list(dataclasses.asdict(r).values()) for r in records], True 93 ) 94 return values 95 96 97@dataclasses.dataclass 98class User: 99 id: int 100 first_name: str 101 last_name: str 102 email: str 103 residence: str 104 lat: float 105 lon: float 106 107 def __hash__(self) -\u0026gt; int: 108 return self.id 109 110 @classmethod 111 def from_json(cls, r: dict): 112 return cls(**r) 113 114 @staticmethod 115 def load(db: DbHelper): 116 stmt = \u0026#34;\u0026#34;\u0026#34; 117 WITH windowed AS ( 118 SELECT 119 *, 120 ROW_NUMBER() OVER (PARTITION BY id ORDER BY created_at DESC) AS rn 121 FROM staging.users 122 ) 123 SELECT id, first_name, last_name, email, residence, lat, lon 124 FROM windowed 125 WHERE rn = 1; 126 \u0026#34;\u0026#34;\u0026#34; 127 return [User.from_json(r) for r in db.fetch_records(stmt)] 128 129 @staticmethod 130 def update(db: DbHelper, percent: float = 0.5): 131 stmt = \u0026#34;INSERT INTO staging.users(id, first_name, last_name, email, residence, lat, lon) VALUES %s RETURNING id, email\u0026#34; 132 users = User.load(db) 133 records = set(random.choices(users, k=int(len(users) * percent))) 134 for r in records: 135 r.email = f\u0026#34;{\u0026#39;\u0026#39;.join(random.choices(string.ascii_letters, k=5)).lower()}@email.com\u0026#34; 136 values = db.update_records( 137 stmt, [list(dataclasses.asdict(r).values()) for r in records], True 138 ) 139 return values 140 141 142@dataclasses.dataclass 143class Order: 144 user_id: int 145 items: str 146 147 @classmethod 148 def create(cls): 149 order_items = [ 150 {\u0026#34;product_id\u0026#34;: id, \u0026#34;quantity\u0026#34;: random.randint(1, 5)} 151 for id in set(random.choices(range(1, 82), k=random.randint(1, 10))) 152 ] 153 return cls( 154 user_id=random.randint(1, 10000), 155 items=json.dumps([item for item in order_items]), 156 ) 157 158 @staticmethod 159 def append(db: DbHelper, num_orders: int = 5000): 160 stmt = \u0026#34;INSERT INTO staging.orders(user_id, items) VALUES %s RETURNING id\u0026#34; 161 records = [Order.create() for _ in range(num_orders)] 162 values = db.update_records( 163 stmt, [list(dataclasses.asdict(r).values()) for r in records], True 164 ) 165 return values 166 167 168def main(): 169 db = DbHelper() 170 ## update product and user records 171 print(f\u0026#34;{len(Product.update(db))} product records updated\u0026#34;) 172 print(f\u0026#34;{len(User.update(db))} user records updated\u0026#34;) 173 ## created order records 174 print(f\u0026#34;{len(Order.append(db))} order records created\u0026#34;) The details of the ETL job can be found on the Airflow web server as shown below.\nRun ETL Below shows example product dimension records after the ETL job is completed twice. The product is updated in the second job and a new surrogate key is assigned as well as the valid_from and valid_to column values are updated accordingly.\n1SELECT product_key, price, created_at, valid_from, valid_to 2FROM dev.dim_products 3WHERE product_id = 61 4 5product_key |price|created_at |valid_from |valid_to | 6--------------------------------+-----+-------------------+-------------------+-------------------+ 76a326183ce19f0db7f4af2ab779cc2dd|185.0|2024-01-16 18:18:12|2024-01-16 18:18:12|2024-01-16 18:22:19| 8d2e27002c0f474d507c60772161673aa|195.0|2024-01-16 18:22:19|2024-01-16 18:22:19|2199-12-31 00:00:00| When I query the fact table for orders with the same product ID, I can check correct product key values are mapped - note value_to is not inclusive.\n1SELECT o.order_key, o.product_key, o.product_id, p.price, o.created_at 2FROM dev.fct_orders o 3JOIN dev.dim_products p ON o.product_key = p.product_key 4WHERE o.product_id = 61 AND order_id IN (5938, 28146) 5 6order_key |product_key |product_id|price|created_at | 7--------------------------------+--------------------------------+----------+-----+-------------------+ 8cb650e3bb22e3be1d112d59a44482560|6a326183ce19f0db7f4af2ab779cc2dd| 61|185.0|2024-01-16 18:18:12| 9470c2fa24f97a2a8dfac2510cc49f942|d2e27002c0f474d507c60772161673aa| 61|195.0|2024-01-16 18:22:19| Summary In this series of posts, we discuss data warehouse/lakehouse examples using data build tool (dbt) including ETL orchestration with Apache Airflow. In this post, we discussed how to set up an ETL process on the dbt project developed in Part 1 using Airflow. A demo ETL job was created that updates records followed by running and testing the dbt project. Finally, the result of ETL job was validated by checking sample records.\n","date":"January 25, 2024","img":"/blog/2024-01-25-dbt-pizza-shop-2/featured.png","lang":"en","langName":"English","largeImg":"/blog/2024-01-25-dbt-pizza-shop-2/featured_hu32c9a8b2df36b3a1704029fc725738dd_77355_500x0_resize_box_3.png","permalink":"/blog/2024-01-25-dbt-pizza-shop-2/","series":[{"title":"DBT Pizza Shop Demo","url":"/series/dbt-pizza-shop-demo/"}],"smallImg":"/blog/2024-01-25-dbt-pizza-shop-2/featured_hu32c9a8b2df36b3a1704029fc725738dd_77355_180x0_resize_box_3.png","tags":[{"title":"Data Build Tool (DBT)","url":"/tags/data-build-tool-dbt/"},{"title":"PostgreSQL","url":"/tags/postgresql/"},{"title":"Docker","url":"/tags/docker/"},{"title":"Docker Compose","url":"/tags/docker-compose/"},{"title":"Apache Airflow","url":"/tags/apache-airflow/"},{"title":"Python","url":"/tags/python/"}],"timestamp":1706140800,"title":"Data Build Tool (Dbt) Pizza Shop Demo - Part 2 ETL on PostgreSQL via Airflow"},{"categories":[{"title":"Data Engineering","url":"/categories/data-engineering/"}],"content":"The data build tool (dbt) is a popular data transformation tool for data warehouse development. Moreover, it can be used for data lakehouse development thanks to open table formats such as Apache Iceberg, Apache Hudi and Delta Lake. dbt supports key AWS analytics services and I wrote a series of posts that discuss how to utilise dbt with Redshift, Glue, EMR on EC2, EMR on EKS, and Athena. Those posts focus on platform integration, however, they do not show realistic ETL scenarios.\nIn this series of posts, we discuss practical data warehouse/lakehouse examples including ETL orchestration with Apache Airflow. As a starting point, we develop a dbt project on PostgreSQL using fictional pizza shop data in this post.\nPart 1 Modelling on PostgreSQL (this post) Part 2 ETL on PostgreSQL via Airflow Part 3 Modelling on BigQuery Part 4 ETL on BigQuery via Airflow Part 5 Modelling on Amazon Athena Part 6 ETL on Amazon Athena via Airflow Setup Database PostgreSQL is used in the post, and it is deployed locally using Docker Compose. The source can be found in the GitHub repository of this post.\nPrepare Data Fictional pizza shop data from Building Real-Time Analytics Systems is used in this post. There are three data sets - products, users and orders. The first two data sets are copied from the book\u0026rsquo;s GitHub repository and saved into initdb/data folder. The last order data set is generated by the following Python script.\n1# initdb/generate_orders.py 2import os 3import csv 4import dataclasses 5import random 6import json 7 8 9@dataclasses.dataclass 10class Order: 11 user_id: int 12 items: str 13 14 @staticmethod 15 def create(): 16 order_items = [ 17 {\u0026#34;product_id\u0026#34;: id, \u0026#34;quantity\u0026#34;: random.randint(1, 5)} 18 for id in set(random.choices(range(1, 82), k=random.randint(1, 10))) 19 ] 20 return Order( 21 user_id=random.randint(1, 10000), 22 items=json.dumps([item for item in order_items]), 23 ) 24 25 26if __name__ == \u0026#34;__main__\u0026#34;: 27 \u0026#34;\u0026#34;\u0026#34; 28 Generate random orders given by the NUM_ORDERS environment variable. 29 - orders.csv will be written to ./data folder 30 31 Example: 32 python generate_orders.py 33 NUM_ORDERS=10000 python generate_orders.py 34 \u0026#34;\u0026#34;\u0026#34; 35 NUM_ORDERS = int(os.getenv(\u0026#34;NUM_ORDERS\u0026#34;, \u0026#34;20000\u0026#34;)) 36 CURRENT_DIR = os.path.dirname(os.path.realpath(__file__)) 37 orders = [Order.create() for _ in range(NUM_ORDERS)] 38 39 filepath = os.path.join(CURRENT_DIR, \u0026#34;data\u0026#34;, \u0026#34;orders.csv\u0026#34;) 40 if os.path.exists(filepath): 41 os.remove(filepath) 42 43 with open(os.path.join(CURRENT_DIR, \u0026#34;data\u0026#34;, \u0026#34;orders.csv\u0026#34;), \u0026#34;w\u0026#34;) as f: 44 writer = csv.writer(f) 45 writer.writerow([\u0026#34;user_id\u0026#34;, \u0026#34;items\u0026#34;]) 46 for order in orders: 47 writer.writerow(dataclasses.asdict(order).values()) Below shows sample order records generated by the script. It includes user ID and order items. As discussed further later, the items will be kept as the JSONB type in the staging table and transposed into rows in the fact table.\n1user_id,items 26845,\u0026#34;[{\u0026#34;\u0026#34;product_id\u0026#34;\u0026#34;: 52, \u0026#34;\u0026#34;quantity\u0026#34;\u0026#34;: 4}, {\u0026#34;\u0026#34;product_id\u0026#34;\u0026#34;: 68, \u0026#34;\u0026#34;quantity\u0026#34;\u0026#34;: 5}]\u0026#34; 36164,\u0026#34;[{\u0026#34;\u0026#34;product_id\u0026#34;\u0026#34;: 77, \u0026#34;\u0026#34;quantity\u0026#34;\u0026#34;: 4}]\u0026#34; 49303,\u0026#34;[{\u0026#34;\u0026#34;product_id\u0026#34;\u0026#34;: 5, \u0026#34;\u0026#34;quantity\u0026#34;\u0026#34;: 2}, {\u0026#34;\u0026#34;product_id\u0026#34;\u0026#34;: 71, \u0026#34;\u0026#34;quantity\u0026#34;\u0026#34;: 3}, {\u0026#34;\u0026#34;product_id\u0026#34;\u0026#34;: 74, \u0026#34;\u0026#34;quantity\u0026#34;\u0026#34;: 2}, {\u0026#34;\u0026#34;product_id\u0026#34;\u0026#34;: 10, \u0026#34;\u0026#34;quantity\u0026#34;\u0026#34;: 5}, {\u0026#34;\u0026#34;product_id\u0026#34;\u0026#34;: 12, \u0026#34;\u0026#34;quantity\u0026#34;\u0026#34;: 2}]\u0026#34; The folder structure of source data sets and order generation script can be found below.\n1$ tree initdb/ -P \u0026#34;*.csv|generate*\u0026#34; -I \u0026#34;scripts\u0026#34; 2initdb/ 3├── data 4│ ├── orders.csv 5│ ├── products.csv 6│ └── users.csv 7└── generate_orders.py Run Database A PostgreSQL server is deployed using Docker Compose. Also, the compose file creates a database (devdb) and user (devuser) by specifying corresponding environment variables (POSTGRES_*). Moreover, the database bootstrap script (bootstrap.sql) is volume-mapped into the docker entry point directory, and it creates necessary schemas and tables followed by loading initial records into the staging tables at startup. See below for details about the bootstrap script.\n1# compose-postgres.yml 2version: \u0026#34;3\u0026#34; 3 4services: 5 postgres: 6 image: postgres:13 7 container_name: postgres 8 ports: 9 - 5432:5432 10 volumes: 11 - ./initdb/scripts:/docker-entrypoint-initdb.d 12 - ./initdb/data:/tmp 13 - postgres_data:/var/lib/postgresql/data 14 environment: 15 - POSTGRES_DB=devdb 16 - POSTGRES_USER=devuser 17 - POSTGRES_PASSWORD=password 18 - TZ=Australia/Sydney 19 20volumes: 21 postgres_data: 22 driver: local 23 name: postgres_data Database Bootstrap Script The bootstrap script begins with creating schemas and granting permission to the development user. Then the tables for the pizza shop data are created in the staging schema, and initial records are copied from data files into corresponding tables.\n1-- initdb/scripts/bootstrap.sql 2 3-- // create schemas and grant permission 4CREATE SCHEMA staging; 5GRANT ALL ON SCHEMA staging TO devuser; 6 7CREATE SCHEMA dev; 8GRANT ALL ON SCHEMA dev TO devuser; 9 10-- // create tables 11DROP TABLE IF EXISTS staging.users; 12DROP TABLE IF EXISTS staging.products; 13DROP TABLE IF EXISTS staging.orders; 14 15CREATE TABLE staging.users 16( 17 id SERIAL, 18 first_name VARCHAR(255), 19 last_name VARCHAR(255), 20 email VARCHAR(255), 21 residence VARCHAR(500), 22 lat DECIMAL(10, 8), 23 lon DECIMAL(10, 8), 24 created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP 25); 26 27CREATE TABLE IF NOT EXISTS staging.products 28( 29 id SERIAL, 30 name VARCHAR(100), 31 description VARCHAR(500), 32 price FLOAT, 33 category VARCHAR(100), 34 image VARCHAR(200), 35 created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP 36); 37 38CREATE TABLE IF NOT EXISTS staging.orders 39( 40 id SERIAL, 41 user_id INT, 42 items JSONB, 43 created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP 44); 45 46-- // copy data 47COPY staging.users(first_name, last_name, email, residence, lat, lon) 48FROM \u0026#39;/tmp/users.csv\u0026#39; DELIMITER \u0026#39;,\u0026#39; CSV HEADER; 49 50COPY staging.products(name, description, price, category, image) 51FROM \u0026#39;/tmp/products.csv\u0026#39; DELIMITER \u0026#39;,\u0026#39; CSV HEADER; 52 53COPY staging.orders(user_id, items) 54FROM \u0026#39;/tmp/orders.csv\u0026#39; DELIMITER \u0026#39;,\u0026#39; CSV HEADER; Setup DBT Project A dbt project named pizza_shop is created using the dbt-postgres package (dbt-postgres==1.7.4). Specifically, it is created using the dbt init command, and it bootstraps the project in the pizza_shop folder as well as adds the project profile to the dbt profiles file as shown below.\n1# $HOME/.dbt/profiles.yml 2pizza_shop: 3 outputs: 4 dev: 5 dbname: devdb 6 host: localhost 7 pass: password 8 port: 5432 9 schema: dev 10 threads: 4 11 type: postgres 12 user: devuser 13 target: dev Project Sources Recall that three tables are created in the staging schema by the database bootstrap script. They are used as sources of the project and their details are kept in sources.yml to be referred easily in other models.\n1# pizza_shop/models/sources.yml 2version: 2 3 4sources: 5 - name: raw 6 schema: staging 7 tables: 8 - name: users 9 identifier: users 10 - name: products 11 identifier: products 12 - name: orders 13 identifier: orders Using the raw sources, three models are created by performing simple transformations such as adding surrogate keys using the dbt_utils package and changing column names. Note that, as the products and users dimension tables are kept by Type 2 slowly changing dimension (SCD type 2), the surrogate keys are used to uniquely identify relevant dimension records.\n1-- pizza_shop/models/src/src_products.sql 2WITH raw_products AS ( 3 SELECT * FROM {{ source(\u0026#39;raw\u0026#39;, \u0026#39;products\u0026#39;) }} 4) 5SELECT 6 {{ dbt_utils.generate_surrogate_key([\u0026#39;name\u0026#39;, \u0026#39;description\u0026#39;, \u0026#39;price\u0026#39;, \u0026#39;category\u0026#39;, \u0026#39;image\u0026#39;]) }} as product_key, 7 id AS product_id, 8 name, 9 description, 10 price, 11 category, 12 image, 13 created_at 14FROM raw_products 1-- pizza_shop/models/src/src_users.sql 2WITH raw_users AS ( 3 SELECT * FROM {{ source(\u0026#39;raw\u0026#39;, \u0026#39;users\u0026#39;) }} 4) 5SELECT 6 {{ dbt_utils.generate_surrogate_key([\u0026#39;first_name\u0026#39;, \u0026#39;last_name\u0026#39;, \u0026#39;email\u0026#39;, \u0026#39;residence\u0026#39;, \u0026#39;lat\u0026#39;, \u0026#39;lon\u0026#39;]) }} as user_key, 7 id AS user_id, 8 first_name, 9 last_name, 10 email, 11 residence, 12 lat AS latitude, 13 lon AS longitude, 14 created_at 15FROM raw_users 1-- pizza_shop/models/src/src_orders.sql 2WITH raw_orders AS ( 3 SELECT * FROM {{ source(\u0026#39;raw\u0026#39;, \u0026#39;orders\u0026#39;) }} 4) 5SELECT 6 id AS order_id, 7 user_id, 8 items, 9 created_at 10FROM raw_orders Data Modelling For SCD type 2, the dimension tables are materialized as table and two additional columns are included - valid_from and valid_to. The extra columns are for setting up a time range where a record is applicable, and they are used to map a relevant surrogate key in the fact table when there are multiple dimension records according to the same natural key. Note that SCD type 2 tables can also be maintained by dbt snapshots.\n1-- pizza_shop/models/dim/dim_products.sql 2{{ 3 config( 4 materialized = \u0026#39;table\u0026#39;, 5 ) 6}} 7WITH src_products AS ( 8 SELECT * FROM {{ ref(\u0026#39;src_products\u0026#39;) }} 9) 10SELECT 11 *, 12 created_at AS valid_from, 13 COALESCE( 14 LEAD(created_at, 1) OVER (PARTITION BY product_id ORDER BY created_at), 15 \u0026#39;2199-12-31\u0026#39;::TIMESTAMP 16 ) AS valid_to 17FROM src_products 1-- pizza_shop/models/dim/dim_users.sql 2{{ 3 config( 4 materialized = \u0026#39;table\u0026#39;, 5 ) 6}} 7WITH src_users AS ( 8 SELECT * FROM {{ ref(\u0026#39;src_users\u0026#39;) }} 9) 10SELECT 11 *, 12 created_at AS valid_from, 13 COALESCE( 14 LEAD(created_at, 1) OVER (PARTITION BY user_id ORDER BY created_at), 15 \u0026#39;2199-12-31\u0026#39;::TIMESTAMP 16 ) AS valid_to 17FROM src_users The transactional fact table is materialized as incremental so that only new records are appended. Also, order items are transposed into rows using the jsonb_array_elements function. Finally, the records are joined with the dimension tables to add relevant surrogate keys from them. Note that the surrogate key of the fact table is constructed by a combination of all natural keys.\n1-- pizza_shop/models/fct/fct_orders.sql 2{{ 3 config( 4 materialized = \u0026#39;incremental\u0026#39; 5 ) 6}} 7WITH dim_products AS ( 8 SELECT * FROM {{ ref(\u0026#39;dim_products\u0026#39;) }} 9), dim_users AS ( 10 SELECT * FROM {{ ref(\u0026#39;dim_users\u0026#39;) }} 11), src_orders AS ( 12 SELECT 13 order_id, 14 user_id, 15 jsonb_array_elements(items) AS order_item, 16 created_at 17 FROM {{ ref(\u0026#39;src_orders\u0026#39;) }} 18), expanded_orders AS ( 19 SELECT 20 order_id, 21 user_id, 22 (order_item -\u0026gt;\u0026gt; \u0026#39;product_id\u0026#39;)::INT AS product_id, 23 (order_item -\u0026gt;\u0026gt; \u0026#39;quantity\u0026#39;)::INT AS quantity, 24 created_at 25 FROM src_orders 26) 27SELECT 28 {{ dbt_utils.generate_surrogate_key([\u0026#39;order_id\u0026#39;, \u0026#39;p.product_id\u0026#39;, \u0026#39;u.user_id\u0026#39;]) }} as order_key, 29 p.product_key, 30 u.user_key, 31 o.order_id, 32 o.user_id, 33 o.product_id, 34 o.quantity, 35 o.created_at 36FROM expanded_orders o 37JOIN dim_products p 38 ON o.product_id = p.product_id 39 AND o.created_at \u0026gt;= p.valid_from 40 AND o.created_at \u0026lt; p.valid_to 41JOIN dim_users u 42 ON o.user_id = u.user_id 43 AND o.created_at \u0026gt;= u.valid_from 44 AND o.created_at \u0026lt; u.valid_to 45{% if is_incremental() %} 46 WHERE o.created_at \u0026gt; (SELECT created_at from {{ this }} ORDER BY created_at DESC LIMIT 1) 47{% endif %} We can keep the final models in a separate YAML file for testing and enhanced documentation.\n1# pizza_shop/models/schema.yml 2version: 2 3 4models: 5 - name: dim_products 6 description: Products table, which is converted into SCD type 2 7 columns: 8 - name: product_key 9 description: | 10 Primary key of the table 11 Surrogate key, which is generated by md5 hash using the following columns 12 - name, description, price, category, image 13 tests: 14 - not_null 15 - unique 16 - name: product_id 17 description: Natural key of products 18 - name: name 19 description: Porduct name 20 - name: description 21 description: Product description 22 - name: price 23 description: Product price 24 - name: category 25 description: Product category 26 - name: image 27 description: Product image 28 - name: created_at 29 description: Timestamp when the record is loaded 30 - name: valid_from 31 description: Effective start timestamp of the corresponding record (inclusive) 32 - name: valid_to 33 description: Effective end timestamp of the corresponding record (exclusive) 34 - name: dim_users 35 description: Users table, which is converted into SCD type 2 36 columns: 37 - name: user_key 38 description: | 39 Primary key of the table 40 Surrogate key, which is generated by md5 hash using the following columns 41 - first_name, last_name, email, residence, lat, lon 42 tests: 43 - not_null 44 - unique 45 - name: user_id 46 description: Natural key of users 47 - name: first_name 48 description: First name 49 - name: last_name 50 description: Last name 51 - name: email 52 description: Email address 53 - name: residence 54 description: User address 55 - name: latitude 56 description: Latitude of user address 57 - name: longitude 58 description: Longitude of user address 59 - name: created_at 60 description: Timestamp when the record is loaded 61 - name: valid_from 62 description: Effective start timestamp of the corresponding record (inclusive) 63 - name: valid_to 64 description: Effective end timestamp of the corresponding record (exclusive) 65 - name: fct_orders 66 description: Orders fact table. Order items are exploded into rows 67 columns: 68 - name: order_key 69 description: | 70 Primary key of the table 71 Surrogate key, which is generated by md5 hash using the following columns 72 - order_id, product_id, user_id 73 tests: 74 - not_null 75 - unique 76 - name: product_key 77 description: Product surrogate key which matches the product dimension record 78 tests: 79 - not_null 80 - relationships: 81 to: ref(\u0026#39;dim_products\u0026#39;) 82 field: product_key 83 - name: user_key 84 description: User surrogate key which matches the user dimension record 85 tests: 86 - not_null 87 - relationships: 88 to: ref(\u0026#39;dim_users\u0026#39;) 89 field: user_key 90 - name: order_id 91 description: Natural key of orders 92 - name: user_id 93 description: Natural key of users 94 - name: product_id 95 description: Natural key of products 96 - name: quantity 97 description: Amount of products ordered 98 - name: created_at 99 description: Timestamp when the record is loaded The project can be executed using the dbt run command as shown below.\n1$ dbt run 204:39:32 Running with dbt=1.7.4 304:39:33 Registered adapter: postgres=1.7.4 404:39:33 Found 6 models, 10 tests, 3 sources, 0 exposures, 0 metrics, 515 macros, 0 groups, 0 semantic models 504:39:33 604:39:33 Concurrency: 4 threads (target=\u0026#39;dev\u0026#39;) 704:39:33 804:39:33 1 of 6 START sql view model dev.src_orders ..................................... [RUN] 904:39:33 2 of 6 START sql view model dev.src_products ................................... [RUN] 1004:39:33 3 of 6 START sql view model dev.src_users ...................................... [RUN] 1104:39:33 2 of 6 OK created sql view model dev.src_products .............................. [CREATE VIEW in 0.14s] 1204:39:33 1 of 6 OK created sql view model dev.src_orders ................................ [CREATE VIEW in 0.15s] 1304:39:33 3 of 6 OK created sql view model dev.src_users ................................. [CREATE VIEW in 0.14s] 1404:39:33 4 of 6 START sql table model dev.dim_products .................................. [RUN] 1504:39:33 5 of 6 START sql table model dev.dim_users ..................................... [RUN] 1604:39:33 5 of 6 OK created sql table model dev.dim_users ................................ [SELECT 10000 in 0.12s] 1704:39:33 4 of 6 OK created sql table model dev.dim_products ............................. [SELECT 81 in 0.13s] 1804:39:33 6 of 6 START sql incremental model dev.fct_orders .............................. [RUN] 1904:39:33 6 of 6 OK created sql incremental model dev.fct_orders ......................... [SELECT 105789 in 0.33s] 2004:39:33 2104:39:33 Finished running 3 view models, 2 table models, 1 incremental model in 0 hours 0 minutes and 0.70 seconds (0.70s). 2204:39:33 2304:39:33 Completed successfully 2404:39:33 2504:39:33 Done. PASS=6 WARN=0 ERROR=0 SKIP=0 TOTAL=6 Also, the project can be tested using the dbt test command.\n1$ dbt test 204:40:53 Running with dbt=1.7.4 304:40:53 Registered adapter: postgres=1.7.4 404:40:53 Found 6 models, 10 tests, 3 sources, 0 exposures, 0 metrics, 515 macros, 0 groups, 0 semantic models 504:40:53 604:40:53 Concurrency: 4 threads (target=\u0026#39;dev\u0026#39;) 704:40:53 804:40:53 1 of 10 START test not_null_dim_products_product_key ........................... [RUN] 904:40:53 2 of 10 START test not_null_dim_users_user_key ................................. [RUN] 1004:40:53 3 of 10 START test not_null_fct_orders_order_key ............................... [RUN] 1104:40:53 4 of 10 START test not_null_fct_orders_product_key ............................. [RUN] 1204:40:53 1 of 10 PASS not_null_dim_products_product_key ................................. [PASS in 0.11s] 1304:40:53 5 of 10 START test not_null_fct_orders_user_key ................................ [RUN] 1404:40:53 2 of 10 PASS not_null_dim_users_user_key ....................................... [PASS in 0.11s] 1504:40:53 4 of 10 PASS not_null_fct_orders_product_key ................................... [PASS in 0.11s] 1604:40:53 3 of 10 PASS not_null_fct_orders_order_key ..................................... [PASS in 0.12s] 1704:40:53 6 of 10 START test relationships_fct_orders_product_key__product_key__ref_dim_products_ [RUN] 1804:40:53 7 of 10 START test relationships_fct_orders_user_key__user_key__ref_dim_users_ . [RUN] 1904:40:53 8 of 10 START test unique_dim_products_product_key ............................. [RUN] 2004:40:53 5 of 10 PASS not_null_fct_orders_user_key ...................................... [PASS in 0.09s] 2104:40:53 9 of 10 START test unique_dim_users_user_key ................................... [RUN] 2204:40:53 8 of 10 PASS unique_dim_products_product_key ................................... [PASS in 0.06s] 2304:40:53 10 of 10 START test unique_fct_orders_order_key ................................ [RUN] 2404:40:53 6 of 10 PASS relationships_fct_orders_product_key__product_key__ref_dim_products_ [PASS in 0.10s] 2504:40:53 7 of 10 PASS relationships_fct_orders_user_key__user_key__ref_dim_users_ ....... [PASS in 0.11s] 2604:40:53 9 of 10 PASS unique_dim_users_user_key ......................................... [PASS in 0.06s] 2704:40:53 10 of 10 PASS unique_fct_orders_order_key ...................................... [PASS in 0.11s] 2804:40:53 2904:40:53 Finished running 10 tests in 0 hours 0 minutes and 0.42 seconds (0.42s). 3004:40:53 3104:40:53 Completed successfully 3204:40:53 3304:40:53 Done. PASS=10 WARN=0 ERROR=0 SKIP=0 TOTAL=10 Update Records Although we will discuss ETL orchestration with Apache Airflow in the next post, here I illustrate how the dimension and fact tables change when records are updated.\nProduct First, a new record is inserted into the products table in the staging schema, and the price is set to increase by 10.\n1-- // update a product record 2INSERT INTO staging.products (id, name, description, price, category, image) 3 SELECT 1, name, description, price + 10, category, image 4 FROM staging.products 5 WHERE id = 1; 6 7SELECT id, name, price, category, created_at 8FROM staging.products 9WHERE id = 1; 10 11id|name |price|category |created_at | 12--+--------------------------------+-----+----------+-------------------+ 13 1|Moroccan Spice Pasta Pizza - Veg|335.0|veg pizzas|2024-01-14 15:38:30| 14 1|Moroccan Spice Pasta Pizza - Veg|345.0|veg pizzas|2024-01-14 15:43:15| When we execute the dbt run command again, we see the corresponding dimension table reflects the change by adding a new record and updating valid_from and valid_to columns accordingly. With this change, any later order record that has this product should be mapped into the new product surrogate key.\n1SELECT product_key, price, created_at, valid_from, valid_to 2FROM dev.dim_products 3WHERE product_id = 1; 4 5product_key |price|created_at |valid_from |valid_to | 6--------------------------------+-----+-------------------+-------------------+-------------------+ 7a8c5f8c082bcf52a164f2eccf2b493f6|335.0|2024-01-14 15:38:30|2024-01-14 15:38:30|2024-01-14 15:43:15| 8c995d7e1ec035da116c0f37e6284d1d5|345.0|2024-01-14 15:43:15|2024-01-14 15:43:15|2199-12-31 00:00:00| User Also, a new record is inserted into the users table in the staging schema while modifying the email address.\n1-- // update a user record 2INSERT INTO staging.users (id, first_name, last_name, email, residence, lat, lon) 3 SELECT 1, first_name, last_name, \u0026#39;john.doe@example.com\u0026#39;, residence, lat, lon 4 FROM staging.users 5 WHERE id = 1; 6 7SELECT id, first_name, last_name, email, created_at 8FROM staging.users 9WHERE id = 1; 10 11id|first_name|last_name|email |created_at | 12--+----------+---------+--------------------------+-------------------+ 13 1|Kismat |Shroff |drishyamallick@hotmail.com|2024-01-14 15:38:30| 14 1|Kismat |Shroff |john.doe@example.com |2024-01-14 15:43:35| Again the corresponding dimension table reflects the change by adding a new record and updating valid_from and valid_to columns accordingly.\n1SELECT user_key, email, valid_from, valid_to 2FROM dev.dim_users 3WHERE user_id = 1; 4 5user_key |email |valid_from |valid_to | 6--------------------------------+--------------------------+-------------------+-------------------+ 77f530277c15881c328b67c4764205a9c|drishyamallick@hotmail.com|2024-01-14 15:38:30|2024-01-14 15:43:35| 8f4b2344f893f50597cdc4a12c7e87e81|john.doe@example.com |2024-01-14 15:43:35|2199-12-31 00:00:00| Order We insert a new order record that has two items where the IDs of the first and second products are 1 and 2 respectively. In this example, we expect the first product maps to the updated product surrogate key while the surrogate key of the second product remains the same. We can check it by querying the new order record together with an existing record that has corresponding products. As expected, the query result shows the product surrogate key is updated only in the new order record.\n1INSERT INTO staging.orders(user_id, items) 2VALUES (1,\u0026#39;[{\u0026#34;product_id\u0026#34;: 1, \u0026#34;quantity\u0026#34;: 2}, {\u0026#34;product_id\u0026#34;: 2, \u0026#34;quantity\u0026#34;: 3}]\u0026#39;); 3 4SELECT o.order_key, o.product_key, o.order_id, o.product_id, p.price, o.quantity, o.created_at 5FROM dev.fct_orders o 6JOIN dev.dim_products p ON o.product_key = p.product_key 7WHERE o.order_id IN (249, 20001) 8ORDER BY o.order_id, o.product_id; 9 10order_key | product_key |order_id|product_id|price|quantity|created_at | 11--------------------------------+----------------------------------+--------+----------+-----+--------+-------------------+ 125c8f7a8bc27d825efdb2e8167c9ae481|* a8c5f8c082bcf52a164f2eccf2b493f6| 249| 1|335.0| 1|2024-01-14 15:38:30| 1342834bd2b8f847cbd182765b43094be0| 8dd51b3981692c787baa9d4335f15345| 249| 2| 60.0| 1|2024-01-14 15:38:30| 1428d937c571bd9601c2e719e618e56f67|* c995d7e1ec035da116c0f37e6284d1d5| 20001| 1|345.0| 2|2024-01-14 15:51:04| 1575e1957ce20f188ab7502c322e2ab7d9| 8dd51b3981692c787baa9d4335f15345| 20001| 2| 60.0| 3|2024-01-14 15:51:04| Summary The data build tool (dbt) is a popular data transformation tool. In this series, we discuss practical examples of data warehouse and lakehouse development where data transformation is performed by the data build tool (dbt) and ETL is managed by Apache Airflow. As a starting point, we developed a dbt project on PostgreSQL using fictional pizza shop data in this post. Two SCD type 2 dimension tables and a single transaction tables were modelled on a dbt project and impacts of record updates were discussed in detail.\n","date":"January 18, 2024","img":"/blog/2024-01-18-dbt-pizza-shop-1/featured.png","lang":"en","langName":"English","largeImg":"/blog/2024-01-18-dbt-pizza-shop-1/featured_hua8fc52ecfe83e5abaf1872cc948e583f_85093_500x0_resize_box_3.png","permalink":"/blog/2024-01-18-dbt-pizza-shop-1/","series":[{"title":"DBT Pizza Shop Demo","url":"/series/dbt-pizza-shop-demo/"}],"smallImg":"/blog/2024-01-18-dbt-pizza-shop-1/featured_hua8fc52ecfe83e5abaf1872cc948e583f_85093_180x0_resize_box_3.png","tags":[{"title":"Data Build Tool (DBT)","url":"/tags/data-build-tool-dbt/"},{"title":"PostgreSQL","url":"/tags/postgresql/"},{"title":"Docker","url":"/tags/docker/"},{"title":"Docker Compose","url":"/tags/docker-compose/"},{"title":"Python","url":"/tags/python/"}],"timestamp":1705536000,"title":"Data Build Tool (Dbt) Pizza Shop Demo - Part 1 Modelling on PostgreSQL"},{"categories":[{"title":"Apache Kafka","url":"/categories/apache-kafka/"}],"content":"Kafka Connect is a tool for scalably and reliably streaming data between Apache Kafka and other systems. It makes it simple to quickly define connectors that move large collections of data into and out of Kafka. In this post, we discuss how to set up a data ingestion pipeline using Kafka connectors. Fake customer and order data is ingested into Kafka topics using the MSK Data Generator. Also, we use the Confluent S3 sink connector to save the messages of the topics into a S3 bucket. The Kafka Connect servers and individual connectors are deployed using the custom resources of Strimzi on Kubernetes.\nPart 1 Cluster Setup Part 2 Producer and Consumer Part 3 Kafka Connect (this post) Kafka Connect We create a Kafka Connect server using the Strimzi custom resource named KafkaConnect. The source can be found in the GitHub repository of this post.\nCreate Secrets As discussed further below, a custom Docker image is built and pushed into an external Docker registry when we create a Kafka Connect server using Strimzi. Therefore, we need the registry secret to push the image. Also, as the sink connector needs permission to write files into a S3 bucket, AWS credentials should be added to the Connect server. Both the secret and credentials will be made available via Kubernetes Secrets and those are created as shown below.\n1# Log in to DockerHub if not done - docker login 2kubectl create secret generic regcred \\ 3 --from-file=.dockerconfigjson=$HOME/.docker/config.json \\ 4 --type=kubernetes.io/dockerconfigjson 1kubectl create -f - \u0026lt;\u0026lt;EOF 2apiVersion: v1 3kind: Secret 4metadata: 5 name: awscred 6stringData: 7 AWS_ACCESS_KEY_ID: $AWS_ACCESS_KEY_ID 8 AWS_SECRET_ACCESS_KEY: $AWS_SECRET_ACCESS_KEY 9EOF Deploy Connect Server The Kafka Connect server is created using the Strimzi custom resource named KafkaConnect. When we create the custom resource, a Docker image is built, the image is pushed into an external Docker registry (output.type: docker), and it is pulled to deploy the Kafka Connect server instances. Therefore, in the build configuration, we need to indicate the location of an external Docker registry and registry secret (pushSecret). Note that the registry secret is referred from the Kubernetes Secret that is created earlier. Also, we can build connector sources together by specifying their types and URLs in build.plugins.\nWhen it comes to the Connect server configuration, two server instances are configured to run (replicas: 2) and the same Kafka version (2.8.1) to the associating Kafka cluster is used. Also, the name and port of the service that exposes the Kafka internal listeners are used to specify the Kafka bootstrap server address. Moreover, AWS credentials are added to environment variables because the sink connector needs permission to write files to a S3 bucket.\n1# manifests/kafka-connect.yaml 2apiVersion: kafka.strimzi.io/v1beta2 3kind: KafkaConnect 4metadata: 5 name: demo-connect 6 annotations: 7 strimzi.io/use-connector-resources: \u0026#34;true\u0026#34; 8spec: 9 version: 2.8.1 10 replicas: 2 11 bootstrapServers: demo-cluster-kafka-bootstrap:9092 12 config: 13 group.id: demo-connect 14 offset.storage.topic: demo-connect-offsets 15 config.storage.topic: demo-connect-configs 16 status.storage.topic: demo-connect-status 17 # -1 means it will use the default replication factor configured in the broker 18 config.storage.replication.factor: -1 19 offset.storage.replication.factor: -1 20 status.storage.replication.factor: -1 21 key.converter: org.apache.kafka.connect.json.JsonConverter 22 value.converter: org.apache.kafka.connect.json.JsonConverter 23 key.converter.schemas.enable: false 24 value.converter.schemas.enable: false 25 externalConfiguration: 26 env: 27 - name: AWS_ACCESS_KEY_ID 28 valueFrom: 29 secretKeyRef: 30 name: awscred 31 key: AWS_ACCESS_KEY_ID 32 - name: AWS_SECRET_ACCESS_KEY 33 valueFrom: 34 secretKeyRef: 35 name: awscred 36 key: AWS_SECRET_ACCESS_KEY 37 build: 38 output: 39 type: docker 40 image: jaehyeon/demo-connect:latest 41 pushSecret: regcred 42 # https://strimzi.io/docs/operators/0.27.1/using#plugins 43 plugins: 44 - name: confluentinc-kafka-connect-s3 45 artifacts: 46 - type: zip 47 url: https://d1i4a15mxbxib1.cloudfront.net/api/plugins/confluentinc/kafka-connect-s3/versions/10.4.3/confluentinc-kafka-connect-s3-10.4.3.zip 48 - name: msk-data-generator 49 artifacts: 50 - type: jar 51 url: https://github.com/awslabs/amazon-msk-data-generator/releases/download/v0.4.0/msk-data-generator-0.4-jar-with-dependencies.jar We assume that a Kafka cluster and management app are deployed on Minikube as discussed in Part 1. The Kafka Connect server can be created using the kubernetes create command as shown below.\n1kubectl create -f manifests/kafka-connect.yaml Once the Connect image is built successfully, we can see that the image is pushed into the external Docker registry.\nThen the Connect server instances run by two Pods, and they are exposed by a service named demo-connect-connect-api on port 8083.\n1kubectl get all -l strimzi.io/cluster=demo-connect 2# NAME READY STATUS RESTARTS AGE 3# pod/demo-connect-connect-559cd588b4-48lhd 1/1 Running 0 2m4s 4# pod/demo-connect-connect-559cd588b4-wzqgs 1/1 Running 0 2m4s 5 6# NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE 7# service/demo-connect-connect-api ClusterIP 10.111.148.128 \u0026lt;none\u0026gt; 8083/TCP 2m4s 8 9# NAME READY UP-TO-DATE AVAILABLE AGE 10# deployment.apps/demo-connect-connect 2/2 2 2 2m4s 11 12# NAME DESIRED CURRENT READY AGE 13# replicaset.apps/demo-connect-connect-559cd588b4 2 2 2 2m4s Kafka Connectors Both the source and sink connectors are created using the Strimzi custom resource named KafkaConnector.\nSource Connector The connector class (spec.class) is set for the MSK Data Generator, and a single worker is allocated to it (tasks.max). Also, the key converter is set to the string converter as the keys of both topics are set to be primitive values (genkp) while the value converter is configured as the json converter. Finally, schemas are not enabled for both the key and value.\nThe remaining properties are specific to the source connector. Basically it sends messages to two topics (customer and order). They are linked by the customer_id attribute of the order topic where the value is from the key of the customer topic. This is useful for practicing stream processing e.g. for joining two streams.\n1# manifests/kafka-connectors.yaml 2apiVersion: kafka.strimzi.io/v1beta2 3kind: KafkaConnector 4metadata: 5 name: order-source 6 labels: 7 strimzi.io/cluster: demo-connect 8spec: 9 class: com.amazonaws.mskdatagen.GeneratorSourceConnector 10 tasksMax: 1 11 config: 12 ## 13 key.converter: org.apache.kafka.connect.storage.StringConverter 14 key.converter.schemas.enable: false 15 value.converter: org.apache.kafka.connect.json.JsonConverter 16 value.converter.schemas.enable: false 17 ## 18 genkp.customer.with: \u0026#34;#{Code.isbn10}\u0026#34; 19 genv.customer.name.with: \u0026#34;#{Name.full_name}\u0026#34; 20 genkp.order.with: \u0026#34;#{Internet.uuid}\u0026#34; 21 genv.order.product_id.with: \u0026#34;#{number.number_between \u0026#39;101\u0026#39;\u0026#39;109\u0026#39;}\u0026#34; 22 genv.order.quantity.with: \u0026#34;#{number.number_between \u0026#39;1\u0026#39;\u0026#39;5\u0026#39;}\u0026#34; 23 genv.order.customer_id.matching: customer.key 24 global.throttle.ms: 500 25 global.history.records.max: 1000 26 27... Sink Connector The connector is configured to write files from both the topics (order and customer) into a S3 bucket (s3.bucket.name) where the file names are prefixed by the partition number (DefaultPartitioner). Also, it invokes file commits every 60 seconds (rotate.schedule.interval.ms) or the number of messages reach 100 (flush.size). Like the source connector, it overrides the converter-related properties.\n1# manifests/kafka-connectors.yaml 2 3... 4 5apiVersion: kafka.strimzi.io/v1beta2 6kind: KafkaConnector 7metadata: 8 name: order-sink 9 labels: 10 strimzi.io/cluster: demo-connect 11spec: 12 class: io.confluent.connect.s3.S3SinkConnector 13 tasksMax: 1 14 config: 15 ## 16 key.converter: org.apache.kafka.connect.storage.StringConverter 17 key.converter.schemas.enable: false 18 value.converter: org.apache.kafka.connect.json.JsonConverter 19 value.converter.schemas.enable: false 20 ## 21 storage.class: io.confluent.connect.s3.storage.S3Storage 22 format.class: io.confluent.connect.s3.format.json.JsonFormat 23 topics: order,customer 24 s3.bucket.name: kafka-dev-on-k8s-ap-southeast-2 25 s3.region: ap-southeast-2 26 flush.size: 100 27 rotate.schedule.interval.ms: 60000 28 timezone: Australia/Sydney 29 partitioner.class: io.confluent.connect.storage.partitioner.DefaultPartitioner 30 errors.log.enable: true Deploy Connectors The Kafka source and sink connectors can be created using the kubernetes create command as shown below. Once created, we can check their details by listing (or describing) the Strimzi custom resource (KafkaConnector). Below shows both the source and sink connectors are in ready status, which indicates they are running.\n1kubectl create -f manifests/kafka-connectors.yaml 2 3kubectl get kafkaconnectors 4# NAME CLUSTER CONNECTOR CLASS MAX TASKS READY 5# order-sink demo-connect io.confluent.connect.s3.S3SinkConnector 1 True 6# order-source demo-connect com.amazonaws.mskdatagen.GeneratorSourceConnector 1 True We can also see the connector status on the Kafka management app (kafka-ui) as shown below.\nThe sink connector writes messages of the two topics (customer and order), and topic names are used as S3 prefixes.\nThe files are generated by \u0026lt;topic\u0026gt;+\u0026lt;partiton\u0026gt;+\u0026lt;start-offset\u0026gt;.json. The sink connector\u0026rsquo;s format class is set to io.confluent.connect.s3.format.json.JsonFormat so that it writes to Json files.\nDelete Resources The Kubernetes resources and Minikube cluster can be removed by the kubectl delete and minikube delete commands respectively.\n1## delete resources 2kubectl delete -f manifests/kafka-connectors.yaml 3kubectl delete -f manifests/kafka-connect.yaml 4kubectl delete secret awscred 5kubectl delete secret regcred 6kubectl delete -f manifests/kafka-cluster.yaml 7kubectl delete -f manifests/kafka-ui.yaml 8kubectl delete -f manifests/strimzi-cluster-operator-$STRIMZI_VERSION.yaml 9 10## delete minikube 11minikube delete Summary Kafka Connect is a tool for scalably and reliably streaming data between Apache Kafka and other systems. In this post, we discussed how to set up a data ingestion pipeline using Kafka connectors. Fake customer and order data was ingested into Kafka topics using the MSK Data Generator. Also, we used the Confluent S3 sink connector to save the messages of the topics into a S3 bucket. The Kafka Connect servers and individual connectors were deployed using the custom resources of Strimzi on Kubernetes.\n","date":"January 11, 2024","img":"/blog/2024-01-11-kafka-development-on-k8s-part-3/featured.png","lang":"en","langName":"English","largeImg":"/blog/2024-01-11-kafka-development-on-k8s-part-3/featured_hu95d566cb28d7f91b2d6baddb7d8bb440_97270_500x0_resize_box_3.png","permalink":"/blog/2024-01-11-kafka-development-on-k8s-part-3/","series":[{"title":"Kafka Development on Kubernetes","url":"/series/kafka-development-on-kubernetes/"}],"smallImg":"/blog/2024-01-11-kafka-development-on-k8s-part-3/featured_hu95d566cb28d7f91b2d6baddb7d8bb440_97270_180x0_resize_box_3.png","tags":[{"title":"Apache Kafka","url":"/tags/apache-kafka/"},{"title":"Kubernetes","url":"/tags/kubernetes/"},{"title":"Minikube","url":"/tags/minikube/"},{"title":"Strimzi","url":"/tags/strimzi/"},{"title":"Docker","url":"/tags/docker/"},{"title":"Kafka Connect","url":"/tags/kafka-connect/"}],"timestamp":1704931200,"title":"Kafka Development on Kubernetes - Part 3 Kafka Connect"},{"categories":[{"title":"Apache Kafka","url":"/categories/apache-kafka/"}],"content":"Apache Kafka has five core APIs, and we can develop applications to send/read streams of data to/from topics in a Kafka cluster using the producer and consumer APIs. While the main Kafka project maintains only the Java APIs, there are several open source projects that provide the Kafka client APIs in Python. In this post, we discuss how to develop Kafka client applications using the kafka-python package on Kubernetes.\nPart 1 Cluster Setup Part 2 Producer and Consumer (this post) Part 3 Kafka Connect Kafka Client Apps We create Kafka producer and consumer apps using the kafka-python package. The source can be found in the GitHub repository of this post.\nProducer The producer app sends fake order data that is generated by the Faker package. The Order class generates one or more order records by the create method where each record includes order ID, order timestamp, user ID and order items. Both the key and value are serialized as JSON.\n1# clients/producer.py 2import os 3import datetime 4import time 5import json 6import typing 7import logging 8import dataclasses 9 10from faker import Faker 11from kafka import KafkaProducer 12 13logging.basicConfig(level=logging.INFO) 14 15 16@dataclasses.dataclass 17class OrderItem: 18 product_id: int 19 quantity: int 20 21 22@dataclasses.dataclass 23class Order: 24 order_id: str 25 ordered_at: datetime.datetime 26 user_id: str 27 order_items: typing.List[OrderItem] 28 29 def asdict(self): 30 return dataclasses.asdict(self) 31 32 @classmethod 33 def auto(cls, fake: Faker = Faker()): 34 user_id = str(fake.random_int(1, 100)).zfill(3) 35 order_items = [ 36 OrderItem(fake.random_int(1, 1000), fake.random_int(1, 10)) 37 for _ in range(fake.random_int(1, 4)) 38 ] 39 return cls(fake.uuid4(), datetime.datetime.utcnow(), user_id, order_items) 40 41 def create(self, num: int): 42 return [self.auto() for _ in range(num)] 43 44 45class Producer: 46 def __init__(self, bootstrap_servers: list, topic: str): 47 self.bootstrap_servers = bootstrap_servers 48 self.topic = topic 49 self.producer = self.create() 50 51 def create(self): 52 return KafkaProducer( 53 bootstrap_servers=self.bootstrap_servers, 54 value_serializer=lambda v: json.dumps(v, default=self.serialize).encode(\u0026#34;utf-8\u0026#34;), 55 key_serializer=lambda v: json.dumps(v, default=self.serialize).encode(\u0026#34;utf-8\u0026#34;), 56 api_version=(2, 8, 1) 57 ) 58 59 def send(self, orders: typing.List[Order]): 60 for order in orders: 61 try: 62 self.producer.send( 63 self.topic, key={\u0026#34;order_id\u0026#34;: order.order_id}, value=order.asdict() 64 ) 65 except Exception as e: 66 raise RuntimeError(\u0026#34;fails to send a message\u0026#34;) from e 67 self.producer.flush() 68 69 def serialize(self, obj): 70 if isinstance(obj, datetime.datetime): 71 return obj.isoformat() 72 if isinstance(obj, datetime.date): 73 return str(obj) 74 return obj 75 76 77if __name__ == \u0026#34;__main__\u0026#34;: 78 producer = Producer( 79 bootstrap_servers=os.environ[\u0026#34;BOOTSTRAP_SERVERS\u0026#34;].split(\u0026#34;,\u0026#34;), 80 topic=os.getenv(\u0026#34;TOPIC_NAME\u0026#34;, \u0026#34;orders\u0026#34;), 81 ) 82 max_run = int(os.getenv(\u0026#34;MAX_RUN\u0026#34;, \u0026#34;-1\u0026#34;)) 83 logging.info(f\u0026#34;max run - {max_run}\u0026#34;) 84 current_run = 0 85 while True: 86 current_run += 1 87 logging.info(f\u0026#34;current run - {current_run}\u0026#34;) 88 if current_run \u0026gt; max_run and max_run \u0026gt;= 0: 89 logging.info(f\u0026#34;exceeds max run, finish\u0026#34;) 90 producer.producer.close() 91 break 92 producer.send(Order.auto().create(100)) 93 time.sleep(1) A sample order record is shown below.\n1{ 2\t\u0026#34;order_id\u0026#34;: \u0026#34;79c0c393-9eca-4a44-8efd-3965752f3e16\u0026#34;, 3\t\u0026#34;ordered_at\u0026#34;: \u0026#34;2023-12-26T18:02:54.510497\u0026#34;, 4\t\u0026#34;user_id\u0026#34;: \u0026#34;050\u0026#34;, 5\t\u0026#34;order_items\u0026#34;: [ 6\t{ 7\t\u0026#34;product_id\u0026#34;: 113, 8\t\u0026#34;quantity\u0026#34;: 9 9\t}, 10\t{ 11\t\u0026#34;product_id\u0026#34;: 58, 12\t\u0026#34;quantity\u0026#34;: 5 13\t} 14\t] 15} Consumer The Consumer class instantiates the KafkaConsumer in the create method. The main consumer configuration values are provided by the constructor arguments, and those are Kafka bootstrap server addresses (bootstrap_servers), topic names (topics) and consumer group ID (group_id). The process method of the class polls messages and logs details of the consumer records.\n1# clients/consumer.py 2import os 3import time 4import logging 5 6from kafka import KafkaConsumer 7from kafka.errors import KafkaError 8 9logging.basicConfig(level=logging.INFO) 10 11 12class Consumer: 13 def __init__(self, bootstrap_servers: list, topics: list, group_id: str) -\u0026gt; None: 14 self.bootstrap_servers = bootstrap_servers 15 self.topics = topics 16 self.group_id = group_id 17 self.consumer = self.create() 18 19 def create(self): 20 return KafkaConsumer( 21 *self.topics, 22 bootstrap_servers=self.bootstrap_servers, 23 auto_offset_reset=\u0026#34;earliest\u0026#34;, 24 enable_auto_commit=True, 25 group_id=self.group_id, 26 key_deserializer=lambda v: v.decode(\u0026#34;utf-8\u0026#34;), 27 value_deserializer=lambda v: v.decode(\u0026#34;utf-8\u0026#34;), 28 api_version=(2, 8, 1) 29 ) 30 31 def process(self): 32 try: 33 while True: 34 msg = self.consumer.poll(timeout_ms=1000) 35 if msg is None: 36 continue 37 self.print_info(msg) 38 time.sleep(1) 39 except KafkaError as error: 40 logging.error(error) 41 42 def print_info(self, msg: dict): 43 for t, v in msg.items(): 44 for r in v: 45 logging.info( 46 f\u0026#34;key={r.key}, value={r.value}, topic={t.topic}, partition={t.partition}, offset={r.offset}, ts={r.timestamp}\u0026#34; 47 ) 48 49 50if __name__ == \u0026#34;__main__\u0026#34;: 51 consumer = Consumer( 52 bootstrap_servers=os.environ[\u0026#34;BOOTSTRAP_SERVERS\u0026#34;].split(\u0026#34;,\u0026#34;), 53 topics=os.getenv(\u0026#34;TOPIC_NAME\u0026#34;, \u0026#34;orders\u0026#34;).split(\u0026#34;,\u0026#34;), 54 group_id=os.getenv(\u0026#34;GROUP_ID\u0026#34;, \u0026#34;orders-group\u0026#34;), 55 ) 56 consumer.process() Test Client Apps on Host We assume that a Kafka cluster and management app are deployed on Minikube as discussed in Part 1. As mentioned in Part 1, the external listener of the Kafka bootstrap server is exposed by a service named demo-cluster-kafka-external-bootstrap. We can use the minikube service command to obtain the Kubernetes URL for the service.\n1minikube service demo-cluster-kafka-external-bootstrap --url 2 3# http://127.0.0.1:42289 4# ❗ Because you are using a Docker driver on linux, the terminal needs to be open to run it. We can execute the Kafka client apps by replacing the bootstrap server address with the URL obtained in the previous step. Note that the apps should run in separate terminals.\n1# terminal 1 2BOOTSTRAP_SERVERS=127.0.0.1:42289 python clients/producer.py 3# terminal 2 4BOOTSTRAP_SERVERS=127.0.0.1:42289 python clients/consumer.py The access URL of the Kafka management app (kafka-ui) can be obtained using the minikube service command as shown below.\n1minikube service kafka-ui --url 2 3# http://127.0.0.1:36477 4# ❗ Because you are using a Docker driver on linux, the terminal needs to be open to run it. On the management app, we can check messages are created in the orders topic. Note that the Kafka cluster is configured to allow automatic creation of topics and the default number of partitions is set to 3.\nAlso, we can see that messages are consumed by a single consumer in the consumer group named orders-group.\nDeploy Client Apps Build Docker Image We need a custom Docker image to deploy the client apps and normally images are pulled from an external Docker registry. Instead of relying on an external registry, however, we can reuse the Docker daemon inside the Minikube cluster, which speeds up local development. It can be achieved by executing the following command.\n1# use docker daemon inside minikube cluster 2eval $(minikube docker-env) 3# Host added: /home/jaehyeon/.ssh/known_hosts ([127.0.0.1]:32772) The Dockerfile copies the client apps into the /app folder and installs dependent pip packages.\n1# clients/Dockerfile 2FROM bitnami/python:3.8 3 4COPY . /app 5RUN pip install -r requirements.txt We can check the image is found in the Docker daemon inside the Minikube cluster after building it with a name of order-clients:0.1.0.\n1docker build -t=order-clients:0.1.0 clients/. 2 3docker images order-clients 4# REPOSITORY TAG IMAGE ID CREATED SIZE 5# order-clients 0.1.0 bc48046837b1 26 seconds ago 589MB Deploy on Minikube We can create the Kafka client apps using Kubernetes Deployments. Both the apps have a single instance and Kafka cluster access details are added to environment variables. Note that, as we use the local Docker image (order-clients:0.1.0), the image pull policy (imagePullPolicy) is set to Never so that it will not be pulled from an external Docker registry.\n1# manifests/kafka-clients.yml 2apiVersion: apps/v1 3kind: Deployment 4metadata: 5 labels: 6 app: order-producer 7 group: client 8 name: order-producer 9spec: 10 replicas: 1 11 selector: 12 matchLabels: 13 app: order-producer 14 template: 15 metadata: 16 labels: 17 app: order-producer 18 group: client 19 spec: 20 containers: 21 - image: order-clients:0.1.0 22 name: producer-container 23 args: [\u0026#34;python\u0026#34;, \u0026#34;producer.py\u0026#34;] 24 env: 25 - name: BOOTSTRAP_SERVERS 26 value: demo-cluster-kafka-bootstrap:9092 27 - name: TOPIC_NAME 28 value: orders 29 - name: TZ 30 value: Australia/Sydney 31 resources: {} 32 imagePullPolicy: Never # shouldn\u0026#39;t be Always 33--- 34apiVersion: apps/v1 35kind: Deployment 36metadata: 37 labels: 38 app: order-consumer 39 group: client 40 name: order-consumer 41spec: 42 replicas: 1 43 selector: 44 matchLabels: 45 app: order-consumer 46 template: 47 metadata: 48 labels: 49 app: order-consumer 50 group: client 51 spec: 52 containers: 53 - image: order-clients:0.1.0 54 name: consumer-container 55 args: [\u0026#34;python\u0026#34;, \u0026#34;consumer.py\u0026#34;] 56 env: 57 - name: BOOTSTRAP_SERVERS 58 value: demo-cluster-kafka-bootstrap:9092 59 - name: TOPIC_NAME 60 value: orders 61 - name: GROUP_ID 62 value: orders-group 63 - name: TZ 64 value: Australia/Sydney 65 resources: {} 66 imagePullPolicy: Never The client apps can be deployed using the kubectl create command, and we can check the producer and consumer apps run on a single pod respectively.\n1kubectl create -f manifests/kafka-clients.yml 2 3kubectl get all -l group=client 4# NAME READY STATUS RESTARTS AGE 5# pod/order-consumer-79785749d5-67bxz 1/1 Running 0 12s 6# pod/order-producer-759d568fb8-rrl6w 1/1 Running 0 12s 7 8# NAME READY UP-TO-DATE AVAILABLE AGE 9# deployment.apps/order-consumer 1/1 1 1 12s 10# deployment.apps/order-producer 1/1 1 1 12s 11 12# NAME DESIRED CURRENT READY AGE 13# replicaset.apps/order-consumer-79785749d5 1 1 1 12s 14# replicaset.apps/order-producer-759d568fb8 1 1 1 12s We can monitor the client apps using the log messages. Below shows the last 3 messages of them, and we see that they run as expected.\n1kubectl logs deploy/order-producer --tail=3 2# INFO:root:current run - 104 3# INFO:root:current run - 105 4# INFO:root:current run - 106 5 6kubectl logs deploy/order-consumer --tail=3 7# INFO:root:key={\u0026#34;order_id\u0026#34;: \u0026#34;d9b9e577-7a02-401e-b0e1-2d0cdcda51a3\u0026#34;}, value={\u0026#34;order_id\u0026#34;: \u0026#34;d9b9e577-7a02-401e-b0e1-2d0cdcda51a3\u0026#34;, \u0026#34;ordered_at\u0026#34;: \u0026#34;2023-12-18T18:17:13.065654\u0026#34;, \u0026#34;user_id\u0026#34;: \u0026#34;061\u0026#34;, \u0026#34;order_items\u0026#34;: [{\u0026#34;product_id\u0026#34;: 866, \u0026#34;quantity\u0026#34;: 7}, {\u0026#34;product_id\u0026#34;: 970, \u0026#34;quantity\u0026#34;: 1}]}, topic=orders, partition=1, offset=7000, ts=1702923433077 8# INFO:root:key={\u0026#34;order_id\u0026#34;: \u0026#34;dfbd2e1f-1b18-4772-83b9-c689a3da4c03\u0026#34;}, value={\u0026#34;order_id\u0026#34;: \u0026#34;dfbd2e1f-1b18-4772-83b9-c689a3da4c03\u0026#34;, \u0026#34;ordered_at\u0026#34;: \u0026#34;2023-12-18T18:17:13.065754\u0026#34;, \u0026#34;user_id\u0026#34;: \u0026#34;016\u0026#34;, \u0026#34;order_items\u0026#34;: [{\u0026#34;product_id\u0026#34;: 853, \u0026#34;quantity\u0026#34;: 10}]}, topic=orders, partition=1, offset=7001, ts=1702923433077 9# INFO:root:key={\u0026#34;order_id\u0026#34;: \u0026#34;eaf43f0b-53ed-419a-ba75-1d74bd0525a4\u0026#34;}, value={\u0026#34;order_id\u0026#34;: \u0026#34;eaf43f0b-53ed-419a-ba75-1d74bd0525a4\u0026#34;, \u0026#34;ordered_at\u0026#34;: \u0026#34;2023-12-18T18:17:13.065795\u0026#34;, \u0026#34;user_id\u0026#34;: \u0026#34;072\u0026#34;, \u0026#34;order_items\u0026#34;: [{\u0026#34;product_id\u0026#34;: 845, \u0026#34;quantity\u0026#34;: 1}, {\u0026#34;product_id\u0026#34;: 944, \u0026#34;quantity\u0026#34;: 7}, {\u0026#34;product_id\u0026#34;: 454, \u0026#34;quantity\u0026#34;: 10}, {\u0026#34;product_id\u0026#34;: 834, \u0026#34;quantity\u0026#34;: 9}]}, topic=orders, partition=1, offset=7002, ts=1702923433078 Delete Resources The Kubernetes resources and Minikube cluster can be removed by the kubectl delete and minikube delete commands respectively.\n1## delete resources 2kubectl delete -f manifests/kafka-clients.yml 3kubectl delete -f manifests/kafka-cluster.yaml 4kubectl delete -f manifests/kafka-ui.yaml 5kubectl delete -f manifests/strimzi-cluster-operator-$STRIMZI_VERSION.yaml 6 7## delete minikube 8minikube delete Summary Apache Kafka has five core APIs, and we can develop applications to send/read streams of data to/from topics in a Kafka cluster using the producer and consumer APIs. In this post, we discussed how to develop Kafka client applications using the kafka-python package on Kubernetes.\n","date":"January 4, 2024","img":"/blog/2024-01-04-kafka-development-on-k8s-part-2/featured.png","lang":"en","langName":"English","largeImg":"/blog/2024-01-04-kafka-development-on-k8s-part-2/featured_hu866886d7082b1c38bf0c33066b35072f_75889_500x0_resize_box_3.png","permalink":"/blog/2024-01-04-kafka-development-on-k8s-part-2/","series":[{"title":"Kafka Development on Kubernetes","url":"/series/kafka-development-on-kubernetes/"}],"smallImg":"/blog/2024-01-04-kafka-development-on-k8s-part-2/featured_hu866886d7082b1c38bf0c33066b35072f_75889_180x0_resize_box_3.png","tags":[{"title":"Apache Kafka","url":"/tags/apache-kafka/"},{"title":"Kubernetes","url":"/tags/kubernetes/"},{"title":"Minikube","url":"/tags/minikube/"},{"title":"Strimzi","url":"/tags/strimzi/"},{"title":"Docker","url":"/tags/docker/"},{"title":"Python","url":"/tags/python/"}],"timestamp":1704326400,"title":"Kafka Development on Kubernetes - Part 2 Producer and Consumer"},{"categories":[{"title":"Apache Kafka","url":"/categories/apache-kafka/"}],"content":"Apache Kafka is one of the key technologies for implementing data streaming architectures. Strimzi provides a way to run an Apache Kafka cluster and related resources on Kubernetes in various deployment configurations. In this series of posts, we will discuss how to create a Kafka cluster, to develop Kafka client applications in Python and to build a data pipeline using Kafka connectors on Kubernetes.\nPart 1 Cluster Setup (this post) Part 2 Producer and Consumer Part 3 Kafka Connect Setup Kafka Cluster The Kafka cluster is deployed using the Strimzi Operator on a Minikube cluster. We install Strimzi version 0.27.1 and Kubernetes version 1.24.7 as we use Kafka version 2.8.1 - see this page for details about Kafka version compatibility. Once the Minikube CLI and Docker are installed, a Minikube cluster can be created by specifying the desired Kubernetes version as shown below.\n1minikube start --cpus=\u0026#39;max\u0026#39; --memory=10240 --addons=metrics-server --kubernetes-version=v1.24.7 Deploy Strimzi Operator The project repository keeps manifest files that can be used to deploy the Strimzi Operator and related resources. We can download the relevant manifest file by specifying the desired version. By default, the manifest file assumes the resources are deployed in the myproject namespace. As we deploy them in the default namespace, however, we need to change the resource namespace accordingly using sed. The source can be found in the GitHub repository of this post.\nThe resources that are associated with the Strimzi Operator can be deployed using the kubectl create command. Note that over 20 resources are deployed from the manifest file including Kafka-related custom resources. Among those, we use the Kafka, KafkaConnect and KafkaConnector custom resources in this series.\n1## download and update strimzi oeprator manifest 2STRIMZI_VERSION=\u0026#34;0.27.1\u0026#34; 3DOWNLOAD_URL=https://github.com/strimzi/strimzi-kafka-operator/releases/download/$STRIMZI_VERSION/strimzi-cluster-operator-$STRIMZI_VERSION.yaml 4curl -L -o manifests/strimzi-cluster-operator-$STRIMZI_VERSION.yaml ${DOWNLOAD_URL} 5# update namespace from myproject to default 6sed -i \u0026#39;s/namespace: .*/namespace: default/\u0026#39; manifests/strimzi-cluster-operator-$STRIMZI_VERSION.yaml 7 8## deploy strimzi cluster operator 9kubectl create -f manifests/strimzi-cluster-operator-$STRIMZI_VERSION.yaml 10 11# rolebinding.rbac.authorization.k8s.io/strimzi-cluster-operator-entity-operator-delegation created 12# customresourcedefinition.apiextensions.k8s.io/strimzipodsets.core.strimzi.io created 13# clusterrole.rbac.authorization.k8s.io/strimzi-kafka-client created 14# deployment.apps/strimzi-cluster-operator created 15# customresourcedefinition.apiextensions.k8s.io/kafkarebalances.kafka.strimzi.io created 16# clusterrolebinding.rbac.authorization.k8s.io/strimzi-cluster-operator-kafka-broker-delegation created 17# configmap/strimzi-cluster-operator created 18# rolebinding.rbac.authorization.k8s.io/strimzi-cluster-operator created 19# customresourcedefinition.apiextensions.k8s.io/kafkamirrormakers.kafka.strimzi.io created 20# clusterrole.rbac.authorization.k8s.io/strimzi-entity-operator created 21# clusterrole.rbac.authorization.k8s.io/strimzi-kafka-broker created 22# customresourcedefinition.apiextensions.k8s.io/kafkaconnects.kafka.strimzi.io created 23# customresourcedefinition.apiextensions.k8s.io/kafkamirrormaker2s.kafka.strimzi.io created 24# customresourcedefinition.apiextensions.k8s.io/kafkausers.kafka.strimzi.io created 25# customresourcedefinition.apiextensions.k8s.io/kafkaconnectors.kafka.strimzi.io created 26# customresourcedefinition.apiextensions.k8s.io/kafkas.kafka.strimzi.io created 27# serviceaccount/strimzi-cluster-operator created 28# customresourcedefinition.apiextensions.k8s.io/kafkatopics.kafka.strimzi.io created 29# clusterrolebinding.rbac.authorization.k8s.io/strimzi-cluster-operator created 30# clusterrole.rbac.authorization.k8s.io/strimzi-cluster-operator-global created 31# clusterrolebinding.rbac.authorization.k8s.io/strimzi-cluster-operator-kafka-client-delegation created 32# customresourcedefinition.apiextensions.k8s.io/kafkabridges.kafka.strimzi.io created 33# clusterrole.rbac.authorization.k8s.io/strimzi-cluster-operator-namespaced created We can check the Strimzi Operator runs as a Deployment.\n1kubectl get deploy,rs,po 2# NAME READY UP-TO-DATE AVAILABLE AGE 3# deployment.apps/strimzi-cluster-operator 1/1 1 1 3m44s 4 5# NAME DESIRED CURRENT READY AGE 6# replicaset.apps/strimzi-cluster-operator-7cc4b5759c 1 1 1 3m44s 7 8# NAME READY STATUS RESTARTS AGE 9# pod/strimzi-cluster-operator-7cc4b5759c-q45nv 1/1 Running 0 3m43s Deploy Kafka Cluster We deploy a Kafka cluster with two brokers and one Zookeeper node. It has both internal and external listeners on port 9092 and 29092 respectively. Note that the external listener is configured as the nodeport type so that the associating service can be accessed from the host machine. Also, the cluster is configured to allow automatic creation of topics (auto.create.topics.enable: \u0026ldquo;true\u0026rdquo;) and the default number of partition is set to 3 (num.partitions: 3).\n1# manifests/kafka-cluster.yaml 2apiVersion: kafka.strimzi.io/v1beta2 3kind: Kafka 4metadata: 5 name: demo-cluster 6spec: 7 kafka: 8 replicas: 2 9 version: 2.8.1 10 listeners: 11 - name: plain 12 port: 9092 13 type: internal 14 tls: false 15 - name: external 16 port: 29092 17 type: nodeport 18 tls: false 19 storage: 20 type: jbod 21 volumes: 22 - id: 0 23 type: persistent-claim 24 size: 20Gi 25 deleteClaim: true 26 config: 27 offsets.topic.replication.factor: 1 28 transaction.state.log.replication.factor: 1 29 transaction.state.log.min.isr: 1 30 default.replication.factor: 1 31 min.insync.replicas: 1 32 inter.broker.protocol.version: 2.8 33 auto.create.topics.enable: \u0026#34;true\u0026#34; 34 num.partitions: 3 35 zookeeper: 36 replicas: 1 37 storage: 38 type: persistent-claim 39 size: 10Gi 40 deleteClaim: true The Kafka cluster can be deployed using the kubectl create command as shown below.\n1kubectl create -f manifests/kafka-cluster.yaml The Kafka and Zookeeper nodes are deployed as StatefulSets as it provides guarantees about the ordering and uniqueness of the associating Pods. It also creates multiple services, and we use the following services in this series.\ncommunication within Kubernetes cluster demo-cluster-kafka-bootstrap - to access Kafka brokers from the client and management apps demo-cluster-zookeeper-client - to access Zookeeper node from the management app communication from host demo-cluster-kafka-external-bootstrap - to access Kafka brokers from the client apps 1kubectl get all -l app.kubernetes.io/instance=demo-cluster 2# NAME READY STATUS RESTARTS AGE 3# pod/demo-cluster-kafka-0 1/1 Running 0 67s 4# pod/demo-cluster-kafka-1 1/1 Running 0 67s 5# pod/demo-cluster-zookeeper-0 1/1 Running 0 109s 6 7# NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE 8# service/demo-cluster-kafka-bootstrap ClusterIP 10.106.148.7 \u0026lt;none\u0026gt; 9091/TCP,9092/TCP 67s 9# service/demo-cluster-kafka-brokers ClusterIP None \u0026lt;none\u0026gt; 9090/TCP,9091/TCP,9092/TCP 67s 10# service/demo-cluster-kafka-external-0 NodePort 10.109.118.191 \u0026lt;none\u0026gt; 29092:31733/TCP 67s 11# service/demo-cluster-kafka-external-1 NodePort 10.105.36.159 \u0026lt;none\u0026gt; 29092:31342/TCP 67s 12# service/demo-cluster-kafka-external-bootstrap NodePort 10.97.88.48 \u0026lt;none\u0026gt; 29092:30247/TCP 67s 13# service/demo-cluster-zookeeper-client ClusterIP 10.97.131.183 \u0026lt;none\u0026gt; 2181/TCP 109s 14# service/demo-cluster-zookeeper-nodes ClusterIP None \u0026lt;none\u0026gt; 2181/TCP,2888/TCP,3888/TCP 109s 15 16# NAME READY AGE 17# statefulset.apps/demo-cluster-kafka 2/2 67s 18# statefulset.apps/demo-cluster-zookeeper 1/1 109s Deploy Kafka UI UI for Apache Kafka (kafka-ui) is a free and open-source Kafka management application, and it is deployed as a Kubernetes Deployment. The Deployment is configured to have a single instance, and the Kafka cluster access details are specified as environment variables. The app is associated by a service of the NodePort type for external access.\n1# manifests/kafka-ui.yaml 2apiVersion: v1 3kind: Service 4metadata: 5 labels: 6 app: kafka-ui 7 name: kafka-ui 8spec: 9 type: NodePort 10 ports: 11 - port: 8080 12 targetPort: 8080 13 selector: 14 app: kafka-ui 15--- 16apiVersion: apps/v1 17kind: Deployment 18metadata: 19 labels: 20 app: kafka-ui 21 name: kafka-ui 22spec: 23 replicas: 1 24 selector: 25 matchLabels: 26 app: kafka-ui 27 template: 28 metadata: 29 labels: 30 app: kafka-ui 31 spec: 32 containers: 33 - image: provectuslabs/kafka-ui:v0.7.1 34 name: kafka-ui-container 35 ports: 36 - containerPort: 8080 37 env: 38 - name: KAFKA_CLUSTERS_0_NAME 39 value: demo-cluster 40 - name: KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS 41 value: demo-cluster-kafka-bootstrap:9092 42 - name: KAFKA_CLUSTERS_0_ZOOKEEPER 43 value: demo-cluster-zookeeper-client:2181 44 resources: {} The Kafka management app (kafka-ui) can be deployed using the kubectl create command as shown below.\n1kubectl create -f manifests/kafka-ui.yaml 2 3kubectl get all -l app=kafka-ui 4# NAME READY STATUS RESTARTS AGE 5# pod/kafka-ui-6d55c756b8-tznlp 1/1 Running 0 54s 6 7# NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE 8# service/kafka-ui NodePort 10.99.20.37 \u0026lt;none\u0026gt; 8080:31119/TCP 54s 9 10# NAME READY UP-TO-DATE AVAILABLE AGE 11# deployment.apps/kafka-ui 1/1 1 1 54s 12 13# NAME DESIRED CURRENT READY AGE 14# replicaset.apps/kafka-ui-6d55c756b8 1 1 1 54s We can use the minikube service command to obtain the Kubernetes URL for the kafka-ui service.\n1minikube service kafka-ui --url 2# http://127.0.0.1:38257 3# ❗ Because you are using a Docker driver on linux, the terminal needs to be open to run it. Produce and Consume Messages We can produce and consume messages with Kafka command utilities by creating two Pods in separate terminals. The Pods start the producer (kafka-console-producer.sh) and consumer (kafka-console-consumer.sh) scripts respectively while specifying necessary arguments such as the bootstrap server address and topic name. We can see that the records created by the producer are appended in the consumer terminal.\n1kubectl run kafka-producer --image=quay.io/strimzi/kafka:0.27.1-kafka-2.8.1 --rm -it --restart=Never \\ 2 -- bin/kafka-console-producer.sh --bootstrap-server demo-cluster-kafka-bootstrap:9092 --topic demo-topic 3 4# \u0026gt;product: apples, quantity: 5 5# \u0026gt;product: lemons, quantity: 7 1kubectl run kafka-consumer --image=quay.io/strimzi/kafka:0.27.1-kafka-2.8.1 --rm -it --restart=Never \\ 2 -- bin/kafka-console-consumer.sh --bootstrap-server demo-cluster-kafka-bootstrap:9092 --topic demo-topic --from-beginning 3 4# product: apples, quantity: 5 5# product: lemons, quantity: 7 We can also check the messages in the management app as shown below.\nDelete Resources The Kubernetes resources and Minikube cluster can be removed by the kubectl delete and minikube delete commands respectively.\n1## delete resources 2kubectl delete -f manifests/kafka-cluster.yaml 3kubectl delete -f manifests/kafka-ui.yaml 4kubectl delete -f manifests/strimzi-cluster-operator-$STRIMZI_VERSION.yaml 5 6## delete minikube 7minikube delete Summary Apache Kafka is one of the key technologies for implementing data streaming architectures. Strimzi provides a way to run an Apache Kafka cluster and related resources on Kubernetes in various deployment configurations. In this post, we discussed how to deploy a Kafka cluster on a Minikube cluster using the Strimzi Operator. Also, it is demonstrated how to produce and consume messages using the Kafka command line utilities.\n","date":"December 21, 2023","img":"/blog/2023-12-21-kafka-development-on-k8s-part-1/featured.png","lang":"en","langName":"English","largeImg":"/blog/2023-12-21-kafka-development-on-k8s-part-1/featured_hu216964ff6c7a4996550e95c3e0e5f209_108975_500x0_resize_box_3.png","permalink":"/blog/2023-12-21-kafka-development-on-k8s-part-1/","series":[{"title":"Kafka Development on Kubernetes","url":"/series/kafka-development-on-kubernetes/"}],"smallImg":"/blog/2023-12-21-kafka-development-on-k8s-part-1/featured_hu216964ff6c7a4996550e95c3e0e5f209_108975_180x0_resize_box_3.png","tags":[{"title":"Apache Kafka","url":"/tags/apache-kafka/"},{"title":"Kubernetes","url":"/tags/kubernetes/"},{"title":"Minikube","url":"/tags/minikube/"},{"title":"Strimzi","url":"/tags/strimzi/"},{"title":"Docker","url":"/tags/docker/"}],"timestamp":1703116800,"title":"Kafka Development on Kubernetes - Part 1 Cluster Setup"},{"categories":[{"title":"Data Streaming","url":"/categories/data-streaming/"}],"content":"Amazon MSK can be configured as an event source of a Lambda function. Lambda internally polls for new messages from the event source and then synchronously invokes the target Lambda function. With this feature, we can develop a Kafka consumer application in serverless environment where developers can focus on application logic. In this lab, we will discuss how to create a Kafka consumer using a Lambda function.\nIntroduction Lab 1 Produce data to Kafka using Lambda Lab 2 Write data to Kafka from S3 using Flink Lab 3 Transform and write data to S3 from Kafka using Flink Lab 4 Clean, Aggregate, and Enrich Events with Flink Lab 5 Write data to DynamoDB using Kafka Connect Lab 6 Consume data from Kafka using Lambda (this post) Architecture Fake taxi ride data is sent to a Kafka topic by the Kafka producer application that is discussed in Lab 1. The messages of the taxi-rides topic are consumed by a Lambda function where the MSK cluster is configured as an event source of the function.\nInfrastructure The AWS infrastructure is created using Terraform and the source can be found in the GitHub repository of this post. See this earlier post for details about how to create the resources. The key resources cover a VPC, VPN server, MSK cluster and Python Lambda producer app.\nLambda Kafka Consumer The Kafka consumer Lambda function is created additionally for this lab, and it is deployed conditionally by a flag variable called consumer_to_create. Once it is set to true, the function is created by the AWS Lambda Terraform module while referring to the associating configuration variables (local.consumer.*). Note that an event source mapping is created on the Lambda function where the event source is set to the Kafka cluster on Amazon MSK. We can also control which topics to poll by adding one or more topic names in the topics attribute - only a single topic named taxi-rides is specified for this lab.\n1# infra/variables.tf 2variable \u0026#34;consumer_to_create\u0026#34; { 3 description = \u0026#34;Flag to indicate whether to create Kafka consumer\u0026#34; 4 type = bool 5 default = false 6} 7 8... 9 10locals { 11 ... 12 consumer = { 13 to_create = var.consumer_to_create 14 src_path = \u0026#34;../consumer\u0026#34; 15 function_name = \u0026#34;kafka_consumer\u0026#34; 16 handler = \u0026#34;app.lambda_function\u0026#34; 17 timeout = 600 18 memory_size = 128 19 runtime = \u0026#34;python3.8\u0026#34; 20 topic_name = \u0026#34;taxi-rides\u0026#34; 21 starting_position = \u0026#34;TRIM_HORIZON\u0026#34; 22 } 23 ... 24} 25 26# infra/consumer.tf 27module \u0026#34;kafka_consumer\u0026#34; { 28 source = \u0026#34;terraform-aws-modules/lambda/aws\u0026#34; 29 version = \u0026#34;\u0026gt;=5.1.0, \u0026lt;6.0.0\u0026#34; 30 31 create = local.consumer.to_create 32 33 function_name = local.consumer.function_name 34 handler = local.consumer.handler 35 runtime = local.consumer.runtime 36 timeout = local.consumer.timeout 37 memory_size = local.consumer.memory_size 38 source_path = local.consumer.src_path 39 vpc_subnet_ids = module.vpc.private_subnets 40 vpc_security_group_ids = local.consumer.to_create ? [aws_security_group.kafka_consumer[0].id] : null 41 attach_network_policy = true 42 attach_policies = true 43 policies = local.consumer.to_create ? [aws_iam_policy.kafka_consumer[0].arn] : null 44 number_of_policies = 1 45 46 depends_on = [ 47 aws_msk_cluster.msk_data_cluster 48 ] 49 50 tags = local.tags 51} 52 53resource \u0026#34;aws_lambda_event_source_mapping\u0026#34; \u0026#34;kafka_consumer\u0026#34; { 54 count = local.consumer.to_create ? 1 : 0 55 event_source_arn = aws_msk_cluster.msk_data_cluster.arn 56 function_name = module.kafka_consumer.lambda_function_name 57 topics = [local.consumer.topic_name] 58 starting_position = local.consumer.starting_position 59 amazon_managed_kafka_event_source_config { 60 consumer_group_id = \u0026#34;${local.consumer.topic_name}-group-01\u0026#34; 61 } 62} 63 64resource \u0026#34;aws_lambda_permission\u0026#34; \u0026#34;kafka_consumer\u0026#34; { 65 count = local.consumer.to_create ? 1 : 0 66 statement_id = \u0026#34;InvokeLambdaFunction\u0026#34; 67 action = \u0026#34;lambda:InvokeFunction\u0026#34; 68 function_name = local.consumer.function_name 69 principal = \u0026#34;kafka.amazonaws.com\u0026#34; 70 source_arn = aws_msk_cluster.msk_data_cluster.arn 71} IAM Permission The consumer Lambda function needs permission to read messages to the Kafka topic. The following IAM policy is added to the Lambda function as illustrated in this AWS documentation.\n1# infra/consumer.tf 2resource \u0026#34;aws_iam_policy\u0026#34; \u0026#34;kafka_consumer\u0026#34; { 3 count = local.consumer.to_create ? 1 : 0 4 name = \u0026#34;${local.consumer.function_name}-msk-lambda-permission\u0026#34; 5 6 policy = jsonencode({ 7 Version = \u0026#34;2012-10-17\u0026#34; 8 Statement = [ 9 { 10 Sid = \u0026#34;PermissionOnKafkaCluster\u0026#34; 11 Action = [ 12 \u0026#34;kafka-cluster:Connect\u0026#34;, 13 \u0026#34;kafka-cluster:DescribeGroup\u0026#34;, 14 \u0026#34;kafka-cluster:AlterGroup\u0026#34;, 15 \u0026#34;kafka-cluster:DescribeTopic\u0026#34;, 16 \u0026#34;kafka-cluster:ReadData\u0026#34;, 17 \u0026#34;kafka-cluster:DescribeClusterDynamicConfiguration\u0026#34; 18 ] 19 Effect = \u0026#34;Allow\u0026#34; 20 Resource = [ 21 \u0026#34;arn:aws:kafka:${local.region}:${data.aws_caller_identity.current.account_id}:cluster/${local.name}-msk-cluster/*\u0026#34;, 22 \u0026#34;arn:aws:kafka:${local.region}:${data.aws_caller_identity.current.account_id}:topic/${local.name}-msk-cluster/*\u0026#34;, 23 \u0026#34;arn:aws:kafka:${local.region}:${data.aws_caller_identity.current.account_id}:group/${local.name}-msk-cluster/*\u0026#34; 24 ] 25 }, 26 { 27 Sid = \u0026#34;PermissionOnKafka\u0026#34; 28 Action = [ 29 \u0026#34;kafka:DescribeCluster\u0026#34;, 30 \u0026#34;kafka:GetBootstrapBrokers\u0026#34; 31 ] 32 Effect = \u0026#34;Allow\u0026#34; 33 Resource = \u0026#34;*\u0026#34; 34 }, 35 { 36 Sid = \u0026#34;PermissionOnNetwork\u0026#34; 37 Action = [ 38 # The first three actions also exist in netwrok policy attachment in lambda module 39 # \u0026#34;ec2:CreateNetworkInterface\u0026#34;, 40 # \u0026#34;ec2:DescribeNetworkInterfaces\u0026#34;, 41 # \u0026#34;ec2:DeleteNetworkInterface\u0026#34;, 42 \u0026#34;ec2:DescribeVpcs\u0026#34;, 43 \u0026#34;ec2:DescribeSubnets\u0026#34;, 44 \u0026#34;ec2:DescribeSecurityGroups\u0026#34; 45 ] 46 Effect = \u0026#34;Allow\u0026#34; 47 Resource = \u0026#34;*\u0026#34; 48 } 49 ] 50 }) 51} Function Source The records attribute of the Lambda event payload includes Kafka consumer records. Each of the records contains details of the Amazon MSK topic and partition identifier, together with a timestamp and a base64-encoded message (key and value). The Lambda function simply prints those records after decoding the message key and value as well as formatting the timestamp into the ISO format.\n1# consumer/app.py 2import json 3import base64 4import datetime 5 6 7class ConsumerRecord: 8 def __init__(self, record: dict): 9 self.topic = record[\u0026#34;topic\u0026#34;] 10 self.partition = record[\u0026#34;partition\u0026#34;] 11 self.offset = record[\u0026#34;offset\u0026#34;] 12 self.timestamp = record[\u0026#34;timestamp\u0026#34;] 13 self.timestamp_type = record[\u0026#34;timestampType\u0026#34;] 14 self.key = record[\u0026#34;key\u0026#34;] 15 self.value = record[\u0026#34;value\u0026#34;] 16 self.headers = record[\u0026#34;headers\u0026#34;] 17 18 def parse_record( 19 self, 20 to_str: bool = True, 21 to_json: bool = True, 22 ): 23 rec = { 24 **self.__dict__, 25 **{ 26 \u0026#34;key\u0026#34;: json.loads(base64.b64decode(self.key).decode()), 27 \u0026#34;value\u0026#34;: json.loads(base64.b64decode(self.value).decode()), 28 \u0026#34;timestamp\u0026#34;: ConsumerRecord.format_timestamp(self.timestamp, to_str), 29 }, 30 } 31 return json.dumps(rec, default=ConsumerRecord.serialize) if to_json else rec 32 33 @staticmethod 34 def format_timestamp(value, to_str: bool = True): 35 ts = datetime.datetime.fromtimestamp(value / 1000) 36 return ts.isoformat(timespec=\u0026#34;milliseconds\u0026#34;) if to_str else ts 37 38 @staticmethod 39 def serialize(obj): 40 if isinstance(obj, datetime.datetime): 41 return obj.isoformat() 42 if isinstance(obj, datetime.date): 43 return str(obj) 44 return obj 45 46 47def lambda_function(event, context): 48 for _, records in event[\u0026#34;records\u0026#34;].items(): 49 for record in records: 50 cr = ConsumerRecord(record) 51 print(cr.parse_record()) Deployment The infrastructure can be deployed (as well as destroyed) using Terraform CLI. Note that the Lambda consumer function is created only when the consumer_to_create variable is set to true.\n1# initialize 2terraform init 3# create an execution plan 4terraform plan -var \u0026#39;producer_to_create=true\u0026#39; -var \u0026#39;consumer_to_create=true\u0026#39; 5# execute the actions proposed in a Terraform plan 6terraform apply -auto-approve=true -var \u0026#39;producer_to_create=true\u0026#39; -var \u0026#39;consumer_to_create=true\u0026#39; 7 8# destroy all remote objects 9# terraform destroy -auto-approve=true -var \u0026#39;producer_to_create=true\u0026#39; -var \u0026#39;consumer_to_create=true\u0026#39; Once the resources are deployed, we can check the Lambda function on AWS Console. Note that the MSK cluster is configured as the Lambda trigger as expected.\nApplication Result Kafka Topic We can see the topic (taxi-rides) is created, and the details of the topic can be found on the Topics menu on localhost:3000. Note that, if the Kafka monitoring app (kpow) is not started, we can run it using compose-ui.yml - see this post for details about kpow configuration.\nConsumer Output We can check the outputs of the Lambda function on CloudWatch Logs. As expected, the message key and value are decoded properly.\nSummary Amazon MSK can be configured as an event source of a Lambda function. Lambda internally polls for new messages from the event source and then synchronously invokes the target Lambda function. With this feature, we can develop a Kafka consumer application in serverless environment where developers can focus on application logic. In this lab, we discussed how to create a Kafka consumer using a Lambda function.\n","date":"December 14, 2023","img":"/blog/2023-12-14-real-time-streaming-with-kafka-and-flink-7/featured.png","lang":"en","langName":"English","largeImg":"/blog/2023-12-14-real-time-streaming-with-kafka-and-flink-7/featured_hu093e82cf28681343fcef47b169b89c91_138986_500x0_resize_box_3.png","permalink":"/blog/2023-12-14-real-time-streaming-with-kafka-and-flink-7/","series":[{"title":"Real Time Streaming With Kafka and Flink","url":"/series/real-time-streaming-with-kafka-and-flink/"}],"smallImg":"/blog/2023-12-14-real-time-streaming-with-kafka-and-flink-7/featured_hu093e82cf28681343fcef47b169b89c91_138986_180x0_resize_box_3.png","tags":[{"title":"AWS","url":"/tags/aws/"},{"title":"Amazon MSK","url":"/tags/amazon-msk/"},{"title":"AWS Lambda","url":"/tags/aws-lambda/"},{"title":"Apache Kafka","url":"/tags/apache-kafka/"},{"title":"Docker","url":"/tags/docker/"},{"title":"Docker Compose","url":"/tags/docker-compose/"},{"title":"Python","url":"/tags/python/"}],"timestamp":1702512000,"title":"Real Time Streaming With Kafka and Flink - Lab 6 Consume Data From Kafka Using Lambda"},{"categories":[{"title":"Data Engineering","url":"/categories/data-engineering/"}],"content":"Apache Flink became generally available for Amazon EMR on EKS from the EMR 6.15.0 releases, and we are able to pull the Flink (as well as Spark) container images from the ECR Public Gallery. As both of them can be integrated with the Glue Data Catalog, it can be particularly useful if we develop real time data ingestion/processing via Flink and build analytical queries using Spark (or any other tools or services that can access to the Glue Data Catalog).\nIn this post, we will discuss how to set up a local development environment for Apache Flink and Spark using the EMR container images. For the former, a custom Docker image will be created, which downloads dependent connector Jar files into the Flink library folder, fixes process startup issues, and updates Hadoop configurations for Glue Data Catalog integration. For the latter, instead of creating a custom image, the EMR image is used to launch the Spark container where the required configuration updates are added at runtime via volume-mapping. After illustrating the environment setup, we will discuss a solution where data ingestion/processing is performed in real time using Apache Flink and the processed data is consumed by Apache Spark for analysis.\nArchitecture A PyFlink application produces messages into a Kafka topic and those messages are read and processed by another Flink application. For simplicity, the processor just buffers the messages and writes into S3 in Apache Hive style partitions. The sink (target) table is registered in the Glue Data Catalog for sharing the table details with other tools and services. A PySpark application is used to consume the processed data, which queries the Glue table using Spark SQL.\nInfrastructure We create a Flink cluster, Spark container, Kafka cluster and Kafka management app. They are deployed using Docker Compose and the source can be found in the GitHub repository of this post.\nFlink Setup Custom Docker Image Flink 1.17.1 is installed in the EMR Flink image (public.ecr.aws/emr-on-eks/flink/emr-6.15.0-flink:latest) and a custom Docker image is created, which extends it. It begins with downloading dependent Jar files into the Flink library folder (/usr/lib/flink/lib), which are in relation to the Kafka and Flink Faker connectors.\nWhen I started to run the Flink SQL client, I encountered a number of process startup issues. First, I had a runtime exception whose error message is java.lang.RuntimeException: Could not find a free permitted port on the machine. When it gets started, it reserves a port and writes the port details into a folder via the getAvailablePort method of the NetUtils class. Unlike the official Flink Docker image where the details are written to the /tmp folder, the EMR image writes into the /mnt/tmp folder, and it throws an error due to insufficient permission. I was able to fix the issue by creating the /mnt/tmp folder beforehand. Secondly, I also had additional issues that were caused by the NoClassDefFoundError, and they were fixed by adding the Javax Inject and AOP Alliance Jar files into the Flink library folder.\nFor Glue Data Catalog integration, we need Hadoop configuration. The EMR image keeps core-site.xml in the /glue/confs/hadoop/conf folder, and I had to update the file. Specifically I updated the credentials providers from WebIdentityTokenCredentialsProvider to EnvironmentVariableCredentialsProvider. In this way, we are able to access AWS services with AWS credentials in environment variables - the updated Hadoop configuration file can be found below. I also created the /mnt/s3 folder as it is specified as the S3 buffer directory in core-site.xml.\n1# dockers/flink/Dockerfile 2FROM public.ecr.aws/emr-on-eks/flink/emr-6.15.0-flink:latest 3 4ARG FLINK_VERSION 5ENV FLINK_VERSION=${FLINK_VERSION:-1.17.1} 6ARG KAFKA_VERSION 7ENV KAFKA_VERSION=${KAFKA_VERSION:-3.2.3} 8ARG FAKER_VERSION 9ENV FAKER_VERSION=${FAKER_VERSION:-0.5.3} 10 11## 12## add connectors (Kafka and flink faker) and related dependencies 13## 14RUN curl -o /usr/lib/flink/lib/flink-connector-kafka-${FLINK_VERSION}.jar \\ 15 https://repo.maven.apache.org/maven2/org/apache/flink/flink-connector-kafka/${FLINK_VERSION}/flink-connector-kafka-${FLINK_VERSION}.jar \\ 16 \u0026amp;\u0026amp; curl -o /usr/lib/flink/lib/kafka-clients-${KAFKA_VERSION}.jar \\ 17 https://repo.maven.apache.org/maven2/org/apache/kafka/kafka-clients/${KAFKA_VERSION}/kafka-clients-${KAFKA_VERSION}.jar \\ 18 \u0026amp;\u0026amp; curl -o /usr/lib/flink/lib/flink-sql-connector-kafka-${FLINK_VERSION}.jar \\ 19 https://repo.maven.apache.org/maven2/org/apache/flink/flink-sql-connector-kafka/${FLINK_VERSION}/flink-sql-connector-kafka-${FLINK_VERSION}.jar \\ 20 \u0026amp;\u0026amp; curl -L -o /usr/lib/flink/lib/flink-faker-${FAKER_VERSION}.jar \\ 21 https://github.com/knaufk/flink-faker/releases/download/v${FAKER_VERSION}/flink-faker-${FAKER_VERSION}.jar 22 23## 24## fix process startup issues 25## 26# should be able to write a file in /mnt/tmp as getAvailablePort() in NetUtils class writes to /mnt/tmp instead of /tmp 27# see https://stackoverflow.com/questions/77539526/fail-to-start-flink-sql-client-on-emr-on-eks-docker-image 28RUN mkdir -p /mnt/tmp 29 30## add missing jar files 31RUN curl -L -o /usr/lib/flink/lib/javax.inject-1.jar \\ 32 https://repo1.maven.org/maven2/javax/inject/javax.inject/1/javax.inject-1.jar \\ 33 \u0026amp;\u0026amp; curl -L -o /usr/lib/flink/lib/aopalliance-1.0.jar \\ 34 https://repo1.maven.org/maven2/aopalliance/aopalliance/1.0/aopalliance-1.0.jar 35 36## 37## update hadoop configuration for Glue data catalog integration 38## 39## create /mnt/s3 (value of fs.s3.buffer.dir) beforehand 40RUN mkdir -p /mnt/s3 41 42## copy updated core-site.xml 43## update credentials providers and value of fs.s3.buffer.dir to /mnt/s3 only 44USER root 45 46COPY ./core-site.xml /glue/confs/hadoop/conf/core-site.xml 47 48USER flink Here is the updated Hadoop configuration file that is baked into the custom Docker image.\n1\u0026lt;!-- dockers/flink/core-site.xml --\u0026gt; 2\u0026lt;configuration\u0026gt; 3 \u0026lt;property\u0026gt; 4 \u0026lt;name\u0026gt;fs.s3a.aws.credentials.provider\u0026lt;/name\u0026gt; 5 \u0026lt;value\u0026gt;com.amazonaws.auth.EnvironmentVariableCredentialsProvider\u0026lt;/value\u0026gt; 6 \u0026lt;/property\u0026gt; 7 8 \u0026lt;property\u0026gt; 9 \u0026lt;name\u0026gt;fs.s3.customAWSCredentialsProvider\u0026lt;/name\u0026gt; 10 \u0026lt;value\u0026gt;com.amazonaws.auth.EnvironmentVariableCredentialsProvider\u0026lt;/value\u0026gt; 11 \u0026lt;/property\u0026gt; 12 13 \u0026lt;property\u0026gt; 14 \u0026lt;name\u0026gt;fs.s3.impl\u0026lt;/name\u0026gt; 15 \u0026lt;value\u0026gt;com.amazon.ws.emr.hadoop.fs.EmrFileSystem\u0026lt;/value\u0026gt; 16 \u0026lt;/property\u0026gt; 17 18 \u0026lt;property\u0026gt; 19 \u0026lt;name\u0026gt;fs.s3n.impl\u0026lt;/name\u0026gt; 20 \u0026lt;value\u0026gt;com.amazon.ws.emr.hadoop.fs.EmrFileSystem\u0026lt;/value\u0026gt; 21 \u0026lt;/property\u0026gt; 22 23 \u0026lt;property\u0026gt; 24 \u0026lt;name\u0026gt;fs.AbstractFileSystem.s3.impl\u0026lt;/name\u0026gt; 25 \u0026lt;value\u0026gt;org.apache.hadoop.fs.s3.EMRFSDelegate\u0026lt;/value\u0026gt; 26 \u0026lt;/property\u0026gt; 27 28 \u0026lt;property\u0026gt; 29 \u0026lt;name\u0026gt;fs.s3.buffer.dir\u0026lt;/name\u0026gt; 30 \u0026lt;value\u0026gt;/mnt/s3\u0026lt;/value\u0026gt; 31 \u0026lt;final\u0026gt;true\u0026lt;/final\u0026gt; 32 \u0026lt;/property\u0026gt; 33\u0026lt;/configuration\u0026gt; The Docker image can be built using the following command.\n1$ docker build -t emr-6.15.0-flink:local dockers/flink/. Docker Compose Services The Flink cluster is made up of a single master container (jobmanager) and one task container (taskmanager). The master container opens up the port 8081, and we are able to access the Flink Web UI on localhost:8081. Also, the current folder is volume-mapped into the /home/flink/project folder, and it allows us to submit a Flink application in the host folder to the Flink cluster.\nOther than the environment variables of the Kafka bootstrap server addresses and AWS credentials, the following environment variables are important for deploying the Flink cluster and running Flink jobs without an issue.\nK8S_FLINK_GLUE_ENABLED If this environment variable exists, the container entrypoint file (docker-entrypoint.sh) configures Apache Hive. It moves the Hive/Glue related dependencies into the Flink library folder (/usr/lib/flink/lib) for setting up Hive Catalog, which is integrated with the Glue Data Catalog. K8S_FLINK_LOG_URL_STDERR and K8S_FLINK_LOG_URL_STDOUT The container entrypoint file (docker-entrypoint.sh) creates these folders, but I had an error due to insufficient permission. Therefore, I changed the values of those folders within the /tmp folder. HADOOP_CONF_DIR This variable is required when setting up a Hive catalog, or we can add it as an option when creating a catalog (hadoop-config-dir). FLINK_PROPERTIES The properties will be appended into the Flink configuration file (/usr/lib/flink/conf/flink-conf.yaml). Among those, jobmanager.memory.process.size and taskmanager.memory.process.size are mandatory for the containers run without failure. 1# docker-compose.yml 2version: \u0026#34;3.5\u0026#34; 3 4services: 5 jobmanager: 6 image: emr-6.15.0-flink:local 7 container_name: jobmanager 8 command: jobmanager 9 ports: 10 - \u0026#34;8081:8081\u0026#34; 11 networks: 12 - appnet 13 volumes: 14 - ./:/home/flink/project 15 environment: 16 - RUNTIME_ENV=DOCKER 17 - BOOTSTRAP_SERVERS=kafka-0:9092 18 - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID:-not_set} 19 - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY:-not_set} 20 - AWS_REGION=${AWS_REGION:-not_set} 21 - K8S_FLINK_GLUE_ENABLED=true 22 - K8S_FLINK_LOG_URL_STDERR=/tmp/stderr 23 - K8S_FLINK_LOG_URL_STDOUT=/tmp/stdout 24 - HADOOP_CONF_DIR=/glue/confs/hadoop/conf 25 - | 26 FLINK_PROPERTIES= 27 jobmanager.rpc.address: jobmanager 28 state.backend: filesystem 29 state.checkpoints.dir: file:///tmp/flink-checkpoints 30 heartbeat.interval: 1000 31 heartbeat.timeout: 5000 32 rest.flamegraph.enabled: true 33 web.backpressure.refresh-interval: 10000 34 jobmanager.memory.process.size: 1600m 35 taskmanager.memory.process.size: 1728m 36 taskmanager: 37 image: emr-6.15.0-flink:local 38 container_name: taskmanager 39 command: taskmanager 40 networks: 41 - appnet 42 volumes: 43 - ./:/home/flink/project 44 - flink_data:/tmp/ 45 environment: 46 - RUNTIME_ENV=DOCKER 47 - BOOTSTRAP_SERVERS=kafka-0:9092 48 - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID:-not_set} 49 - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY:-not_set} 50 - AWS_REGION=${AWS_REGION:-not_set} 51 - K8S_FLINK_GLUE_ENABLED=true 52 - K8S_FLINK_LOG_URL_STDERR=/tmp/stderr 53 - K8S_FLINK_LOG_URL_STDOUT=/tmp/stdout 54 - HADOOP_CONF_DIR=/glue/confs/hadoop/conf 55 - | 56 FLINK_PROPERTIES= 57 jobmanager.rpc.address: jobmanager 58 taskmanager.numberOfTaskSlots: 5 59 state.backend: filesystem 60 state.checkpoints.dir: file:///tmp/flink-checkpoints 61 heartbeat.interval: 1000 62 heartbeat.timeout: 5000 63 jobmanager.memory.process.size: 1600m 64 taskmanager.memory.process.size: 1728m 65 depends_on: 66 - jobmanager 67 68 ... 69 70networks: 71 appnet: 72 name: app-network 73 74volumes: 75 flink_data: 76 driver: local 77 name: flink_data 78 ... Spark Setup In an earlier post, I illustrated how to set up a local development environment using an EMR container image. That post is based the Dev Containers extension of VS Code, and it is assumed that development takes place after attaching the project folder into a Docker container. Thanks to the Docker extension, however, we no longer have to attach the project folder into a container always because the extension allows us to do so with just a few mouse clicks if necessary - see the screenshot below. Moreover, as Spark applications developed in the host folder can easily be submitted to the Spark container via volume-mapping, we can simplify Spark setup dramatically without creating a custom Docker image. Therefore, Spark will be set up using the EMR image where updated Spark configuration files and the project folder are volume-mapped to the container. Also, the Spark History Server will be running in the container, which allows us to monitor completed and running Spark applications.\nConfiguration Updates Same to Flink Hadoop configuration updates, we need to update credentials providers from WebIdentityTokenCredentialsProvider to EnvironmentVariableCredentialsProvider to access AWS services with AWS credentials in environment variables. Also, we should specify the catalog implementation to hive and set AWSGlueDataCatalogHiveClientFactory as the Hive metastore factory class.\n1# dockers/spark/spark-defaults.conf 2 3... 4 5## 6## Update credentials providers 7## 8spark.hadoop.fs.s3.customAWSCredentialsProvider com.amazonaws.auth.EnvironmentVariableCredentialsProvider 9spark.hadoop.dynamodb.customAWSCredentialsProvider com.amazonaws.auth.EnvironmentVariableCredentialsProvider 10# spark.authenticate true 11 12## 13## Update to use Glue catalog 14## 15spark.sql.catalogImplementation hive 16spark.hadoop.hive.metastore.client.factory.class com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory Besides, I updated the log properties so that the log level of the root logger is set to be warn and warning messages due to EC2 metadata access failure are not logged.\n1# dockers/spark/log4j2.properties 2 3... 4 5# Set everything to be logged to the console 6rootLogger.level = warn 7 8... 9 10## Ignore warn messages related to EC2 metadata access failure 11logger.InstanceMetadataServiceResourceFetcher.name = com.amazonaws.internal.InstanceMetadataServiceResourceFetcher 12logger.InstanceMetadataServiceResourceFetcher.level = fatal 13logger.EC2MetadataUtils.name = com.amazonaws.util.EC2MetadataUtils 14logger.EC2MetadataUtils.level = fatal Docker Compose Service Spark 3.4.1 is installed in the EMR image (public.ecr.aws/emr-on-eks/spark/emr-6.15.0:latest) and the Spark container is created with it. It starts the Spark History Server, which provides an interface to debug and diagnose completed and running Spark applications. Note that the server is configured to run in foreground (SPARK_NO_DAEMONIZE=true) in order for the container to keep alive. As mentioned, the updated Spark configuration files and the project folder are volume-mapped.\n1# docker-compose.yml 2version: \u0026#34;3.5\u0026#34; 3 4services: 5 6 ... 7 8 spark: 9 image: public.ecr.aws/emr-on-eks/spark/emr-6.15.0:latest 10 container_name: spark 11 command: /usr/lib/spark/sbin/start-history-server.sh 12 ports: 13 - \u0026#34;18080:18080\u0026#34; 14 networks: 15 - appnet 16 environment: 17 - BOOTSTRAP_SERVERS=kafka-0:9092 18 - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID:-not_set} 19 - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY:-not_set} 20 - AWS_REGION=${AWS_REGION:-not_set} 21 - SPARK_NO_DAEMONIZE=true 22 volumes: 23 - ./:/home/hadoop/project 24 - ./dockers/spark/spark-defaults.conf:/usr/lib/spark/conf/spark-defaults.conf 25 - ./dockers/spark/log4j2.properties:/usr/lib/spark/conf/log4j2.properties 26 27 ... 28 29networks: 30 appnet: 31 name: app-network 32 33 ... Kafka Setup Docker Compose Services A Kafka cluster with a single broker and zookeeper node is used in this post. The broker has two listeners and the port 9092 and 29092 are used for internal and external communication respectively. The default number of topic partitions is set to 3. More details about Kafka cluster setup can be found in this post.\nThe UI for Apache Kafka (kafka-ui) is used for monitoring Kafka topics and related resources. The bootstrap server address and zookeeper access url are added as environment variables. See this post for details about Kafka management apps.\n1# docker-compose.yml 2version: \u0026#34;3.5\u0026#34; 3 4services: 5 6 ... 7 8 zookeeper: 9 image: bitnami/zookeeper:3.5 10 container_name: zookeeper 11 ports: 12 - \u0026#34;2181\u0026#34; 13 networks: 14 - appnet 15 environment: 16 - ALLOW_ANONYMOUS_LOGIN=yes 17 volumes: 18 - zookeeper_data:/bitnami/zookeeper 19 kafka-0: 20 image: bitnami/kafka:2.8.1 21 container_name: kafka-0 22 expose: 23 - 9092 24 ports: 25 - \u0026#34;29092:29092\u0026#34; 26 networks: 27 - appnet 28 environment: 29 - ALLOW_PLAINTEXT_LISTENER=yes 30 - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181 31 - KAFKA_CFG_BROKER_ID=0 32 - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT 33 - KAFKA_CFG_LISTENERS=INTERNAL://:9092,EXTERNAL://:29092 34 - KAFKA_CFG_ADVERTISED_LISTENERS=INTERNAL://kafka-0:9092,EXTERNAL://localhost:29092 35 - KAFKA_CFG_INTER_BROKER_LISTENER_NAME=INTERNAL 36 - KAFKA_CFG_NUM_PARTITIONS=3 37 - KAFKA_CFG_DEFAULT_REPLICATION_FACTOR=1 38 volumes: 39 - kafka_0_data:/bitnami/kafka 40 depends_on: 41 - zookeeper 42 kafka-ui: 43 image: provectuslabs/kafka-ui:v0.7.1 44 container_name: kafka-ui 45 ports: 46 - \u0026#34;8080:8080\u0026#34; 47 networks: 48 - appnet 49 environment: 50 KAFKA_CLUSTERS_0_NAME: local 51 KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka-0:9092 52 KAFKA_CLUSTERS_0_ZOOKEEPER: zookeeper:2181 53 depends_on: 54 - zookeeper 55 - kafka-0 56 57networks: 58 appnet: 59 name: app-network 60 61volumes: 62 ... 63 zookeeper_data: 64 driver: local 65 name: zookeeper_data 66 kafka_0_data: 67 driver: local 68 name: kafka_0_data Applications Flink Producer A PyFlink application is created for data ingesting in real time. The app begins with generating timestamps using the DataGen SQL Connector where three records are generated per second. Then extra values (id and value) are added to the records using Python user defined functions and the updated records are ingested into a Kafka topic named orders. Note that the output records are printed in the terminal additionally when the app is running locally for ease of checking them.\n1# apps/flink/producer.py 2import os 3import uuid 4import random 5 6from pyflink.datastream import StreamExecutionEnvironment, RuntimeExecutionMode 7from pyflink.table import StreamTableEnvironment 8from pyflink.table.udf import udf 9 10def _qry_source_table(): 11 stmt = \u0026#34;\u0026#34;\u0026#34; 12 CREATE TABLE seeds ( 13 ts AS PROCTIME() 14 ) WITH ( 15 \u0026#39;connector\u0026#39; = \u0026#39;datagen\u0026#39;, 16 \u0026#39;rows-per-second\u0026#39; = \u0026#39;3\u0026#39; 17 ) 18 \u0026#34;\u0026#34;\u0026#34; 19 print(stmt) 20 return stmt 21 22def _qry_sink_table(table_name: str, topic_name: str, bootstrap_servers: str): 23 stmt = f\u0026#34;\u0026#34;\u0026#34; 24 CREATE TABLE {table_name} ( 25 `id` VARCHAR, 26 `value` INT, 27 `ts` TIMESTAMP(3) 28 ) WITH ( 29 \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39;, 30 \u0026#39;topic\u0026#39; = \u0026#39;{topic_name}\u0026#39;, 31 \u0026#39;properties.bootstrap.servers\u0026#39; = \u0026#39;{bootstrap_servers}\u0026#39;, 32 \u0026#39;format\u0026#39; = \u0026#39;json\u0026#39;, 33 \u0026#39;key.format\u0026#39; = \u0026#39;json\u0026#39;, 34 \u0026#39;key.fields\u0026#39; = \u0026#39;id\u0026#39;, 35 \u0026#39;properties.allow.auto.create.topics\u0026#39; = \u0026#39;true\u0026#39; 36 ) 37 \u0026#34;\u0026#34;\u0026#34; 38 print(stmt) 39 return stmt 40 41 42def _qry_print_table(): 43 stmt = \u0026#34;\u0026#34;\u0026#34; 44 CREATE TABLE print ( 45 `id` VARCHAR, 46 `value` INT, 47 `ts` TIMESTAMP(3) 48 49 ) WITH ( 50 \u0026#39;connector\u0026#39; = \u0026#39;print\u0026#39; 51 ) 52 \u0026#34;\u0026#34;\u0026#34; 53 print(stmt) 54 return stmt 55 56def _qry_insert(target_table: str): 57 stmt = f\u0026#34;\u0026#34;\u0026#34; 58 INSERT INTO {target_table} 59 SELECT 60 add_id(), 61 add_value(), 62 ts 63 FROM seeds 64 \u0026#34;\u0026#34;\u0026#34; 65 print(stmt) 66 return stmt 67 68if __name__ == \u0026#34;__main__\u0026#34;: 69 RUNTIME_ENV = os.environ.get(\u0026#34;RUNTIME_ENV\u0026#34;, \u0026#34;LOCAL\u0026#34;) # DOCKER or LOCAL 70 BOOTSTRAP_SERVERS = os.environ.get(\u0026#34;BOOTSTRAP_SERVERS\u0026#34;, \u0026#34;localhost:29092\u0026#34;) # overwrite app config 71 72 env = StreamExecutionEnvironment.get_execution_environment() 73 env.set_runtime_mode(RuntimeExecutionMode.STREAMING) 74 if RUNTIME_ENV == \u0026#34;LOCAL\u0026#34;: 75 SRC_DIR = os.path.dirname(os.path.realpath(__file__)) 76 JAR_FILES = [\u0026#34;flink-sql-connector-kafka-1.17.1.jar\u0026#34;] # should exist where producer.py exists 77 JAR_PATHS = tuple( 78 [f\u0026#34;file://{os.path.join(SRC_DIR, name)}\u0026#34; for name in JAR_FILES] 79 ) 80 env.add_jars(*JAR_PATHS) 81 print(JAR_PATHS) 82 83 t_env = StreamTableEnvironment.create(stream_execution_environment=env) 84 t_env.get_config().set_local_timezone(\u0026#34;Australia/Sydney\u0026#34;) 85 t_env.create_temporary_function( 86 \u0026#34;add_id\u0026#34;, udf(lambda: str(uuid.uuid4()), result_type=\u0026#34;STRING\u0026#34;) 87 ) 88 t_env.create_temporary_function( 89 \u0026#34;add_value\u0026#34;, udf(lambda: random.randrange(0, 1000), result_type=\u0026#34;INT\u0026#34;) 90 ) 91 ## create source and sink tables 92 SINK_TABLE_NAME = \u0026#34;orders_table\u0026#34; 93 t_env.execute_sql(_qry_source_table()) 94 t_env.execute_sql(_qry_sink_table(SINK_TABLE_NAME, \u0026#34;orders\u0026#34;, BOOTSTRAP_SERVERS)) 95 t_env.execute_sql(_qry_print_table()) 96 ## insert into sink table 97 if RUNTIME_ENV == \u0026#34;LOCAL\u0026#34;: 98 statement_set = t_env.create_statement_set() 99 statement_set.add_insert_sql(_qry_insert(SINK_TABLE_NAME)) 100 statement_set.add_insert_sql(_qry_insert(\u0026#34;print\u0026#34;)) 101 statement_set.execute().wait() 102 else: 103 table_result = t_env.execute_sql(_qry_insert(SINK_TABLE_NAME)) 104 print(table_result.get_job_client().get_job_status()) The application can be submitted into the Flink cluster as shown below. Note that the dependent Jar files for the Kafka connector exist in the Flink library folder (/usr/lib/flink/lib) and we don\u0026rsquo;t have to specify them separately.\n1docker exec jobmanager /usr/lib/flink/bin/flink run \\ 2 --python /home/flink/project/apps/flink/producer.py \\ 3 -d Once the app runs, we can see the status of the Flink job on the Flink Web UI (localhost:8081).\nAlso, we can check the topic (orders) is created and messages are ingested on kafka-ui (localhost:8080).\nFlink Processor The Flink processor application is created using Flink SQL on the Flink SQL client. The SQL client can be started by executing docker exec -it jobmanager ./bin/sql-client.sh.\nSource Table in Default Catalog We first create a source table that reads messages from the orders topic of the Kafka cluster. As the table is not necessarily be shared by other tools or services, it is created on the default catalog, not on the Glue catalog.\n1-- apps/flink/processor.sql 2-- // create the source table, metadata not registered in glue datalog 3CREATE TABLE source_tbl( 4 `id` STRING, 5 `value` INT, 6 `ts` TIMESTAMP(3) 7) WITH ( 8 \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39;, 9 \u0026#39;topic\u0026#39; = \u0026#39;orders\u0026#39;, 10 \u0026#39;properties.bootstrap.servers\u0026#39; = \u0026#39;kafka-0:9092\u0026#39;, 11 \u0026#39;properties.group.id\u0026#39; = \u0026#39;orders-source\u0026#39;, 12 \u0026#39;format\u0026#39; = \u0026#39;json\u0026#39;, 13 \u0026#39;scan.startup.mode\u0026#39; = \u0026#39;latest-offset\u0026#39; 14); Sink Table in Glue Catalog We create the sink table on the Glue Data Catalog because it needs to be accessed by a Spark application. First, we create a Hive catalog named glue_catalog with the Hive configuration that integrates with the Glue Data Catalog. The EMR Flink image includes the required Hive configuration file, and we can specify the corresponding path in the container (/glue/confs/hive/conf).\n1-- apps/flink/processor.sql 2--// create a hive catalogs that integrates with the glue catalog 3CREATE CATALOG glue_catalog WITH ( 4 \u0026#39;type\u0026#39; = \u0026#39;hive\u0026#39;, 5 \u0026#39;default-database\u0026#39; = \u0026#39;default\u0026#39;, 6 \u0026#39;hive-conf-dir\u0026#39; = \u0026#39;/glue/confs/hive/conf\u0026#39; 7); 8 9-- Flink SQL\u0026gt; show catalogs; 10-- +-----------------+ 11-- | catalog name | 12-- +-----------------+ 13-- | default_catalog | 14-- | glue_catalog | 15-- +-----------------+ Below shows the Hive configuration file (/glue/confs/hive/conf/hive-site.xml). Same as the Spark configuration, AWSGlueDataCatalogHiveClientFactory is specified as the Hive metastore factory class, which enables to use the Glue Data Catalog as the metastore of Hive databases and tables.\n1\u0026lt;configuration\u0026gt; 2 \u0026lt;property\u0026gt; 3 \u0026lt;name\u0026gt;hive.metastore.client.factory.class\u0026lt;/name\u0026gt; 4 \u0026lt;value\u0026gt;com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory\u0026lt;/value\u0026gt; 5 \u0026lt;/property\u0026gt; 6 7 \u0026lt;property\u0026gt; 8 \u0026lt;name\u0026gt;hive.metastore.uris\u0026lt;/name\u0026gt; 9 \u0026lt;value\u0026gt;thrift://dummy:9083\u0026lt;/value\u0026gt; 10 \u0026lt;/property\u0026gt; 11\u0026lt;/configuration\u0026gt; Secondly, we create a database named demo by specifying the S3 location URI - s3://demo-ap-southeast-2/warehouse/.\n1-- apps/flink/processor.sql 2-- // create a database named demo 3CREATE DATABASE IF NOT EXISTS glue_catalog.demo 4 WITH (\u0026#39;hive.database.location-uri\u0026#39;= \u0026#39;s3://demo-ap-southeast-2/warehouse/\u0026#39;); Once succeeded, we are able to see the database is created in the Glue Data Catalog.\nFinally, we create the sink table in the Glue database. The Hive SQL dialect is used to create the table, and it is partitioned by year, month, date and hour.\n1-- apps/flink/processor.sql 2-- // create the sink table using hive dialect 3SET table.sql-dialect=hive; 4CREATE TABLE glue_catalog.demo.sink_tbl( 5 `id` STRING, 6 `value` INT, 7 `ts` TIMESTAMP(9) 8) 9PARTITIONED BY (`year` STRING, `month` STRING, `date` STRING, `hour` STRING) 10STORED AS parquet 11TBLPROPERTIES ( 12 \u0026#39;partition.time-extractor.timestamp-pattern\u0026#39;=\u0026#39;$year-$month-$date $hour:00:00\u0026#39;, 13 \u0026#39;sink.partition-commit.trigger\u0026#39;=\u0026#39;partition-time\u0026#39;, 14 \u0026#39;sink.partition-commit.delay\u0026#39;=\u0026#39;1 h\u0026#39;, 15 \u0026#39;sink.partition-commit.policy.kind\u0026#39;=\u0026#39;metastore,success-file\u0026#39; 16); We can check the sink table is created in the Glue database on AWS Console as shown below.\nFlink Job The Flink processor job gets submitted when we execute the INSERT statement on the SQL client. Note that the checkpoint interval is set to 60 seconds to simplify monitoring, and it is expected that a new file is added per minute.\n1-- apps/flink/processor.sql 2SET \u0026#39;state.checkpoints.dir\u0026#39; = \u0026#39;file:///tmp/checkpoints/\u0026#39;; 3SET \u0026#39;execution.checkpointing.interval\u0026#39; = \u0026#39;60000\u0026#39;; 4 5SET table.sql-dialect=hive; 6-- // insert into the sink table 7INSERT INTO TABLE glue_catalog.demo.sink_tbl 8SELECT 9 `id`, 10 `value`, 11 `ts`, 12 DATE_FORMAT(`ts`, \u0026#39;yyyy\u0026#39;) AS `year`, 13 DATE_FORMAT(`ts`, \u0026#39;MM\u0026#39;) AS `month`, 14 DATE_FORMAT(`ts`, \u0026#39;dd\u0026#39;) AS `date`, 15 DATE_FORMAT(`ts`, \u0026#39;HH\u0026#39;) AS `hour` 16FROM source_tbl; Once the Flink app is submitted, we can check the Flink job on the Flink Web UI (localhost:8081).\nAs expected, the output files are written into S3 in Apache Hive style partitions, and they are created in one minute interval.\nSpark Consumer A simple PySpark application is created to query the output table. Note that, as the partitions are not added dynamically, MSCK REPAIR TABLE command is executed before querying the table.\n1# apps/spark/consumer.py 2from pyspark.sql import SparkSession 3 4if __name__ == \u0026#34;__main__\u0026#34;: 5 spark = SparkSession.builder.appName(\u0026#34;Consume Orders\u0026#34;).getOrCreate() 6 spark.sparkContext.setLogLevel(\u0026#34;FATAL\u0026#34;) 7 spark.sql(\u0026#34;MSCK REPAIR TABLE demo.sink_tbl\u0026#34;) 8 spark.sql(\u0026#34;SELECT * FROM demo.sink_tbl\u0026#34;).show() The Spark app can be submitted as shown below. Note that the application is accessible in the container because the project folder is volume-mapped to the container folder (/home/hadoop/project).\n1## spark submit 2docker exec spark spark-submit \\ 3 --master local[*] \\ 4 --deploy-mode client \\ 5 /home/hadoop/project/apps/spark/consumer.py The app queries the output table successfully and shows the result as expected.\nWe can check the performance of the Spark application on the Spark History Server (localhost:18080).\nSummary In this post, we discussed how to set up a local development environment for Apache Flink and Spark using the EMR container images. For the former, a custom Docker image was created, which downloads dependent connector Jar files into the Flink library folder, fixes process startup issues, and updates Hadoop configurations for Glue Data Catalog integration. For the latter, instead of creating a custom image, the EMR image was used to launch the Spark container where the required configuration updates are added at runtime via volume-mapping. After illustrating the environment setup, we discussed a solution where data ingestion/processing is performed in real time using Apache Flink and the processed data is consumed by Apache Spark for analysis.\n","date":"December 7, 2023","img":"/blog/2023-12-07-flink-spark-local-dev/featured.png","lang":"en","langName":"English","largeImg":"/blog/2023-12-07-flink-spark-local-dev/featured_hu7a73c8ca7d8b8a4ddf49d1a9b8fe40bd_133053_500x0_resize_box_3.png","permalink":"/blog/2023-12-07-flink-spark-local-dev/","series":[],"smallImg":"/blog/2023-12-07-flink-spark-local-dev/featured_hu7a73c8ca7d8b8a4ddf49d1a9b8fe40bd_133053_180x0_resize_box_3.png","tags":[{"title":"Apache Flink","url":"/tags/apache-flink/"},{"title":"PyFlink","url":"/tags/pyflink/"},{"title":"Apache Spark","url":"/tags/apache-spark/"},{"title":"PySpark","url":"/tags/pyspark/"},{"title":"Apache Kafka","url":"/tags/apache-kafka/"},{"title":"Amazon EMR","url":"/tags/amazon-emr/"},{"title":"Docker","url":"/tags/docker/"},{"title":"Docker Compose","url":"/tags/docker-compose/"},{"title":"Python","url":"/tags/python/"}],"timestamp":1701907200,"title":"Setup Local Development Environment for Apache Flink and Spark Using EMR Container Images"},{"categories":[{"title":"Data Streaming","url":"/categories/data-streaming/"}],"content":"Kafka Connect is a tool for scalably and reliably streaming data between Apache Kafka and other systems. It makes it simple to quickly define connectors that move large collections of data into and out of Kafka. In this lab, we will discuss how to create a data pipeline that ingests data from a Kafka topic into a DynamoDB table using the Camel DynamoDB sink connector.\nIntroduction Lab 1 Produce data to Kafka using Lambda Lab 2 Write data to Kafka from S3 using Flink Lab 3 Transform and write data to S3 from Kafka using Flink Lab 4 Clean, Aggregate, and Enrich Events with Flink Lab 5 Write data to DynamoDB using Kafka Connect (this post) Lab 6 Consume data from Kafka using Lambda Architecture Fake taxi ride data is sent to a Kafka topic by the Kafka producer application that is discussed in Lab 1. The messages of the topic are written into a DynamoDB table by a Kafka sink connector, which is deployed on Amazon MSK Connect.\nInfrastructure The AWS infrastructure is created using Terraform and the source can be found in the GitHub repository of this post. See this earlier post for details about how to create the resources. The key resources cover a VPC, VPN server, MSK cluster and Python Lambda producer app.\nMSK Connect For this lab, a Kafka sink connector and DynamoDB table are created additionally, and their details are illustrated below.\nDownload Connector Source Before we deploy the sink connector, its source should be downloaded into the infra/connectors folder. From there, the source can be saved into a S3 bucket followed by being used to create a custom plugin. The connector source has multiple Jar files, and they should be compressed as the zip format. The archive file can be created by executing the download.sh file.\n1# download.sh 2#!/usr/bin/env bash 3SCRIPT_DIR=\u0026#34;$(cd $(dirname \u0026#34;$0\u0026#34;); pwd)\u0026#34; 4 5SRC_PATH=${SCRIPT_DIR}/infra/connectors 6rm -rf ${SRC_PATH} \u0026amp;\u0026amp; mkdir -p ${SRC_PATH} 7 8## Download camel dynamodb sink connector 9echo \u0026#34;download camel dynamodb sink connector...\u0026#34; 10DOWNLOAD_URL=https://repo.maven.apache.org/maven2/org/apache/camel/kafkaconnector/camel-aws-ddb-sink-kafka-connector/3.20.3/camel-aws-ddb-sink-kafka-connector-3.20.3-package.tar.gz 11 12# decompress and zip contents to create custom plugin of msk connect later 13curl -o ${SRC_PATH}/camel-aws-ddb-sink-kafka-connector.tar.gz ${DOWNLOAD_URL} \\ 14 \u0026amp;\u0026amp; tar -xvzf ${SRC_PATH}/camel-aws-ddb-sink-kafka-connector.tar.gz -C ${SRC_PATH} \\ 15 \u0026amp;\u0026amp; cd ${SRC_PATH}/camel-aws-ddb-sink-kafka-connector \\ 16 \u0026amp;\u0026amp; zip -r camel-aws-ddb-sink-kafka-connector.zip . \\ 17 \u0026amp;\u0026amp; mv camel-aws-ddb-sink-kafka-connector.zip ${SRC_PATH} \\ 18 \u0026amp;\u0026amp; rm ${SRC_PATH}/camel-aws-ddb-sink-kafka-connector.tar.gz Below shows the sink connector source. As mentioned, the zip file will be used to create a custom plugin. Note that the unarchived connect source is kept because we can use it on a local Kafka Connect server deployed on Docker, which can be used for local development.\n1$ tree infra/connectors -P \u0026#39;camel-aws-ddb-sink-kafka-connector*\u0026#39; -I \u0026#39;docs\u0026#39; 2infra/connectors 3├── camel-aws-ddb-sink-kafka-connector 4│ └── camel-aws-ddb-sink-kafka-connector-3.20.3.jar 5└── camel-aws-ddb-sink-kafka-connector.zip DynamoDB Sink Connector The connector is configured to write messages from the taxi-rides topic into a DynamoDB table. It requires to specify the table name, AWS region, operation, write capacity and whether to use the default credential provider - see the documentation for details. Note that, if you don\u0026rsquo;t use the default credential provider, you have to specify the access key id and secret access key. Note also that the camel.sink.unmarshal option is to convert data from the internal java.util.HashMap into the required java.io.InputStream. Without this configuration, the connector fails with the org.apache.camel.NoTypeConversionAvailableException error.\n1# infra/msk-connect.tf 2resource \u0026#34;aws_mskconnect_connector\u0026#34; \u0026#34;taxi_rides_sink\u0026#34; { 3 count = local.connect.to_create ? 1 : 0 4 5 name = \u0026#34;${local.name}-taxi-rides-sink\u0026#34; 6 7 kafkaconnect_version = \u0026#34;2.7.1\u0026#34; 8 9 capacity { 10 provisioned_capacity { 11 mcu_count = 1 12 worker_count = 1 13 } 14 } 15 16 connector_configuration = { 17 # connector configuration 18 \u0026#34;connector.class\u0026#34; = \u0026#34;org.apache.camel.kafkaconnector.awsddbsink.CamelAwsddbsinkSinkConnector\u0026#34;, 19 \u0026#34;tasks.max\u0026#34; = \u0026#34;1\u0026#34;, 20 \u0026#34;key.converter\u0026#34; = \u0026#34;org.apache.kafka.connect.storage.StringConverter\u0026#34;, 21 \u0026#34;key.converter.schemas.enable\u0026#34; = false, 22 \u0026#34;value.converter\u0026#34; = \u0026#34;org.apache.kafka.connect.json.JsonConverter\u0026#34;, 23 \u0026#34;value.converter.schemas.enable\u0026#34; = false, 24 # camel ddb sink configuration 25 \u0026#34;topics\u0026#34; = \u0026#34;taxi-rides\u0026#34;, 26 \u0026#34;camel.kamelet.aws-ddb-sink.table\u0026#34; = aws_dynamodb_table.taxi_rides.id, 27 \u0026#34;camel.kamelet.aws-ddb-sink.region\u0026#34; = local.region, 28 \u0026#34;camel.kamelet.aws-ddb-sink.operation\u0026#34; = \u0026#34;PutItem\u0026#34;, 29 \u0026#34;camel.kamelet.aws-ddb-sink.writeCapacity\u0026#34; = 1, 30 \u0026#34;camel.kamelet.aws-ddb-sink.useDefaultCredentialsProvider\u0026#34; = true, 31 \u0026#34;camel.sink.unmarshal\u0026#34; = \u0026#34;jackson\u0026#34; 32 } 33 34 kafka_cluster { 35 apache_kafka_cluster { 36 bootstrap_servers = aws_msk_cluster.msk_data_cluster.bootstrap_brokers_sasl_iam 37 38 vpc { 39 security_groups = [aws_security_group.msk.id] 40 subnets = module.vpc.private_subnets 41 } 42 } 43 } 44 45 kafka_cluster_client_authentication { 46 authentication_type = \u0026#34;IAM\u0026#34; 47 } 48 49 kafka_cluster_encryption_in_transit { 50 encryption_type = \u0026#34;TLS\u0026#34; 51 } 52 53 plugin { 54 custom_plugin { 55 arn = aws_mskconnect_custom_plugin.camel_ddb_sink[0].arn 56 revision = aws_mskconnect_custom_plugin.camel_ddb_sink[0].latest_revision 57 } 58 } 59 60 log_delivery { 61 worker_log_delivery { 62 cloudwatch_logs { 63 enabled = true 64 log_group = aws_cloudwatch_log_group.camel_ddb_sink[0].name 65 } 66 s3 { 67 enabled = true 68 bucket = aws_s3_bucket.default_bucket.id 69 prefix = \u0026#34;logs/msk/connect/taxi-rides-sink\u0026#34; 70 } 71 } 72 } 73 74 service_execution_role_arn = aws_iam_role.kafka_connector_role[0].arn 75} 76 77resource \u0026#34;aws_mskconnect_custom_plugin\u0026#34; \u0026#34;camel_ddb_sink\u0026#34; { 78 count = local.connect.to_create ? 1 : 0 79 80 name = \u0026#34;${local.name}-camel-ddb-sink\u0026#34; 81 content_type = \u0026#34;ZIP\u0026#34; 82 83 location { 84 s3 { 85 bucket_arn = aws_s3_bucket.default_bucket.arn 86 file_key = aws_s3_object.camel_ddb_sink[0].key 87 } 88 } 89} 90 91resource \u0026#34;aws_s3_object\u0026#34; \u0026#34;camel_ddb_sink\u0026#34; { 92 count = local.connect.to_create ? 1 : 0 93 94 bucket = aws_s3_bucket.default_bucket.id 95 key = \u0026#34;plugins/camel-aws-ddb-sink-kafka-connector.zip\u0026#34; 96 source = \u0026#34;connectors/camel-aws-ddb-sink-kafka-connector.zip\u0026#34; 97 98 etag = filemd5(\u0026#34;connectors/camel-aws-ddb-sink-kafka-connector.zip\u0026#34;) 99} 100 101resource \u0026#34;aws_cloudwatch_log_group\u0026#34; \u0026#34;camel_ddb_sink\u0026#34; { 102 count = local.connect.to_create ? 1 : 0 103 104 name = \u0026#34;/msk/connect/camel-ddb-sink\u0026#34; 105 106 retention_in_days = 1 107 108 tags = local.tags 109} Connector IAM Role The managed policy of the connector role has permission on MSK cluster resources (cluster, topic and group). It also has permission on S3 bucket and CloudWatch Log for logging. Finally, as the connector should be able to create records in a DynamoDB table, the DynamoDB full access policy (AmazonDynamoDBFullAccess) is attached to the role. Note that the DynamoDB policy is too generous, and it is recommended limiting its scope in production environment.\n1# infra/msk-connect.tf 2resource \u0026#34;aws_iam_role\u0026#34; \u0026#34;kafka_connector_role\u0026#34; { 3 count = local.connect.to_create ? 1 : 0 4 5 name = \u0026#34;${local.name}-connector-role\u0026#34; 6 7 assume_role_policy = jsonencode({ 8 Version = \u0026#34;2012-10-17\u0026#34; 9 Statement = [ 10 { 11 Action = \u0026#34;sts:AssumeRole\u0026#34; 12 Effect = \u0026#34;Allow\u0026#34; 13 Sid = \u0026#34;\u0026#34; 14 Principal = { 15 Service = \u0026#34;kafkaconnect.amazonaws.com\u0026#34; 16 } 17 }, 18 ] 19 }) 20 managed_policy_arns = [ 21 \u0026#34;arn:aws:iam::aws:policy/AmazonDynamoDBFullAccess\u0026#34;, 22 aws_iam_policy.kafka_connector_policy[0].arn 23 ] 24} 25 26resource \u0026#34;aws_iam_policy\u0026#34; \u0026#34;kafka_connector_policy\u0026#34; { 27 count = local.connect.to_create ? 1 : 0 28 29 name = \u0026#34;${local.name}-connector-policy\u0026#34; 30 31 policy = jsonencode({ 32 Version = \u0026#34;2012-10-17\u0026#34; 33 Statement = [ 34 { 35 Sid = \u0026#34;PermissionOnCluster\u0026#34; 36 Action = [ 37 \u0026#34;kafka-cluster:Connect\u0026#34;, 38 \u0026#34;kafka-cluster:AlterCluster\u0026#34;, 39 \u0026#34;kafka-cluster:DescribeCluster\u0026#34; 40 ] 41 Effect = \u0026#34;Allow\u0026#34; 42 Resource = \u0026#34;arn:aws:kafka:${local.region}:${data.aws_caller_identity.current.account_id}:cluster/${local.name}-msk-cluster/*\u0026#34; 43 }, 44 { 45 Sid = \u0026#34;PermissionOnTopics\u0026#34; 46 Action = [ 47 \u0026#34;kafka-cluster:*Topic*\u0026#34;, 48 \u0026#34;kafka-cluster:WriteData\u0026#34;, 49 \u0026#34;kafka-cluster:ReadData\u0026#34; 50 ] 51 Effect = \u0026#34;Allow\u0026#34; 52 Resource = \u0026#34;arn:aws:kafka:${local.region}:${data.aws_caller_identity.current.account_id}:topic/${local.name}-msk-cluster/*\u0026#34; 53 }, 54 { 55 Sid = \u0026#34;PermissionOnGroups\u0026#34; 56 Action = [ 57 \u0026#34;kafka-cluster:AlterGroup\u0026#34;, 58 \u0026#34;kafka-cluster:DescribeGroup\u0026#34; 59 ] 60 Effect = \u0026#34;Allow\u0026#34; 61 Resource = \u0026#34;arn:aws:kafka:${local.region}:${data.aws_caller_identity.current.account_id}:group/${local.name}-msk-cluster/*\u0026#34; 62 }, 63 { 64 Sid = \u0026#34;PermissionOnDataBucket\u0026#34; 65 Action = [ 66 \u0026#34;s3:ListBucket\u0026#34;, 67 \u0026#34;s3:*Object\u0026#34; 68 ] 69 Effect = \u0026#34;Allow\u0026#34; 70 Resource = [ 71 \u0026#34;${aws_s3_bucket.default_bucket.arn}\u0026#34;, 72 \u0026#34;${aws_s3_bucket.default_bucket.arn}/*\u0026#34; 73 ] 74 }, 75 { 76 Sid = \u0026#34;LoggingPermission\u0026#34; 77 Action = [ 78 \u0026#34;logs:CreateLogStream\u0026#34;, 79 \u0026#34;logs:CreateLogGroup\u0026#34;, 80 \u0026#34;logs:PutLogEvents\u0026#34; 81 ] 82 Effect = \u0026#34;Allow\u0026#34; 83 Resource = \u0026#34;*\u0026#34; 84 }, 85 ] 86 }) 87} DynamoDB Table A DynamoDB table is defined, and it will be used to export the topic messages. The table has the id attribute as the partition key.\n1# infra/msk-connect.tf 2resource \u0026#34;aws_dynamodb_table\u0026#34; \u0026#34;taxi_rides\u0026#34; { 3 name = \u0026#34;${local.name}-taxi-rides\u0026#34; 4 billing_mode = \u0026#34;PROVISIONED\u0026#34; 5 read_capacity = 1 6 write_capacity = 1 7 hash_key = \u0026#34;id\u0026#34; 8 9 attribute { 10 name = \u0026#34;id\u0026#34; 11 type = \u0026#34;S\u0026#34; 12 } 13 14 tags = local.tags 15} The infrastructure can be deployed (as well as destroyed) using Terraform CLI. Note that the sink connector and DynamoDB table are created only when the connect_to_create variable is set to true.\n1# initialize 2terraform init 3# create an execution plan 4terraform plan -var \u0026#39;producer_to_create=true\u0026#39; -var \u0026#39;connect_to_create=true\u0026#39; 5# execute the actions proposed in a Terraform plan 6terraform apply -auto-approve=true -var \u0026#39;producer_to_create=true\u0026#39; -var \u0026#39;connect_to_create=true\u0026#39; 7 8# destroy all remote objects 9# terraform destroy -auto-approve=true -var \u0026#39;producer_to_create=true\u0026#39; -var \u0026#39;connect_to_create=true\u0026#39; Once the resources are deployed, we can check the sink connector on AWS Console.\nLocal Development (Optional) Create Kafka Connect on Docker As discussed further later, we can use a local Kafka cluster deployed on Docker instead of one on Amazon MSK. For this option, we need to deploy a local Kafka Connect server on Docker as well, and it can be created by the following Docker Compose file. See this post for details about how to set up a Kafka Connect server on Docker.\n1# compose-extra.yml 2version: \u0026#34;3.5\u0026#34; 3 4services: 5 6 ... 7 8 kafka-connect: 9 image: bitnami/kafka:2.8.1 10 container_name: connect 11 command: \u0026gt; 12 /opt/bitnami/kafka/bin/connect-distributed.sh 13 /opt/bitnami/kafka/config/connect-distributed.properties 14 ports: 15 - \u0026#34;8083:8083\u0026#34; 16 networks: 17 - appnet 18 environment: 19 AWS_ACCESS_KEY_ID: $AWS_ACCESS_KEY_ID 20 AWS_SECRET_ACCESS_KEY: $AWS_SECRET_ACCESS_KEY 21 volumes: 22 - \u0026#34;./configs/connect-distributed.properties:/opt/bitnami/kafka/config/connect-distributed.properties\u0026#34; 23 - \u0026#34;./infra/connectors/camel-aws-ddb-sink-kafka-connector:/opt/connectors/camel-aws-ddb-sink-kafka-connector\u0026#34; 24 25networks: 26 appnet: 27 external: true 28 name: app-network 29 30 ... We can create a local Kafka cluster and Kafka Connect server as following.\n1## set aws credentials environment variables 2export AWS_ACCESS_KEY_ID=\u0026lt;aws-access-key-id\u0026gt; 3export AWS_SECRET_ACCESS_KEY=\u0026lt;aws-secret-access-key\u0026gt; 4 5# create kafka cluster 6docker-compose -f compose-local-kafka.yml up -d 7# create kafka connect server 8docker-compose -f compose-extra.yml up -d Create DynamoDB Table with CLI We still need to create a DynamoDB table, and it can be created using the AWS CLI as shown below.\n1// configs/ddb.json 2{ 3 \u0026#34;TableName\u0026#34;: \u0026#34;real-time-streaming-taxi-rides\u0026#34;, 4 \u0026#34;KeySchema\u0026#34;: [{ \u0026#34;AttributeName\u0026#34;: \u0026#34;id\u0026#34;, \u0026#34;KeyType\u0026#34;: \u0026#34;HASH\u0026#34; }], 5 \u0026#34;AttributeDefinitions\u0026#34;: [{ \u0026#34;AttributeName\u0026#34;: \u0026#34;id\u0026#34;, \u0026#34;AttributeType\u0026#34;: \u0026#34;S\u0026#34; }], 6 \u0026#34;ProvisionedThroughput\u0026#34;: { 7 \u0026#34;ReadCapacityUnits\u0026#34;: 1, 8 \u0026#34;WriteCapacityUnits\u0026#34;: 1 9 } 10} 1$ aws dynamodb create-table --cli-input-json file://configs/ddb.json Deploy Sink Connector Locally As Kafka Connect provides a REST API that manages connectors, we can create a connector programmatically. The REST endpoint requires a JSON payload that includes connector configurations.\n1// configs/sink.json 2{ 3 \u0026#34;name\u0026#34;: \u0026#34;real-time-streaming-taxi-rides-sink\u0026#34;, 4 \u0026#34;config\u0026#34;: { 5 \u0026#34;connector.class\u0026#34;: \u0026#34;org.apache.camel.kafkaconnector.awsddbsink.CamelAwsddbsinkSinkConnector\u0026#34;, 6 \u0026#34;tasks.max\u0026#34;: \u0026#34;1\u0026#34;, 7 \u0026#34;key.converter\u0026#34;: \u0026#34;org.apache.kafka.connect.storage.StringConverter\u0026#34;, 8 \u0026#34;key.converter.schemas.enable\u0026#34;: false, 9 \u0026#34;value.converter\u0026#34;: \u0026#34;org.apache.kafka.connect.json.JsonConverter\u0026#34;, 10 \u0026#34;value.converter.schemas.enable\u0026#34;: false, 11 \u0026#34;topics\u0026#34;: \u0026#34;taxi-rides\u0026#34;, 12 13 \u0026#34;camel.kamelet.aws-ddb-sink.table\u0026#34;: \u0026#34;real-time-streaming-taxi-rides\u0026#34;, 14 \u0026#34;camel.kamelet.aws-ddb-sink.region\u0026#34;: \u0026#34;ap-southeast-2\u0026#34;, 15 \u0026#34;camel.kamelet.aws-ddb-sink.operation\u0026#34;: \u0026#34;PutItem\u0026#34;, 16 \u0026#34;camel.kamelet.aws-ddb-sink.writeCapacity\u0026#34;: 1, 17 \u0026#34;camel.kamelet.aws-ddb-sink.useDefaultCredentialsProvider\u0026#34;: true, 18 \u0026#34;camel.sink.unmarshal\u0026#34;: \u0026#34;jackson\u0026#34; 19 } 20} The connector can be created (as well as deleted) using Curl as shown below.\n1# deploy sink connector 2$ curl -i -X POST -H \u0026#34;Accept:application/json\u0026#34; -H \u0026#34;Content-Type:application/json\u0026#34; \\ 3 http://localhost:8083/connectors/ -d @configs/sink.json 4 5# check status 6$ curl http://localhost:8083/connectors/real-time-streaming-taxi-rides-sink/status 7 8# # delete sink connector 9# $ curl -X DELETE http://localhost:8083/connectors/real-time-streaming-taxi-rides-sink Application Result Kafka Topic We can see the topic (taxi-rides) is created, and the details of the topic can be found on the Topics menu on localhost:3000. Note that, if the Kafka monitoring app (kpow) is not started, we can run it using compose-ui.yml - see this post for details about kpow configuration.\nTable Records We can check the ingested records on the DynamoDB table items view. Below shows a list of scanned records.\nSummary Kafka Connect is a tool for scalably and reliably streaming data between Apache Kafka and other systems. It makes it simple to quickly define connectors that move large collections of data into and out of Kafka. In this lab, we discussed how to create a data pipeline that ingests data from a Kafka topic into a DynamoDB table using the Camel DynamoDB sink connector.\n","date":"November 30, 2023","img":"/blog/2023-11-30-real-time-streaming-with-kafka-and-flink-6/featured.png","lang":"en","langName":"English","largeImg":"/blog/2023-11-30-real-time-streaming-with-kafka-and-flink-6/featured_hu102db49e706d92c6c08c818a4ddcaaa6_113252_500x0_resize_box_3.png","permalink":"/blog/2023-11-30-real-time-streaming-with-kafka-and-flink-6/","series":[{"title":"Real Time Streaming With Kafka and Flink","url":"/series/real-time-streaming-with-kafka-and-flink/"}],"smallImg":"/blog/2023-11-30-real-time-streaming-with-kafka-and-flink-6/featured_hu102db49e706d92c6c08c818a4ddcaaa6_113252_180x0_resize_box_3.png","tags":[{"title":"AWS","url":"/tags/aws/"},{"title":"Amazon MSK","url":"/tags/amazon-msk/"},{"title":"Amazon MSK Connect","url":"/tags/amazon-msk-connect/"},{"title":"Amazon DynamoDB","url":"/tags/amazon-dynamodb/"},{"title":"Apache Kafka","url":"/tags/apache-kafka/"},{"title":"Kafka Connect","url":"/tags/kafka-connect/"},{"title":"Docker","url":"/tags/docker/"},{"title":"Docker Compose","url":"/tags/docker-compose/"}],"timestamp":1701302400,"title":"Real Time Streaming With Kafka and Flink - Lab 5 Write Data to DynamoDB Using Kafka Connect"},{"categories":[{"title":"Data Streaming","url":"/categories/data-streaming/"}],"content":"The value of data can be maximised when it is used without delay. With Apache Flink, we can build streaming analytics applications that incorporate the latest events with low latency. In this lab, we will create a Pyflink application that writes accumulated taxi rides data into an OpenSearch cluster. It aggregates the number of trips/passengers and trip durations by vendor ID for a window of 5 seconds. The data is then used to create a chart that monitors the status of taxi rides in the OpenSearch Dashboard.\nIntroduction Lab 1 Produce data to Kafka using Lambda Lab 2 Write data to Kafka from S3 using Flink Lab 3 Transform and write data to S3 from Kafka using Flink Lab 4 Clean, Aggregate, and Enrich Events with Flink (this post) Lab 5 Write data to DynamoDB using Kafka Connect Lab 6 Consume data from Kafka using Lambda Architecture Fake taxi ride data is sent to a Kafka topic by the Kafka producer application that is discussed in Lab 1. The Pyflink app aggregates the number of trips/passengers and trip durations by vendor ID for a window of 5 seconds and sends the accumulated records into an OpenSearch cluster. The data is then used to create a chart that monitors the status of taxi rides in the OpenSearch Dashboard.\nInfrastructure The AWS infrastructure is created using Terraform and the source can be found in the GitHub repository of this post. See this earlier post for details about how to create the resources. The key resources cover a VPC, VPN server, MSK cluster and Python Lambda producer app.\nOpenSearch Cluster For this lab, an OpenSearch cluster is created additionally, and it is deployed with the m5.large.search instance type in private subnets. For simplicity, anonymous authentication is enabled so that we don\u0026rsquo;t have to specify user credentials when making an HTTP request. Overall only network-level security is enforced on the OpenSearch domain. Note that the cluster is created only when the opensearch_to_create variable is set to true.\n1# infra/variables.tf 2variable \u0026#34;opensearch_to_create\u0026#34; { 3 description = \u0026#34;Flag to indicate whether to create OpenSearch cluster\u0026#34; 4 type = bool 5 default = false 6} 7 8... 9 10locals { 11 12 ... 13 14 opensearch = { 15 to_create = var.opensearch_to_create 16 engine_version = \u0026#34;2.7\u0026#34; 17 instance_type = \u0026#34;m5.large.search\u0026#34; 18 instance_count = 2 19 } 20 21 ... 22 23} 24 25# infra/opensearch.tf 26resource \u0026#34;aws_opensearch_domain\u0026#34; \u0026#34;opensearch\u0026#34; { 27 count = local.opensearch.to_create ? 1 : 0 28 29 domain_name = local.name 30 engine_version = \u0026#34;OpenSearch_${local.opensearch.engine_version}\u0026#34; 31 32 cluster_config { 33 dedicated_master_enabled = false 34 instance_type = local.opensearch.instance_type # m5.large.search 35 instance_count = local.opensearch.instance_count # 2 36 zone_awareness_enabled = true 37 } 38 39 advanced_security_options { 40 enabled = false 41 anonymous_auth_enabled = true 42 internal_user_database_enabled = true 43 } 44 45 domain_endpoint_options { 46 enforce_https = true 47 tls_security_policy = \u0026#34;Policy-Min-TLS-1-2-2019-07\u0026#34; 48 custom_endpoint_enabled = false 49 } 50 51 ebs_options { 52 ebs_enabled = true 53 volume_size = 10 54 } 55 56 log_publishing_options { 57 cloudwatch_log_group_arn = aws_cloudwatch_log_group.opensearch_log_group_index_slow_logs[0].arn 58 log_type = \u0026#34;INDEX_SLOW_LOGS\u0026#34; 59 } 60 61 vpc_options { 62 subnet_ids = slice(module.vpc.private_subnets, 0, local.opensearch.instance_count) 63 security_group_ids = [aws_security_group.opensearch[0].id] 64 } 65 66 access_policies = jsonencode({ 67 Version = \u0026#34;2012-10-17\u0026#34; 68 Statement = [ 69 { 70 Action = \u0026#34;es:*\u0026#34;, 71 Principal = \u0026#34;*\u0026#34;, 72 Effect = \u0026#34;Allow\u0026#34;, 73 Resource = \u0026#34;arn:aws:es:${data.aws_region.current.name}:${data.aws_caller_identity.current.account_id}:domain/${local.name}/*\u0026#34; 74 } 75 ] 76 }) 77} OpenSearch Security Group The security group of the OpenSearch domain has inbound rules that allow connection from the security groups of the VPN server. It is important to configure those rules because the Pyflink application will be executed in the developer machine and access to the OpenSearch cluster will be made through the VPN server. Only port 443 and 9200 are open for accessing the OpenSearch Dashboard and making HTTP requests.\n1# infra/opensearch.tf 2resource \u0026#34;aws_security_group\u0026#34; \u0026#34;opensearch\u0026#34; { 3 count = local.opensearch.to_create ? 1 : 0 4 5 name = \u0026#34;${local.name}-opensearch-sg\u0026#34; 6 vpc_id = module.vpc.vpc_id 7 8 lifecycle { 9 create_before_destroy = true 10 } 11 12 tags = local.tags 13} 14 15... 16 17resource \u0026#34;aws_security_group_rule\u0026#34; \u0026#34;opensearch_vpn_inbound_https\u0026#34; { 18 count = local.vpn.to_create \u0026amp;\u0026amp; local.opensearch.to_create ? 1 : 0 19 type = \u0026#34;ingress\u0026#34; 20 description = \u0026#34;Allow inbound traffic for OpenSearch Dashboard from VPN\u0026#34; 21 security_group_id = aws_security_group.opensearch[0].id 22 protocol = \u0026#34;tcp\u0026#34; 23 from_port = 443 24 to_port = 443 25 source_security_group_id = aws_security_group.vpn[0].id 26} 27 28resource \u0026#34;aws_security_group_rule\u0026#34; \u0026#34;opensearch_vpn_inbound_rest\u0026#34; { 29 count = local.vpn.to_create \u0026amp;\u0026amp; local.opensearch.to_create ? 1 : 0 30 type = \u0026#34;ingress\u0026#34; 31 description = \u0026#34;Allow inbound traffic for OpenSearch REST API from VPN\u0026#34; 32 security_group_id = aws_security_group.opensearch[0].id 33 protocol = \u0026#34;tcp\u0026#34; 34 from_port = 9200 35 to_port = 9200 36 source_security_group_id = aws_security_group.vpn[0].id 37} The infrastructure can be deployed (as well as destroyed) using Terraform CLI as shown below.\n1# initialize 2terraform init 3# create an execution plan 4terraform plan -var \u0026#39;producer_to_create=true\u0026#39; -var \u0026#39;opensearch_to_create=true\u0026#39; 5# execute the actions proposed in a Terraform plan 6terraform apply -auto-approve=true -var \u0026#39;producer_to_create=true\u0026#39; -var \u0026#39;opensearch_to_create=true\u0026#39; 7 8# destroy all remote objects 9# terraform destroy -auto-approve=true -var \u0026#39;producer_to_create=true\u0026#39; -var \u0026#39;opensearch_to_create=true\u0026#39; Once the resources are deployed, we can check the OpenSearch cluster on AWS Console as shown below.\nLocal OpenSearch Cluster on Docker (Optional) As discussed further later, we can use a local Kafka cluster deployed on Docker instead of one on Amazon MSK. For this option, we need to deploy a local OpenSearch cluster on Docker and the following Docker Compose file defines a single node OpenSearch Cluster and OpenSearch Dashboard services.\n1# compose-extra.yml 2version: \u0026#34;3.5\u0026#34; 3 4services: 5 opensearch: 6 image: opensearchproject/opensearch:2.7.0 7 container_name: opensearch 8 environment: 9 - discovery.type=single-node 10 - node.name=opensearch 11 - DISABLE_SECURITY_PLUGIN=true 12 - \u0026#34;OPENSEARCH_JAVA_OPTS=-Xms512m -Xmx512m\u0026#34; 13 volumes: 14 - opensearch_data:/usr/share/opensearch/data 15 ports: 16 - 9200:9200 17 - 9600:9600 18 networks: 19 - appnet 20 opensearch-dashboards: 21 image: opensearchproject/opensearch-dashboards:2.7.0 22 container_name: opensearch-dashboards 23 ports: 24 - 5601:5601 25 expose: 26 - \u0026#34;5601\u0026#34; 27 environment: 28 OPENSEARCH_HOSTS: \u0026#39;[\u0026#34;http://opensearch:9200\u0026#34;]\u0026#39; 29 DISABLE_SECURITY_DASHBOARDS_PLUGIN: true 30 networks: 31 - appnet 32 33networks: 34 appnet: 35 external: true 36 name: app-network 37 38volumes: 39 opensearch_data: 40 driver: local 41 name: opensearch_data Flink Cluster on Docker Compose There are two Docker Compose files that deploy a Flink Cluster locally. The first one (compose-msk.yml) relies on the Kafka cluster on Amazon MSK while a local Kafka cluster is created together with a Flink cluster in the second file (compose-local-kafka.yml) - see Lab 2 and Lab 3 respectively for details about them. Note that, if we use a local Kafka and Flink clusters, we don\u0026rsquo;t have to deploy the AWS resources. Instead, we can use a local OpenSearch cluster, and it can be deployed by using compose-extra.yml.\nThe Docker Compose services can be deployed as shown below.\n1## flink cluster with msk cluster 2$ docker-compose -f compose-msk.yml up -d 3 4## local kafka/flink cluster 5$ docker-compose -f compose-local-kafka.yml up -d 6# opensearch cluster 7$ docker-compose -f compose-extra.yml up -d Pyflink Application Flink Pipeline Jar The application has multiple dependencies and a single Jar file is created so that it can be specified in the --jarfile option. Note that we have to build the Jar file based on the Apache Kafka Connector instead of the Apache Kafka SQL Connector because the MSK IAM Auth library is not compatible with the latter due to shade relocation. The dependencies are grouped as shown below.\nKafka Connector flink-connector-base flink-connector-kafka aws-msk-iam-auth (for IAM authentication) OpenSearch Connector flink-connector-opensearch opensearch opensearch-rest-high-level-client org.apache.httpcomponents httpcore-nio A single Jar file is created for this lab as the app may need to be deployed via Amazon Managed Flink potentially. If we do not have to deploy on it, however, they can be added to the Flink library folder (/opt/flink/lib) separately.\nThe single Uber jar file is created by the following POM file and the Jar file is named as lab4-pipeline-1.0.0.jar.\n1\u0026lt;!-- package/lab4-pipeline/pom.xml --\u0026gt; 2\u0026lt;project xmlns=\u0026#34;http://maven.apache.org/POM/4.0.0\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; 3\txsi:schemaLocation=\u0026#34;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\u0026#34;\u0026gt; 4\t\u0026lt;modelVersion\u0026gt;4.0.0\u0026lt;/modelVersion\u0026gt; 5 6\t\u0026lt;groupId\u0026gt;com.amazonaws.services.kinesisanalytics\u0026lt;/groupId\u0026gt; 7\t\u0026lt;artifactId\u0026gt;lab4-pipeline\u0026lt;/artifactId\u0026gt; 8\t\u0026lt;version\u0026gt;1.0.0\u0026lt;/version\u0026gt; 9\t\u0026lt;packaging\u0026gt;jar\u0026lt;/packaging\u0026gt; 10 11\t\u0026lt;name\u0026gt;Uber Jar for Lab 4\u0026lt;/name\u0026gt; 12 13\t\u0026lt;properties\u0026gt; 14\t\u0026lt;project.build.sourceEncoding\u0026gt;UTF-8\u0026lt;/project.build.sourceEncoding\u0026gt; 15\t\u0026lt;flink.version\u0026gt;1.17.1\u0026lt;/flink.version\u0026gt; 16\t\u0026lt;jdk.version\u0026gt;11\u0026lt;/jdk.version\u0026gt; 17\t\u0026lt;kafka.clients.version\u0026gt;3.2.3\u0026lt;/kafka.clients.version\u0026gt; 18\t\u0026lt;log4j.version\u0026gt;2.17.1\u0026lt;/log4j.version\u0026gt; 19\t\u0026lt;aws-msk-iam-auth.version\u0026gt;1.1.7\u0026lt;/aws-msk-iam-auth.version\u0026gt; 20\t\u0026lt;opensearch.connector.version\u0026gt;1.0.1-1.17\u0026lt;/opensearch.connector.version\u0026gt; 21\t\u0026lt;opensearch.version\u0026gt;2.7.0\u0026lt;/opensearch.version\u0026gt; 22\t\u0026lt;httpcore-nio.version\u0026gt;4.4.12\u0026lt;/httpcore-nio.version\u0026gt; 23\t\u0026lt;/properties\u0026gt; 24 25\t\u0026lt;repositories\u0026gt; 26\t\u0026lt;repository\u0026gt; 27\t\u0026lt;id\u0026gt;apache.snapshots\u0026lt;/id\u0026gt; 28\t\u0026lt;name\u0026gt;Apache Development Snapshot Repository\u0026lt;/name\u0026gt; 29\t\u0026lt;url\u0026gt;https://repository.apache.org/content/repositories/snapshots/\u0026lt;/url\u0026gt; 30\t\u0026lt;releases\u0026gt; 31\t\u0026lt;enabled\u0026gt;false\u0026lt;/enabled\u0026gt; 32\t\u0026lt;/releases\u0026gt; 33\t\u0026lt;snapshots\u0026gt; 34\t\u0026lt;enabled\u0026gt;true\u0026lt;/enabled\u0026gt; 35\t\u0026lt;/snapshots\u0026gt; 36\t\u0026lt;/repository\u0026gt; 37\t\u0026lt;/repositories\u0026gt; 38 39\t\u0026lt;dependencies\u0026gt; 40 41\t\u0026lt;!-- Kafkf Connector --\u0026gt; 42 43\t\u0026lt;dependency\u0026gt; 44\t\u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; 45\t\u0026lt;artifactId\u0026gt;flink-connector-base\u0026lt;/artifactId\u0026gt; 46\t\u0026lt;version\u0026gt;${flink.version}\u0026lt;/version\u0026gt; 47\t\u0026lt;/dependency\u0026gt; 48 49\t\u0026lt;dependency\u0026gt; 50\t\u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; 51\t\u0026lt;artifactId\u0026gt;flink-connector-kafka\u0026lt;/artifactId\u0026gt; 52\t\u0026lt;version\u0026gt;${flink.version}\u0026lt;/version\u0026gt; 53\t\u0026lt;/dependency\u0026gt; 54 55\t\u0026lt;dependency\u0026gt; 56\t\u0026lt;groupId\u0026gt;org.apache.kafka\u0026lt;/groupId\u0026gt; 57\t\u0026lt;artifactId\u0026gt;kafka-clients\u0026lt;/artifactId\u0026gt; 58\t\u0026lt;version\u0026gt;${kafka.clients.version}\u0026lt;/version\u0026gt; 59\t\u0026lt;/dependency\u0026gt; 60 61\t\u0026lt;dependency\u0026gt; 62\t\u0026lt;groupId\u0026gt;software.amazon.msk\u0026lt;/groupId\u0026gt; 63\t\u0026lt;artifactId\u0026gt;aws-msk-iam-auth\u0026lt;/artifactId\u0026gt; 64\t\u0026lt;version\u0026gt;${aws-msk-iam-auth.version}\u0026lt;/version\u0026gt; 65\t\u0026lt;/dependency\u0026gt; 66 67\t\u0026lt;!-- OpenSearch --\u0026gt; 68 69\t\u0026lt;dependency\u0026gt; 70\t\u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; 71\t\u0026lt;artifactId\u0026gt;flink-connector-opensearch\u0026lt;/artifactId\u0026gt; 72\t\u0026lt;version\u0026gt;${opensearch.connector.version}\u0026lt;/version\u0026gt; 73\t\u0026lt;/dependency\u0026gt; 74 75\t\u0026lt;dependency\u0026gt; 76\t\u0026lt;groupId\u0026gt;org.opensearch\u0026lt;/groupId\u0026gt; 77\t\u0026lt;artifactId\u0026gt;opensearch\u0026lt;/artifactId\u0026gt; 78\t\u0026lt;version\u0026gt;${opensearch.version}\u0026lt;/version\u0026gt; 79\t\u0026lt;/dependency\u0026gt; 80 81\t\u0026lt;dependency\u0026gt; 82\t\u0026lt;groupId\u0026gt;org.opensearch.client\u0026lt;/groupId\u0026gt; 83\t\u0026lt;artifactId\u0026gt;opensearch-rest-high-level-client\u0026lt;/artifactId\u0026gt; 84\t\u0026lt;version\u0026gt;${opensearch.version}\u0026lt;/version\u0026gt; 85\t\u0026lt;exclusions\u0026gt; 86\t\u0026lt;exclusion\u0026gt; 87\t\u0026lt;groupId\u0026gt;org.apache.httpcomponents\u0026lt;/groupId\u0026gt; 88\t\u0026lt;artifactId\u0026gt;httpcore-nio\u0026lt;/artifactId\u0026gt; 89\t\u0026lt;/exclusion\u0026gt; 90\t\u0026lt;/exclusions\u0026gt; 91\t\u0026lt;/dependency\u0026gt; 92 93\t\u0026lt;!-- We need to include httpcore-nio again in the correct version due to the exclusion above --\u0026gt; 94\t\u0026lt;dependency\u0026gt; 95\t\u0026lt;groupId\u0026gt;org.apache.httpcomponents\u0026lt;/groupId\u0026gt; 96\t\u0026lt;artifactId\u0026gt;httpcore-nio\u0026lt;/artifactId\u0026gt; 97\t\u0026lt;version\u0026gt;${httpcore-nio.version}\u0026lt;/version\u0026gt; 98\t\u0026lt;/dependency\u0026gt; 99 100\t\u0026lt;!-- Add logging framework, to produce console output when running in the IDE. --\u0026gt; 101\t\u0026lt;!-- These dependencies are excluded from the application JAR by default. --\u0026gt; 102\t\u0026lt;dependency\u0026gt; 103\t\u0026lt;groupId\u0026gt;org.apache.logging.log4j\u0026lt;/groupId\u0026gt; 104\t\u0026lt;artifactId\u0026gt;log4j-slf4j-impl\u0026lt;/artifactId\u0026gt; 105\t\u0026lt;version\u0026gt;${log4j.version}\u0026lt;/version\u0026gt; 106\t\u0026lt;scope\u0026gt;runtime\u0026lt;/scope\u0026gt; 107\t\u0026lt;/dependency\u0026gt; 108\t\u0026lt;dependency\u0026gt; 109\t\u0026lt;groupId\u0026gt;org.apache.logging.log4j\u0026lt;/groupId\u0026gt; 110\t\u0026lt;artifactId\u0026gt;log4j-api\u0026lt;/artifactId\u0026gt; 111\t\u0026lt;version\u0026gt;${log4j.version}\u0026lt;/version\u0026gt; 112\t\u0026lt;scope\u0026gt;runtime\u0026lt;/scope\u0026gt; 113\t\u0026lt;/dependency\u0026gt; 114\t\u0026lt;dependency\u0026gt; 115\t\u0026lt;groupId\u0026gt;org.apache.logging.log4j\u0026lt;/groupId\u0026gt; 116\t\u0026lt;artifactId\u0026gt;log4j-core\u0026lt;/artifactId\u0026gt; 117\t\u0026lt;version\u0026gt;${log4j.version}\u0026lt;/version\u0026gt; 118\t\u0026lt;scope\u0026gt;runtime\u0026lt;/scope\u0026gt; 119\t\u0026lt;/dependency\u0026gt; 120\t\u0026lt;/dependencies\u0026gt; 121 122\t\u0026lt;build\u0026gt; 123\t\u0026lt;plugins\u0026gt; 124 125\t\u0026lt;!-- Java Compiler --\u0026gt; 126\t\u0026lt;plugin\u0026gt; 127\t\u0026lt;groupId\u0026gt;org.apache.maven.plugins\u0026lt;/groupId\u0026gt; 128\t\u0026lt;artifactId\u0026gt;maven-compiler-plugin\u0026lt;/artifactId\u0026gt; 129\t\u0026lt;version\u0026gt;3.8.0\u0026lt;/version\u0026gt; 130\t\u0026lt;configuration\u0026gt; 131\t\u0026lt;source\u0026gt;${jdk.version}\u0026lt;/source\u0026gt; 132\t\u0026lt;target\u0026gt;${jdk.version}\u0026lt;/target\u0026gt; 133\t\u0026lt;/configuration\u0026gt; 134\t\u0026lt;/plugin\u0026gt; 135 136\t\u0026lt;!-- We use the maven-shade plugin to create a fat jar that contains all necessary dependencies. --\u0026gt; 137\t\u0026lt;!-- Change the value of \u0026lt;mainClass\u0026gt;...\u0026lt;/mainClass\u0026gt; if your program entry point changes. --\u0026gt; 138\t\u0026lt;plugin\u0026gt; 139\t\u0026lt;groupId\u0026gt;org.apache.maven.plugins\u0026lt;/groupId\u0026gt; 140\t\u0026lt;artifactId\u0026gt;maven-shade-plugin\u0026lt;/artifactId\u0026gt; 141\t\u0026lt;version\u0026gt;3.4.1\u0026lt;/version\u0026gt; 142\t\u0026lt;executions\u0026gt; 143\t\u0026lt;!-- Run shade goal on package phase --\u0026gt; 144\t\u0026lt;execution\u0026gt; 145\t\u0026lt;phase\u0026gt;package\u0026lt;/phase\u0026gt; 146\t\u0026lt;goals\u0026gt; 147\t\u0026lt;goal\u0026gt;shade\u0026lt;/goal\u0026gt; 148\t\u0026lt;/goals\u0026gt; 149\t\u0026lt;configuration\u0026gt; 150\t\u0026lt;artifactSet\u0026gt; 151\t\u0026lt;excludes\u0026gt; 152\t\u0026lt;exclude\u0026gt;org.apache.flink:force-shading\u0026lt;/exclude\u0026gt; 153\t\u0026lt;exclude\u0026gt;com.google.code.findbugs:jsr305\u0026lt;/exclude\u0026gt; 154\t\u0026lt;exclude\u0026gt;org.slf4j:*\u0026lt;/exclude\u0026gt; 155\t\u0026lt;exclude\u0026gt;org.apache.logging.log4j:*\u0026lt;/exclude\u0026gt; 156\t\u0026lt;/excludes\u0026gt; 157\t\u0026lt;/artifactSet\u0026gt; 158\t\u0026lt;filters\u0026gt; 159\t\u0026lt;filter\u0026gt; 160\t\u0026lt;!-- Do not copy the signatures in the META-INF folder. 161\tOtherwise, this might cause SecurityExceptions when using the JAR. --\u0026gt; 162\t\u0026lt;artifact\u0026gt;*:*\u0026lt;/artifact\u0026gt; 163\t\u0026lt;excludes\u0026gt; 164\t\u0026lt;exclude\u0026gt;META-INF/*.SF\u0026lt;/exclude\u0026gt; 165\t\u0026lt;exclude\u0026gt;META-INF/*.DSA\u0026lt;/exclude\u0026gt; 166\t\u0026lt;exclude\u0026gt;META-INF/*.RSA\u0026lt;/exclude\u0026gt; 167\t\u0026lt;/excludes\u0026gt; 168\t\u0026lt;/filter\u0026gt; 169\t\u0026lt;/filters\u0026gt; 170\t\u0026lt;transformers\u0026gt; 171\t\u0026lt;transformer implementation=\u0026#34;org.apache.maven.plugins.shade.resource.ServicesResourceTransformer\u0026#34;/\u0026gt; 172\t\u0026lt;/transformers\u0026gt; 173\t\u0026lt;/configuration\u0026gt; 174\t\u0026lt;/execution\u0026gt; 175\t\u0026lt;/executions\u0026gt; 176\t\u0026lt;/plugin\u0026gt; 177\t\u0026lt;/plugins\u0026gt; 178 179\t\u0026lt;pluginManagement\u0026gt; 180\t\u0026lt;plugins\u0026gt; 181 182\t\u0026lt;!-- This improves the out-of-the-box experience in Eclipse by resolving some warnings. --\u0026gt; 183\t\u0026lt;plugin\u0026gt; 184\t\u0026lt;groupId\u0026gt;org.eclipse.m2e\u0026lt;/groupId\u0026gt; 185\t\u0026lt;artifactId\u0026gt;lifecycle-mapping\u0026lt;/artifactId\u0026gt; 186\t\u0026lt;version\u0026gt;1.0.0\u0026lt;/version\u0026gt; 187\t\u0026lt;configuration\u0026gt; 188\t\u0026lt;lifecycleMappingMetadata\u0026gt; 189\t\u0026lt;pluginExecutions\u0026gt; 190\t\u0026lt;pluginExecution\u0026gt; 191\t\u0026lt;pluginExecutionFilter\u0026gt; 192\t\u0026lt;groupId\u0026gt;org.apache.maven.plugins\u0026lt;/groupId\u0026gt; 193\t\u0026lt;artifactId\u0026gt;maven-shade-plugin\u0026lt;/artifactId\u0026gt; 194\t\u0026lt;versionRange\u0026gt;[3.1.1,)\u0026lt;/versionRange\u0026gt; 195\t\u0026lt;goals\u0026gt; 196\t\u0026lt;goal\u0026gt;shade\u0026lt;/goal\u0026gt; 197\t\u0026lt;/goals\u0026gt; 198\t\u0026lt;/pluginExecutionFilter\u0026gt; 199\t\u0026lt;action\u0026gt; 200\t\u0026lt;ignore/\u0026gt; 201\t\u0026lt;/action\u0026gt; 202\t\u0026lt;/pluginExecution\u0026gt; 203\t\u0026lt;pluginExecution\u0026gt; 204\t\u0026lt;pluginExecutionFilter\u0026gt; 205\t\u0026lt;groupId\u0026gt;org.apache.maven.plugins\u0026lt;/groupId\u0026gt; 206\t\u0026lt;artifactId\u0026gt;maven-compiler-plugin\u0026lt;/artifactId\u0026gt; 207\t\u0026lt;versionRange\u0026gt;[3.1,)\u0026lt;/versionRange\u0026gt; 208\t\u0026lt;goals\u0026gt; 209\t\u0026lt;goal\u0026gt;testCompile\u0026lt;/goal\u0026gt; 210\t\u0026lt;goal\u0026gt;compile\u0026lt;/goal\u0026gt; 211\t\u0026lt;/goals\u0026gt; 212\t\u0026lt;/pluginExecutionFilter\u0026gt; 213\t\u0026lt;action\u0026gt; 214\t\u0026lt;ignore/\u0026gt; 215\t\u0026lt;/action\u0026gt; 216\t\u0026lt;/pluginExecution\u0026gt; 217\t\u0026lt;/pluginExecutions\u0026gt; 218\t\u0026lt;/lifecycleMappingMetadata\u0026gt; 219\t\u0026lt;/configuration\u0026gt; 220\t\u0026lt;/plugin\u0026gt; 221\t\u0026lt;/plugins\u0026gt; 222\t\u0026lt;/pluginManagement\u0026gt; 223\t\u0026lt;/build\u0026gt; 224\u0026lt;/project\u0026gt; The Uber Jar file can be built using the following script (build.sh).\n1# build.sh 2#!/usr/bin/env bash 3SCRIPT_DIR=\u0026#34;$(cd $(dirname \u0026#34;$0\u0026#34;); pwd)\u0026#34; 4SRC_PATH=$SCRIPT_DIR/package 5 6# remove contents under $SRC_PATH (except for the folders beginging with lab) 7shopt -s extglob 8rm -rf $SRC_PATH/!(lab*) 9 10## Generate Uber jar file for individual labs 11echo \u0026#34;generate Uber jar for PyFlink app...\u0026#34; 12mkdir $SRC_PATH/lib 13 14... 15 16mvn clean install -f $SRC_PATH/lab4-pipeline/pom.xml \\ 17 \u0026amp;\u0026amp; mv $SRC_PATH/lab4-pipeline/target/lab4-pipeline-1.0.0.jar $SRC_PATH/lib \\ 18 \u0026amp;\u0026amp; rm -rf $SRC_PATH/lab4-pipeline/target Application Source A source table is created to read messages from the taxi-rides topic using the Kafka Connector. The source records are bucketed in a window of 5 seconds using the TUMBLE windowing table-valued function, and those within buckets are aggregated by vendor ID, window start and window end variables. Then they are inserted into the sink table, which leads to ingesting them into an OpenSearch index named trip_stats using the OpenSearch connector.\nIn the main method, we create all the source and sink tables after mapping relevant application properties. Then the output records are written to the OpenSearch index. Note that the output records are printed in the terminal additionally when the app is running locally for checking them easily.\n1# forwarder/processor.py 2import os 3import re 4import json 5 6from pyflink.datastream import StreamExecutionEnvironment, RuntimeExecutionMode 7from pyflink.table import StreamTableEnvironment 8 9RUNTIME_ENV = os.environ.get(\u0026#34;RUNTIME_ENV\u0026#34;, \u0026#34;LOCAL\u0026#34;) # LOCAL or DOCKER 10BOOTSTRAP_SERVERS = os.environ.get(\u0026#34;BOOTSTRAP_SERVERS\u0026#34;) # overwrite app config 11OPENSEARCH_HOSTS = os.environ.get(\u0026#34;OPENSEARCH_HOSTS\u0026#34;) # overwrite app config 12 13env = StreamExecutionEnvironment.get_execution_environment() 14env.set_runtime_mode(RuntimeExecutionMode.STREAMING) 15env.enable_checkpointing(5000) 16 17if RUNTIME_ENV == \u0026#34;LOCAL\u0026#34;: 18 CURRENT_DIR = os.path.dirname(os.path.realpath(__file__)) 19 PARENT_DIR = os.path.dirname(CURRENT_DIR) 20 APPLICATION_PROPERTIES_FILE_PATH = os.path.join( 21 CURRENT_DIR, \u0026#34;application_properties.json\u0026#34; 22 ) 23 JAR_FILES = [\u0026#34;lab4-pipeline-1.0.0.jar\u0026#34;] 24 JAR_PATHS = tuple( 25 [f\u0026#34;file://{os.path.join(PARENT_DIR, \u0026#39;jars\u0026#39;, name)}\u0026#34; for name in JAR_FILES] 26 ) 27 print(JAR_PATHS) 28 env.add_jars(*JAR_PATHS) 29else: 30 APPLICATION_PROPERTIES_FILE_PATH = ( 31 \u0026#34;/etc/flink/forwarder/application_properties.json\u0026#34; 32 ) 33 34table_env = StreamTableEnvironment.create(stream_execution_environment=env) 35 36 37def get_application_properties(): 38 if os.path.isfile(APPLICATION_PROPERTIES_FILE_PATH): 39 with open(APPLICATION_PROPERTIES_FILE_PATH, \u0026#34;r\u0026#34;) as file: 40 contents = file.read() 41 properties = json.loads(contents) 42 return properties 43 else: 44 raise RuntimeError( 45 f\u0026#34;A file at \u0026#39;{APPLICATION_PROPERTIES_FILE_PATH}\u0026#39; was not found\u0026#34; 46 ) 47 48 49def property_map(props: dict, property_group_id: str): 50 for prop in props: 51 if prop[\u0026#34;PropertyGroupId\u0026#34;] == property_group_id: 52 return prop[\u0026#34;PropertyMap\u0026#34;] 53 54 55def inject_security_opts(opts: dict, bootstrap_servers: str): 56 if re.search(\u0026#34;9098$\u0026#34;, bootstrap_servers): 57 opts = { 58 **opts, 59 **{ 60 \u0026#34;properties.security.protocol\u0026#34;: \u0026#34;SASL_SSL\u0026#34;, 61 \u0026#34;properties.sasl.mechanism\u0026#34;: \u0026#34;AWS_MSK_IAM\u0026#34;, 62 \u0026#34;properties.sasl.jaas.config\u0026#34;: \u0026#34;software.amazon.msk.auth.iam.IAMLoginModule required;\u0026#34;, 63 \u0026#34;properties.sasl.client.callback.handler.class\u0026#34;: \u0026#34;software.amazon.msk.auth.iam.IAMClientCallbackHandler\u0026#34;, 64 }, 65 } 66 return \u0026#34;, \u0026#34;.join({f\u0026#34;\u0026#39;{k}\u0026#39; = \u0026#39;{v}\u0026#39;\u0026#34; for k, v in opts.items()}) 67 68 69def create_source_table(table_name: str, topic_name: str, bootstrap_servers: str): 70 opts = { 71 \u0026#34;connector\u0026#34;: \u0026#34;kafka\u0026#34;, 72 \u0026#34;topic\u0026#34;: topic_name, 73 \u0026#34;properties.bootstrap.servers\u0026#34;: bootstrap_servers, 74 \u0026#34;properties.group.id\u0026#34;: \u0026#34;soruce-group\u0026#34;, 75 \u0026#34;format\u0026#34;: \u0026#34;json\u0026#34;, 76 \u0026#34;scan.startup.mode\u0026#34;: \u0026#34;latest-offset\u0026#34;, 77 } 78 79 stmt = f\u0026#34;\u0026#34;\u0026#34; 80 CREATE TABLE {table_name} ( 81 id VARCHAR, 82 vendor_id INT, 83 pickup_date VARCHAR, 84 dropoff_date VARCHAR, 85 passenger_count INT, 86 pickup_longitude VARCHAR, 87 pickup_latitude VARCHAR, 88 dropoff_longitude VARCHAR, 89 dropoff_latitude VARCHAR, 90 store_and_fwd_flag VARCHAR, 91 gc_distance INT, 92 trip_duration INT, 93 google_distance INT, 94 google_duration INT, 95 process_time AS PROCTIME() 96 ) WITH ( 97 {inject_security_opts(opts, bootstrap_servers)} 98 ) 99 \u0026#34;\u0026#34;\u0026#34; 100 print(stmt) 101 return stmt 102 103 104def create_sink_table(table_name: str, os_hosts: str, os_index: str): 105 stmt = f\u0026#34;\u0026#34;\u0026#34; 106 CREATE TABLE {table_name} ( 107 vendor_id VARCHAR, 108 trip_count BIGINT NOT NULL, 109 passenger_count INT, 110 trip_duration INT, 111 window_start TIMESTAMP(3) NOT NULL, 112 window_end TIMESTAMP(3) NOT NULL 113 ) WITH ( 114 \u0026#39;connector\u0026#39;= \u0026#39;opensearch\u0026#39;, 115 \u0026#39;hosts\u0026#39; = \u0026#39;{os_hosts}\u0026#39;, 116 \u0026#39;index\u0026#39; = \u0026#39;{os_index}\u0026#39; 117 ) 118 \u0026#34;\u0026#34;\u0026#34; 119 print(stmt) 120 return stmt 121 122 123def create_print_table(table_name: str): 124 stmt = f\u0026#34;\u0026#34;\u0026#34; 125 CREATE TABLE {table_name} ( 126 vendor_id VARCHAR, 127 trip_count BIGINT NOT NULL, 128 passenger_count INT, 129 trip_duration INT, 130 window_start TIMESTAMP(3) NOT NULL, 131 window_end TIMESTAMP(3) NOT NULL 132 ) WITH ( 133 \u0026#39;connector\u0026#39;= \u0026#39;print\u0026#39; 134 ) 135 \u0026#34;\u0026#34;\u0026#34; 136 print(stmt) 137 return stmt 138 139 140def set_insert_sql(source_table_name: str, sink_table_name: str): 141 stmt = f\u0026#34;\u0026#34;\u0026#34; 142 INSERT INTO {sink_table_name} 143 SELECT 144 CAST(vendor_id AS STRING) AS vendor_id, 145 COUNT(id) AS trip_count, 146 SUM(passenger_count) AS passenger_count, 147 SUM(trip_duration) AS trip_duration, 148 window_start, 149 window_end 150 FROM TABLE( 151 TUMBLE(TABLE {source_table_name}, DESCRIPTOR(process_time), INTERVAL \u0026#39;5\u0026#39; SECONDS)) 152 GROUP BY vendor_id, window_start, window_end 153 \u0026#34;\u0026#34;\u0026#34; 154 print(stmt) 155 return stmt 156 157 158def main(): 159 #### map source/sink properties 160 props = get_application_properties() 161 ## source 162 source_property_group_key = \u0026#34;source.config.0\u0026#34; 163 source_properties = property_map(props, source_property_group_key) 164 print(\u0026#34;\u0026gt;\u0026gt; source properties\u0026#34;) 165 print(source_properties) 166 source_table_name = source_properties[\u0026#34;table.name\u0026#34;] 167 source_topic_name = source_properties[\u0026#34;topic.name\u0026#34;] 168 source_bootstrap_servers = ( 169 BOOTSTRAP_SERVERS or source_properties[\u0026#34;bootstrap.servers\u0026#34;] 170 ) 171 ## sink 172 sink_property_group_key = \u0026#34;sink.config.0\u0026#34; 173 sink_properties = property_map(props, sink_property_group_key) 174 print(\u0026#34;\u0026gt;\u0026gt; sink properties\u0026#34;) 175 print(sink_properties) 176 sink_table_name = sink_properties[\u0026#34;table.name\u0026#34;] 177 sink_os_hosts = OPENSEARCH_HOSTS or sink_properties[\u0026#34;os_hosts\u0026#34;] 178 sink_os_index = sink_properties[\u0026#34;os_index\u0026#34;] 179 ## print 180 print_table_name = \u0026#34;sink_print\u0026#34; 181 #### create tables 182 table_env.execute_sql( 183 create_source_table( 184 source_table_name, source_topic_name, source_bootstrap_servers 185 ) 186 ) 187 table_env.execute_sql( 188 create_sink_table(sink_table_name, sink_os_hosts, sink_os_index) 189 ) 190 table_env.execute_sql(create_print_table(print_table_name)) 191 #### insert into sink tables 192 if RUNTIME_ENV == \u0026#34;LOCAL\u0026#34;: 193 statement_set = table_env.create_statement_set() 194 statement_set.add_insert_sql(set_insert_sql(source_table_name, sink_table_name)) 195 statement_set.add_insert_sql(set_insert_sql(print_table_name, sink_table_name)) 196 statement_set.execute().wait() 197 else: 198 table_result = table_env.execute_sql( 199 set_insert_sql(source_table_name, sink_table_name) 200 ) 201 print(table_result.get_job_client().get_job_status()) 202 203 204if __name__ == \u0026#34;__main__\u0026#34;: 205 main() 1// forwarder/application_properties.json 2[ 3 { 4 \u0026#34;PropertyGroupId\u0026#34;: \u0026#34;kinesis.analytics.flink.run.options\u0026#34;, 5 \u0026#34;PropertyMap\u0026#34;: { 6 \u0026#34;python\u0026#34;: \u0026#34;processor.py\u0026#34;, 7 \u0026#34;jarfile\u0026#34;: \u0026#34;package/lib/lab4-pipeline-1.0.0.jar\u0026#34; 8 } 9 }, 10 { 11 \u0026#34;PropertyGroupId\u0026#34;: \u0026#34;source.config.0\u0026#34;, 12 \u0026#34;PropertyMap\u0026#34;: { 13 \u0026#34;table.name\u0026#34;: \u0026#34;taxi_rides_src\u0026#34;, 14 \u0026#34;topic.name\u0026#34;: \u0026#34;taxi-rides\u0026#34;, 15 \u0026#34;bootstrap.servers\u0026#34;: \u0026#34;localhost:29092\u0026#34; 16 } 17 }, 18 { 19 \u0026#34;PropertyGroupId\u0026#34;: \u0026#34;sink.config.0\u0026#34;, 20 \u0026#34;PropertyMap\u0026#34;: { 21 \u0026#34;table.name\u0026#34;: \u0026#34;trip_stats_sink\u0026#34;, 22 \u0026#34;os_hosts\u0026#34;: \u0026#34;http://opensearch:9200\u0026#34;, 23 \u0026#34;os_index\u0026#34;: \u0026#34;trip_stats\u0026#34; 24 } 25 } 26] Run Application Execute on Local Flink Cluster We can run the application in the Flink cluster on Docker and the steps are shown below. Either the Kafka cluster on Amazon MSK or a local Kafka cluster can be used depending on which Docker Compose file we use. In either way, we can check the job details on the Flink web UI on localhost:8081. Note that, if we use the local Kafka cluster option, we have to start the producer application in a different terminal as well as to deploy a local OpenSearch cluster.\n1## set aws credentials environment variables 2export AWS_ACCESS_KEY_ID=\u0026lt;aws-access-key-id\u0026gt; 3export AWS_SECRET_ACCESS_KEY=\u0026lt;aws-secret-access-key\u0026gt; 4 5# set addtional environment variables when using AWS services if using compose-msk.yml 6# values can be obtained in Terraform outputs or AWS Console 7export BOOTSTRAP_SERVERS=\u0026lt;bootstrap-servers\u0026gt; 8export OPENSEARCH_HOSTS=\u0026lt;opensearch-hosts\u0026gt; 9 10## run docker compose service 11# or with msk cluster 12docker-compose -f compose-msk.yml up -d 13 14# # with local kafka and opensearch cluster 15# docker-compose -f compose-local-kafka.yml up -d 16# docker-compose -f compose-extra.yml up -d 17 18## run the producer application in another terminal if using a local cluster 19# python producer/app.py 20 21## submit pyflink application 22docker exec jobmanager /opt/flink/bin/flink run \\ 23 --python /etc/flink/forwarder/processor.py \\ 24 --jarfile /etc/flink/package/lib/lab4-pipeline-1.0.0.jar \\ 25 -d Once the Pyflink application is submitted, we can check the details of it on the Flink UI as shown below.\nApplication Result Kafka Topic We can see the topic (taxi-rides) is created, and the details of the topic can be found on the Topics menu on localhost:3000.\nOpenSearch Index The ingested data can be checked easily using the Query Workbench as shown below.\nTo monitor the status of taxi rides, a horizontal bar chart is created in the OpenSearch Dashboard. The average trip duration is selected as the metric, and the records are grouped by vendor ID. We can see the values change while new records arrives.\nSummary In this lab, we created a Pyflink application that writes accumulated taxi rides data into an OpenSearch cluster. It aggregated the number of trips/passengers and trip durations by vendor ID for a window of 5 seconds. The data was then used to create a chart that monitors the status of taxi rides in the OpenSearch Dashboard.\n","date":"November 23, 2023","img":"/blog/2023-11-23-real-time-streaming-with-kafka-and-flink-5/featured.png","lang":"en","langName":"English","largeImg":"/blog/2023-11-23-real-time-streaming-with-kafka-and-flink-5/featured_hu934f3fa21a75bb8ccc480bfe067ce5a2_112340_500x0_resize_box_3.png","permalink":"/blog/2023-11-23-real-time-streaming-with-kafka-and-flink-5/","series":[{"title":"Real Time Streaming With Kafka and Flink","url":"/series/real-time-streaming-with-kafka-and-flink/"}],"smallImg":"/blog/2023-11-23-real-time-streaming-with-kafka-and-flink-5/featured_hu934f3fa21a75bb8ccc480bfe067ce5a2_112340_180x0_resize_box_3.png","tags":[{"title":"AWS","url":"/tags/aws/"},{"title":"Amazon MSK","url":"/tags/amazon-msk/"},{"title":"Amazon OpenSearch Service","url":"/tags/amazon-opensearch-service/"},{"title":"Apache Kafka","url":"/tags/apache-kafka/"},{"title":"Apache Flink","url":"/tags/apache-flink/"},{"title":"OpenSearch","url":"/tags/opensearch/"},{"title":"PyFlink","url":"/tags/pyflink/"},{"title":"Docker","url":"/tags/docker/"},{"title":"Docker Compose","url":"/tags/docker-compose/"},{"title":"Python","url":"/tags/python/"}],"timestamp":1700697600,"title":"Real Time Streaming With Kafka and Flink - Lab 4 Clean, Aggregate, and Enrich Events With Flink"},{"categories":[{"title":"Data Streaming","url":"/categories/data-streaming/"}],"content":"In this lab, we will create a Pyflink application that exports Kafka topic messages into a S3 bucket. The app enriches the records by adding a new column using a user defined function and writes them via the FileSystem SQL connector. This allows us to achieve a simpler architecture compared to the original lab where the records are sent into Amazon Kinesis Data Firehose, enriched by a separate Lambda function and written to a S3 bucket afterwards. While the records are being written to the S3 bucket, a Glue table will be created to query them on Amazon Athena.\nIntroduction Lab 1 Produce data to Kafka using Lambda Lab 2 Write data to Kafka from S3 using Flink Lab 3 Transform and write data to S3 from Kafka using Flink (this post) Lab 4 Clean, Aggregate, and Enrich Events with Flink Lab 5 Write data to DynamoDB using Kafka Connect Lab 6 Consume data from Kafka using Lambda [Update 2023-11-22] Amazon MSK now supports fully managed data delivery to Amazon S3 using Kinesis Data Firehose, and you may consider this feature rather than relying on a Flink application. See this page for details.\nArchitecture Fake taxi ride data is sent to a Kafka topic by the Kafka producer application that is discussed in Lab 1. The records are read by a Pyflink application, and it writes them into a S3 bucket. The app enriches the records by adding a new column named source using a user defined function. The records in the S3 bucket can be queried on Amazon Athena after creating an external table that sources the bucket.\nInfrastructure AWS Infrastructure The AWS infrastructure is created using Terraform and the source can be found in the GitHub repository of this post - see this earlier post for details about how to create the resources. The infrastructure can be deployed (as well as destroyed) using Terraform CLI as shown below.\n1# initialize 2terraform init 3# create an execution plan 4terraform plan -var \u0026#39;producer_to_create=true\u0026#39; 5# execute the actions proposed in a Terraform plan 6terraform apply -auto-approve=true -var \u0026#39;producer_to_create=true\u0026#39; 7 8# destroy all remote objects 9# terraform destroy -auto-approve=true -var \u0026#39;producer_to_create=true\u0026#39; Note that deploying the AWS infrastructure is optional because we can create a local Kafka and Flink cluster on Docker for testing easily. We will not use a Kafka cluster deployed on MSK in this post.\nKafka and Flink Cluster on Docker Compose In the previous post, we discussed how to create a local Flink cluster on Docker. We can add additional Docker Compose services (zookeeper and kafka-0) for a Kafka cluster and the updated compose file can be found below. See this post for details how to set up a Kafka cluster on Docker.\n1# compose-local-kafka.yml 2# see compose-msk.yml for an msk cluster instead of a local kafka cluster 3version: \u0026#34;3.5\u0026#34; 4 5services: 6 jobmanager: 7 image: real-time-streaming-aws:1.17.1 8 command: jobmanager 9 container_name: jobmanager 10 ports: 11 - \u0026#34;8081:8081\u0026#34; 12 networks: 13 - appnet 14 volumes: 15 - ./:/etc/flink 16 environment: 17 - BOOTSTRAP_SERVERS=kafka-0:9092 18 - RUNTIME_ENV=DOCKER 19 - AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID 20 - AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY 21 # - AWS_SESSION_TOKEN=$AWS_SESSION_TOKEN 22 - | 23 FLINK_PROPERTIES= 24 jobmanager.rpc.address: jobmanager 25 state.backend: filesystem 26 state.checkpoints.dir: file:///tmp/flink-checkpoints 27 heartbeat.interval: 1000 28 heartbeat.timeout: 5000 29 rest.flamegraph.enabled: true 30 web.backpressure.refresh-interval: 10000 31 taskmanager: 32 image: real-time-streaming-aws:1.17.1 33 command: taskmanager 34 container_name: taskmanager 35 networks: 36 - appnet 37 volumes: 38 - flink_data:/tmp/ 39 - ./:/etc/flink 40 environment: 41 - BOOTSTRAP_SERVERS=kafka-0:9092 42 - RUNTIME_ENV=DOCKER 43 - AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID 44 - AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY 45 # - AWS_SESSION_TOKEN=$AWS_SESSION_TOKEN 46 - | 47 FLINK_PROPERTIES= 48 jobmanager.rpc.address: jobmanager 49 taskmanager.numberOfTaskSlots: 5 50 state.backend: filesystem 51 state.checkpoints.dir: file:///tmp/flink-checkpoints 52 heartbeat.interval: 1000 53 heartbeat.timeout: 5000 54 depends_on: 55 - jobmanager 56 zookeeper: 57 image: bitnami/zookeeper:3.5 58 container_name: zookeeper 59 ports: 60 - \u0026#34;2181\u0026#34; 61 networks: 62 - appnet 63 environment: 64 - ALLOW_ANONYMOUS_LOGIN=yes 65 volumes: 66 - zookeeper_data:/bitnami/zookeeper 67 kafka-0: 68 image: bitnami/kafka:2.8.1 69 container_name: kafka-0 70 expose: 71 - 9092 72 ports: 73 - \u0026#34;29092:29092\u0026#34; 74 networks: 75 - appnet 76 environment: 77 - ALLOW_PLAINTEXT_LISTENER=yes 78 - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181 79 - KAFKA_CFG_BROKER_ID=0 80 - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT 81 - KAFKA_CFG_LISTENERS=INTERNAL://:9092,EXTERNAL://:29092 82 - KAFKA_CFG_ADVERTISED_LISTENERS=INTERNAL://kafka-0:9092,EXTERNAL://localhost:29092 83 - KAFKA_CFG_INTER_BROKER_LISTENER_NAME=INTERNAL 84 - KAFKA_CFG_NUM_PARTITIONS=5 85 - KAFKA_CFG_DEFAULT_REPLICATION_FACTOR=1 86 - KAFKA_CFG_AUTO_CREATE_TOPICS_ENABLE=true 87 volumes: 88 - kafka_0_data:/bitnami/kafka 89 depends_on: 90 - zookeeper 91 kpow: 92 image: factorhouse/kpow-ce:91.5.1 93 container_name: kpow 94 ports: 95 - \u0026#34;3000:3000\u0026#34; 96 networks: 97 - appnet 98 environment: 99 BOOTSTRAP: kafka-0:9092 100 env_file: # https://kpow.io/get-started/#individual 101 - ./kpow.env 102 depends_on: 103 - zookeeper 104 - kafka-0 105 106networks: 107 appnet: 108 name: app-network 109 110volumes: 111 zookeeper_data: 112 driver: local 113 name: zookeeper_data 114 kafka_0_data: 115 driver: local 116 name: kafka_0_data 117 flink_data: 118 driver: local 119 name: flink_data The Docker Compose services can be deployed as shown below.\n1$ docker-compose -f compose-local-kafka.yml up -d Pyflink Application Flink Pipeline Jar The application has multiple dependencies and a single Jar file is created so that it can be specified in the --jarfile option. Note that we have to build the Jar file based on the Apache Kafka Connector instead of the Apache Kafka SQL Connector because the MSK IAM Auth library is not compatible with the latter due to shade relocation. The dependencies can be grouped as shown below.\nFlink Connectors flink-connector-base flink-connector-files flink-connector-kafka Each of them can be placed in the Flink library folder (/opt/flink/lib) Parquet Format flink-parquet hadoop dependencies parquet-hadoop hadoop-common hadoop-mapreduce-client-core All of them can be combined as a single Jar file and be placed in the Flink library folder (/opt/flink/lib) Kafka Communication and IAM Authentication kafka-clients for communicating with a Kafka cluster It can be placed in the Flink library folder (/opt/flink/lib) aws-msk-iam-auth for IAM authentication It can be placed in the Flink library folder (/opt/flink/lib) A single Jar file is created in this post as the app may need to be deployed via Amazon Managed Flink potentially. If we do not have to deploy on it, however, they can be added to the Flink library folder separately, which is a more comprehensive way of managing dependency in my opinion. I added comments about how they may be placed into the Flink library folder in the dependencies list above. Note that the S3 file system Jar file (flink-s3-fs-hadoop-1.17.1.jar) is placed under the plugins (/opt/flink/plugins/s3-fs-hadoop) folder of the custom Docker image according to the Flink documentation.\nThe single Uber jar file is created by the following POM file and the Jar file is named as lab3-pipeline-1.0.0.jar.\n1\u0026lt;!-- package/lab3-pipeline/pom.xml --\u0026gt; 2\u0026lt;project xmlns=\u0026#34;http://maven.apache.org/POM/4.0.0\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; 3\txsi:schemaLocation=\u0026#34;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\u0026#34;\u0026gt; 4\t\u0026lt;modelVersion\u0026gt;4.0.0\u0026lt;/modelVersion\u0026gt; 5 6\t\u0026lt;groupId\u0026gt;com.amazonaws.services.kinesisanalytics\u0026lt;/groupId\u0026gt; 7\t\u0026lt;artifactId\u0026gt;lab3-pipeline\u0026lt;/artifactId\u0026gt; 8\t\u0026lt;version\u0026gt;1.0.0\u0026lt;/version\u0026gt; 9\t\u0026lt;packaging\u0026gt;jar\u0026lt;/packaging\u0026gt; 10 11\t\u0026lt;name\u0026gt;Uber Jar for Lab 3\u0026lt;/name\u0026gt; 12 13\t\u0026lt;properties\u0026gt; 14\t\u0026lt;project.build.sourceEncoding\u0026gt;UTF-8\u0026lt;/project.build.sourceEncoding\u0026gt; 15\t\u0026lt;flink.version\u0026gt;1.17.1\u0026lt;/flink.version\u0026gt; 16\t\u0026lt;target.java.version\u0026gt;1.11\u0026lt;/target.java.version\u0026gt; 17\t\u0026lt;jdk.version\u0026gt;11\u0026lt;/jdk.version\u0026gt; 18\t\u0026lt;scala.binary.version\u0026gt;2.12\u0026lt;/scala.binary.version\u0026gt; 19\t\u0026lt;kda.connectors.version\u0026gt;2.0.0\u0026lt;/kda.connectors.version\u0026gt; 20\t\u0026lt;kda.runtime.version\u0026gt;1.2.0\u0026lt;/kda.runtime.version\u0026gt; 21\t\u0026lt;kafka.clients.version\u0026gt;3.2.3\u0026lt;/kafka.clients.version\u0026gt; 22\t\u0026lt;hadoop.version\u0026gt;3.2.4\u0026lt;/hadoop.version\u0026gt; 23\t\u0026lt;flink.format.parquet.version\u0026gt;1.12.3\u0026lt;/flink.format.parquet.version\u0026gt; 24\t\u0026lt;log4j.version\u0026gt;2.17.1\u0026lt;/log4j.version\u0026gt; 25\t\u0026lt;aws-msk-iam-auth.version\u0026gt;1.1.7\u0026lt;/aws-msk-iam-auth.version\u0026gt; 26\t\u0026lt;/properties\u0026gt; 27 28\t\u0026lt;repositories\u0026gt; 29\t\u0026lt;repository\u0026gt; 30\t\u0026lt;id\u0026gt;apache.snapshots\u0026lt;/id\u0026gt; 31\t\u0026lt;name\u0026gt;Apache Development Snapshot Repository\u0026lt;/name\u0026gt; 32\t\u0026lt;url\u0026gt;https://repository.apache.org/content/repositories/snapshots/\u0026lt;/url\u0026gt; 33\t\u0026lt;releases\u0026gt; 34\t\u0026lt;enabled\u0026gt;false\u0026lt;/enabled\u0026gt; 35\t\u0026lt;/releases\u0026gt; 36\t\u0026lt;snapshots\u0026gt; 37\t\u0026lt;enabled\u0026gt;true\u0026lt;/enabled\u0026gt; 38\t\u0026lt;/snapshots\u0026gt; 39\t\u0026lt;/repository\u0026gt; 40\t\u0026lt;/repositories\u0026gt; 41 42\t\u0026lt;dependencies\u0026gt; 43 44\t\u0026lt;!-- Flink Connectors --\u0026gt; 45 46\t\u0026lt;dependency\u0026gt; 47\t\u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; 48\t\u0026lt;artifactId\u0026gt;flink-connector-base\u0026lt;/artifactId\u0026gt; 49\t\u0026lt;version\u0026gt;${flink.version}\u0026lt;/version\u0026gt; 50\t\u0026lt;/dependency\u0026gt; 51 52\t\u0026lt;dependency\u0026gt; 53\t\u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; 54\t\u0026lt;artifactId\u0026gt;flink-connector-kafka\u0026lt;/artifactId\u0026gt; 55\t\u0026lt;version\u0026gt;${flink.version}\u0026lt;/version\u0026gt; 56\t\u0026lt;/dependency\u0026gt; 57 58\t\u0026lt;dependency\u0026gt; 59\t\u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; 60\t\u0026lt;artifactId\u0026gt;flink-connector-files\u0026lt;/artifactId\u0026gt; 61\t\u0026lt;version\u0026gt;${flink.version}\u0026lt;/version\u0026gt; 62\t\u0026lt;/dependency\u0026gt; 63 64\t\u0026lt;!-- Parquet --\u0026gt; 65\t\u0026lt;!-- See https://github.com/apache/flink/blob/release-1.17/flink-formats/flink-parquet/pom.xml --\u0026gt; 66 67\t\u0026lt;dependency\u0026gt; 68\t\u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; 69\t\u0026lt;artifactId\u0026gt;flink-parquet\u0026lt;/artifactId\u0026gt; 70\t\u0026lt;version\u0026gt;${flink.version}\u0026lt;/version\u0026gt; 71\t\u0026lt;/dependency\u0026gt; 72 73\t\u0026lt;!-- Hadoop is needed by Parquet --\u0026gt; 74 75\t\u0026lt;dependency\u0026gt; 76\t\u0026lt;groupId\u0026gt;org.apache.parquet\u0026lt;/groupId\u0026gt; 77\t\u0026lt;artifactId\u0026gt;parquet-hadoop\u0026lt;/artifactId\u0026gt; 78\t\u0026lt;version\u0026gt;${flink.format.parquet.version}\u0026lt;/version\u0026gt; 79\t\u0026lt;exclusions\u0026gt; 80\t\u0026lt;exclusion\u0026gt; 81\t\u0026lt;groupId\u0026gt;org.xerial.snappy\u0026lt;/groupId\u0026gt; 82\t\u0026lt;artifactId\u0026gt;snappy-java\u0026lt;/artifactId\u0026gt; 83\t\u0026lt;/exclusion\u0026gt; 84\t\u0026lt;exclusion\u0026gt; 85\t\u0026lt;groupId\u0026gt;commons-cli\u0026lt;/groupId\u0026gt; 86\t\u0026lt;artifactId\u0026gt;commons-cli\u0026lt;/artifactId\u0026gt; 87\t\u0026lt;/exclusion\u0026gt; 88\t\u0026lt;/exclusions\u0026gt; 89\t\u0026lt;/dependency\u0026gt; 90 91\t\u0026lt;dependency\u0026gt; 92\t\u0026lt;groupId\u0026gt;org.apache.hadoop\u0026lt;/groupId\u0026gt; 93\t\u0026lt;artifactId\u0026gt;hadoop-common\u0026lt;/artifactId\u0026gt; 94\t\u0026lt;version\u0026gt;${hadoop.version}\u0026lt;/version\u0026gt; 95\t\u0026lt;exclusions\u0026gt; 96\t\u0026lt;exclusion\u0026gt; 97\t\u0026lt;groupId\u0026gt;com.google.protobuf\u0026lt;/groupId\u0026gt; 98\t\u0026lt;artifactId\u0026gt;protobuf-java\u0026lt;/artifactId\u0026gt; 99\t\u0026lt;/exclusion\u0026gt; 100\t\u0026lt;exclusion\u0026gt; 101\t\u0026lt;groupId\u0026gt;ch.qos.reload4j\u0026lt;/groupId\u0026gt; 102\t\u0026lt;artifactId\u0026gt;reload4j\u0026lt;/artifactId\u0026gt; 103\t\u0026lt;/exclusion\u0026gt; 104\t\u0026lt;exclusion\u0026gt; 105\t\u0026lt;groupId\u0026gt;org.slf4j\u0026lt;/groupId\u0026gt; 106\t\u0026lt;artifactId\u0026gt;slf4j-reload4j\u0026lt;/artifactId\u0026gt; 107\t\u0026lt;/exclusion\u0026gt; 108\t\u0026lt;exclusion\u0026gt; 109\t\u0026lt;groupId\u0026gt;commons-cli\u0026lt;/groupId\u0026gt; 110\t\u0026lt;artifactId\u0026gt;commons-cli\u0026lt;/artifactId\u0026gt; 111\t\u0026lt;/exclusion\u0026gt; 112\t\u0026lt;/exclusions\u0026gt; 113\t\u0026lt;/dependency\u0026gt; 114 115\t\u0026lt;dependency\u0026gt; 116\t\u0026lt;groupId\u0026gt;org.apache.hadoop\u0026lt;/groupId\u0026gt; 117\t\u0026lt;artifactId\u0026gt;hadoop-mapreduce-client-core\u0026lt;/artifactId\u0026gt; 118\t\u0026lt;version\u0026gt;${hadoop.version}\u0026lt;/version\u0026gt; 119\t\u0026lt;exclusions\u0026gt; 120\t\u0026lt;exclusion\u0026gt; 121\t\u0026lt;groupId\u0026gt;com.google.protobuf\u0026lt;/groupId\u0026gt; 122\t\u0026lt;artifactId\u0026gt;protobuf-java\u0026lt;/artifactId\u0026gt; 123\t\u0026lt;/exclusion\u0026gt; 124\t\u0026lt;exclusion\u0026gt; 125\t\u0026lt;groupId\u0026gt;ch.qos.reload4j\u0026lt;/groupId\u0026gt; 126\t\u0026lt;artifactId\u0026gt;reload4j\u0026lt;/artifactId\u0026gt; 127\t\u0026lt;/exclusion\u0026gt; 128\t\u0026lt;exclusion\u0026gt; 129\t\u0026lt;groupId\u0026gt;org.slf4j\u0026lt;/groupId\u0026gt; 130\t\u0026lt;artifactId\u0026gt;slf4j-reload4j\u0026lt;/artifactId\u0026gt; 131\t\u0026lt;/exclusion\u0026gt; 132\t\u0026lt;exclusion\u0026gt; 133\t\u0026lt;groupId\u0026gt;commons-cli\u0026lt;/groupId\u0026gt; 134\t\u0026lt;artifactId\u0026gt;commons-cli\u0026lt;/artifactId\u0026gt; 135\t\u0026lt;/exclusion\u0026gt; 136\t\u0026lt;/exclusions\u0026gt; 137\t\u0026lt;/dependency\u0026gt; 138 139\t\u0026lt;!-- Kafka Client and MSK IAM Auth Lib --\u0026gt; 140 141\t\u0026lt;dependency\u0026gt; 142\t\u0026lt;groupId\u0026gt;org.apache.kafka\u0026lt;/groupId\u0026gt; 143\t\u0026lt;artifactId\u0026gt;kafka-clients\u0026lt;/artifactId\u0026gt; 144\t\u0026lt;version\u0026gt;${kafka.clients.version}\u0026lt;/version\u0026gt; 145\t\u0026lt;/dependency\u0026gt; 146 147\t\u0026lt;dependency\u0026gt; 148\t\u0026lt;groupId\u0026gt;software.amazon.msk\u0026lt;/groupId\u0026gt; 149\t\u0026lt;artifactId\u0026gt;aws-msk-iam-auth\u0026lt;/artifactId\u0026gt; 150\t\u0026lt;version\u0026gt;${aws-msk-iam-auth.version}\u0026lt;/version\u0026gt; 151\t\u0026lt;/dependency\u0026gt; 152 153\t\u0026lt;!-- Add logging framework, to produce console output when running in the IDE. --\u0026gt; 154\t\u0026lt;!-- These dependencies are excluded from the application JAR by default. --\u0026gt; 155\t\u0026lt;dependency\u0026gt; 156\t\u0026lt;groupId\u0026gt;org.apache.logging.log4j\u0026lt;/groupId\u0026gt; 157\t\u0026lt;artifactId\u0026gt;log4j-slf4j-impl\u0026lt;/artifactId\u0026gt; 158\t\u0026lt;version\u0026gt;${log4j.version}\u0026lt;/version\u0026gt; 159\t\u0026lt;scope\u0026gt;runtime\u0026lt;/scope\u0026gt; 160\t\u0026lt;/dependency\u0026gt; 161\t\u0026lt;dependency\u0026gt; 162\t\u0026lt;groupId\u0026gt;org.apache.logging.log4j\u0026lt;/groupId\u0026gt; 163\t\u0026lt;artifactId\u0026gt;log4j-api\u0026lt;/artifactId\u0026gt; 164\t\u0026lt;version\u0026gt;${log4j.version}\u0026lt;/version\u0026gt; 165\t\u0026lt;scope\u0026gt;runtime\u0026lt;/scope\u0026gt; 166\t\u0026lt;/dependency\u0026gt; 167\t\u0026lt;dependency\u0026gt; 168\t\u0026lt;groupId\u0026gt;org.apache.logging.log4j\u0026lt;/groupId\u0026gt; 169\t\u0026lt;artifactId\u0026gt;log4j-core\u0026lt;/artifactId\u0026gt; 170\t\u0026lt;version\u0026gt;${log4j.version}\u0026lt;/version\u0026gt; 171\t\u0026lt;scope\u0026gt;runtime\u0026lt;/scope\u0026gt; 172\t\u0026lt;/dependency\u0026gt; 173\t\u0026lt;/dependencies\u0026gt; 174 175\t\u0026lt;build\u0026gt; 176\t\u0026lt;plugins\u0026gt; 177 178\t\u0026lt;!-- Java Compiler --\u0026gt; 179\t\u0026lt;plugin\u0026gt; 180\t\u0026lt;groupId\u0026gt;org.apache.maven.plugins\u0026lt;/groupId\u0026gt; 181\t\u0026lt;artifactId\u0026gt;maven-compiler-plugin\u0026lt;/artifactId\u0026gt; 182\t\u0026lt;version\u0026gt;3.8.0\u0026lt;/version\u0026gt; 183\t\u0026lt;configuration\u0026gt; 184\t\u0026lt;source\u0026gt;${jdk.version}\u0026lt;/source\u0026gt; 185\t\u0026lt;target\u0026gt;${jdk.version}\u0026lt;/target\u0026gt; 186\t\u0026lt;/configuration\u0026gt; 187\t\u0026lt;/plugin\u0026gt; 188 189\t\u0026lt;!-- We use the maven-shade plugin to create a fat jar that contains all necessary dependencies. --\u0026gt; 190\t\u0026lt;!-- Change the value of \u0026lt;mainClass\u0026gt;...\u0026lt;/mainClass\u0026gt; if your program entry point changes. --\u0026gt; 191\t\u0026lt;plugin\u0026gt; 192\t\u0026lt;groupId\u0026gt;org.apache.maven.plugins\u0026lt;/groupId\u0026gt; 193\t\u0026lt;artifactId\u0026gt;maven-shade-plugin\u0026lt;/artifactId\u0026gt; 194\t\u0026lt;version\u0026gt;3.4.1\u0026lt;/version\u0026gt; 195\t\u0026lt;executions\u0026gt; 196\t\u0026lt;!-- Run shade goal on package phase --\u0026gt; 197\t\u0026lt;execution\u0026gt; 198\t\u0026lt;phase\u0026gt;package\u0026lt;/phase\u0026gt; 199\t\u0026lt;goals\u0026gt; 200\t\u0026lt;goal\u0026gt;shade\u0026lt;/goal\u0026gt; 201\t\u0026lt;/goals\u0026gt; 202\t\u0026lt;configuration\u0026gt; 203\t\u0026lt;artifactSet\u0026gt; 204\t\u0026lt;excludes\u0026gt; 205\t\u0026lt;exclude\u0026gt;org.apache.flink:force-shading\u0026lt;/exclude\u0026gt; 206\t\u0026lt;exclude\u0026gt;com.google.code.findbugs:jsr305\u0026lt;/exclude\u0026gt; 207\t\u0026lt;exclude\u0026gt;org.slf4j:*\u0026lt;/exclude\u0026gt; 208\t\u0026lt;exclude\u0026gt;org.apache.logging.log4j:*\u0026lt;/exclude\u0026gt; 209\t\u0026lt;/excludes\u0026gt; 210\t\u0026lt;/artifactSet\u0026gt; 211\t\u0026lt;filters\u0026gt; 212\t\u0026lt;filter\u0026gt; 213\t\u0026lt;!-- Do not copy the signatures in the META-INF folder. 214\tOtherwise, this might cause SecurityExceptions when using the JAR. --\u0026gt; 215\t\u0026lt;artifact\u0026gt;*:*\u0026lt;/artifact\u0026gt; 216\t\u0026lt;excludes\u0026gt; 217\t\u0026lt;exclude\u0026gt;META-INF/*.SF\u0026lt;/exclude\u0026gt; 218\t\u0026lt;exclude\u0026gt;META-INF/*.DSA\u0026lt;/exclude\u0026gt; 219\t\u0026lt;exclude\u0026gt;META-INF/*.RSA\u0026lt;/exclude\u0026gt; 220\t\u0026lt;/excludes\u0026gt; 221\t\u0026lt;/filter\u0026gt; 222\t\u0026lt;/filters\u0026gt; 223\t\u0026lt;transformers\u0026gt; 224\t\u0026lt;transformer implementation=\u0026#34;org.apache.maven.plugins.shade.resource.ServicesResourceTransformer\u0026#34;/\u0026gt; 225\t\u0026lt;/transformers\u0026gt; 226\t\u0026lt;/configuration\u0026gt; 227\t\u0026lt;/execution\u0026gt; 228\t\u0026lt;/executions\u0026gt; 229\t\u0026lt;/plugin\u0026gt; 230\t\u0026lt;/plugins\u0026gt; 231 232\t\u0026lt;pluginManagement\u0026gt; 233\t\u0026lt;plugins\u0026gt; 234 235\t\u0026lt;!-- This improves the out-of-the-box experience in Eclipse by resolving some warnings. --\u0026gt; 236\t\u0026lt;plugin\u0026gt; 237\t\u0026lt;groupId\u0026gt;org.eclipse.m2e\u0026lt;/groupId\u0026gt; 238\t\u0026lt;artifactId\u0026gt;lifecycle-mapping\u0026lt;/artifactId\u0026gt; 239\t\u0026lt;version\u0026gt;1.0.0\u0026lt;/version\u0026gt; 240\t\u0026lt;configuration\u0026gt; 241\t\u0026lt;lifecycleMappingMetadata\u0026gt; 242\t\u0026lt;pluginExecutions\u0026gt; 243\t\u0026lt;pluginExecution\u0026gt; 244\t\u0026lt;pluginExecutionFilter\u0026gt; 245\t\u0026lt;groupId\u0026gt;org.apache.maven.plugins\u0026lt;/groupId\u0026gt; 246\t\u0026lt;artifactId\u0026gt;maven-shade-plugin\u0026lt;/artifactId\u0026gt; 247\t\u0026lt;versionRange\u0026gt;[3.1.1,)\u0026lt;/versionRange\u0026gt; 248\t\u0026lt;goals\u0026gt; 249\t\u0026lt;goal\u0026gt;shade\u0026lt;/goal\u0026gt; 250\t\u0026lt;/goals\u0026gt; 251\t\u0026lt;/pluginExecutionFilter\u0026gt; 252\t\u0026lt;action\u0026gt; 253\t\u0026lt;ignore/\u0026gt; 254\t\u0026lt;/action\u0026gt; 255\t\u0026lt;/pluginExecution\u0026gt; 256\t\u0026lt;pluginExecution\u0026gt; 257\t\u0026lt;pluginExecutionFilter\u0026gt; 258\t\u0026lt;groupId\u0026gt;org.apache.maven.plugins\u0026lt;/groupId\u0026gt; 259\t\u0026lt;artifactId\u0026gt;maven-compiler-plugin\u0026lt;/artifactId\u0026gt; 260\t\u0026lt;versionRange\u0026gt;[3.1,)\u0026lt;/versionRange\u0026gt; 261\t\u0026lt;goals\u0026gt; 262\t\u0026lt;goal\u0026gt;testCompile\u0026lt;/goal\u0026gt; 263\t\u0026lt;goal\u0026gt;compile\u0026lt;/goal\u0026gt; 264\t\u0026lt;/goals\u0026gt; 265\t\u0026lt;/pluginExecutionFilter\u0026gt; 266\t\u0026lt;action\u0026gt; 267\t\u0026lt;ignore/\u0026gt; 268\t\u0026lt;/action\u0026gt; 269\t\u0026lt;/pluginExecution\u0026gt; 270\t\u0026lt;/pluginExecutions\u0026gt; 271\t\u0026lt;/lifecycleMappingMetadata\u0026gt; 272\t\u0026lt;/configuration\u0026gt; 273\t\u0026lt;/plugin\u0026gt; 274\t\u0026lt;/plugins\u0026gt; 275\t\u0026lt;/pluginManagement\u0026gt; 276\t\u0026lt;/build\u0026gt; 277\u0026lt;/project\u0026gt; The Uber Jar file can be built using the following script (build.sh).\n1# build.sh 2#!/usr/bin/env bash 3SCRIPT_DIR=\u0026#34;$(cd $(dirname \u0026#34;$0\u0026#34;); pwd)\u0026#34; 4SRC_PATH=$SCRIPT_DIR/package 5 6# remove contents under $SRC_PATH (except for the folders beginging with lab) 7shopt -s extglob 8rm -rf $SRC_PATH/!(lab*) 9 10## Generate Uber jar file for individual labs 11echo \u0026#34;generate Uber jar for PyFlink app...\u0026#34; 12mkdir $SRC_PATH/lib 13mvn clean install -f $SRC_PATH/lab2-pipeline/pom.xml \\ 14 \u0026amp;\u0026amp; mv $SRC_PATH/lab2-pipeline/target/lab2-pipeline-1.0.0.jar $SRC_PATH/lib \\ 15 \u0026amp;\u0026amp; rm -rf $SRC_PATH/lab2-pipeline/target 16 17mvn clean install -f $SRC_PATH/lab3-pipeline/pom.xml \\ 18 \u0026amp;\u0026amp; mv $SRC_PATH/lab3-pipeline/target/lab3-pipeline-1.0.0.jar $SRC_PATH/lib \\ 19 \u0026amp;\u0026amp; rm -rf $SRC_PATH/lab3-pipeline/target Application Source Although the Pyflink application uses the Table API, the FileSystem connector completes file writing when checkpointing is enabled. Therefore, the app creates a DataStream stream execution environment and enables checkpointing every 60 seconds. Then it creates a table environment, and the source and sink tables are created on it. An additional field named source is added to the sink table, and it is obtained by a user defined function (add_source). The sink table is partitioned by year, month, date and hour.\nIn the main method, we create all the source and sink tables after mapping relevant application properties. Then the output records are written to a S3 bucket. Note that the output records are printed in the terminal additionally when the app is running locally for ease of checking them.\n1# exporter/processor.py 2import os 3import re 4import json 5 6from pyflink.datastream import StreamExecutionEnvironment, RuntimeExecutionMode 7from pyflink.table import StreamTableEnvironment 8from pyflink.table.udf import udf 9 10RUNTIME_ENV = os.environ.get(\u0026#34;RUNTIME_ENV\u0026#34;, \u0026#34;LOCAL\u0026#34;) # LOCAL or DOCKER 11BOOTSTRAP_SERVERS = os.environ.get(\u0026#34;BOOTSTRAP_SERVERS\u0026#34;) # overwrite app config 12 13 14env = StreamExecutionEnvironment.get_execution_environment() 15env.set_runtime_mode(RuntimeExecutionMode.STREAMING) 16env.enable_checkpointing(60000) 17 18if RUNTIME_ENV == \u0026#34;LOCAL\u0026#34;: 19 CURRENT_DIR = os.path.dirname(os.path.realpath(__file__)) 20 PARENT_DIR = os.path.dirname(CURRENT_DIR) 21 APPLICATION_PROPERTIES_FILE_PATH = os.path.join( 22 CURRENT_DIR, \u0026#34;application_properties.json\u0026#34; 23 ) 24 JAR_FILES = [\u0026#34;lab3-pipeline-1.0.0.jar\u0026#34;] 25 JAR_PATHS = tuple( 26 [f\u0026#34;file://{os.path.join(PARENT_DIR, \u0026#39;jars\u0026#39;, name)}\u0026#34; for name in JAR_FILES] 27 ) 28 print(JAR_PATHS) 29 env.add_jars(*JAR_PATHS) 30else: 31 APPLICATION_PROPERTIES_FILE_PATH = \u0026#34;/etc/flink/exporter/application_properties.json\u0026#34; 32 33table_env = StreamTableEnvironment.create(stream_execution_environment=env) 34table_env.create_temporary_function( 35 \u0026#34;add_source\u0026#34;, udf(lambda: \u0026#34;NYCTAXI\u0026#34;, result_type=\u0026#34;STRING\u0026#34;) 36) 37 38 39def get_application_properties(): 40 if os.path.isfile(APPLICATION_PROPERTIES_FILE_PATH): 41 with open(APPLICATION_PROPERTIES_FILE_PATH, \u0026#34;r\u0026#34;) as file: 42 contents = file.read() 43 properties = json.loads(contents) 44 return properties 45 else: 46 raise RuntimeError( 47 f\u0026#34;A file at \u0026#39;{APPLICATION_PROPERTIES_FILE_PATH}\u0026#39; was not found\u0026#34; 48 ) 49 50 51def property_map(props: dict, property_group_id: str): 52 for prop in props: 53 if prop[\u0026#34;PropertyGroupId\u0026#34;] == property_group_id: 54 return prop[\u0026#34;PropertyMap\u0026#34;] 55 56 57def inject_security_opts(opts: dict, bootstrap_servers: str): 58 if re.search(\u0026#34;9098$\u0026#34;, bootstrap_servers): 59 opts = { 60 **opts, 61 **{ 62 \u0026#34;properties.security.protocol\u0026#34;: \u0026#34;SASL_SSL\u0026#34;, 63 \u0026#34;properties.sasl.mechanism\u0026#34;: \u0026#34;AWS_MSK_IAM\u0026#34;, 64 \u0026#34;properties.sasl.jaas.config\u0026#34;: \u0026#34;software.amazon.msk.auth.iam.IAMLoginModule required;\u0026#34;, 65 \u0026#34;properties.sasl.client.callback.handler.class\u0026#34;: \u0026#34;software.amazon.msk.auth.iam.IAMClientCallbackHandler\u0026#34;, 66 }, 67 } 68 return \u0026#34;, \u0026#34;.join({f\u0026#34;\u0026#39;{k}\u0026#39; = \u0026#39;{v}\u0026#39;\u0026#34; for k, v in opts.items()}) 69 70 71def create_source_table(table_name: str, topic_name: str, bootstrap_servers: str): 72 opts = { 73 \u0026#34;connector\u0026#34;: \u0026#34;kafka\u0026#34;, 74 \u0026#34;topic\u0026#34;: topic_name, 75 \u0026#34;properties.bootstrap.servers\u0026#34;: bootstrap_servers, 76 \u0026#34;properties.group.id\u0026#34;: \u0026#34;soruce-group\u0026#34;, 77 \u0026#34;format\u0026#34;: \u0026#34;json\u0026#34;, 78 \u0026#34;scan.startup.mode\u0026#34;: \u0026#34;latest-offset\u0026#34;, 79 } 80 81 stmt = f\u0026#34;\u0026#34;\u0026#34; 82 CREATE TABLE {table_name} ( 83 id VARCHAR, 84 vendor_id INT, 85 pickup_date VARCHAR, 86 pickup_datetime AS TO_TIMESTAMP(REPLACE(pickup_date, \u0026#39;T\u0026#39;, \u0026#39; \u0026#39;)), 87 dropoff_date VARCHAR, 88 dropoff_datetime AS TO_TIMESTAMP(REPLACE(dropoff_date, \u0026#39;T\u0026#39;, \u0026#39; \u0026#39;)), 89 passenger_count INT, 90 pickup_longitude VARCHAR, 91 pickup_latitude VARCHAR, 92 dropoff_longitude VARCHAR, 93 dropoff_latitude VARCHAR, 94 store_and_fwd_flag VARCHAR, 95 gc_distance INT, 96 trip_duration INT, 97 google_distance INT, 98 google_duration INT 99 ) WITH ( 100 {inject_security_opts(opts, bootstrap_servers)} 101 ) 102 \u0026#34;\u0026#34;\u0026#34; 103 print(stmt) 104 return stmt 105 106 107def create_sink_table(table_name: str, file_path: str): 108 stmt = f\u0026#34;\u0026#34;\u0026#34; 109 CREATE TABLE {table_name} ( 110 id VARCHAR, 111 vendor_id INT, 112 pickup_datetime TIMESTAMP, 113 dropoff_datetime TIMESTAMP, 114 passenger_count INT, 115 pickup_longitude VARCHAR, 116 pickup_latitude VARCHAR, 117 dropoff_longitude VARCHAR, 118 dropoff_latitude VARCHAR, 119 store_and_fwd_flag VARCHAR, 120 gc_distance INT, 121 trip_duration INT, 122 google_distance INT, 123 google_duration INT, 124 source VARCHAR, 125 `year` VARCHAR, 126 `month` VARCHAR, 127 `date` VARCHAR, 128 `hour` VARCHAR 129 ) PARTITIONED BY (`year`, `month`, `date`, `hour`) WITH ( 130 \u0026#39;connector\u0026#39;= \u0026#39;filesystem\u0026#39;, 131 \u0026#39;path\u0026#39; = \u0026#39;{file_path}\u0026#39;, 132 \u0026#39;format\u0026#39; = \u0026#39;parquet\u0026#39;, 133 \u0026#39;sink.partition-commit.delay\u0026#39;=\u0026#39;1 h\u0026#39;, 134 \u0026#39;sink.partition-commit.policy.kind\u0026#39;=\u0026#39;success-file\u0026#39; 135 ) 136 \u0026#34;\u0026#34;\u0026#34; 137 print(stmt) 138 return stmt 139 140 141def create_print_table(table_name: str): 142 stmt = f\u0026#34;\u0026#34;\u0026#34; 143 CREATE TABLE {table_name} ( 144 id VARCHAR, 145 vendor_id INT, 146 pickup_datetime TIMESTAMP, 147 dropoff_datetime TIMESTAMP, 148 passenger_count INT, 149 pickup_longitude VARCHAR, 150 pickup_latitude VARCHAR, 151 dropoff_longitude VARCHAR, 152 dropoff_latitude VARCHAR, 153 store_and_fwd_flag VARCHAR, 154 gc_distance INT, 155 trip_duration INT, 156 google_distance INT, 157 google_duration INT, 158 source VARCHAR, 159 `year` VARCHAR, 160 `month` VARCHAR, 161 `date` VARCHAR, 162 `hour` VARCHAR 163 ) WITH ( 164 \u0026#39;connector\u0026#39;= \u0026#39;print\u0026#39; 165 ) 166 \u0026#34;\u0026#34;\u0026#34; 167 print(stmt) 168 return stmt 169 170 171def set_insert_sql(source_table_name: str, sink_table_name: str): 172 stmt = f\u0026#34;\u0026#34;\u0026#34; 173 INSERT INTO {sink_table_name} 174 SELECT 175 id, 176 vendor_id, 177 pickup_datetime, 178 dropoff_datetime, 179 passenger_count, 180 pickup_longitude, 181 pickup_latitude, 182 dropoff_longitude, 183 dropoff_latitude, 184 store_and_fwd_flag, 185 gc_distance, 186 trip_duration, 187 google_distance, 188 google_duration, 189 add_source() AS source, 190 DATE_FORMAT(pickup_datetime, \u0026#39;yyyy\u0026#39;) AS `year`, 191 DATE_FORMAT(pickup_datetime, \u0026#39;MM\u0026#39;) AS `month`, 192 DATE_FORMAT(pickup_datetime, \u0026#39;dd\u0026#39;) AS `date`, 193 DATE_FORMAT(pickup_datetime, \u0026#39;HH\u0026#39;) AS `hour` 194 FROM {source_table_name} 195 \u0026#34;\u0026#34;\u0026#34; 196 print(stmt) 197 return stmt 198 199 200def main(): 201 #### map source/sink properties 202 props = get_application_properties() 203 ## source 204 source_property_group_key = \u0026#34;source.config.0\u0026#34; 205 source_properties = property_map(props, source_property_group_key) 206 print(\u0026#34;\u0026gt;\u0026gt; source properties\u0026#34;) 207 print(source_properties) 208 source_table_name = source_properties[\u0026#34;table.name\u0026#34;] 209 source_topic_name = source_properties[\u0026#34;topic.name\u0026#34;] 210 source_bootstrap_servers = ( 211 BOOTSTRAP_SERVERS or source_properties[\u0026#34;bootstrap.servers\u0026#34;] 212 ) 213 ## sink 214 sink_property_group_key = \u0026#34;sink.config.0\u0026#34; 215 sink_properties = property_map(props, sink_property_group_key) 216 print(\u0026#34;\u0026gt;\u0026gt; sink properties\u0026#34;) 217 print(sink_properties) 218 sink_table_name = sink_properties[\u0026#34;table.name\u0026#34;] 219 sink_file_path = sink_properties[\u0026#34;file.path\u0026#34;] 220 ## print 221 print_table_name = \u0026#34;sink_print\u0026#34; 222 #### create tables 223 table_env.execute_sql( 224 create_source_table( 225 source_table_name, source_topic_name, source_bootstrap_servers 226 ) 227 ) 228 table_env.execute_sql(create_sink_table(sink_table_name, sink_file_path)) 229 table_env.execute_sql(create_print_table(print_table_name)) 230 #### insert into sink tables 231 if RUNTIME_ENV == \u0026#34;LOCAL\u0026#34;: 232 statement_set = table_env.create_statement_set() 233 statement_set.add_insert_sql(set_insert_sql(source_table_name, sink_table_name)) 234 statement_set.add_insert_sql(set_insert_sql(print_table_name, sink_table_name)) 235 statement_set.execute().wait() 236 else: 237 table_result = table_env.execute_sql( 238 set_insert_sql(source_table_name, sink_table_name) 239 ) 240 print(table_result.get_job_client().get_job_status()) 241 242 243if __name__ == \u0026#34;__main__\u0026#34;: 244 main() 1// exporter/application_properties.json 2[ 3 { 4 \u0026#34;PropertyGroupId\u0026#34;: \u0026#34;kinesis.analytics.flink.run.options\u0026#34;, 5 \u0026#34;PropertyMap\u0026#34;: { 6 \u0026#34;python\u0026#34;: \u0026#34;processor.py\u0026#34;, 7 \u0026#34;jarfile\u0026#34;: \u0026#34;package/lib/lab3-pipeline-1.0.0.jar\u0026#34; 8 } 9 }, 10 { 11 \u0026#34;PropertyGroupId\u0026#34;: \u0026#34;source.config.0\u0026#34;, 12 \u0026#34;PropertyMap\u0026#34;: { 13 \u0026#34;table.name\u0026#34;: \u0026#34;taxi_rides_src\u0026#34;, 14 \u0026#34;topic.name\u0026#34;: \u0026#34;taxi-rides\u0026#34;, 15 \u0026#34;bootstrap.servers\u0026#34;: \u0026#34;localhost:29092\u0026#34; 16 } 17 }, 18 { 19 \u0026#34;PropertyGroupId\u0026#34;: \u0026#34;sink.config.0\u0026#34;, 20 \u0026#34;PropertyMap\u0026#34;: { 21 \u0026#34;table.name\u0026#34;: \u0026#34;taxi_rides_sink\u0026#34;, 22 \u0026#34;file.path\u0026#34;: \u0026#34;s3://\u0026lt;s3-bucket-name-to-replace\u0026gt;/taxi-rides/\u0026#34; 23 } 24 } 25] Run Application Execute on Local Flink Cluster We can run the application in the Flink cluster on Docker and the steps are shown below. Either the Kafka cluster on Amazon MSK or a local Kafka cluster can be used depending on which Docker Compose file we use. In either way, we can check the job details on the Flink web UI on localhost:8081. Note that, if we use the local Kafka cluster option, we have to start the producer application in a different terminal.\n1## prep - update s3 bucket name in loader/application_properties.json 2 3## set aws credentials environment variables 4export AWS_ACCESS_KEY_ID=\u0026lt;aws-access-key-id\u0026gt; 5export AWS_SECRET_ACCESS_KEY=\u0026lt;aws-secret-access-key\u0026gt; 6# export AWS_SESSION_TOKEN=\u0026lt;aws-session-token\u0026gt; 7 8## run docker compose service 9# with local kafka cluster 10docker-compose -f compose-local-kafka.yml up -d 11# # or with msk cluster 12# docker-compose -f compose-msk.yml up -d 13 14## run the producer application in another terminal 15# python producer/app.py 16 17## submit pyflink application 18docker exec jobmanager /opt/flink/bin/flink run \\ 19 --python /etc/flink/exporter/processor.py \\ 20 --jarfile /etc/flink/package/lib/lab3-pipeline-1.0.0.jar \\ 21 -d Application Result Kafka Topic We can see the topic (taxi-rides) is created, and the details of the topic can be found on the Topics menu on localhost:3000.\nAlso, we can inspect topic messages in the Data tab as shown below.\nS3 Files We can see the Pyflink app writes the records into the S3 bucket as expected. The files are written in Apache Hive style partitions and only completed files are found. Note that, when I tested the sink connector on the local file system, the file names of in-progress and pending files begin with dot (.) and the dot is removed when they are completed. I don\u0026rsquo;t see those incomplete files in the S3 bucket, and it seems that only completed files are moved. Note also that, as the checkpoint interval is set to 60 seconds, new files are created every minute. We can adjust the interval if it creates too many small files.\nAthena Table To query the output records, we can create a partitioned table on Amazon Athena by specifying the S3 location. It can be created by executing the following SQL statement.\n1CREATE EXTERNAL TABLE taxi_rides ( 2 id STRING, 3 vendor_id INT, 4 pickup_datetime TIMESTAMP, 5 dropoff_datetime TIMESTAMP, 6 passenger_count INT, 7 pickup_longitude STRING, 8 pickup_latitude STRING, 9 dropoff_longitude STRING, 10 dropoff_latitude STRING, 11 store_and_fwd_flag STRING, 12 gc_distance INT, 13 trip_duration INT, 14 google_distance INT, 15 google_duration INT, 16 source STRING 17) 18PARTITIONED BY (year STRING, month STRING, date STRING, hour STRING) 19STORED AS parquet 20LOCATION \u0026#39;s3://real-time-streaming-ap-southeast-2/taxi-rides/\u0026#39;; Then we can add the existing partition by updating the metadata in the catalog i.e. run MSCK REPAIR TABLE taxi_rides;.\n1Partitions not in metastore:\ttaxi_rides:year=2023/month=11/date=14/hour=15 2Repair: Added partition to metastore taxi_rides:year=2023/month=11/date=14/hour=15 After the partition is added, we can query the records as shown below.\nSummary In this lab, we created a Pyflink application that exports Kafka topic messages into a S3 bucket. The app enriched the records by adding a new column using a user defined function and wrote them via the FileSystem SQL connector. While the records were being written to the S3 bucket, a Glue table was created to query them on Amazon Athena.\n","date":"November 16, 2023","img":"/blog/2023-11-16-real-time-streaming-with-kafka-and-flink-4/featured.png","lang":"en","langName":"English","largeImg":"/blog/2023-11-16-real-time-streaming-with-kafka-and-flink-4/featured_hua0f43666abbaff872f0e4bf2da3a3c1a_160359_500x0_resize_box_3.png","permalink":"/blog/2023-11-16-real-time-streaming-with-kafka-and-flink-4/","series":[{"title":"Real Time Streaming With Kafka and Flink","url":"/series/real-time-streaming-with-kafka-and-flink/"}],"smallImg":"/blog/2023-11-16-real-time-streaming-with-kafka-and-flink-4/featured_hua0f43666abbaff872f0e4bf2da3a3c1a_160359_180x0_resize_box_3.png","tags":[{"title":"AWS","url":"/tags/aws/"},{"title":"Amazon MSK","url":"/tags/amazon-msk/"},{"title":"Amazon S3","url":"/tags/amazon-s3/"},{"title":"Amazon Athena","url":"/tags/amazon-athena/"},{"title":"Apache Kafka","url":"/tags/apache-kafka/"},{"title":"Apache Flink","url":"/tags/apache-flink/"},{"title":"PyFlink","url":"/tags/pyflink/"},{"title":"Docker","url":"/tags/docker/"},{"title":"Docker Compose","url":"/tags/docker-compose/"},{"title":"Python","url":"/tags/python/"}],"timestamp":1700092800,"title":"Real Time Streaming With Kafka and Flink - Lab 3 Transform and Write Data to S3 From Kafka Using Flink"},{"categories":[{"title":"Data Streaming","url":"/categories/data-streaming/"}],"content":"In this lab, we will create a Pyflink application that reads records from S3 and sends them into a Kafka topic. A custom pipeline Jar file will be created as the Kafka cluster is authenticated by IAM, and it will be demonstrated how to execute the app in a Flink cluster deployed on Docker as well as locally as a typical Python app. We can assume the S3 data is static metadata that needs to be joined into another stream, and this exercise can be useful for data enrichment.\nIntroduction Lab 1 Produce data to Kafka using Lambda Lab 2 Write data to Kafka from S3 using Flink (this post) Lab 3 Transform and write data to S3 from Kafka using Flink Lab 4 Clean, Aggregate, and Enrich Events with Flink Lab 5 Write data to DynamoDB using Kafka Connect Lab 6 Consume data from Kafka using Lambda [Update 2023-11-06] Initially I planned to deploy Pyflink applications on Amazon Managed Service for Apache Flink, but I changed the plan to use a local Flink cluster deployed on Docker. The main reasons are\nIt is not clear how to configure a Pyflink application for the managed service. For example, Apache Flink supports pluggable file systems and the required dependency (eg flink-s3-fs-hadoop-1.15.2.jar) should be placed under the plugins folder. However, the sample Pyflink applications from pyflink-getting-started and amazon-kinesis-data-analytics-blueprints either ignore the S3 jar file for deployment or package it together with other dependencies - none of them uses the S3 jar file as a plugin. I tried multiple different configurations, but all ended up with an error whose code is CodeError.InvalidApplicationCode. I don\u0026rsquo;t have such an issue when I deploy the app on a local Flink cluster and I haven\u0026rsquo;t found a way to configure the app for the managed service as yet. The Pyflink app for Lab 4 requires the OpenSearch sink connector and the connector is available on 1.16.0+. However, the latest Flink version of the managed service is still 1.15.2 and the sink connector is not available on it. Normally the latest version of the managed service is behind two minor versions of the official release, but it seems to take a little longer to catch up at the moment - the version 1.18.0 was released a while ago. Architecture Sample taxi ride data is stored in a S3 bucket, and a Pyflink application reads and ingests it into a Kafka topic on Amazon MSK. As Apache Flink supports both stream and batch processing, we are able to process static data without an issue. We can assume the S3 data is static metadata that needs to be joined into a stream, and this exercise can be useful for data enrichment.\nInfrastructure AWS Infrastructure The AWS infrastructure is created using Terraform and the source can be found in the GitHub repository of this post - see the previous post for details. The infrastructure can be deployed (as well as destroyed) using Terraform CLI as shown below.\n1# initialize 2$ terraform init 3# create an execution plan 4$ terraform plan 5# execute the actions proposed in a Terraform plan 6$ terraform apply -auto-approve=true 7 8# # destroy all remote objects 9# $ terraform destroy -auto-approve=true Flink Cluster on Docker Docker Image with Python and Pyflink The official Flink docker image doesn\u0026rsquo;t include Python and the Pyflink package, and we need to build a custom image from it. Beginning with placing the S3 jar file (flink-s3-fs-hadoop-1.15.2.jar) under the plugins folder, the following image instals Python and the Pyflink package. It can be built as follows.\n1$ docker build -t=real-time-streaming-aws:1.17.1 . 1# Dockerfile 2FROM flink:1.17.1 3 4ARG PYTHON_VERSION 5ENV PYTHON_VERSION=${PYTHON_VERSION:-3.8.10} 6ARG FLINK_VERSION 7ENV FLINK_VERSION=${FLINK_VERSION:-1.17.1} 8 9RUN mkdir ./plugins/s3-fs-hadoop \\ 10 \u0026amp;\u0026amp; cp ./opt/flink-s3-fs-hadoop-${FLINK_VERSION}.jar ./plugins/s3-fs-hadoop 11 12RUN apt-get update -y \u0026amp;\u0026amp; \\ 13 apt-get install -y build-essential libssl-dev zlib1g-dev libbz2-dev libffi-dev liblzma-dev \u0026amp;\u0026amp; \\ 14 wget https://www.python.org/ftp/python/${PYTHON_VERSION}/Python-${PYTHON_VERSION}.tgz \u0026amp;\u0026amp; \\ 15 tar -xvf Python-${PYTHON_VERSION}.tgz \u0026amp;\u0026amp; \\ 16 cd Python-${PYTHON_VERSION} \u0026amp;\u0026amp; \\ 17 ./configure --without-tests --enable-shared \u0026amp;\u0026amp; \\ 18 make -j6 \u0026amp;\u0026amp; \\ 19 make install \u0026amp;\u0026amp; \\ 20 ldconfig /usr/local/lib \u0026amp;\u0026amp; \\ 21 cd .. \u0026amp;\u0026amp; rm -f Python-${PYTHON_VERSION}.tgz \u0026amp;\u0026amp; rm -rf Python-${PYTHON_VERSION} \u0026amp;\u0026amp; \\ 22 ln -s /usr/local/bin/python3 /usr/local/bin/python \u0026amp;\u0026amp; \\ 23 apt-get clean \u0026amp;\u0026amp; \\ 24 rm -rf /var/lib/apt/lists/* 25 26# install PyFlink 27RUN pip3 install apache-flink==${FLINK_VERSION} 28 29# add kafka client for Flink SQL client, will be added manually 30RUN wget -P /etc/lib/ https://repo.maven.apache.org/maven2/org/apache/kafka/kafka-clients/3.2.3/kafka-clients-3.2.3.jar; Flink Cluster on Docker Compose The docker compose file includes services for a Flink cluster and Kpow Community Edition. For the Flink cluster, both a single master container (jobmanager) and one task container (taskmanager) are created. The former runs the job Dispatcher and ResourceManager while TaskManager is run in the latter. Once a Flink app (job) is submitted to the Dispatcher, it spawns a JobManager thread and provides the JobGraph for execution. The JobManager requests the necessary processing slots from the ResourceManager and deploys the job for execution once the requested slots have been received.\nKafka bootstrap server addresses and AWS credentials are required for the Flink cluster and kpow app, which are specified as environment variables. The bootstrap server addresses can be obtained via terraform (terraform output -json | jq -r '.msk_bootstrap_brokers_sasl_iam.value') or from AWS Console.\nFinally, see the previous post for details about how to configure the kpow app.\n1# compose-msk.yml 2# see compose-local-kafka.yml for a local kafka cluster instead of msk 3version: \u0026#34;3.5\u0026#34; 4 5services: 6 jobmanager: 7 image: real-time-streaming-aws:1.17.1 8 command: jobmanager 9 container_name: jobmanager 10 ports: 11 - \u0026#34;8081:8081\u0026#34; 12 networks: 13 - appnet 14 volumes: 15 - ./loader:/etc/flink 16 - ./exporter:/etc/flink/exporter 17 - ./package:/etc/package 18 environment: 19 - BOOTSTRAP_SERVERS=$BOOTSTRAP_SERVERS 20 - RUNTIME_ENV=DOCKER 21 - AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID 22 - AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY 23 # - AWS_SESSION_TOKEN=$AWS_SESSION_TOKEN 24 - | 25 FLINK_PROPERTIES= 26 jobmanager.rpc.address: jobmanager 27 state.backend: filesystem 28 state.checkpoints.dir: file:///tmp/flink-checkpoints 29 heartbeat.interval: 1000 30 heartbeat.timeout: 5000 31 rest.flamegraph.enabled: true 32 web.backpressure.refresh-interval: 10000 33 taskmanager: 34 image: real-time-streaming-aws:1.17.1 35 command: taskmanager 36 container_name: taskmanager 37 networks: 38 - appnet 39 volumes: 40 - flink_data:/tmp/ 41 - ./:/etc/flink 42 environment: 43 - BOOTSTRAP_SERVERS=$BOOTSTRAP_SERVERS 44 - RUNTIME_ENV=DOCKER 45 - AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID 46 - AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY 47 # - AWS_SESSION_TOKEN=$AWS_SESSION_TOKEN 48 - | 49 FLINK_PROPERTIES= 50 jobmanager.rpc.address: jobmanager 51 taskmanager.numberOfTaskSlots: 5 52 state.backend: filesystem 53 state.checkpoints.dir: file:///tmp/flink-checkpoints 54 heartbeat.interval: 1000 55 heartbeat.timeout: 5000 56 depends_on: 57 - jobmanager 58 kpow: 59 image: factorhouse/kpow-ce:91.5.1 60 container_name: kpow 61 ports: 62 - \u0026#34;3000:3000\u0026#34; 63 networks: 64 - appnet 65 environment: 66 AWS_ACCESS_KEY_ID: $AWS_ACCESS_KEY_ID 67 AWS_SECRET_ACCESS_KEY: $AWS_SECRET_ACCESS_KEY 68 # AWS_SESSION_TOKEN: $AWS_SESSION_TOKEN 69 BOOTSTRAP: $BOOTSTRAP_SERVERS 70 SECURITY_PROTOCOL: SASL_SSL 71 SASL_MECHANISM: AWS_MSK_IAM 72 SASL_JAAS_CONFIG: software.amazon.msk.auth.iam.IAMLoginModule required; 73 SASL_CLIENT_CALLBACK_HANDLER_CLASS: software.amazon.msk.auth.iam.IAMClientCallbackHandler 74 env_file: # https://kpow.io/get-started/#individual 75 - ./kpow.env 76 77networks: 78 appnet: 79 name: app-network 80 81volumes: 82 flink_data: 83 driver: local 84 name: flink_data The Docker Compose services can be deployed as shown below.\n1$ docker-compose -f compose-msk.yml up -d Pyflink Application Flink Pipeline Jar We are going to include all dependent Jar files with the --jarfile option, and it only accepts a single Jar file. Therefore, we have to create a custom Uber jar file that consolidates all dependent Jar files. On top of the Apache Kafka SQL Connector, we also need the Amazon MSK Library for AWS Identity and Access Management (MSK IAM Auth) as the MSK cluster is authenticated via IAM. Note that we have to build the Jar file based on the Apache Kafka Connector instead of the Apache Kafka SQL Connector because the MSK IAM Auth library is not compatible with the latter due to shade relocation. After some search, I found an example from the amazon-kinesis-data-analytics-blueprints and was able to modify the POM file with necessary dependencies for this post. The modified POM file can be shown below, and it creates the Uber Jar for this post - lab2-pipeline-1.0.0.jar.\n1\u0026lt;!-- package/lab2-pipeline/pom.xml --\u0026gt; 2\u0026lt;project xmlns=\u0026#34;http://maven.apache.org/POM/4.0.0\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; 3\txsi:schemaLocation=\u0026#34;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\u0026#34;\u0026gt; 4\t\u0026lt;modelVersion\u0026gt;4.0.0\u0026lt;/modelVersion\u0026gt; 5 6\t\u0026lt;groupId\u0026gt;com.amazonaws.services.kinesisanalytics\u0026lt;/groupId\u0026gt; 7\t\u0026lt;artifactId\u0026gt;lab2-pipeline\u0026lt;/artifactId\u0026gt; 8\t\u0026lt;version\u0026gt;1.0.0\u0026lt;/version\u0026gt; 9\t\u0026lt;packaging\u0026gt;jar\u0026lt;/packaging\u0026gt; 10 11\t\u0026lt;name\u0026gt;Uber Jar for PyFlink App\u0026lt;/name\u0026gt; 12 13\t\u0026lt;properties\u0026gt; 14\t\u0026lt;project.build.sourceEncoding\u0026gt;UTF-8\u0026lt;/project.build.sourceEncoding\u0026gt; 15\t\u0026lt;flink.version\u0026gt;1.17.1\u0026lt;/flink.version\u0026gt; 16\t\u0026lt;target.java.version\u0026gt;1.11\u0026lt;/target.java.version\u0026gt; 17\t\u0026lt;jdk.version\u0026gt;11\u0026lt;/jdk.version\u0026gt; 18\t\u0026lt;scala.binary.version\u0026gt;2.12\u0026lt;/scala.binary.version\u0026gt; 19\t\u0026lt;kda.connectors.version\u0026gt;2.0.0\u0026lt;/kda.connectors.version\u0026gt; 20\t\u0026lt;kda.runtime.version\u0026gt;1.2.0\u0026lt;/kda.runtime.version\u0026gt; 21\t\u0026lt;kafka.clients.version\u0026gt;2.8.1\u0026lt;/kafka.clients.version\u0026gt; 22\t\u0026lt;log4j.version\u0026gt;2.17.1\u0026lt;/log4j.version\u0026gt; 23\t\u0026lt;aws-msk-iam-auth.version\u0026gt;1.1.7\u0026lt;/aws-msk-iam-auth.version\u0026gt; 24\t\u0026lt;/properties\u0026gt; 25 26\t\u0026lt;repositories\u0026gt; 27\t\u0026lt;repository\u0026gt; 28\t\u0026lt;id\u0026gt;apache.snapshots\u0026lt;/id\u0026gt; 29\t\u0026lt;name\u0026gt;Apache Development Snapshot Repository\u0026lt;/name\u0026gt; 30\t\u0026lt;url\u0026gt;https://repository.apache.org/content/repositories/snapshots/\u0026lt;/url\u0026gt; 31\t\u0026lt;releases\u0026gt; 32\t\u0026lt;enabled\u0026gt;false\u0026lt;/enabled\u0026gt; 33\t\u0026lt;/releases\u0026gt; 34\t\u0026lt;snapshots\u0026gt; 35\t\u0026lt;enabled\u0026gt;true\u0026lt;/enabled\u0026gt; 36\t\u0026lt;/snapshots\u0026gt; 37\t\u0026lt;/repository\u0026gt; 38\t\u0026lt;/repositories\u0026gt; 39 40\t\u0026lt;dependencies\u0026gt; 41 42\t\u0026lt;dependency\u0026gt; 43\t\u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; 44\t\u0026lt;artifactId\u0026gt;flink-connector-base\u0026lt;/artifactId\u0026gt; 45\t\u0026lt;version\u0026gt;${flink.version}\u0026lt;/version\u0026gt; 46\t\u0026lt;/dependency\u0026gt; 47 48\t\u0026lt;dependency\u0026gt; 49\t\u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; 50\t\u0026lt;artifactId\u0026gt;flink-connector-kafka\u0026lt;/artifactId\u0026gt; 51\t\u0026lt;version\u0026gt;${flink.version}\u0026lt;/version\u0026gt; 52\t\u0026lt;/dependency\u0026gt; 53 54\t\u0026lt;dependency\u0026gt; 55\t\u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; 56\t\u0026lt;artifactId\u0026gt;flink-connector-files\u0026lt;/artifactId\u0026gt; 57\t\u0026lt;version\u0026gt;${flink.version}\u0026lt;/version\u0026gt; 58\t\u0026lt;/dependency\u0026gt; 59 60\t\u0026lt;dependency\u0026gt; 61\t\u0026lt;groupId\u0026gt;org.apache.kafka\u0026lt;/groupId\u0026gt; 62\t\u0026lt;artifactId\u0026gt;kafka-clients\u0026lt;/artifactId\u0026gt; 63\t\u0026lt;version\u0026gt;${kafka.clients.version}\u0026lt;/version\u0026gt; 64\t\u0026lt;/dependency\u0026gt; 65 66\t\u0026lt;dependency\u0026gt; 67\t\u0026lt;groupId\u0026gt;software.amazon.msk\u0026lt;/groupId\u0026gt; 68\t\u0026lt;artifactId\u0026gt;aws-msk-iam-auth\u0026lt;/artifactId\u0026gt; 69\t\u0026lt;version\u0026gt;${aws-msk-iam-auth.version}\u0026lt;/version\u0026gt; 70\t\u0026lt;/dependency\u0026gt; 71 72\t\u0026lt;!-- Add logging framework, to produce console output when running in the IDE. --\u0026gt; 73\t\u0026lt;!-- These dependencies are excluded from the application JAR by default. --\u0026gt; 74\t\u0026lt;dependency\u0026gt; 75\t\u0026lt;groupId\u0026gt;org.apache.logging.log4j\u0026lt;/groupId\u0026gt; 76\t\u0026lt;artifactId\u0026gt;log4j-slf4j-impl\u0026lt;/artifactId\u0026gt; 77\t\u0026lt;version\u0026gt;${log4j.version}\u0026lt;/version\u0026gt; 78\t\u0026lt;scope\u0026gt;runtime\u0026lt;/scope\u0026gt; 79\t\u0026lt;/dependency\u0026gt; 80\t\u0026lt;dependency\u0026gt; 81\t\u0026lt;groupId\u0026gt;org.apache.logging.log4j\u0026lt;/groupId\u0026gt; 82\t\u0026lt;artifactId\u0026gt;log4j-api\u0026lt;/artifactId\u0026gt; 83\t\u0026lt;version\u0026gt;${log4j.version}\u0026lt;/version\u0026gt; 84\t\u0026lt;scope\u0026gt;runtime\u0026lt;/scope\u0026gt; 85\t\u0026lt;/dependency\u0026gt; 86\t\u0026lt;dependency\u0026gt; 87\t\u0026lt;groupId\u0026gt;org.apache.logging.log4j\u0026lt;/groupId\u0026gt; 88\t\u0026lt;artifactId\u0026gt;log4j-core\u0026lt;/artifactId\u0026gt; 89\t\u0026lt;version\u0026gt;${log4j.version}\u0026lt;/version\u0026gt; 90\t\u0026lt;scope\u0026gt;runtime\u0026lt;/scope\u0026gt; 91\t\u0026lt;/dependency\u0026gt; 92\t\u0026lt;/dependencies\u0026gt; 93 94\t\u0026lt;build\u0026gt; 95\t\u0026lt;plugins\u0026gt; 96 97\t\u0026lt;!-- Java Compiler --\u0026gt; 98\t\u0026lt;plugin\u0026gt; 99\t\u0026lt;groupId\u0026gt;org.apache.maven.plugins\u0026lt;/groupId\u0026gt; 100\t\u0026lt;artifactId\u0026gt;maven-compiler-plugin\u0026lt;/artifactId\u0026gt; 101\t\u0026lt;version\u0026gt;3.8.0\u0026lt;/version\u0026gt; 102\t\u0026lt;configuration\u0026gt; 103\t\u0026lt;source\u0026gt;${jdk.version}\u0026lt;/source\u0026gt; 104\t\u0026lt;target\u0026gt;${jdk.version}\u0026lt;/target\u0026gt; 105\t\u0026lt;/configuration\u0026gt; 106\t\u0026lt;/plugin\u0026gt; 107 108\t\u0026lt;!-- We use the maven-shade plugin to create a fat jar that contains all necessary dependencies. --\u0026gt; 109\t\u0026lt;!-- Change the value of \u0026lt;mainClass\u0026gt;...\u0026lt;/mainClass\u0026gt; if your program entry point changes. --\u0026gt; 110\t\u0026lt;plugin\u0026gt; 111\t\u0026lt;groupId\u0026gt;org.apache.maven.plugins\u0026lt;/groupId\u0026gt; 112\t\u0026lt;artifactId\u0026gt;maven-shade-plugin\u0026lt;/artifactId\u0026gt; 113\t\u0026lt;version\u0026gt;3.4.1\u0026lt;/version\u0026gt; 114\t\u0026lt;executions\u0026gt; 115\t\u0026lt;!-- Run shade goal on package phase --\u0026gt; 116\t\u0026lt;execution\u0026gt; 117\t\u0026lt;phase\u0026gt;package\u0026lt;/phase\u0026gt; 118\t\u0026lt;goals\u0026gt; 119\t\u0026lt;goal\u0026gt;shade\u0026lt;/goal\u0026gt; 120\t\u0026lt;/goals\u0026gt; 121\t\u0026lt;configuration\u0026gt; 122\t\u0026lt;artifactSet\u0026gt; 123\t\u0026lt;excludes\u0026gt; 124\t\u0026lt;exclude\u0026gt;org.apache.flink:force-shading\u0026lt;/exclude\u0026gt; 125\t\u0026lt;exclude\u0026gt;com.google.code.findbugs:jsr305\u0026lt;/exclude\u0026gt; 126\t\u0026lt;exclude\u0026gt;org.slf4j:*\u0026lt;/exclude\u0026gt; 127\t\u0026lt;exclude\u0026gt;org.apache.logging.log4j:*\u0026lt;/exclude\u0026gt; 128\t\u0026lt;/excludes\u0026gt; 129\t\u0026lt;/artifactSet\u0026gt; 130\t\u0026lt;filters\u0026gt; 131\t\u0026lt;filter\u0026gt; 132\t\u0026lt;!-- Do not copy the signatures in the META-INF folder. 133\tOtherwise, this might cause SecurityExceptions when using the JAR. --\u0026gt; 134\t\u0026lt;artifact\u0026gt;*:*\u0026lt;/artifact\u0026gt; 135\t\u0026lt;excludes\u0026gt; 136\t\u0026lt;exclude\u0026gt;META-INF/*.SF\u0026lt;/exclude\u0026gt; 137\t\u0026lt;exclude\u0026gt;META-INF/*.DSA\u0026lt;/exclude\u0026gt; 138\t\u0026lt;exclude\u0026gt;META-INF/*.RSA\u0026lt;/exclude\u0026gt; 139\t\u0026lt;/excludes\u0026gt; 140\t\u0026lt;/filter\u0026gt; 141\t\u0026lt;/filters\u0026gt; 142\t\u0026lt;transformers\u0026gt; 143\t\u0026lt;transformer implementation=\u0026#34;org.apache.maven.plugins.shade.resource.ServicesResourceTransformer\u0026#34;/\u0026gt; 144\t\u0026lt;/transformers\u0026gt; 145\t\u0026lt;/configuration\u0026gt; 146\t\u0026lt;/execution\u0026gt; 147\t\u0026lt;/executions\u0026gt; 148\t\u0026lt;/plugin\u0026gt; 149\t\u0026lt;/plugins\u0026gt; 150 151\t\u0026lt;pluginManagement\u0026gt; 152\t\u0026lt;plugins\u0026gt; 153 154\t\u0026lt;!-- This improves the out-of-the-box experience in Eclipse by resolving some warnings. --\u0026gt; 155\t\u0026lt;plugin\u0026gt; 156\t\u0026lt;groupId\u0026gt;org.eclipse.m2e\u0026lt;/groupId\u0026gt; 157\t\u0026lt;artifactId\u0026gt;lifecycle-mapping\u0026lt;/artifactId\u0026gt; 158\t\u0026lt;version\u0026gt;1.0.0\u0026lt;/version\u0026gt; 159\t\u0026lt;configuration\u0026gt; 160\t\u0026lt;lifecycleMappingMetadata\u0026gt; 161\t\u0026lt;pluginExecutions\u0026gt; 162\t\u0026lt;pluginExecution\u0026gt; 163\t\u0026lt;pluginExecutionFilter\u0026gt; 164\t\u0026lt;groupId\u0026gt;org.apache.maven.plugins\u0026lt;/groupId\u0026gt; 165\t\u0026lt;artifactId\u0026gt;maven-shade-plugin\u0026lt;/artifactId\u0026gt; 166\t\u0026lt;versionRange\u0026gt;[3.1.1,)\u0026lt;/versionRange\u0026gt; 167\t\u0026lt;goals\u0026gt; 168\t\u0026lt;goal\u0026gt;shade\u0026lt;/goal\u0026gt; 169\t\u0026lt;/goals\u0026gt; 170\t\u0026lt;/pluginExecutionFilter\u0026gt; 171\t\u0026lt;action\u0026gt; 172\t\u0026lt;ignore/\u0026gt; 173\t\u0026lt;/action\u0026gt; 174\t\u0026lt;/pluginExecution\u0026gt; 175\t\u0026lt;pluginExecution\u0026gt; 176\t\u0026lt;pluginExecutionFilter\u0026gt; 177\t\u0026lt;groupId\u0026gt;org.apache.maven.plugins\u0026lt;/groupId\u0026gt; 178\t\u0026lt;artifactId\u0026gt;maven-compiler-plugin\u0026lt;/artifactId\u0026gt; 179\t\u0026lt;versionRange\u0026gt;[3.1,)\u0026lt;/versionRange\u0026gt; 180\t\u0026lt;goals\u0026gt; 181\t\u0026lt;goal\u0026gt;testCompile\u0026lt;/goal\u0026gt; 182\t\u0026lt;goal\u0026gt;compile\u0026lt;/goal\u0026gt; 183\t\u0026lt;/goals\u0026gt; 184\t\u0026lt;/pluginExecutionFilter\u0026gt; 185\t\u0026lt;action\u0026gt; 186\t\u0026lt;ignore/\u0026gt; 187\t\u0026lt;/action\u0026gt; 188\t\u0026lt;/pluginExecution\u0026gt; 189\t\u0026lt;/pluginExecutions\u0026gt; 190\t\u0026lt;/lifecycleMappingMetadata\u0026gt; 191\t\u0026lt;/configuration\u0026gt; 192\t\u0026lt;/plugin\u0026gt; 193\t\u0026lt;/plugins\u0026gt; 194\t\u0026lt;/pluginManagement\u0026gt; 195\t\u0026lt;/build\u0026gt; 196\u0026lt;/project\u0026gt; The Uber Jar file can be built using the following script (build.sh).\n1# build.sh 2#!/usr/bin/env bash 3SCRIPT_DIR=\u0026#34;$(cd $(dirname \u0026#34;$0\u0026#34;); pwd)\u0026#34; 4SRC_PATH=$SCRIPT_DIR/package 5 6# remove contents under $SRC_PATH (except for the folders beginging with lab) 7shopt -s extglob 8rm -rf $SRC_PATH/!(lab*) 9 10## Generate Uber Jar for PyFlink app for MSK cluster with IAM authN 11echo \u0026#34;generate Uber jar for PyFlink app...\u0026#34; 12mkdir $SRC_PATH/lib 13mvn clean install -f $SRC_PATH/lab2-pipeline/pom.xml \\ 14 \u0026amp;\u0026amp; mv $SRC_PATH/lab2-pipeline/target/lab2-pipeline-1.0.0.jar $SRC_PATH/lib \\ 15 \u0026amp;\u0026amp; rm -rf $SRC_PATH/lab2-pipeline/target Application Source The Flink application is developed using the Table API. The source uses the FileSystem SQL Connector and a table is created to read records in a S3 bucket. As mentioned earlier, the S3 file system is accessible as the S3 jar file (flink-s3-fs-hadoop-1.17.1.jar) is placed under the plugins folder of the custom Docker image. The sink table is created to write the source records into a Kafka topic. As the Kafka cluster is authenticated via IAM, additional table options are configured.\nIn the main method, we create all the source and sink tables after mapping relevant application properties. Then the output records are inserted into the output Kafka topic. Note that the output records are printed in the terminal additionally when the app is running locally for ease of checking them.\n1# loader/processor.py 2import os 3import re 4import json 5 6from pyflink.table import EnvironmentSettings, TableEnvironment 7 8RUNTIME_ENV = os.environ.get(\u0026#34;RUNTIME_ENV\u0026#34;, \u0026#34;LOCAL\u0026#34;) # LOCAL or DOCKER 9BOOTSTRAP_SERVERS = os.environ.get(\u0026#34;BOOTSTRAP_SERVERS\u0026#34;) # overwrite app config 10 11env_settings = EnvironmentSettings.in_streaming_mode() 12table_env = TableEnvironment.create(env_settings) 13 14if RUNTIME_ENV == \u0026#34;LOCAL\u0026#34;: 15 CURRENT_DIR = os.path.dirname(os.path.realpath(__file__)) 16 PARENT_DIR = os.path.dirname(CURRENT_DIR) 17 PIPELINE_JAR = \u0026#34;lab2-pipeline-1.0.0.jar\u0026#34; 18 APPLICATION_PROPERTIES_FILE_PATH = os.path.join(CURRENT_DIR, \u0026#34;application_properties.json\u0026#34;) 19 print(f\u0026#34;file://{os.path.join(PARENT_DIR, \u0026#39;package\u0026#39;, \u0026#39;lib\u0026#39;, PIPELINE_JAR)}\u0026#34;) 20 table_env.get_config().set( 21 \u0026#34;pipeline.jars\u0026#34;, 22 f\u0026#34;file://{os.path.join(PARENT_DIR, \u0026#39;package\u0026#39;, \u0026#39;lib\u0026#39;, PIPELINE_JAR)}\u0026#34;, 23 ) 24else: 25 APPLICATION_PROPERTIES_FILE_PATH = \u0026#34;/etc/flink/loader/application_properties.json\u0026#34; 26 27 28def get_application_properties(): 29 if os.path.isfile(APPLICATION_PROPERTIES_FILE_PATH): 30 with open(APPLICATION_PROPERTIES_FILE_PATH, \u0026#34;r\u0026#34;) as file: 31 contents = file.read() 32 properties = json.loads(contents) 33 return properties 34 else: 35 raise RuntimeError(f\u0026#34;A file at \u0026#39;{APPLICATION_PROPERTIES_FILE_PATH}\u0026#39; was not found\u0026#34;) 36 37 38def property_map(props: dict, property_group_id: str): 39 for prop in props: 40 if prop[\u0026#34;PropertyGroupId\u0026#34;] == property_group_id: 41 return prop[\u0026#34;PropertyMap\u0026#34;] 42 43 44def inject_security_opts(opts: dict, bootstrap_servers: str): 45 if re.search(\u0026#34;9098$\u0026#34;, bootstrap_servers): 46 opts = { 47 **opts, 48 **{ 49 \u0026#34;properties.security.protocol\u0026#34;: \u0026#34;SASL_SSL\u0026#34;, 50 \u0026#34;properties.sasl.mechanism\u0026#34;: \u0026#34;AWS_MSK_IAM\u0026#34;, 51 \u0026#34;properties.sasl.jaas.config\u0026#34;: \u0026#34;software.amazon.msk.auth.iam.IAMLoginModule required;\u0026#34;, 52 \u0026#34;properties.sasl.client.callback.handler.class\u0026#34;: \u0026#34;software.amazon.msk.auth.iam.IAMClientCallbackHandler\u0026#34;, 53 }, 54 } 55 return \u0026#34;, \u0026#34;.join({f\u0026#34;\u0026#39;{k}\u0026#39; = \u0026#39;{v}\u0026#39;\u0026#34; for k, v in opts.items()}) 56 57 58def create_source_table(table_name: str, file_path: str): 59 stmt = f\u0026#34;\u0026#34;\u0026#34; 60 CREATE TABLE {table_name} ( 61 id VARCHAR, 62 vendor_id INT, 63 pickup_datetime VARCHAR, 64 dropoff_datetime VARCHAR, 65 passenger_count INT, 66 pickup_longitude VARCHAR, 67 pickup_latitude VARCHAR, 68 dropoff_longitude VARCHAR, 69 dropoff_latitude VARCHAR, 70 store_and_fwd_flag VARCHAR, 71 gc_distance DOUBLE, 72 trip_duration INT, 73 google_distance VARCHAR, 74 google_duration VARCHAR 75 ) WITH ( 76 \u0026#39;connector\u0026#39;= \u0026#39;filesystem\u0026#39;, 77 \u0026#39;format\u0026#39; = \u0026#39;csv\u0026#39;, 78 \u0026#39;path\u0026#39; = \u0026#39;{file_path}\u0026#39; 79 ) 80 \u0026#34;\u0026#34;\u0026#34; 81 print(stmt) 82 return stmt 83 84 85def create_sink_table(table_name: str, topic_name: str, bootstrap_servers: str): 86 opts = { 87 \u0026#34;connector\u0026#34;: \u0026#34;kafka\u0026#34;, 88 \u0026#34;topic\u0026#34;: topic_name, 89 \u0026#34;properties.bootstrap.servers\u0026#34;: bootstrap_servers, 90 \u0026#34;format\u0026#34;: \u0026#34;json\u0026#34;, 91 \u0026#34;key.format\u0026#34;: \u0026#34;json\u0026#34;, 92 \u0026#34;key.fields\u0026#34;: \u0026#34;id\u0026#34;, 93 \u0026#34;properties.allow.auto.create.topics\u0026#34;: \u0026#34;true\u0026#34;, 94 } 95 96 stmt = f\u0026#34;\u0026#34;\u0026#34; 97 CREATE TABLE {table_name} ( 98 id VARCHAR, 99 vendor_id INT, 100 pickup_datetime VARCHAR, 101 dropoff_datetime VARCHAR, 102 passenger_count INT, 103 pickup_longitude VARCHAR, 104 pickup_latitude VARCHAR, 105 dropoff_longitude VARCHAR, 106 dropoff_latitude VARCHAR, 107 store_and_fwd_flag VARCHAR, 108 gc_distance DOUBLE, 109 trip_duration INT, 110 google_distance VARCHAR, 111 google_duration VARCHAR 112 ) WITH ( 113 {inject_security_opts(opts, bootstrap_servers)} 114 ) 115 \u0026#34;\u0026#34;\u0026#34; 116 print(stmt) 117 return stmt 118 119 120def create_print_table(table_name: str): 121 stmt = f\u0026#34;\u0026#34;\u0026#34; 122 CREATE TABLE sink_print ( 123 id VARCHAR, 124 vendor_id INT, 125 pickup_datetime VARCHAR, 126 dropoff_datetime VARCHAR, 127 passenger_count INT, 128 pickup_longitude VARCHAR, 129 pickup_latitude VARCHAR, 130 dropoff_longitude VARCHAR, 131 dropoff_latitude VARCHAR, 132 store_and_fwd_flag VARCHAR, 133 gc_distance DOUBLE, 134 trip_duration INT, 135 google_distance VARCHAR, 136 google_duration VARCHAR 137 ) WITH ( 138 \u0026#39;connector\u0026#39;= \u0026#39;print\u0026#39; 139 ) 140 \u0026#34;\u0026#34;\u0026#34; 141 print(stmt) 142 return stmt 143 144 145def main(): 146 #### map source/sink properties 147 props = get_application_properties() 148 ## source 149 source_property_group_key = \u0026#34;source.config.0\u0026#34; 150 source_properties = property_map(props, source_property_group_key) 151 print(\u0026#34;\u0026gt;\u0026gt; source properties\u0026#34;) 152 print(source_properties) 153 source_table_name = source_properties[\u0026#34;table.name\u0026#34;] 154 source_file_path = source_properties[\u0026#34;file.path\u0026#34;] 155 ## sink 156 sink_property_group_key = \u0026#34;sink.config.0\u0026#34; 157 sink_properties = property_map(props, sink_property_group_key) 158 print(\u0026#34;\u0026gt;\u0026gt; sink properties\u0026#34;) 159 print(sink_properties) 160 sink_table_name = sink_properties[\u0026#34;table.name\u0026#34;] 161 sink_topic_name = sink_properties[\u0026#34;topic.name\u0026#34;] 162 sink_bootstrap_servers = BOOTSTRAP_SERVERS or sink_properties[\u0026#34;bootstrap.servers\u0026#34;] 163 ## print 164 print_table_name = \u0026#34;sink_print\u0026#34; 165 #### create tables 166 table_env.execute_sql(create_source_table(source_table_name, source_file_path)) 167 table_env.execute_sql( 168 create_sink_table(sink_table_name, sink_topic_name, sink_bootstrap_servers) 169 ) 170 table_env.execute_sql(create_print_table(print_table_name)) 171 #### insert into sink tables 172 if RUNTIME_ENV == \u0026#34;LOCAL\u0026#34;: 173 source_table = table_env.from_path(source_table_name) 174 statement_set = table_env.create_statement_set() 175 statement_set.add_insert(sink_table_name, source_table) 176 statement_set.add_insert(print_table_name, source_table) 177 statement_set.execute().wait() 178 else: 179 table_result = table_env.execute_sql( 180 f\u0026#34;INSERT INTO {sink_table_name} SELECT * FROM {source_table_name}\u0026#34; 181 ) 182 print(table_result.get_job_client().get_job_status()) 183 184 185if __name__ == \u0026#34;__main__\u0026#34;: 186 main() 1// loader/application_properties.json 2[ 3 { 4 \u0026#34;PropertyGroupId\u0026#34;: \u0026#34;kinesis.analytics.flink.run.options\u0026#34;, 5 \u0026#34;PropertyMap\u0026#34;: { 6 \u0026#34;python\u0026#34;: \u0026#34;processor.py\u0026#34;, 7 \u0026#34;jarfile\u0026#34;: \u0026#34;package/lib/lab2-pipeline-1.0.0.jar\u0026#34; 8 } 9 }, 10 { 11 \u0026#34;PropertyGroupId\u0026#34;: \u0026#34;source.config.0\u0026#34;, 12 \u0026#34;PropertyMap\u0026#34;: { 13 \u0026#34;table.name\u0026#34;: \u0026#34;taxi_trip_source\u0026#34;, 14 \u0026#34;file.path\u0026#34;: \u0026#34;s3://\u0026lt;s3-bucket-name-to-replace\u0026gt;/taxi-csv/\u0026#34; 15 } 16 }, 17 { 18 \u0026#34;PropertyGroupId\u0026#34;: \u0026#34;sink.config.0\u0026#34;, 19 \u0026#34;PropertyMap\u0026#34;: { 20 \u0026#34;table.name\u0026#34;: \u0026#34;taxi_trip_sink\u0026#34;, 21 \u0026#34;topic.name\u0026#34;: \u0026#34;taxi-trip\u0026#34;, 22 \u0026#34;bootstrap.servers\u0026#34;: \u0026#34;localhost:29092\u0026#34; 23 } 24 } 25] Run Application Execute on Local Flink Cluster We can run the application in the Flink cluster on Docker and the steps are shown below. Either the Kafka cluster on Amazon MSK or a local Kafka cluster can be used depending on which Docker Compose file we use. In either way, we can check the job details on the Flink web UI on localhost:8081. Note that, if we use the local Kafka cluster option, we have to start the producer application in a different terminal.\n1## prep - update s3 bucket name in loader/application_properties.json 2 3## set aws credentials environment variables 4export AWS_ACCESS_KEY_ID=\u0026lt;aws-access-key-id\u0026gt; 5export AWS_SECRET_ACCESS_KEY=\u0026lt;aws-secret-access-key\u0026gt; 6# export AWS_SESSION_TOKEN=\u0026lt;aws-session-token\u0026gt; 7 8## run docker compose service 9# with msk cluster 10docker-compose -f compose-msk.yml up -d 11# # or with local Kafka cluster 12# docker-compose -f compose-local-kafka.yml up -d 13 14## run the producer application in another terminal 15# python producer/app.py 16 17## submit pyflink application 18docker exec jobmanager /opt/flink/bin/flink run \\ 19 --python /etc/flink/loader/processor.py \\ 20 --jarfile /etc/flink/package/lib/lab2-pipeline-1.0.0.jar \\ 21 -d Execute Locally The application can also be executed locally by specifying the runtime environment (RUNTIME_ENV) and bootstrap server addresses (BOOTSTRAP_SERVERS) as shown below.\n1$ RUNTIME_ENV=LOCAL BOOTSTRAP_SERVERS=localhost:29092 python loader/processor.py Note, in order for the Flink app to be able to access the S3 file system, we have to place the S3 jar file (flink-s3-fs-hadoop-1.17.1.jar) in the lib folder of the Pyflink package. For example, my virtual environment is in the venv folder and I can add the Jar file in the venv/lib/python3.8/site-packages/pyflink/lib folder. The package also has the plugins folder but it didn\u0026rsquo;t work when I placed the Jar file under it.\nMonitor Topic We can see the topic (taxi-rides) is created, and the details of the topic can be found on the Topics menu on localhost:3000.\nAlso, we can inspect topic messages in the Data tab as shown below.\nSummary In this lab, we created a Pyflink application that reads records from S3 and sends them into a Kafka topic. A custom pipeline Jar file was created as the Kafka cluster is authenticated by IAM, and it was demonstrated how to execute the app in a Flink cluster deployed on Docker as well as locally as a typical Python app.\n","date":"November 9, 2023","img":"/blog/2023-11-09-real-time-streaming-with-kafka-and-flink-3/featured.png","lang":"en","langName":"English","largeImg":"/blog/2023-11-09-real-time-streaming-with-kafka-and-flink-3/featured_hucc8053169eb3042cf56156294e716152_139114_500x0_resize_box_3.png","permalink":"/blog/2023-11-09-real-time-streaming-with-kafka-and-flink-3/","series":[{"title":"Real Time Streaming With Kafka and Flink","url":"/series/real-time-streaming-with-kafka-and-flink/"}],"smallImg":"/blog/2023-11-09-real-time-streaming-with-kafka-and-flink-3/featured_hucc8053169eb3042cf56156294e716152_139114_180x0_resize_box_3.png","tags":[{"title":"AWS","url":"/tags/aws/"},{"title":"Amazon MSK","url":"/tags/amazon-msk/"},{"title":"Apache Kafka","url":"/tags/apache-kafka/"},{"title":"Apache Flink","url":"/tags/apache-flink/"},{"title":"PyFlink","url":"/tags/pyflink/"},{"title":"Docker","url":"/tags/docker/"},{"title":"Docker Compose","url":"/tags/docker-compose/"},{"title":"Python","url":"/tags/python/"}],"timestamp":1699488000,"title":"Real Time Streaming With Kafka and Flink - Lab 2 Write Data to Kafka From S3 Using Flink"},{"categories":[{"title":"Data Streaming","url":"/categories/data-streaming/"}],"content":"Stream processing technology is becoming more and more popular with companies big and small because it provides superior solutions for many established use cases such as data analytics, ETL, and transactional applications, but also facilitates novel applications, software architectures, and business opportunities. Beginning with traditional data infrastructures and application/data development patterns, this post introduces stateful stream processing and demonstrates to what extent it can improve the traditional development patterns. A consulting company can partner with her clients on their journeys of adopting stateful stream processing, and it can bring huge opportunities. Those opportunities are summarised at the end.\nNote Section 1 and 2 are based on Stream Processing with Apache Flink by Fabian Hueske and Vasiliki Kalavri (O\u0026rsquo;Reilly). You can see the original article on the publisher website.\n1. Traditional Data Infrastructures 1.1 Transactional Processing Applications are usually connected to external services or face human users and continuously process incoming events such as orders, email, or clicks on a website. When an event is processed, an application reads its state or updates it by running transactions against the remote database system. Often, a database system serves multiple applications that sometimes access the same databases or tables. Since multiple applications might work on the same data representation or share the same infrastructure, changing the schema of a table or scaling a database system requires careful planning and a lot of effort.\nA recent approach to overcoming the tight bundling of applications is the microservices design pattern. Microservices are designed as small, self-contained, and independent applications. More complex applications are built by connecting several microservices with each other that only communicate over standardised interfaces such as RESTful HTTP or gRPC connections.\n1.2 Analytical Processing Transactional data is often distributed across several disconnected database systems and is more valuable when it can be jointly analysed. Moreover, the data often needs to be transformed into a common format. Therefore the data is typically replicated to a data warehouse, a dedicated datastore for analytical query workloads.\nAn ETL process extracts data from a transactional database, transforms it into a common representation that might include validation, value normalisation, encoding, deduplication, and schema transformation, and finally loads it into the analytical database. ETL processes can be quite complex and often require technically sophisticated solutions to meet performance requirements. ETL processes need to run periodically to keep the data in the data warehouse synchronised.\n2. Stateful Stream Processing Stateful stream processing is an application design pattern for processing unbounded streams of events and is applicable to many different use cases in the IT infrastructure of a company.\nStateful stream processing applications often ingest their incoming events from an event log. An event log stores and distributes event streams. Events are written to a durable, append-only log, which means that the order of written events cannot be changed. A stream that is written to an event log can be read many times by the same or different consumers. Due to the append-only property of the log, events are always published to all consumers in exactly the same order.\n2.1 Event-Driven Applications Event-driven applications are stateful streaming applications that ingest event streams and process the events with application-specific business logic. Depending on the business logic, an event-driven application can trigger actions such as sending an alert or an email or write events to an outgoing event stream to be consumed by another event-driven application.\nTypical use cases for event-driven applications include\nReal-time recommendations (e.g., for recommending products while customers browse a retailer\u0026rsquo;s website) Pattern detection or complex event processing (e.g., for fraud detection in credit card transactions) Anomaly detection (e.g., to detect attempts to intrude a computer network) Event-driven applications are an evolution of microservices. They communicate via event logs instead of REST or gRPC calls and hold application data as local state instead of writing it to and reading it from an external datastore, such as a relational database or key-value store.\nEvent logs decouple senders and receivers and provide asynchronous, non-blocking event transfer. Applications can be stateful and can locally manage their own state without accessing external datastores. Also, they can be individually operated and scaled.\nEvent-driven applications offer several benefits compared to transactional applications or microservices. Local state access provides very good performance compared to reading and writing queries against remote datastores. Scaling and fault tolerance are handled by the stream processor, and, by leveraging an event log as the input source, the complete input of an application is reliably stored and can be deterministically replayed.\n2.2 Data Pipelines Today\u0026rsquo;s IT architectures include many different datastores, such as relational and special-purpose database systems, event logs, distributed filesystems, in-memory caches, and search indexes.\nA traditional approach to synchronise data in different storage systems is periodic ETL jobs. However, they do not meet the latency requirements for many of today\u0026rsquo;s use cases. An alternative is to use an event log to distribute updates. The updates are written to and distributed by the event log. Consumers of the log incorporate the updates into the affected data stores. Depending on the use case, the transferred data may need to be normalised, enriched with external data, or aggregated before it is ingested by the target data store.\nIngesting, transforming, and inserting data with low latency is another common use case for stateful stream processing applications. This type of application is called a data pipeline. Data pipelines must be able to process large amounts of data in a short time. A stream processor that operates a data pipeline should also feature many source and sink connectors to read data from and write data to various storage systems.\n2.3 Streaming Analytics ETL jobs periodically import data into a datastore and the data is processed by ad-hoc or scheduled queries, which adds considerable latency to the analytics pipeline.\nInstead of waiting to be periodically triggered, a streaming analytics application continuously ingests streams of events and updates its result by incorporating the latest events with low latency. Typically, streaming applications store their result in an external data store that supports efficient updates, such as a database, key-value store or search engine. The live updated results of a streaming analytics application can be used to power dashboard applications.\nStreaming analytics applications are commonly used for:\nMonitoring the quality of cellphone networks Analysing user behaviour in mobile applications Ad-hoc analysis of live data in consumer technology 3. Opportunities of Stateful Stream Processing Each of the application areas of the stateful stream processing pattern mentioned above can bring opportunities and it falls into either application modernisation and data engineering.\nEvent-driven application is in relation to application modernisation. On AWS, I don\u0026rsquo;t see many examples of implementing stateful stream processing in event-driven application development. Rather they focus on how to make use of their serverless services (EventBridge, Step Functions, SQS, SNS and Lambda) together, but it could miss some benefits of stateful stream processing such as local state access and deterministic replay of complete input. I find application architecture can be simpler with stateful stream processing than combining a wide range of AWS services, and it would bring value to customers.\nData pipeline and streaming analytics are key opportunities in data engineering. The former is already popular especially thanks to Change Data Capture (CDC) and I see many customers implement it already or are highly interested in it. I still don\u0026rsquo;t see wide-spread adoption of streaming analytics among customers. However, as the latest generation of stream processors including Apache Flink provide accurate stream processing with high throughput and low latency at scale, I consider it is a matter of time until streaming analytics plays a central role for serving analytical processing needs.\nBuilding and maintaining stream processing infrastructure for clients can be an opportunity in application modernisation as well. First of all, Apache Kafka can be used as a distributed event store and it plays a key role because stream processing applications tend to read from or write to an event store. Kafka Connect can also be important as a tool for streaming data between Apache Kafka and other data systems by connectors in a scalable and reliable way. On AWS, there are multiple options and some of them cover Amazon MSK, Confluent Platform via Amazon Marketplace and self-managed cluster on Amazon EKS. Secondly, Apache Flink can be used as the main stream processor as it supports key requirements - state handling, event-time processing, exactly-once state consistency, recovery from failure to name a few. On AWS, Amazon Managed Service for Apache Flink is the easiest option. Moreover EMR on EKS supports Flink workloads (in preview) and self-managed applications can run on Amazon EKS. Lastly, Amazon EKS can be beneficial for deploying applications and workloads related to stateful stream processing in a more efficient manner.\n","date":"November 2, 2023","img":"/blog/2023-11-02-stateful-stream-processing/featured.png","lang":"en","langName":"English","largeImg":"/blog/2023-11-02-stateful-stream-processing/featured_hua1cae1e37e5d79dce66730fc758b0280_244920_500x0_resize_box_3.png","permalink":"/blog/2023-11-02-stateful-stream-processing/","series":[],"smallImg":"/blog/2023-11-02-stateful-stream-processing/featured_hua1cae1e37e5d79dce66730fc758b0280_244920_180x0_resize_box_3.png","tags":[{"title":"Stateful Stream Processing","url":"/tags/stateful-stream-processing/"},{"title":"Event Driven Architecture","url":"/tags/event-driven-architecture/"},{"title":"Data Pipeline","url":"/tags/data-pipeline/"},{"title":"Streaming Analytics","url":"/tags/streaming-analytics/"},{"title":"Apache Flink","url":"/tags/apache-flink/"},{"title":"Apache Kafka","url":"/tags/apache-kafka/"}],"timestamp":1698883200,"title":"Benefits and Opportunities of Stateful Stream Processing"},{"categories":[{"title":"Data Streaming","url":"/categories/data-streaming/"}],"content":"In the previous post, we discussed how to develop a data pipeline from Apache Kafka into OpenSearch locally using Docker. The pipeline will be deployed on AWS using Amazon MSK, Amazon MSK Connect and Amazon OpenSearch Service using Terraform in this post. First the infrastructure will be deployed that covers a Virtual Private Cloud (VPC), Virtual Private Network (VPN) server, MSK Cluster and OpenSearch domain. Then Kafka source and sink connectors will be deployed on MSK Connect, followed by performing quick data analysis.\nPart 1 Introduction Part 2 Develop Camel DynamoDB Sink Connector Part 3 Deploy Camel DynamoDB Sink Connector Part 4 Develop Aiven OpenSearch Sink Connector Part 5 Deploy Aiven OpenSearch Sink Connector (this post) Architecture Fake impressions and clicks events are generated by the Amazon MSK Data Generator, and they are pushed into the corresponding Kafka topics. The topic records are ingested into OpenSearch indexes with the same names for near real-time analysis using the Aiven\u0026rsquo;s OpenSearch Connector for Apache Kafka. Note that, as the Kafka cluster and OpenSearch Service domain are deployed in private subnets, a VPN server is used to access them from the developer machine.\nInfrastructure The infrastructure is created using Terraform and the source can be found in the GitHub repository of this post.\nVPC and VPN A VPC with 3 public and private subnets is created using the AWS VPC Terraform module (vpc.tf). Also, a SoftEther VPN server is deployed in order to access the resources in the private subnets from the developer machine (vpn.tf). The details about how to configure the VPN server can be found in this post.\nOpenSearch Cluster An OpenSearch domain that has two instances is created. It is deployed with the m5.large.search instance type in private subnets. For simplicity, anonymous authentication is enabled so that we don\u0026rsquo;t have to specify user credentials when making an HTTP request. Overall only network-level security is enforced on the OpenSearch domain.\n1# variable.tf 2locals { 3 ... 4 opensearch = { 5 engine_version = \u0026#34;2.7\u0026#34; 6 instance_type = \u0026#34;m5.large.search\u0026#34; 7 instance_count = 2 8 } 9 ... 10} 11 12# opensearch.tf 13resource \u0026#34;aws_opensearch_domain\u0026#34; \u0026#34;opensearch\u0026#34; { 14 domain_name = local.name 15 engine_version = \u0026#34;OpenSearch_${local.opensearch.engine_version}\u0026#34; 16 17 cluster_config { 18 dedicated_master_enabled = false 19 instance_type = local.opensearch.instance_type # m5.large.search 20 instance_count = local.opensearch.instance_count # 2 21 zone_awareness_enabled = true 22 } 23 24 advanced_security_options { 25 enabled = false 26 anonymous_auth_enabled = true 27 internal_user_database_enabled = true 28 } 29 30 domain_endpoint_options { 31 enforce_https = true 32 tls_security_policy = \u0026#34;Policy-Min-TLS-1-2-2019-07\u0026#34; 33 custom_endpoint_enabled = false 34 } 35 36 ebs_options { 37 ebs_enabled = true 38 volume_size = 10 39 } 40 41 log_publishing_options { 42 cloudwatch_log_group_arn = aws_cloudwatch_log_group.opensearch_log_group_index_slow_logs.arn 43 log_type = \u0026#34;INDEX_SLOW_LOGS\u0026#34; 44 } 45 46 vpc_options { 47 subnet_ids = slice(module.vpc.private_subnets, 0, local.opensearch.instance_count) 48 security_group_ids = [aws_security_group.opensearch.id] 49 } 50 51 access_policies = jsonencode({ 52 Version = \u0026#34;2012-10-17\u0026#34; 53 Statement = [ 54 { 55 Action = \u0026#34;es:*\u0026#34;, 56 Principal = \u0026#34;*\u0026#34;, 57 Effect = \u0026#34;Allow\u0026#34;, 58 Resource = \u0026#34;arn:aws:es:${data.aws_region.current.name}:${data.aws_caller_identity.current.account_id}:domain/${local.name}/*\u0026#34; 59 } 60 ] 61 }) 62} OpenSearch Security Group The security group of the OpenSearch domain has inbound rules that allow connection from the security groups of the MSK cluster and VPN server. Only port 443 and 9200 are open for accessing the OpenSearch Dashboards and making HTTP requests.\n1# opensearch.tf 2resource \u0026#34;aws_security_group\u0026#34; \u0026#34;opensearch\u0026#34; { 3 name = \u0026#34;${local.name}-opensearch-sg\u0026#34; 4 vpc_id = module.vpc.vpc_id 5 6 lifecycle { 7 create_before_destroy = true 8 } 9 10 tags = local.tags 11} 12 13resource \u0026#34;aws_security_group_rule\u0026#34; \u0026#34;opensearch_msk_inbound_https\u0026#34; { 14 type = \u0026#34;ingress\u0026#34; 15 description = \u0026#34;Allow inbound traffic for OpenSearch Dashboard from MSK\u0026#34; 16 security_group_id = aws_security_group.opensearch.id 17 protocol = \u0026#34;tcp\u0026#34; 18 from_port = 443 19 to_port = 443 20 source_security_group_id = aws_security_group.msk.id 21} 22 23resource \u0026#34;aws_security_group_rule\u0026#34; \u0026#34;opensearch_msk_inbound_rest\u0026#34; { 24 type = \u0026#34;ingress\u0026#34; 25 description = \u0026#34;Allow inbound traffic for OpenSearch REST API from MSK\u0026#34; 26 security_group_id = aws_security_group.opensearch.id 27 protocol = \u0026#34;tcp\u0026#34; 28 from_port = 9200 29 to_port = 9200 30 source_security_group_id = aws_security_group.msk.id 31} 32 33resource \u0026#34;aws_security_group_rule\u0026#34; \u0026#34;opensearch_vpn_inbound_https\u0026#34; { 34 count = local.vpn.to_create ? 1 : 0 35 type = \u0026#34;ingress\u0026#34; 36 description = \u0026#34;Allow inbound traffic for OpenSearch Dashboard from VPN\u0026#34; 37 security_group_id = aws_security_group.opensearch.id 38 protocol = \u0026#34;tcp\u0026#34; 39 from_port = 443 40 to_port = 443 41 source_security_group_id = aws_security_group.vpn[0].id 42} 43 44resource \u0026#34;aws_security_group_rule\u0026#34; \u0026#34;opensearch_vpn_inbound_rest\u0026#34; { 45 count = local.vpn.to_create ? 1 : 0 46 type = \u0026#34;ingress\u0026#34; 47 description = \u0026#34;Allow inbound traffic for OpenSearch REST API from VPN\u0026#34; 48 security_group_id = aws_security_group.opensearch.id 49 protocol = \u0026#34;tcp\u0026#34; 50 from_port = 9200 51 to_port = 9200 52 source_security_group_id = aws_security_group.vpn[0].id 53} MSK Cluster An MSK cluster with 2 brokers is created. The broker nodes are deployed with the kafka.m5.large instance type in private subnets and IAM authentication is used for the client authentication method. Finally, additional server configurations are added such as enabling auto creation of topics and topic deletion.\n1# variable.tf 2locals { 3 ... 4 msk = { 5 version = \u0026#34;2.8.1\u0026#34; 6 instance_size = \u0026#34;kafka.m5.large\u0026#34; 7 ebs_volume_size = 20 8 log_retention_ms = 604800000 # 7 days 9 number_of_broker_nodes = 2 10 num_partitions = 2 11 default_replication_factor = 2 12 } 13 ... 14} 15 16# msk.tf 17resource \u0026#34;aws_msk_cluster\u0026#34; \u0026#34;msk_data_cluster\u0026#34; { 18 cluster_name = \u0026#34;${local.name}-msk-cluster\u0026#34; 19 kafka_version = local.msk.version 20 number_of_broker_nodes = local.msk.number_of_broker_nodes 21 configuration_info { 22 arn = aws_msk_configuration.msk_config.arn 23 revision = aws_msk_configuration.msk_config.latest_revision 24 } 25 26 broker_node_group_info { 27 instance_type = local.msk.instance_size 28 client_subnets = slice(module.vpc.private_subnets, 0, local.msk.number_of_broker_nodes) 29 security_groups = [aws_security_group.msk.id] 30 storage_info { 31 ebs_storage_info { 32 volume_size = local.msk.ebs_volume_size 33 } 34 } 35 } 36 37 client_authentication { 38 sasl { 39 iam = true 40 } 41 } 42 43 logging_info { 44 broker_logs { 45 cloudwatch_logs { 46 enabled = true 47 log_group = aws_cloudwatch_log_group.msk_cluster_lg.name 48 } 49 s3 { 50 enabled = true 51 bucket = aws_s3_bucket.default_bucket.id 52 prefix = \u0026#34;logs/msk/cluster/\u0026#34; 53 } 54 } 55 } 56 57 tags = local.tags 58 59 depends_on = [aws_msk_configuration.msk_config] 60} 61 62resource \u0026#34;aws_msk_configuration\u0026#34; \u0026#34;msk_config\u0026#34; { 63 name = \u0026#34;${local.name}-msk-configuration\u0026#34; 64 65 kafka_versions = [local.msk.version] 66 67 server_properties = \u0026lt;\u0026lt;PROPERTIES 68 auto.create.topics.enable = true 69 delete.topic.enable = true 70 log.retention.ms = ${local.msk.log_retention_ms} 71 num.partitions = ${local.msk.num_partitions} 72 default.replication.factor = ${local.msk.default_replication_factor} 73 PROPERTIES 74} MSK Security Group The security group of the MSK cluster allows all inbound traffic from itself and all outbound traffic into all IP addresses. The Kafka connectors will use the same security group and the former is necessary. Both the rules are configured too generously, however, we can limit the protocol and port ranges in production. Also, the security group has an additional inbound rule that permits it to connect on port 9098 from the security group of the VPN server.\n1resource \u0026#34;aws_security_group\u0026#34; \u0026#34;msk\u0026#34; { 2 name = \u0026#34;${local.name}-msk-sg\u0026#34; 3 vpc_id = module.vpc.vpc_id 4 5 lifecycle { 6 create_before_destroy = true 7 } 8 9 tags = local.tags 10} 11 12resource \u0026#34;aws_security_group_rule\u0026#34; \u0026#34;msk_self_inbound_all\u0026#34; { 13 type = \u0026#34;ingress\u0026#34; 14 description = \u0026#34;Allow ingress from itself - required for MSK Connect\u0026#34; 15 security_group_id = aws_security_group.msk.id 16 protocol = \u0026#34;-1\u0026#34; 17 from_port = \u0026#34;0\u0026#34; 18 to_port = \u0026#34;0\u0026#34; 19 source_security_group_id = aws_security_group.msk.id 20} 21 22resource \u0026#34;aws_security_group_rule\u0026#34; \u0026#34;msk_self_outbound_all\u0026#34; { 23 type = \u0026#34;egress\u0026#34; 24 description = \u0026#34;Allow outbound all\u0026#34; 25 security_group_id = aws_security_group.msk.id 26 protocol = \u0026#34;-1\u0026#34; 27 from_port = \u0026#34;0\u0026#34; 28 to_port = \u0026#34;0\u0026#34; 29 cidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;] 30} 31 32resource \u0026#34;aws_security_group_rule\u0026#34; \u0026#34;msk_vpn_inbound\u0026#34; { 33 count = local.vpn.to_create ? 1 : 0 34 type = \u0026#34;ingress\u0026#34; 35 description = \u0026#34;Allow VPN access\u0026#34; 36 security_group_id = aws_security_group.msk.id 37 protocol = \u0026#34;tcp\u0026#34; 38 from_port = 9098 39 to_port = 9098 40 source_security_group_id = aws_security_group.vpn[0].id 41} Deploy Infrastructure To separate infrastructure deployment from Kafka connector creation, I added a Terraform variable called to_create_connector. As can be seen below, if we set the value to false, the Terraform resources will be created without MSK connectors. Below shows how to deploy the infrastructure.\n1$ terraform init 2$ terraform plan -var to_create_connector=false 3$ terraform apply --auto-approve=true -var to_create_connector=false Once completed, we can check the two key resources on AWS Console - OpenSearch domain and MSK cluster.\nKafka Management App A Kafka management app can be a good companion for development as it helps monitor and manage resources on an easy-to-use user interface. We\u0026rsquo;ll use Kpow Community Edition in this post, and we can link a single Kafka cluster, Kafka connect server and schema registry. Note that the community edition is valid for 12 months and the license can be requested in this page. Once requested, the license details will be emailed, and they can be added as an environment file (env_file).\nThe app needs additional configurations in environment variables because the Kafka cluster on Amazon MSK is authenticated by IAM. The bootstrap server address can be found on AWS Console or executing the following Terraform command.\n1$ terraform output -json | jq -r \u0026#39;.msk_bootstrap_brokers_sasl_iam.value\u0026#39; It also requires AWS credentials and SASL security configurations for IAM Authentication. Finally, I added an environment variable that indicates in which AWS region the MSK connectors are deployed. For further details, see the Kpow documentation.\n1# docker-compose.yml 2version: \u0026#34;3\u0026#34; 3 4services: 5 kpow: 6 image: factorhouse/kpow-ce:91.2.1 7 container_name: kpow 8 ports: 9 - \u0026#34;3000:3000\u0026#34; 10 networks: 11 - appnet 12 environment: 13 AWS_ACCESS_KEY_ID: $AWS_ACCESS_KEY_ID 14 AWS_SECRET_ACCESS_KEY: $AWS_SECRET_ACCESS_KEY 15 AWS_SESSION_TOKEN: $AWS_SESSION_TOKEN 16 # MSK cluster 17 BOOTSTRAP: $BOOTSTRAP_SERVERS 18 SECURITY_PROTOCOL: SASL_SSL 19 SASL_MECHANISM: AWS_MSK_IAM 20 SASL_CLIENT_CALLBACK_HANDLER_CLASS: software.amazon.msk.auth.iam.IAMClientCallbackHandler 21 SASL_JAAS_CONFIG: software.amazon.msk.auth.iam.IAMLoginModule required; 22 # MSK connect 23 CONNECT_AWS_REGION: $AWS_DEFAULT_REGION 24 25networks: 26 appnet: 27 name: app-network Data Pipeline Preparation Download Connector Sources The connector sources need to be downloaded into the ./connectors path so that they can be volume-mapped to the container\u0026rsquo;s plugin path (/opt/connectors). The MSK Data Generator is a single Jar file, and it can be kept as it is. On the other hand, the Aiven OpenSearch sink connector is an archive file, and it should be decompressed. Note the zip file will be used to create a custom plugin of MSK Connect in the next post. The following script downloads the connector sources into the host path.\n1# download.sh 2#!/usr/bin/env bash 3shopt -s extglob 4 5SCRIPT_DIR=\u0026#34;$(cd $(dirname \u0026#34;$0\u0026#34;); pwd)\u0026#34; 6 7SRC_PATH=${SCRIPT_DIR}/connectors 8rm -rf ${SRC_PATH} \u0026amp;\u0026amp; mkdir ${SRC_PATH} 9 10## Avien opensearch sink connector 11echo \u0026#34;downloading opensearch sink connector...\u0026#34; 12DOWNLOAD_URL=https://github.com/Aiven-Open/opensearch-connector-for-apache-kafka/releases/download/v3.1.0/opensearch-connector-for-apache-kafka-3.1.0.zip 13 14curl -L -o ${SRC_PATH}/tmp.zip ${DOWNLOAD_URL} \\ 15 \u0026amp;\u0026amp; unzip -qq ${SRC_PATH}/tmp.zip -d ${SRC_PATH} \\ 16 \u0026amp;\u0026amp; rm -rf $SRC_PATH/!(opensearch-connector-for-apache-kafka-3.1.0) \\ 17 \u0026amp;\u0026amp; mv $SRC_PATH/opensearch-connector-for-apache-kafka-3.1.0 $SRC_PATH/opensearch-connector \\ 18 \u0026amp;\u0026amp; cd $SRC_PATH/opensearch-connector \\ 19 \u0026amp;\u0026amp; zip ../opensearch-connector.zip * 20 21## MSK Data Generator Souce Connector 22echo \u0026#34;downloading msk data generator...\u0026#34; 23DOWNLOAD_URL=https://github.com/awslabs/amazon-msk-data-generator/releases/download/v0.4.0/msk-data-generator-0.4-jar-with-dependencies.jar 24 25mkdir ${SRC_PATH}/msk-datagen \\ 26 \u0026amp;\u0026amp; curl -L -o ${SRC_PATH}/msk-datagen/msk-data-generator.jar ${DOWNLOAD_URL} Below shows the folder structure after the connectors are downloaded successfully.\n1$ tree connectors/ 2connectors/ 3├── msk-datagen 4│ └── msk-data-generator.jar 5├── opensearch-connector 6 7... 8 9│ ├── opensearch-2.6.0.jar 10│ ├── opensearch-cli-2.6.0.jar 11│ ├── opensearch-common-2.6.0.jar 12│ ├── opensearch-connector-for-apache-kafka-3.1.0.jar 13│ ├── opensearch-core-2.6.0.jar 14 15... 16 17└── opensearch-connector.zip 18 192 directories, 56 files Create Index Mappings The topic messages include a timestamp field (created_at), but its type is not identified correctly via dynamic mapping. Instead, indexes are created explicitly as shown below. Note that we don\u0026rsquo;t have to specify user credentials because anonymous authentication is enabled. Make sure to connect to the VPN server before executing this script.\n1# configs/create-index-mappings.sh 2#!/usr/bin/env bash 3OPENSEARCH_ENDPOINT=$(terraform output -json | jq -r \u0026#39;.opensearch_domain_endpoint.value\u0026#39;) 4echo \u0026#34;OpenSearch endpoint - $OPENSEARCH_ENDPOINT ...\u0026#34; 5 6echo \u0026#34;Create impressions index and field mapping\u0026#34; 7curl -X PUT \u0026#34;https://$OPENSEARCH_ENDPOINT/impressions\u0026#34; -H \u0026#39;Content-Type: application/json\u0026#39; -d\u0026#39; 8{ 9 \u0026#34;mappings\u0026#34;: { 10 \u0026#34;properties\u0026#34;: { 11 \u0026#34;bid_id\u0026#34;: { 12 \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34; 13 }, 14 \u0026#34;created_at\u0026#34;: { 15 \u0026#34;type\u0026#34;: \u0026#34;date\u0026#34;, 16 \u0026#34;format\u0026#34;: \u0026#34;yyyy-MM-dd HH:mm:ss\u0026#34; 17 }, 18 \u0026#34;campaign_id\u0026#34;: { 19 \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34; 20 }, 21 \u0026#34;creative_details\u0026#34;: { 22 \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34; 23 }, 24 \u0026#34;country_code\u0026#34;: { 25 \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34; 26 } 27 } 28 } 29}\u0026#39; 30 31echo 32echo \u0026#34;Create clicks index and field mapping\u0026#34; 33curl -X PUT \u0026#34;https://$OPENSEARCH_ENDPOINT/clicks\u0026#34; -H \u0026#39;Content-Type: application/json\u0026#39; -d\u0026#39; 34{ 35 \u0026#34;mappings\u0026#34;: { 36 \u0026#34;properties\u0026#34;: { 37 \u0026#34;correlation_id\u0026#34;: { 38 \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34; 39 }, 40 \u0026#34;created_at\u0026#34;: { 41 \u0026#34;type\u0026#34;: \u0026#34;date\u0026#34;, 42 \u0026#34;format\u0026#34;: \u0026#34;yyyy-MM-dd HH:mm:ss\u0026#34; 43 }, 44 \u0026#34;tracker\u0026#34;: { 45 \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34; 46 } 47 } 48 } 49}\u0026#39; 50 51echo Once created, we can check them in the OpenSearch Dashboards where its endpoint can be found by executing the following Terraform command.\n1$ terraform output -json | jq -r \u0026#39;.opensearch_domain_dashboard_endpoint.value\u0026#39; Connector IAM Role For simplicity, a single IAM role will be used for both the source and sink connectors. The custom managed policy has permission on MSK cluster resources (cluster, topic and group). It also has permission on S3 bucket and CloudWatch Log for logging.\n1# msk-connect.tf 2resource \u0026#34;aws_iam_role\u0026#34; \u0026#34;kafka_connector_role\u0026#34; { 3 name = \u0026#34;${local.name}-connector-role\u0026#34; 4 5 assume_role_policy = jsonencode({ 6 Version = \u0026#34;2012-10-17\u0026#34; 7 Statement = [ 8 { 9 Action = \u0026#34;sts:AssumeRole\u0026#34; 10 Effect = \u0026#34;Allow\u0026#34; 11 Sid = \u0026#34;\u0026#34; 12 Principal = { 13 Service = \u0026#34;kafkaconnect.amazonaws.com\u0026#34; 14 } 15 }, 16 ] 17 }) 18 managed_policy_arns = [ 19 \u0026#34;arn:aws:iam::aws:policy/AmazonDynamoDBFullAccess\u0026#34;, 20 aws_iam_policy.kafka_connector_policy.arn 21 ] 22} 23 24resource \u0026#34;aws_iam_policy\u0026#34; \u0026#34;kafka_connector_policy\u0026#34; { 25 name = \u0026#34;${local.name}-connector-policy\u0026#34; 26 27 policy = jsonencode({ 28 Version = \u0026#34;2012-10-17\u0026#34; 29 Statement = [ 30 { 31 Sid = \u0026#34;PermissionOnCluster\u0026#34; 32 Action = [ 33 \u0026#34;kafka-cluster:Connect\u0026#34;, 34 \u0026#34;kafka-cluster:AlterCluster\u0026#34;, 35 \u0026#34;kafka-cluster:DescribeCluster\u0026#34; 36 ] 37 Effect = \u0026#34;Allow\u0026#34; 38 Resource = \u0026#34;arn:aws:kafka:${local.region}:${data.aws_caller_identity.current.account_id}:cluster/${local.name}-msk-cluster/*\u0026#34; 39 }, 40 { 41 Sid = \u0026#34;PermissionOnTopics\u0026#34; 42 Action = [ 43 \u0026#34;kafka-cluster:*Topic*\u0026#34;, 44 \u0026#34;kafka-cluster:WriteData\u0026#34;, 45 \u0026#34;kafka-cluster:ReadData\u0026#34; 46 ] 47 Effect = \u0026#34;Allow\u0026#34; 48 Resource = \u0026#34;arn:aws:kafka:${local.region}:${data.aws_caller_identity.current.account_id}:topic/${local.name}-msk-cluster/*\u0026#34; 49 }, 50 { 51 Sid = \u0026#34;PermissionOnGroups\u0026#34; 52 Action = [ 53 \u0026#34;kafka-cluster:AlterGroup\u0026#34;, 54 \u0026#34;kafka-cluster:DescribeGroup\u0026#34; 55 ] 56 Effect = \u0026#34;Allow\u0026#34; 57 Resource = \u0026#34;arn:aws:kafka:${local.region}:${data.aws_caller_identity.current.account_id}:group/${local.name}-msk-cluster/*\u0026#34; 58 }, 59 { 60 Sid = \u0026#34;PermissionOnDataBucket\u0026#34; 61 Action = [ 62 \u0026#34;s3:ListBucket\u0026#34;, 63 \u0026#34;s3:*Object\u0026#34; 64 ] 65 Effect = \u0026#34;Allow\u0026#34; 66 Resource = [ 67 \u0026#34;${aws_s3_bucket.default_bucket.arn}\u0026#34;, 68 \u0026#34;${aws_s3_bucket.default_bucket.arn}/*\u0026#34; 69 ] 70 }, 71 { 72 Sid = \u0026#34;LoggingPermission\u0026#34; 73 Action = [ 74 \u0026#34;logs:CreateLogStream\u0026#34;, 75 \u0026#34;logs:CreateLogGroup\u0026#34;, 76 \u0026#34;logs:PutLogEvents\u0026#34; 77 ] 78 Effect = \u0026#34;Allow\u0026#34; 79 Resource = \u0026#34;*\u0026#34; 80 }, 81 ] 82 }) 83} Source Connector The connector source will be uploaded into S3 followed by creating a customer plugin. Then the source connector will be created using the custom plugin and deployed in private subnets.\nWhen it comes to the connector configuration, the first six attributes are in relation to general configurations. The connector class (connector.class) is required for any connector and I set it for the MSK Data Generator. Also, two tasks are allocated to it (tasks.max). The message key is set to be converted into string (key.converter) while the value to json (value.converter). The former is because the keys are configured to have string primitive values (genkp) by the source connector. Finally, schemas are not enabled for both the key and value.\nThe remaining attributes are for the MSK Data Generator. Two topics named impressions and clicks will be created, and the messages attributes are generated by the Java faker library. Interestingly the bid ID of the impression message and the correlation ID of the click message share the same value sometimes. This is because only a fraction of impressions results in clicks in practice.\n1resource \u0026#34;aws_mskconnect_connector\u0026#34; \u0026#34;msk_data_generator\u0026#34; { 2 count = var.to_create_connector ? 1 : 0 3 name = \u0026#34;${local.name}-ad-tech-source\u0026#34; 4 5 kafkaconnect_version = \u0026#34;2.7.1\u0026#34; 6 7 capacity { 8 provisioned_capacity { 9 mcu_count = 1 10 worker_count = 1 11 } 12 } 13 14 connector_configuration = { 15 # connector configuration 16 \u0026#34;connector.class\u0026#34; = \u0026#34;com.amazonaws.mskdatagen.GeneratorSourceConnector\u0026#34;, 17 \u0026#34;tasks.max\u0026#34; = \u0026#34;2\u0026#34;, 18 \u0026#34;key.converter\u0026#34; = \u0026#34;org.apache.kafka.connect.storage.StringConverter\u0026#34;, 19 \u0026#34;key.converter.schemas.enable\u0026#34; = false, 20 \u0026#34;value.converter\u0026#34; = \u0026#34;org.apache.kafka.connect.json.JsonConverter\u0026#34;, 21 \u0026#34;value.converter.schemas.enable\u0026#34; = false, 22 # msk data generator configuration 23 \u0026#34;genkp.impressions.with\u0026#34; = \u0026#34;#{Code.isbn10}\u0026#34; 24 \u0026#34;genv.impressions.bid_id.with\u0026#34; = \u0026#34;#{Code.isbn10}\u0026#34; 25 \u0026#34;genv.impressions.campaign_id.with\u0026#34; = \u0026#34;#{Code.isbn10}\u0026#34; 26 \u0026#34;genv.impressions.creative_details.with\u0026#34; = \u0026#34;#{Color.name}\u0026#34; 27 \u0026#34;genv.impressions.country_code.with\u0026#34; = \u0026#34;#{Address.countryCode}\u0026#34; 28 \u0026#34;genkp.clicks.with\u0026#34; = \u0026#34;#{Code.isbn10}\u0026#34; 29 \u0026#34;genv.clicks.correlation_id.sometimes.matching\u0026#34; = \u0026#34;impressions.value.bid_id\u0026#34; 30 \u0026#34;genv.clicks.correlation_id.sometimes.with\u0026#34; = \u0026#34;NA\u0026#34; 31 \u0026#34;genv.clicks.tracker.with\u0026#34; = \u0026#34;#{Lorem.characters \u0026#39;15\u0026#39;}\u0026#34; 32 \u0026#34;global.throttle.ms\u0026#34; = \u0026#34;500\u0026#34; 33 \u0026#34;global.history.records.max\u0026#34; = \u0026#34;1000\u0026#34; 34 } 35 36 kafka_cluster { 37 apache_kafka_cluster { 38 bootstrap_servers = aws_msk_cluster.msk_data_cluster.bootstrap_brokers_sasl_iam 39 40 vpc { 41 security_groups = [aws_security_group.msk.id] 42 subnets = module.vpc.private_subnets 43 } 44 } 45 } 46 47 kafka_cluster_client_authentication { 48 authentication_type = \u0026#34;IAM\u0026#34; 49 } 50 51 kafka_cluster_encryption_in_transit { 52 encryption_type = \u0026#34;TLS\u0026#34; 53 } 54 55 plugin { 56 custom_plugin { 57 arn = aws_mskconnect_custom_plugin.msk_data_generator.arn 58 revision = aws_mskconnect_custom_plugin.msk_data_generator.latest_revision 59 } 60 } 61 62 log_delivery { 63 worker_log_delivery { 64 cloudwatch_logs { 65 enabled = true 66 log_group = aws_cloudwatch_log_group.msk_data_generator.name 67 } 68 s3 { 69 enabled = true 70 bucket = aws_s3_bucket.default_bucket.id 71 prefix = \u0026#34;logs/msk/connect/msk-data-generator\u0026#34; 72 } 73 } 74 } 75 76 service_execution_role_arn = aws_iam_role.kafka_connector_role.arn 77} 78 79resource \u0026#34;aws_mskconnect_custom_plugin\u0026#34; \u0026#34;msk_data_generator\u0026#34; { 80 name = \u0026#34;${local.name}-msk-data-generator\u0026#34; 81 content_type = \u0026#34;JAR\u0026#34; 82 83 location { 84 s3 { 85 bucket_arn = aws_s3_bucket.default_bucket.arn 86 file_key = aws_s3_object.msk_data_generator.key 87 } 88 } 89} 90 91resource \u0026#34;aws_s3_object\u0026#34; \u0026#34;msk_data_generator\u0026#34; { 92 bucket = aws_s3_bucket.default_bucket.id 93 key = \u0026#34;plugins/msk-data-generator.jar\u0026#34; 94 source = \u0026#34;connectors/msk-datagen/msk-data-generator.jar\u0026#34; 95 96 etag = filemd5(\u0026#34;connectors/msk-datagen/msk-data-generator.jar\u0026#34;) 97} 98 99resource \u0026#34;aws_cloudwatch_log_group\u0026#34; \u0026#34;msk_data_generator\u0026#34; { 100 name = \u0026#34;/msk/connect/msk-data-generator\u0026#34; 101 102 retention_in_days = 1 103 104 tags = local.tags 105} Sink Connector Similar to the source connector, the sink connector will be deployed using a customer plugin where its source is uploaded into S3. It is marked to depend on the source connector so that it will be created only after the source connector is deployed successfully.\nFor connector configuration, it is configured to write messages from the impressions and clicks topics into the OpenSearch indexes created earlier. It uses the same key and value converters to the source connector, and schemas are not enabled for both the key and value.\nThe OpenSearch domain endpoint is added to the connection URL attribute (connection.url), and this is the only necessary attribute for making HTTP requests thanks to anonymous authentication. Also, as the topics are append-only logs, we can set the document ID to be [topic-name].[partition].[offset] by setting key.ignore to true. See the connector configuration document for more details.\nHaving an event timestamp attribute can be useful for performing temporal analysis. As I don\u0026rsquo;t find a comprehensive way to set it up in the source connector, a new field called created_at is added using single message transforms (SMTs). Specifically I added two transforms - insertTS and formatTS. As the name suggests, the former inserts the system timestamp value while it is formatted into yyyy-MM-dd HH:mm:ss by the latter.\n1resource \u0026#34;aws_mskconnect_connector\u0026#34; \u0026#34;opensearch_sink\u0026#34; { 2 count = var.to_create_connector ? 1 : 0 3 name = \u0026#34;${local.name}-ad-tech-sink\u0026#34; 4 5 kafkaconnect_version = \u0026#34;2.7.1\u0026#34; 6 7 capacity { 8 provisioned_capacity { 9 mcu_count = 1 10 worker_count = 1 11 } 12 } 13 14 connector_configuration = { 15 # connector configuration 16 \u0026#34;connector.class\u0026#34; = \u0026#34;io.aiven.kafka.connect.opensearch.OpensearchSinkConnector\u0026#34;, 17 \u0026#34;tasks.max\u0026#34; = \u0026#34;2\u0026#34;, 18 \u0026#34;topics\u0026#34; = \u0026#34;impressions,clicks\u0026#34;, 19 \u0026#34;key.converter\u0026#34; = \u0026#34;org.apache.kafka.connect.storage.StringConverter\u0026#34;, 20 \u0026#34;key.converter.schemas.enable\u0026#34; = false, 21 \u0026#34;value.converter\u0026#34; = \u0026#34;org.apache.kafka.connect.json.JsonConverter\u0026#34;, 22 \u0026#34;value.converter.schemas.enable\u0026#34; = false, 23 # opensearch sink configuration 24 \u0026#34;connection.url\u0026#34; = \u0026#34;https://${aws_opensearch_domain.opensearch.endpoint}\u0026#34;, 25 \u0026#34;schema.ignore\u0026#34; = true, 26 \u0026#34;key.ignore\u0026#34; = true, 27 \u0026#34;type.name\u0026#34; = \u0026#34;_doc\u0026#34;, 28 \u0026#34;behavior.on.malformed.documents\u0026#34; = \u0026#34;fail\u0026#34;, 29 \u0026#34;behavior.on.null.values\u0026#34; = \u0026#34;ignore\u0026#34;, 30 \u0026#34;behavior.on.version.conflict\u0026#34; = \u0026#34;ignore\u0026#34;, 31 # dead-letter-queue configuration 32 \u0026#34;errors.deadletterqueue.topic.name\u0026#34; = \u0026#34;ad-tech-dl\u0026#34;, 33 \u0026#34;errors.tolerance\u0026#34; = \u0026#34;all\u0026#34;, 34 \u0026#34;errors.deadletterqueue.context.headers.enable\u0026#34; = true, 35 \u0026#34;errors.deadletterqueue.topic.replication.factor\u0026#34; = \u0026#34;1\u0026#34;, 36 # single message transforms 37 \u0026#34;transforms\u0026#34; = \u0026#34;insertTS,formatTS\u0026#34;, 38 \u0026#34;transforms.insertTS.type\u0026#34; = \u0026#34;org.apache.kafka.connect.transforms.InsertField$Value\u0026#34;, 39 \u0026#34;transforms.insertTS.timestamp.field\u0026#34; = \u0026#34;created_at\u0026#34;, 40 \u0026#34;transforms.formatTS.type\u0026#34; = \u0026#34;org.apache.kafka.connect.transforms.TimestampConverter$Value\u0026#34;, 41 \u0026#34;transforms.formatTS.format\u0026#34; = \u0026#34;yyyy-MM-dd HH:mm:ss\u0026#34;, 42 \u0026#34;transforms.formatTS.field\u0026#34; = \u0026#34;created_at\u0026#34;, 43 \u0026#34;transforms.formatTS.target.type\u0026#34; = \u0026#34;string\u0026#34; 44 } 45 46 kafka_cluster { 47 apache_kafka_cluster { 48 bootstrap_servers = aws_msk_cluster.msk_data_cluster.bootstrap_brokers_sasl_iam 49 50 vpc { 51 security_groups = [aws_security_group.msk.id] 52 subnets = module.vpc.private_subnets 53 } 54 } 55 } 56 57 kafka_cluster_client_authentication { 58 authentication_type = \u0026#34;IAM\u0026#34; 59 } 60 61 kafka_cluster_encryption_in_transit { 62 encryption_type = \u0026#34;TLS\u0026#34; 63 } 64 65 plugin { 66 custom_plugin { 67 arn = aws_mskconnect_custom_plugin.opensearch_sink.arn 68 revision = aws_mskconnect_custom_plugin.opensearch_sink.latest_revision 69 } 70 } 71 72 log_delivery { 73 worker_log_delivery { 74 cloudwatch_logs { 75 enabled = true 76 log_group = aws_cloudwatch_log_group.opensearch_sink.name 77 } 78 s3 { 79 enabled = true 80 bucket = aws_s3_bucket.default_bucket.id 81 prefix = \u0026#34;logs/msk/connect/opensearch-sink\u0026#34; 82 } 83 } 84 } 85 86 service_execution_role_arn = aws_iam_role.kafka_connector_role.arn 87 88 depends_on = [ 89 aws_mskconnect_connector.msk_data_generator 90 ] 91} 92 93resource \u0026#34;aws_mskconnect_custom_plugin\u0026#34; \u0026#34;opensearch_sink\u0026#34; { 94 name = \u0026#34;${local.name}-opensearch-sink\u0026#34; 95 content_type = \u0026#34;ZIP\u0026#34; 96 97 location { 98 s3 { 99 bucket_arn = aws_s3_bucket.default_bucket.arn 100 file_key = aws_s3_object.opensearch_sink.key 101 } 102 } 103} 104 105resource \u0026#34;aws_s3_object\u0026#34; \u0026#34;opensearch_sink\u0026#34; { 106 bucket = aws_s3_bucket.default_bucket.id 107 key = \u0026#34;plugins/opensearch-connector.zip\u0026#34; 108 source = \u0026#34;connectors/opensearch-connector.zip\u0026#34; 109 110 etag = filemd5(\u0026#34;connectors/opensearch-connector.zip\u0026#34;) 111} 112 113resource \u0026#34;aws_cloudwatch_log_group\u0026#34; \u0026#34;opensearch_sink\u0026#34; { 114 name = \u0026#34;/msk/connect/opensearch-sink\u0026#34; 115 116 retention_in_days = 1 117 118 tags = local.tags 119} Deploy Connectors The connectors can be deployed while setting the value of to_create_connector to true as shown below.\n1$ terraform plan -var to_create_connector=true 2$ terraform apply --auto-approve=true -var to_create_connector=true Once completed, we can check the source and sink connectors on AWS Console as following.\nSource Data We can use Kpow to see the details of the impressions and clicks topics on localhost:3000. Make sure to connect to the VPN server, or it fails to access the MSK cluster.\nAs mentioned earlier, only a fraction of correlation IDs of the click messages has actual values, and we can see that by inspecting the messages of the clicks topic.\nOpenSearch Dashboard In OpenSearch Dashboards, we can search clicks that are associated with impressions. As expected, only a small portion of clicks are searched.\nMoreover, we can join correlated impressions and clicks quickly using the Query Workbench. Below shows a simple SQL query that joins impressions and associating clicks that are created after a certain time point.\nDestroy Resources As all resources are created by Terraform, they can be destroyed by a single command as shown below.\n1$ terraform destroy --auto-approve=true -var to_create_connector=true Summary In the previous post, we discussed how to develop a data pipeline from Apache Kafka into OpenSearch locally using Docker. The pipeline was deployed on AWS using Amazon MSK, Amazon MSK Connect and Amazon OpenSearch Service using Terraform in this post. First the infrastructure was deployed that covers a Virtual Private Cloud (VPC), Virtual Private Network (VPN) server, MSK Cluster and OpenSearch domain. Then Kafka source and sink connectors were deployed on MSK Connect, followed by performing quick data analysis. It turns out that Kafka Connect can be effective for ingesting data from Kafka into OpenSearch.\n","date":"October 30, 2023","img":"/blog/2023-10-30-kafka-connect-for-aws-part-5/featured.png","lang":"en","langName":"English","largeImg":"/blog/2023-10-30-kafka-connect-for-aws-part-5/featured_hue6ace805e51d39c2a79d7a3e7795d500_85575_500x0_resize_box_3.png","permalink":"/blog/2023-10-30-kafka-connect-for-aws-part-5/","series":[{"title":"Kafka Connect for AWS Services Integration","url":"/series/kafka-connect-for-aws-services-integration/"}],"smallImg":"/blog/2023-10-30-kafka-connect-for-aws-part-5/featured_hue6ace805e51d39c2a79d7a3e7795d500_85575_180x0_resize_box_3.png","tags":[{"title":"AWS","url":"/tags/aws/"},{"title":"Amazon MSK","url":"/tags/amazon-msk/"},{"title":"MSK Connect","url":"/tags/msk-connect/"},{"title":"Amazon OpenSearch Service","url":"/tags/amazon-opensearch-service/"},{"title":"Apache Kafka","url":"/tags/apache-kafka/"},{"title":"Kafka Connect","url":"/tags/kafka-connect/"},{"title":"OpenSearch","url":"/tags/opensearch/"},{"title":"Docker","url":"/tags/docker/"},{"title":"Docker Compose","url":"/tags/docker-compose/"}],"timestamp":1698624000,"title":"Kafka Connect for AWS Services Integration - Part 5 Deploy Aiven OpenSearch Sink Connector"},{"categories":[{"title":"Data Streaming","url":"/categories/data-streaming/"}],"content":"In this lab, we will create a Kafka producer application using AWS Lambda, which sends fake taxi ride data into a Kafka topic on Amazon MSK. A configurable number of the producer Lambda function will be invoked by an Amazon EventBridge schedule rule. In this way we are able to generate test data concurrently based on the desired volume of messages.\nIntroduction Lab 1 Produce data to Kafka using Lambda (this post) Lab 2 Write data to Kafka from S3 using Flink Lab 3 Transform and write data to S3 from Kafka using Flink Lab 4 Clean, Aggregate, and Enrich Events with Flink Lab 5 Write data to DynamoDB using Kafka Connect Lab 6 Consume data from Kafka using Lambda [Update 2023-11-06] Initially I planned to deploy Pyflink applications on Amazon Managed Service for Apache Flink, but I changed the plan to use a local Flink cluster deployed on Docker. The main reasons are\nIt is not clear how to configure a Pyflink application for the managed service. For example, Apache Flink supports pluggable file systems and the required dependency (eg flink-s3-fs-hadoop-1.15.2.jar) should be placed under the plugins folder. However, the sample Pyflink applications from pyflink-getting-started and amazon-kinesis-data-analytics-blueprints either ignore the S3 jar file for deployment or package it together with other dependencies - none of them uses the S3 jar file as a plugin. I tried multiple different configurations, but all ended up with having an error whose code is CodeError.InvalidApplicationCode. I don\u0026rsquo;t have such an issue when I deploy the app on a local Flink cluster and I haven\u0026rsquo;t found a way to configure the app for the managed service as yet. The Pyflink app for Lab 4 requires the OpenSearch sink connector and the connector is available on 1.16.0+. However, the latest Flink version of the managed service is still 1.15.2 and the sink connector is not available on it. Normally the latest version of the managed service is behind two minor versions of the official release, but it seems to take a little longer to catch up at the moment - the version 1.18.0 was released a while ago. Architecture Fake taxi ride data is generated by multiple Kafka Lambda producer functions that are invoked by an EventBridge schedule rule. The schedule is set to run every minute and the associating rule has a configurable number (e.g. 5) of targets. Each target points to the same Lambda function. In this way we are able to generate test data using multiple Lambda functions based on the desired volume of messages.\nInfrastructure The infrastructure is created using Terraform and the source can be found in the GitHub repository of this post.\nVPC and VPN A VPC with 3 public and private subnets is created using the AWS VPC Terraform module (infra/vpc.tf). Also, a SoftEther VPN server is deployed in order to access the resources in the private subnets from the developer machine (infra/vpn.tf). The details about how to configure the VPN server can be found in this post.\nMSK Cluster An MSK cluster with 2 brokers is created. The broker nodes are deployed with the kafka.m5.large instance type in private subnets and IAM authentication is used for the client authentication method. Finally, additional server configurations are added such as enabling topic auto-creation/deletion, (default) number of partitions and default replication factor.\n1# infra/variables.tf 2locals { 3 ... 4 msk = { 5 version = \u0026#34;2.8.1\u0026#34; 6 instance_size = \u0026#34;kafka.m5.large\u0026#34; 7 ebs_volume_size = 20 8 log_retention_ms = 604800000 # 7 days 9 number_of_broker_nodes = 2 10 num_partitions = 5 11 default_replication_factor = 2 12 } 13 ... 14} 15 16# infra/msk.tf 17resource \u0026#34;aws_msk_cluster\u0026#34; \u0026#34;msk_data_cluster\u0026#34; { 18 cluster_name = \u0026#34;${local.name}-msk-cluster\u0026#34; 19 kafka_version = local.msk.version 20 number_of_broker_nodes = local.msk.number_of_broker_nodes 21 configuration_info { 22 arn = aws_msk_configuration.msk_config.arn 23 revision = aws_msk_configuration.msk_config.latest_revision 24 } 25 26 broker_node_group_info { 27 instance_type = local.msk.instance_size 28 client_subnets = slice(module.vpc.private_subnets, 0, local.msk.number_of_broker_nodes) 29 security_groups = [aws_security_group.msk.id] 30 storage_info { 31 ebs_storage_info { 32 volume_size = local.msk.ebs_volume_size 33 } 34 } 35 } 36 37 client_authentication { 38 sasl { 39 iam = true 40 } 41 } 42 43 logging_info { 44 broker_logs { 45 cloudwatch_logs { 46 enabled = true 47 log_group = aws_cloudwatch_log_group.msk_cluster_lg.name 48 } 49 s3 { 50 enabled = true 51 bucket = aws_s3_bucket.default_bucket.id 52 prefix = \u0026#34;logs/msk/cluster/\u0026#34; 53 } 54 } 55 } 56 57 tags = local.tags 58 59 depends_on = [aws_msk_configuration.msk_config] 60} 61 62resource \u0026#34;aws_msk_configuration\u0026#34; \u0026#34;msk_config\u0026#34; { 63 name = \u0026#34;${local.name}-msk-configuration\u0026#34; 64 65 kafka_versions = [local.msk.version] 66 67 server_properties = \u0026lt;\u0026lt;PROPERTIES 68 auto.create.topics.enable = true 69 delete.topic.enable = true 70 log.retention.ms = ${local.msk.log_retention_ms} 71 num.partitions = ${local.msk.num_partitions} 72 default.replication.factor = ${local.msk.default_replication_factor} 73 PROPERTIES 74} Security Group The security group of the MSK cluster allows all inbound traffic from itself and all outbound traffic into all IP addresses. These are necessary for Kafka connectors on MSK Connect that we will develop in later posts. Note that both the rules are too generous, however, we can limit the protocol and port ranges in production. Also, the security group has additional inbound rules that can be accessed on port 9098 from the security groups of the VPN server and Lambda producer function.\n1# infra/msk.tf 2resource \u0026#34;aws_security_group\u0026#34; \u0026#34;msk\u0026#34; { 3 name = \u0026#34;${local.name}-msk-sg\u0026#34; 4 vpc_id = module.vpc.vpc_id 5 6 lifecycle { 7 create_before_destroy = true 8 } 9 10 tags = local.tags 11} 12 13resource \u0026#34;aws_security_group_rule\u0026#34; \u0026#34;msk_self_inbound_all\u0026#34; { 14 type = \u0026#34;ingress\u0026#34; 15 description = \u0026#34;Allow ingress from itself - required for MSK Connect\u0026#34; 16 security_group_id = aws_security_group.msk.id 17 protocol = \u0026#34;-1\u0026#34; 18 from_port = \u0026#34;0\u0026#34; 19 to_port = \u0026#34;0\u0026#34; 20 source_security_group_id = aws_security_group.msk.id 21} 22 23resource \u0026#34;aws_security_group_rule\u0026#34; \u0026#34;msk_self_outbound_all\u0026#34; { 24 type = \u0026#34;egress\u0026#34; 25 description = \u0026#34;Allow outbound all\u0026#34; 26 security_group_id = aws_security_group.msk.id 27 protocol = \u0026#34;-1\u0026#34; 28 from_port = \u0026#34;0\u0026#34; 29 to_port = \u0026#34;0\u0026#34; 30 cidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;] 31} 32 33resource \u0026#34;aws_security_group_rule\u0026#34; \u0026#34;msk_vpn_inbound\u0026#34; { 34 count = local.vpn.to_create ? 1 : 0 35 type = \u0026#34;ingress\u0026#34; 36 description = \u0026#34;Allow VPN access\u0026#34; 37 security_group_id = aws_security_group.msk.id 38 protocol = \u0026#34;tcp\u0026#34; 39 from_port = 9092 40 to_port = 9098 41 source_security_group_id = aws_security_group.vpn[0].id 42} 43 44resource \u0026#34;aws_security_group_rule\u0026#34; \u0026#34;msk_kafka_producer_inbound\u0026#34; { 45 count = local.producer.to_create ? 1 : 0 46 type = \u0026#34;ingress\u0026#34; 47 description = \u0026#34;lambda kafka producer access\u0026#34; 48 security_group_id = aws_security_group.msk.id 49 protocol = \u0026#34;tcp\u0026#34; 50 from_port = 9098 51 to_port = 9098 52 source_security_group_id = aws_security_group.kafka_producer[0].id 53} Lambda Function The Kafka producer Lambda function is deployed conditionally by a flag variable called producer_to_create. Once it is set to true, the function is created by the AWS Lambda Terraform module while referring to the associating configuration variables (local.producer.*).\n1# infra/variables.tf 2variable \u0026#34;producer_to_create\u0026#34; { 3 description = \u0026#34;Flag to indicate whether to create Lambda Kafka producer\u0026#34; 4 type = bool 5 default = false 6} 7 8... 9 10locals { 11 ... 12 producer = { 13 to_create = var.producer_to_create 14 src_path = \u0026#34;../producer\u0026#34; 15 function_name = \u0026#34;kafka_producer\u0026#34; 16 handler = \u0026#34;app.lambda_function\u0026#34; 17 concurrency = 5 18 timeout = 90 19 memory_size = 128 20 runtime = \u0026#34;python3.8\u0026#34; 21 schedule_rate = \u0026#34;rate(1 minute)\u0026#34; 22 environment = { 23 topic_name = \u0026#34;taxi-rides\u0026#34; 24 max_run_sec = 60 25 } 26 } 27 ... 28} 29 30# infra/producer.tf 31module \u0026#34;kafka_producer\u0026#34; { 32 source = \u0026#34;terraform-aws-modules/lambda/aws\u0026#34; 33 version = \u0026#34;\u0026gt;=5.1.0, \u0026lt;6.0.0\u0026#34; 34 35 create = local.producer.to_create 36 37 function_name = local.producer.function_name 38 handler = local.producer.handler 39 runtime = local.producer.runtime 40 timeout = local.producer.timeout 41 memory_size = local.producer.memory_size 42 source_path = local.producer.src_path 43 vpc_subnet_ids = module.vpc.private_subnets 44 vpc_security_group_ids = local.producer.to_create ? [aws_security_group.kafka_producer[0].id] : null 45 attach_network_policy = true 46 attach_policies = true 47 policies = local.producer.to_create ? [aws_iam_policy.kafka_producer[0].arn] : null 48 number_of_policies = 1 49 environment_variables = { 50 BOOTSTRAP_SERVERS = aws_msk_cluster.msk_data_cluster.bootstrap_brokers_sasl_iam 51 TOPIC_NAME = local.producer.environment.topic_name 52 MAX_RUN_SEC = local.producer.environment.max_run_sec 53 } 54 55 depends_on = [ 56 aws_msk_cluster.msk_data_cluster 57 ] 58 59 tags = local.tags 60} 61 62resource \u0026#34;aws_lambda_function_event_invoke_config\u0026#34; \u0026#34;kafka_producer\u0026#34; { 63 count = local.producer.to_create ? 1 : 0 64 65 function_name = module.kafka_producer.lambda_function_name 66 maximum_retry_attempts = 0 67} 68 69resource \u0026#34;aws_lambda_permission\u0026#34; \u0026#34;allow_eventbridge\u0026#34; { 70 count = local.producer.to_create ? 1 : 0 71 statement_id = \u0026#34;InvokeLambdaFunction\u0026#34; 72 action = \u0026#34;lambda:InvokeFunction\u0026#34; 73 function_name = local.producer.function_name 74 principal = \u0026#34;events.amazonaws.com\u0026#34; 75 source_arn = module.eventbridge.eventbridge_rule_arns[\u0026#34;crons\u0026#34;] 76 77 depends_on = [ 78 module.eventbridge 79 ] 80} IAM Permission The producer Lambda function needs permission to send messages to the Kafka topic. The following IAM policy is added to the Lambda function as illustrated in this AWS documentation.\n1# infra/producer.tf 2resource \u0026#34;aws_iam_policy\u0026#34; \u0026#34;kafka_producer\u0026#34; { 3 count = local.producer.to_create ? 1 : 0 4 name = \u0026#34;${local.producer.function_name}-msk-lambda-permission\u0026#34; 5 6 policy = jsonencode({ 7 Version = \u0026#34;2012-10-17\u0026#34; 8 Statement = [ 9 { 10 Sid = \u0026#34;PermissionOnCluster\u0026#34; 11 Action = [ 12 \u0026#34;kafka-cluster:Connect\u0026#34;, 13 \u0026#34;kafka-cluster:AlterCluster\u0026#34;, 14 \u0026#34;kafka-cluster:DescribeCluster\u0026#34; 15 ] 16 Effect = \u0026#34;Allow\u0026#34; 17 Resource = \u0026#34;arn:aws:kafka:${local.region}:${data.aws_caller_identity.current.account_id}:cluster/${local.name}-msk-cluster/*\u0026#34; 18 }, 19 { 20 Sid = \u0026#34;PermissionOnTopics\u0026#34; 21 Action = [ 22 \u0026#34;kafka-cluster:*Topic*\u0026#34;, 23 \u0026#34;kafka-cluster:WriteData\u0026#34;, 24 \u0026#34;kafka-cluster:ReadData\u0026#34; 25 ] 26 Effect = \u0026#34;Allow\u0026#34; 27 Resource = \u0026#34;arn:aws:kafka:${local.region}:${data.aws_caller_identity.current.account_id}:topic/${local.name}-msk-cluster/*\u0026#34; 28 }, 29 { 30 Sid = \u0026#34;PermissionOnGroups\u0026#34; 31 Action = [ 32 \u0026#34;kafka-cluster:AlterGroup\u0026#34;, 33 \u0026#34;kafka-cluster:DescribeGroup\u0026#34; 34 ] 35 Effect = \u0026#34;Allow\u0026#34; 36 Resource = \u0026#34;arn:aws:kafka:${local.region}:${data.aws_caller_identity.current.account_id}:group/${local.name}-msk-cluster/*\u0026#34; 37 } 38 ] 39 }) 40} Lambda Security Group We also need to add an outbound rule to the Lambda function\u0026rsquo;s security group so that it can access the MSK cluster.\n1# infra/producer.tf 2resource \u0026#34;aws_security_group\u0026#34; \u0026#34;kafka_producer\u0026#34; { 3 count = local.producer.to_create ? 1 : 0 4 5 name = \u0026#34;${local.name}-lambda-sg\u0026#34; 6 vpc_id = module.vpc.vpc_id 7 8 egress { 9 from_port = 9098 10 to_port = 9098 11 protocol = \u0026#34;tcp\u0026#34; 12 cidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;] 13 } 14 15 lifecycle { 16 create_before_destroy = true 17 } 18 19 tags = local.tags 20} Function Source The TaxiRide class generates one or more taxi ride records by the _create _ method where random records are populated by the random module. The Lambda function sends 100 records at a time followed by sleeping for 1 second. It repeats until it reaches MAX_RUN_SEC (e.g. 60) environment variable value. A Kafka message is made up of an ID as the key and a taxi ride record as the value. Both the key and value are serialised as JSON. Note that the stable version of the kafka-python package does not support the IAM authentication method. Therefore, we need to install the package from a forked repository as discussed in this GitHub issue.\n1# producer/app.py 2import os 3import datetime 4import random 5import json 6import re 7import time 8import typing 9import dataclasses 10 11from kafka import KafkaProducer 12 13 14@dataclasses.dataclass 15class TaxiRide: 16 id: str 17 vendor_id: int 18 pickup_date: str 19 dropoff_date: str 20 passenger_count: int 21 pickup_longitude: str 22 pickup_latitude: str 23 dropoff_longitude: str 24 dropoff_latitude: str 25 store_and_fwd_flag: str 26 gc_distance: int 27 trip_duration: int 28 google_distance: int 29 google_duration: int 30 31 def asdict(self): 32 return dataclasses.asdict(self) 33 34 @classmethod 35 def auto(cls): 36 pickup_lon, pickup_lat = tuple(TaxiRide.get_latlon().split(\u0026#34;,\u0026#34;)) 37 dropoff_lon, dropoff_lat = tuple(TaxiRide.get_latlon().split(\u0026#34;,\u0026#34;)) 38 distance, duration = random.randint(1, 7), random.randint(8, 10000) 39 return cls( 40 id=f\u0026#34;id{random.randint(1665586, 8888888)}\u0026#34;, 41 vendor_id=random.randint(1, 5), 42 pickup_date=datetime.datetime.now().isoformat(timespec=\u0026#34;milliseconds\u0026#34;), 43 dropoff_date=( 44 datetime.datetime.now() + datetime.timedelta(minutes=random.randint(30, 100)) 45 ).isoformat(timespec=\u0026#34;milliseconds\u0026#34;), 46 passenger_count=random.randint(1, 9), 47 pickup_longitude=pickup_lon, 48 pickup_latitude=pickup_lat, 49 dropoff_longitude=dropoff_lon, 50 dropoff_latitude=dropoff_lat, 51 store_and_fwd_flag=[\u0026#34;Y\u0026#34;, \u0026#34;N\u0026#34;][random.randint(0, 1)], 52 gc_distance=distance, 53 trip_duration=duration, 54 google_distance=distance, 55 google_duration=duration, 56 ) 57 58 @staticmethod 59 def create(num: int): 60 return [TaxiRide.auto() for _ in range(num)] 61 62 # fmt: off 63 @staticmethod 64 def get_latlon(): 65 location_list = [ 66 \u0026#34;-73.98174286,40.71915817\u0026#34;, \u0026#34;-73.98508453,40.74716568\u0026#34;, \u0026#34;-73.97333527,40.76407242\u0026#34;, \u0026#34;-73.99310303,40.75263214\u0026#34;, 67 \u0026#34;-73.98229218,40.75133133\u0026#34;, \u0026#34;-73.96527863,40.80104065\u0026#34;, \u0026#34;-73.97010803,40.75979996\u0026#34;, \u0026#34;-73.99373627,40.74176025\u0026#34;, 68 \u0026#34;-73.98544312,40.73571014\u0026#34;, \u0026#34;-73.97686005,40.68337631\u0026#34;, \u0026#34;-73.9697876,40.75758362\u0026#34;, \u0026#34;-73.99397278,40.74086761\u0026#34;, 69 \u0026#34;-74.00531769,40.72866058\u0026#34;, \u0026#34;-73.99013519,40.74885178\u0026#34;, \u0026#34;-73.9595108,40.76280975\u0026#34;, \u0026#34;-73.99025726,40.73703384\u0026#34;, 70 \u0026#34;-73.99495697,40.745121\u0026#34;, \u0026#34;-73.93579865,40.70730972\u0026#34;, \u0026#34;-73.99046326,40.75100708\u0026#34;, \u0026#34;-73.9536438,40.77526093\u0026#34;, 71 \u0026#34;-73.98226166,40.75159073\u0026#34;, \u0026#34;-73.98831177,40.72318649\u0026#34;, \u0026#34;-73.97222137,40.67683029\u0026#34;, \u0026#34;-73.98626709,40.73276901\u0026#34;, 72 \u0026#34;-73.97852325,40.78910065\u0026#34;, \u0026#34;-73.97612,40.74908066\u0026#34;, \u0026#34;-73.98240662,40.73148727\u0026#34;, \u0026#34;-73.98776245,40.75037384\u0026#34;, 73 \u0026#34;-73.97187042,40.75840378\u0026#34;, \u0026#34;-73.87303925,40.77410507\u0026#34;, \u0026#34;-73.9921875,40.73451996\u0026#34;, \u0026#34;-73.98435974,40.74898529\u0026#34;, 74 \u0026#34;-73.98092651,40.74196243\u0026#34;, \u0026#34;-74.00701904,40.72573853\u0026#34;, \u0026#34;-74.00798798,40.74022675\u0026#34;, \u0026#34;-73.99419403,40.74555969\u0026#34;, 75 \u0026#34;-73.97737885,40.75883865\u0026#34;, \u0026#34;-73.97051239,40.79664993\u0026#34;, \u0026#34;-73.97693634,40.7599144\u0026#34;, \u0026#34;-73.99306488,40.73812866\u0026#34;, 76 \u0026#34;-74.00775146,40.74528885\u0026#34;, \u0026#34;-73.98532867,40.74198914\u0026#34;, \u0026#34;-73.99037933,40.76152802\u0026#34;, \u0026#34;-73.98442078,40.74978638\u0026#34;, 77 \u0026#34;-73.99173737,40.75437927\u0026#34;, \u0026#34;-73.96742249,40.78820801\u0026#34;, \u0026#34;-73.97813416,40.72935867\u0026#34;, \u0026#34;-73.97171021,40.75943375\u0026#34;, 78 \u0026#34;-74.00737,40.7431221\u0026#34;, \u0026#34;-73.99498749,40.75517654\u0026#34;, \u0026#34;-73.91600037,40.74634933\u0026#34;, \u0026#34;-73.99924469,40.72764587\u0026#34;, 79 \u0026#34;-73.98488617,40.73621368\u0026#34;, \u0026#34;-73.98627472,40.74737167\u0026#34;, 80 ] 81 return location_list[random.randint(0, len(location_list) - 1)] 82 83 84# fmt: on 85class Producer: 86 def __init__(self, bootstrap_servers: list, topic: str): 87 self.bootstrap_servers = bootstrap_servers 88 self.topic = topic 89 self.producer = self.create() 90 91 def create(self): 92 kwargs = { 93 \u0026#34;bootstrap_servers\u0026#34;: self.bootstrap_servers, 94 \u0026#34;value_serializer\u0026#34;: lambda v: json.dumps(v, default=self.serialize).encode(\u0026#34;utf-8\u0026#34;), 95 \u0026#34;key_serializer\u0026#34;: lambda v: json.dumps(v, default=self.serialize).encode(\u0026#34;utf-8\u0026#34;), 96 \u0026#34;api_version\u0026#34;: (2, 8, 1), 97 } 98 if re.search(\u0026#34;9098$\u0026#34;, next(iter(self.bootstrap_servers))): 99 kwargs = { 100 **kwargs, 101 **{ 102 \u0026#34;security_protocol\u0026#34;: \u0026#34;SASL_SSL\u0026#34;, 103 \u0026#34;sasl_mechanism\u0026#34;: \u0026#34;AWS_MSK_IAM\u0026#34;, 104 }, 105 } 106 return KafkaProducer(**kwargs) 107 108 def send(self, items: typing.List[TaxiRide]): 109 for item in items: 110 self.producer.send(self.topic, key={\u0026#34;id\u0026#34;: item.id}, value=item.asdict()) 111 self.producer.flush() 112 113 def serialize(self, obj): 114 if isinstance(obj, datetime.datetime): 115 return obj.isoformat() 116 if isinstance(obj, datetime.date): 117 return str(obj) 118 return obj 119 120 121def lambda_function(event, context): 122 producer = Producer( 123 bootstrap_servers=os.environ[\u0026#34;BOOTSTRAP_SERVERS\u0026#34;].split(\u0026#34;,\u0026#34;), topic=os.environ[\u0026#34;TOPIC_NAME\u0026#34;] 124 ) 125 s = datetime.datetime.now() 126 total_records = 0 127 while True: 128 items = TaxiRide.create(10) 129 producer.send(items) 130 total_records += len(items) 131 print(f\u0026#34;sent {len(items)} messages\u0026#34;) 132 elapsed_sec = (datetime.datetime.now() - s).seconds 133 if elapsed_sec \u0026gt; int(os.environ[\u0026#34;MAX_RUN_SEC\u0026#34;]): 134 print(f\u0026#34;{total_records} records are sent in {elapsed_sec} seconds ...\u0026#34;) 135 break 136 time.sleep(1) A sample taxi ride record is shown below.\n1{ 2\t\u0026#34;id\u0026#34;: \u0026#34;id3464573\u0026#34;, 3\t\u0026#34;vendor_id\u0026#34;: 5, 4\t\u0026#34;pickup_date\u0026#34;: \u0026#34;2023-10-13T01:59:05.422\u0026#34;, 5\t\u0026#34;dropoff_date\u0026#34;: \u0026#34;2023-10-13T02:52:05.422\u0026#34;, 6\t\u0026#34;passenger_count\u0026#34;: 9, 7\t\u0026#34;pickup_longitude\u0026#34;: \u0026#34;-73.97813416\u0026#34;, 8\t\u0026#34;pickup_latitude\u0026#34;: \u0026#34;40.72935867\u0026#34;, 9\t\u0026#34;dropoff_longitude\u0026#34;: \u0026#34;-73.91600037\u0026#34;, 10\t\u0026#34;dropoff_latitude\u0026#34;: \u0026#34;40.74634933\u0026#34;, 11\t\u0026#34;store_and_fwd_flag\u0026#34;: \u0026#34;Y\u0026#34;, 12\t\u0026#34;gc_distance\u0026#34;: 3, 13\t\u0026#34;trip_duration\u0026#34;: 4731, 14\t\u0026#34;google_distance\u0026#34;: 3, 15\t\u0026#34;google_duration\u0026#34;: 4731 16} EventBridge Rule The AWS EventBridge Terraform module is used to create the EventBridge schedule rule and targets. Note that the rule named crons has a configurable number of targets (eg 5) and each target points to the same Lambda producer function. Therefore, we are able to generate test data using multiple Lambda functions based on the desired volume of messages.\n1# infra/producer.tf 2module \u0026#34;eventbridge\u0026#34; { 3 source = \u0026#34;terraform-aws-modules/eventbridge/aws\u0026#34; 4 version = \u0026#34;\u0026gt;=2.3.0, \u0026lt;3.0.0\u0026#34; 5 6 create = local.producer.to_create 7 create_bus = false 8 9 rules = { 10 crons = { 11 description = \u0026#34;Kafka producer lambda schedule\u0026#34; 12 schedule_expression = local.producer.schedule_rate 13 } 14 } 15 16 targets = { 17 crons = [for i in range(local.producer.concurrency) : { 18 name = \u0026#34;lambda-target-${i}\u0026#34; 19 arn = module.kafka_producer.lambda_function_arn 20 }] 21 } 22 23 depends_on = [ 24 module.kafka_producer 25 ] 26 27 tags = local.tags 28} Deployment The application can be deployed (as well as destroyed) using Terraform CLI as shown below. As the default value of producer_to_create is false, we need to set it to true in order to create the Lambda producer function.\n1# initialize 2terraform init 3# create an execution plan 4terraform plan -var \u0026#39;producer_to_create=true\u0026#39; 5# execute the actions proposed in a Terraform plan 6terraform apply -auto-approve=true -var \u0026#39;producer_to_create=true\u0026#39; 7 8# destroy all remote objects 9# terraform destroy -auto-approve=true -var \u0026#39;producer_to_create=true\u0026#39; Once deployed, we can see that the schedule rule has 5 targets of the same Lambda function among others.\nMonitor Topic A Kafka management app can be a good companion for development as it helps monitor and manage resources on an easy-to-use user interface. We\u0026rsquo;ll use Kpow Community Edition in this post, which allows you to link a single Kafka cluster, Kafka connect server and schema registry. Note that the community edition is valid for 12 months and the licence can be requested on this page. Once requested, the licence details will be emailed, and they can be added as an environment file (env_file).\nThe app needs additional configurations in environment variables because the Kafka cluster on Amazon MSK is authenticated by IAM - see this page for details. The bootstrap server address can be found on AWS Console or executing the following Terraform command.\n1$ terraform output -json | jq -r \u0026#39;.msk_bootstrap_brokers_sasl_iam.value\u0026#39; Note that we need to specify the compose file name when starting it because the file name (compose-ui.yml) is different from the default file name (docker-compose.yml). We can run it by docker-compose -f compose-ui.yml up -d and access on a browser via localhost:3000.\n1# compose-ui.yml 2version: \u0026#34;3\u0026#34; 3 4services: 5 kpow: 6 image: factorhouse/kpow-ce:91.5.1 7 container_name: kpow 8 ports: 9 - \u0026#34;3000:3000\u0026#34; 10 networks: 11 - kafkanet 12 environment: 13 AWS_ACCESS_KEY_ID: $AWS_ACCESS_KEY_ID 14 AWS_SECRET_ACCESS_KEY: $AWS_SECRET_ACCESS_KEY 15 AWS_SESSION_TOKEN: $AWS_SESSION_TOKEN 16 BOOTSTRAP: $BOOTSTRAP_SERVERS 17 SECURITY_PROTOCOL: SASL_SSL 18 SASL_MECHANISM: AWS_MSK_IAM 19 SASL_JAAS_CONFIG: software.amazon.msk.auth.iam.IAMLoginModule required; 20 SASL_CLIENT_CALLBACK_HANDLER_CLASS: software.amazon.msk.auth.iam.IAMClientCallbackHandler 21 env_file: # https://kpow.io/get-started/#individual 22 - ./kpow.env 23 24networks: 25 kafkanet: 26 name: kafka-network We can see the topic (taxi-rides) is created, and it has 5 partitions, which is the default number of partitions.\nAlso, we can inspect topic messages in the Data tab as shown below.\nSummary In this lab, we created a Kafka producer application using AWS Lambda, which sends fake taxi ride data into a Kafka topic on Amazon MSK. It was developed so that a configurable number of the producer Lambda function can be invoked by an Amazon EventBridge schedule rule. In this way, we are able to generate test data concurrently based on the desired volume of messages.\n","date":"October 26, 2023","img":"/blog/2023-10-26-real-time-streaming-with-kafka-and-flink-2/featured.png","lang":"en","langName":"English","largeImg":"/blog/2023-10-26-real-time-streaming-with-kafka-and-flink-2/featured_hufd07a51d08fbb1dcf00355523276b259_138560_500x0_resize_box_3.png","permalink":"/blog/2023-10-26-real-time-streaming-with-kafka-and-flink-2/","series":[{"title":"Real Time Streaming With Kafka and Flink","url":"/series/real-time-streaming-with-kafka-and-flink/"}],"smallImg":"/blog/2023-10-26-real-time-streaming-with-kafka-and-flink-2/featured_hufd07a51d08fbb1dcf00355523276b259_138560_180x0_resize_box_3.png","tags":[{"title":"AWS","url":"/tags/aws/"},{"title":"Amazon MSK","url":"/tags/amazon-msk/"},{"title":"AWS Lambda","url":"/tags/aws-lambda/"},{"title":"Apache Kafka","url":"/tags/apache-kafka/"},{"title":"Docker","url":"/tags/docker/"},{"title":"Docker Compose","url":"/tags/docker-compose/"},{"title":"Python","url":"/tags/python/"}],"timestamp":1698278400,"title":"Real Time Streaming With Kafka and Flink - Lab 1 Produce Data to Kafka Using Lambda"},{"categories":[{"title":"Data Streaming","url":"/categories/data-streaming/"}],"content":"OpenSearch is a popular search and analytics engine and its use cases cover log analytics, real-time application monitoring, and clickstream analysis. OpenSearch can be deployed on its own or via Amazon OpenSearch Service. Apache Kafka is a distributed event store and stream-processing platform, and it aims to provide a unified, high-throughput, low-latency platform for handling real-time data feeds. On AWS, Apache Kafka can be deployed via Amazon Managed Streaming for Apache Kafka (MSK).\nWhen it comes to ingesting data from Apache Kafka into OpenSearch, OpenSearch has a tool called Data Prepper and Amazon OpenSearch Service has a feature called Amazon OpenSearch Ingestion. Alternatively we can use Kafka Connect, which is a tool for scalably and reliably streaming data between Apache Kafka and other systems. On AWS, we can run fully managed Kafka workload using Amazon MSK Connect.\nIn this post, we will discuss how to develop a data pipeline from Apache Kafka into OpenSearch locally using Docker while the pipeline will be deployed on AWS in the next post. Fake impressions and clicks data will be pushed into Kafka topics using a Kafka source connector and those records will be ingested into OpenSearch indexes using a sink connector for near-real time analytics.\nPart 1 Introduction Part 2 Develop Camel DynamoDB Sink Connector Part 3 Deploy Camel DynamoDB Sink Connector Part 4 Develop Aiven OpenSearch Sink Connector (this post) Part 5 Deploy Aiven OpenSearch Sink Connector Architecture Fake impressions and clicks events are generated by the Amazon MSK Data Generator, and they are pushed into the corresponding Kafka topics. The topic records are ingested into OpenSearch indexes with the same names for near real-time analysis using the Aiven\u0026rsquo;s OpenSearch Connector for Apache Kafka. The infrastructure is created using Docker and the source can be found in the GitHub repository of this post.\nOpenSearch Cluster We can create OpenSearch and OpenSearch Dashboards using Docker Compose as illustrated in the OpenSearch Quickstart documentation. The OpenSearch stack consists of two cluster nodes and a single dashboard service. Note that you should disable memory paging and swapping performance on the host before creating those - see the documentation for details.\n1# docker-compose.yml 2version: \u0026#34;3.5\u0026#34; 3 4services: 5 6 ... 7 8 opensearch-node1: 9 image: opensearchproject/opensearch:2.9.0 10 container_name: opensearch-node1 11 environment: 12 - cluster.name=opensearch-cluster 13 - node.name=opensearch-node1 14 - discovery.seed_hosts=opensearch-node1,opensearch-node2 15 - cluster.initial_cluster_manager_nodes=opensearch-node1,opensearch-node2 16 - bootstrap.memory_lock=true 17 - \u0026#34;OPENSEARCH_JAVA_OPTS=-Xms512m -Xmx512m\u0026#34; 18 ulimits: 19 memlock: 20 soft: -1 21 hard: -1 22 nofile: 23 soft: 65536 24 hard: 65536 25 volumes: 26 - opensearch-node1-data:/usr/share/opensearch/data 27 ports: 28 - 9200:9200 29 - 9600:9600 30 networks: 31 - service-net 32 opensearch-node2: 33 image: opensearchproject/opensearch:2.9.0 34 container_name: opensearch-node2 35 environment: 36 - cluster.name=opensearch-cluster 37 - node.name=opensearch-node2 38 - discovery.seed_hosts=opensearch-node1,opensearch-node2 39 - cluster.initial_cluster_manager_nodes=opensearch-node1,opensearch-node2 40 - bootstrap.memory_lock=true 41 - \u0026#34;OPENSEARCH_JAVA_OPTS=-Xms512m -Xmx512m\u0026#34; 42 ulimits: 43 memlock: 44 soft: -1 45 hard: -1 46 nofile: 47 soft: 65536 48 hard: 65536 49 volumes: 50 - opensearch-node1-data:/usr/share/opensearch/data 51 networks: 52 - service-net 53 opensearch-dashboards: 54 image: opensearchproject/opensearch-dashboards:2.9.0 55 container_name: opensearch-dashboards 56 ports: 57 - 5601:5601 58 expose: 59 - \u0026#34;5601\u0026#34; 60 environment: 61 OPENSEARCH_HOSTS: \u0026#39;[\u0026#34;https://opensearch-node1:9200\u0026#34;,\u0026#34;https://opensearch-node2:9200\u0026#34;]\u0026#39; 62 networks: 63 - service-net 64 65networks: 66 service-net: 67 name: service-net 68 69volumes: 70 71 ... 72 73 opensearch-node1-data: 74 driver: local 75 name: opensearch-node1-data 76 opensearch-node2-data: 77 driver: local 78 name: opensearch-node2-data Kafka Cluster We will create a Kafka cluster with a single broker and Zookeeper node using the Bitnami container images. Kafka 2.8.1 is used as it is the recommended Kafka version by Amazon MSK. The cluster communicates on port 9092 internally and the bootstrap server of the broker becomes kafka-0:9092 for those that run in the same network.\n1# docker-compose.yml 2version: \u0026#34;3.5\u0026#34; 3 4services: 5 zookeeper: 6 image: bitnami/zookeeper:3.5 7 container_name: zookeeper 8 ports: 9 - \u0026#34;2181\u0026#34; 10 networks: 11 - service-net 12 environment: 13 - ALLOW_ANONYMOUS_LOGIN=yes 14 volumes: 15 - zookeeper-data:/bitnami/zookeeper 16 kafka-0: 17 image: bitnami/kafka:2.8.1 18 container_name: kafka-0 19 expose: 20 - 9092 21 ports: 22 - \u0026#34;29092:29092\u0026#34; 23 networks: 24 - service-net 25 environment: 26 - ALLOW_PLAINTEXT_LISTENER=yes 27 - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181 28 - KAFKA_CFG_BROKER_ID=0 29 - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT 30 - KAFKA_CFG_LISTENERS=INTERNAL://:9092,EXTERNAL://:29092 31 - KAFKA_CFG_ADVERTISED_LISTENERS=INTERNAL://kafka-0:9092,EXTERNAL://localhost:29092 32 - KAFKA_CFG_INTER_BROKER_LISTENER_NAME=INTERNAL 33 - KAFKA_CFG_NUM_PARTITIONS=2 34 - KAFKA_CFG_DEFAULT_REPLICATION_FACTOR=1 35 volumes: 36 - kafka-0-data:/bitnami/kafka 37 depends_on: 38 - zookeeper 39 40 ... 41 42volumes: 43 zookeeper-data: 44 driver: local 45 name: zookeeper-data 46 kafka-0-data: 47 driver: local 48 name: kafka-0-data 49 50 ... Kafka Connect We can use the same Docker image as Kafka Connect is a part of the Kafka distribution. The Kafka connect server runs in the distributed mode so that multiple connectors can be deployed to the same server. Note the Kafka bootstrap server and plugin path configuration values in the connect configuration file (connect-distributed.properties) listed below. As mentioned earlier, the bootstrap server can be accessed on kafka-0:9092 as it is deployed in the same network. Also, we will locate connector source files in subdirectories of the plugin path.\nbootstrap.servers=kafka-0:9092 plugin.path=/opt/connectors 1# docker-compose.yml 2version: \u0026#34;3.5\u0026#34; 3 4services: 5 6 ... 7 8 kafka-connect: 9 image: bitnami/kafka:2.8.1 10 container_name: connect 11 command: \u0026gt; 12 /opt/bitnami/kafka/bin/connect-distributed.sh 13 /opt/bitnami/kafka/config/connect-distributed.properties 14 ports: 15 - \u0026#34;8083:8083\u0026#34; 16 networks: 17 - service-net 18 volumes: 19 - \u0026#34;./configs/connect-distributed.properties:/opt/bitnami/kafka/config/connect-distributed.properties\u0026#34; 20 - \u0026#34;./connectors/opensearch-connector:/opt/connectors/opensearch-connector\u0026#34; 21 - \u0026#34;./connectors/msk-datagen:/opt/connectors/msk-datagen\u0026#34; 22 depends_on: 23 - zookeeper 24 - kafka-0 25 26 ... Download Connectors The connector sources need to be downloaded into the ./connectors path so that they can be volume-mapped to the container\u0026rsquo;s plugin path (/opt/connectors). The MSK Data Generator is a single Jar file, and it can be kept as it is. On the other hand, the Aiven OpenSearch sink connector is an archive file, and it should be decompressed. Note the zip file will be used to create a custom plugin of MSK Connect in the next post. The following script downloads the connector sources into the host path.\n1# download.sh 2#!/usr/bin/env bash 3shopt -s extglob 4 5SCRIPT_DIR=\u0026#34;$(cd $(dirname \u0026#34;$0\u0026#34;); pwd)\u0026#34; 6 7SRC_PATH=${SCRIPT_DIR}/connectors 8rm -rf ${SRC_PATH} \u0026amp;\u0026amp; mkdir ${SRC_PATH} 9 10## Avien opensearch sink connector 11echo \u0026#34;downloading opensearch sink connector...\u0026#34; 12DOWNLOAD_URL=https://github.com/Aiven-Open/opensearch-connector-for-apache-kafka/releases/download/v3.1.0/opensearch-connector-for-apache-kafka-3.1.0.zip 13 14curl -L -o ${SRC_PATH}/tmp.zip ${DOWNLOAD_URL} \\ 15 \u0026amp;\u0026amp; unzip -qq ${SRC_PATH}/tmp.zip -d ${SRC_PATH} \\ 16 \u0026amp;\u0026amp; rm -rf $SRC_PATH/!(opensearch-connector-for-apache-kafka-3.1.0) \\ 17 \u0026amp;\u0026amp; mv $SRC_PATH/opensearch-connector-for-apache-kafka-3.1.0 $SRC_PATH/opensearch-connector \\ 18 \u0026amp;\u0026amp; cd $SRC_PATH/opensearch-connector \\ 19 \u0026amp;\u0026amp; zip ../opensearch-connector.zip * 20 21## MSK Data Generator Souce Connector 22echo \u0026#34;downloading msk data generator...\u0026#34; 23DOWNLOAD_URL=https://github.com/awslabs/amazon-msk-data-generator/releases/download/v0.4.0/msk-data-generator-0.4-jar-with-dependencies.jar 24 25mkdir ${SRC_PATH}/msk-datagen \\ 26 \u0026amp;\u0026amp; curl -L -o ${SRC_PATH}/msk-datagen/msk-data-generator.jar ${DOWNLOAD_URL} Below shows the folder structure after the connectors are downloaded successfully.\n1$ tree connectors/ 2connectors/ 3├── msk-datagen 4│ └── msk-data-generator.jar 5├── opensearch-connector 6 7... 8 9│ ├── opensearch-2.6.0.jar 10│ ├── opensearch-cli-2.6.0.jar 11│ ├── opensearch-common-2.6.0.jar 12│ ├── opensearch-connector-for-apache-kafka-3.1.0.jar 13│ ├── opensearch-core-2.6.0.jar 14 15... 16 17└── opensearch-connector.zip 18 192 directories, 56 files Kafka Management App A Kafka management app can be a good companion for development as it helps monitor and manage resources on an easy-to-use user interface. We\u0026rsquo;ll use Kpow Community Edition in this post, and we can link a single Kafka cluster, Kafka connect server and schema registry. Note that the community edition is valid for 12 months and the license can be requested in this page. Once requested, the license details will be emailed, and they can be added as an environment file (env_file).\n1# docker-compose.yml 2version: \u0026#34;3.5\u0026#34; 3 4services: 5 6 ... 7 8 kpow: 9 image: factorhouse/kpow-ce:91.5.1 10 container_name: kpow 11 ports: 12 - \u0026#34;3000:3000\u0026#34; 13 networks: 14 - service-net 15 environment: 16 BOOTSTRAP: kafka-0:9092 17 CONNECT_REST_URL: http://kafka-connect:8083 18 env_file: # https://kpow.io/get-started/#individual 19 - ./kpow.env 20 21 ... Data Ingestion to Kafka Topic Create Index Mappings The topic messages include a timestamp field (created_at), but its type is not identified correctly via dynamic mapping. Instead, indexes are created explicitly as shown below.\n1# configs/create-index-mappings.sh 2#!/usr/bin/env bash 3echo \u0026#34;Create impressions index and field mapping\u0026#34; 4curl -X PUT \u0026#34;https://localhost:9200/impressions\u0026#34; -ku admin:admin -H \u0026#39;Content-Type: application/json\u0026#39; -d\u0026#39; 5{ 6 \u0026#34;mappings\u0026#34;: { 7 \u0026#34;properties\u0026#34;: { 8 \u0026#34;bid_id\u0026#34;: { 9 \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34; 10 }, 11 \u0026#34;created_at\u0026#34;: { 12 \u0026#34;type\u0026#34;: \u0026#34;date\u0026#34;, 13 \u0026#34;format\u0026#34;: \u0026#34;yyyy-MM-dd HH:mm:ss\u0026#34; 14 }, 15 \u0026#34;campaign_id\u0026#34;: { 16 \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34; 17 }, 18 \u0026#34;creative_details\u0026#34;: { 19 \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34; 20 }, 21 \u0026#34;country_code\u0026#34;: { 22 \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34; 23 } 24 } 25 } 26}\u0026#39; 27 28echo 29echo \u0026#34;Create clicks index and field mapping\u0026#34; 30curl -X PUT \u0026#34;https://localhost:9200/clicks\u0026#34; -ku admin:admin -H \u0026#39;Content-Type: application/json\u0026#39; -d\u0026#39; 31{ 32 \u0026#34;mappings\u0026#34;: { 33 \u0026#34;properties\u0026#34;: { 34 \u0026#34;correlation_id\u0026#34;: { 35 \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34; 36 }, 37 \u0026#34;created_at\u0026#34;: { 38 \u0026#34;type\u0026#34;: \u0026#34;date\u0026#34;, 39 \u0026#34;format\u0026#34;: \u0026#34;yyyy-MM-dd HH:mm:ss\u0026#34; 40 }, 41 \u0026#34;tracker\u0026#34;: { 42 \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34; 43 } 44 } 45 } 46}\u0026#39; 47 48echo Once created, we can check them in the OpenSearch Dashboards on localhost:5601.\nSource Connector Creation We can create the source connector programmatically using the Connect REST API. The REST endpoint requires a JSON payload that includes the connector name and configurations.\n1$ curl -i -X POST -H \u0026#34;Accept:application/json\u0026#34; -H \u0026#34;Content-Type:application/json\u0026#34; \\ 2 http://localhost:8083/connectors/ -d @configs/source.json Below shows the source connector configuration file.\n1// configs/source.json 2{ 3 \u0026#34;name\u0026#34;: \u0026#34;ad-tech-source\u0026#34;, 4 \u0026#34;config\u0026#34;: { 5 \u0026#34;connector.class\u0026#34;: \u0026#34;com.amazonaws.mskdatagen.GeneratorSourceConnector\u0026#34;, 6 \u0026#34;tasks.max\u0026#34;: \u0026#34;2\u0026#34;, 7 \u0026#34;key.converter\u0026#34;: \u0026#34;org.apache.kafka.connect.storage.StringConverter\u0026#34;, 8 \u0026#34;key.converter.schemas.enable\u0026#34;: false, 9 \u0026#34;value.converter\u0026#34;: \u0026#34;org.apache.kafka.connect.json.JsonConverter\u0026#34;, 10 \u0026#34;value.converter.schemas.enable\u0026#34;: false, 11 12 \u0026#34;genkp.impressions.with\u0026#34;: \u0026#34;#{Code.isbn10}\u0026#34;, 13 \u0026#34;genv.impressions.bid_id.with\u0026#34;: \u0026#34;#{Code.isbn10}\u0026#34;, 14 \u0026#34;genv.impressions.campaign_id.with\u0026#34;: \u0026#34;#{Code.isbn10}\u0026#34;, 15 \u0026#34;genv.impressions.creative_details.with\u0026#34;: \u0026#34;#{Color.name}\u0026#34;, 16 \u0026#34;genv.impressions.country_code.with\u0026#34;: \u0026#34;#{Address.countryCode}\u0026#34;, 17 18 \u0026#34;genkp.clicks.with\u0026#34;: \u0026#34;#{Code.isbn10}\u0026#34;, 19 \u0026#34;genv.clicks.correlation_id.sometimes.matching\u0026#34;: \u0026#34;impressions.value.bid_id\u0026#34;, 20 \u0026#34;genv.clicks.correlation_id.sometimes.with\u0026#34;: \u0026#34;NA\u0026#34;, 21 \u0026#34;genv.clicks.tracker.with\u0026#34;: \u0026#34;#{Lorem.characters \u0026#39;15\u0026#39;}\u0026#34;, 22 23 \u0026#34;global.throttle.ms\u0026#34;: \u0026#34;500\u0026#34;, 24 \u0026#34;global.history.records.max\u0026#34;: \u0026#34;1000\u0026#34; 25 } 26} The first six attributes are in relation to general configurations. The connector class (connector.class) is required for any connector and I set it for the MSK Data Generator. Also, two tasks are allocated to it (tasks.max). The message key is set to be converted into string (key.converter) while the value to json (value.converter). The former is because the keys are configured to have string primitive values (genkp) by the source connector. Finally, schemas are not enabled for both the key and value.\nThe remaining attributes are for the MSK Data Generator. Two topics named impressions and clicks will be created, and the messages attributes are generated by the Java faker library. Interestingly the bid ID of the impression message and the correlation ID of the click message share the same value sometimes. This is because only a fraction of impressions results in clicks in practice.\nOnce created successfully, we can check the connector status as following.\n1$ curl http://localhost:8083/connectors/ad-tech-source/status | json_pp 2{ 3 \u0026#34;connector\u0026#34; : { 4 \u0026#34;state\u0026#34; : \u0026#34;RUNNING\u0026#34;, 5 \u0026#34;worker_id\u0026#34; : \u0026#34;172.19.0.8:8083\u0026#34; 6 }, 7 \u0026#34;name\u0026#34; : \u0026#34;ad-tech-source\u0026#34;, 8 \u0026#34;tasks\u0026#34; : [ 9 { 10 \u0026#34;id\u0026#34; : 0, 11 \u0026#34;state\u0026#34; : \u0026#34;RUNNING\u0026#34;, 12 \u0026#34;worker_id\u0026#34; : \u0026#34;172.19.0.8:8083\u0026#34; 13 }, 14 { 15 \u0026#34;id\u0026#34; : 1, 16 \u0026#34;state\u0026#34; : \u0026#34;RUNNING\u0026#34;, 17 \u0026#34;worker_id\u0026#34; : \u0026#34;172.19.0.8:8083\u0026#34; 18 } 19 ], 20 \u0026#34;type\u0026#34; : \u0026#34;source\u0026#34; 21} Source Data We can check messages are ingested into the impressions and clicks topics in Kpow on localhost:3000.\nAs mentioned earlier, only a fraction of correlation IDs of the click messages has actual values, and we can check that by inspecting the messages.\nData Ingestion to OpenSearch Sink Connector Creation Similar to the source connector, we can create the sink connector using the REST API.\n1$ curl -i -X POST -H \u0026#34;Accept:application/json\u0026#34; -H \u0026#34;Content-Type:application/json\u0026#34; \\ 2 http://localhost:8083/connectors/ -d @configs/sink.json Below shows the sink connector configuration file.\n1// configs/sink.json 2{ 3 \u0026#34;name\u0026#34;: \u0026#34;ad-tech-sink\u0026#34;, 4 \u0026#34;config\u0026#34;: { 5 \u0026#34;connector.class\u0026#34;: \u0026#34;io.aiven.kafka.connect.opensearch.OpensearchSinkConnector\u0026#34;, 6 \u0026#34;tasks.max\u0026#34;: \u0026#34;2\u0026#34;, 7 \u0026#34;topics\u0026#34;: \u0026#34;impressions,clicks\u0026#34;, 8 \u0026#34;key.converter\u0026#34;: \u0026#34;org.apache.kafka.connect.storage.StringConverter\u0026#34;, 9 \u0026#34;key.converter.schemas.enable\u0026#34;: false, 10 \u0026#34;value.converter\u0026#34;: \u0026#34;org.apache.kafka.connect.json.JsonConverter\u0026#34;, 11 \u0026#34;value.converter.schemas.enable\u0026#34;: false, 12 13 \u0026#34;connection.url\u0026#34;: \u0026#34;https://opensearch-node1:9200,https://opensearch-node2:9200\u0026#34;, 14 \u0026#34;connection.username\u0026#34;: \u0026#34;admin\u0026#34;, 15 \u0026#34;connection.password\u0026#34;: \u0026#34;admin\u0026#34;, 16 \u0026#34;schema.ignore\u0026#34;: true, 17 \u0026#34;key.ignore\u0026#34;: true, 18 \u0026#34;type.name\u0026#34;: \u0026#34;_doc\u0026#34;, 19 \u0026#34;behavior.on.malformed.documents\u0026#34;: \u0026#34;fail\u0026#34;, 20 \u0026#34;behavior.on.null.values\u0026#34;: \u0026#34;ignore\u0026#34;, 21 \u0026#34;behavior.on.version.conflict\u0026#34;: \u0026#34;ignore\u0026#34;, 22 23 \u0026#34;errors.deadletterqueue.topic.name\u0026#34;: \u0026#34;ad-tech-dl\u0026#34;, 24 \u0026#34;errors.tolerance\u0026#34;: \u0026#34;all\u0026#34;, 25 \u0026#34;errors.deadletterqueue.context.headers.enable\u0026#34;: true, 26 \u0026#34;errors.deadletterqueue.topic.replication.factor\u0026#34;: 1, 27 28 \u0026#34;transforms\u0026#34;: \u0026#34;insertTS,formatTS\u0026#34;, 29 \u0026#34;transforms.insertTS.type\u0026#34;: \u0026#34;org.apache.kafka.connect.transforms.InsertField$Value\u0026#34;, 30 \u0026#34;transforms.insertTS.timestamp.field\u0026#34;: \u0026#34;created_at\u0026#34;, 31 \u0026#34;transforms.formatTS.type\u0026#34;: \u0026#34;org.apache.kafka.connect.transforms.TimestampConverter$Value\u0026#34;, 32 \u0026#34;transforms.formatTS.format\u0026#34;: \u0026#34;yyyy-MM-dd HH:mm:ss\u0026#34;, 33 \u0026#34;transforms.formatTS.field\u0026#34;: \u0026#34;created_at\u0026#34;, 34 \u0026#34;transforms.formatTS.target.type\u0026#34;: \u0026#34;string\u0026#34; 35 } 36} The connector is configured to write messages from the impressions and clicks topics into the OpenSearch indexes created earlier. It uses the same key and value converters to the source connector, and schemas are not enabled for both the key and value.\nThe cluster request URLs are added to the connection URL attribute (connection.url), and the default username and password are specified accordingly. These values are necessary to make HTTP requests to the cluster as shown earlier when we created indexes with explicit schema mapping. Also, as the topics are append-only logs, we can set the document ID to be [topic-name].[partition].[offset] by setting key.ignore to true. See the connector configuration document for more details.\nHaving an event timestamp attribute can be useful for performing temporal analysis. As I don\u0026rsquo;t find a comprehensive way to set it up in the source connector, a new field called created_at is added using single message transforms (SMTs). Specifically I added two transforms - insertTS and formatTS. As the name suggests, the former inserts the system timestamp value while it is formatted into yyyy-MM-dd HH:mm:ss by the latter.\nOnce created successfully, we can check the connector status as following.\n1$ curl http://localhost:8083/connectors/ad-tech-sink/status | json_pp 2{ 3 \u0026#34;connector\u0026#34; : { 4 \u0026#34;state\u0026#34; : \u0026#34;RUNNING\u0026#34;, 5 \u0026#34;worker_id\u0026#34; : \u0026#34;172.20.0.8:8083\u0026#34; 6 }, 7 \u0026#34;name\u0026#34; : \u0026#34;ad-tech-sink\u0026#34;, 8 \u0026#34;tasks\u0026#34; : [ 9 { 10 \u0026#34;id\u0026#34; : 0, 11 \u0026#34;state\u0026#34; : \u0026#34;RUNNING\u0026#34;, 12 \u0026#34;worker_id\u0026#34; : \u0026#34;172.20.0.8:8083\u0026#34; 13 }, 14 { 15 \u0026#34;id\u0026#34; : 1, 16 \u0026#34;state\u0026#34; : \u0026#34;RUNNING\u0026#34;, 17 \u0026#34;worker_id\u0026#34; : \u0026#34;172.20.0.8:8083\u0026#34; 18 } 19 ], 20 \u0026#34;type\u0026#34; : \u0026#34;sink\u0026#34; 21} OpenSearch Destination In OpenSearch Dashboards, we can search clicks that are associated with impressions. As expected, only a small portion of clicks are searched.\nMoreover, we can join correlated impressions and clicks quickly using the Query Workbench. Below shows a simple SQL query that joins impressions and associating clicks that are created after a certain time point.\nSummary Kafka Connect can be effective to ingesting data from Apache Kafka into OpenSearch. In this post, we discussed how to develop a data pipeline from Apache Kafka into OpenSearch locally using Docker. Fake impressions and clicks data was pushed into Kafka topics using a Kafka source connector and those records will be ingested into OpenSearch indexes using a sink connector for near-real time analytics. In this next post, the pipeline will be deployed on AWS using Amazon OpenSearch Service, Amazon MSK and Amazon MSK Connect.\n","date":"October 23, 2023","img":"/blog/2023-10-23-kafka-connect-for-aws-part-4/featured.png","lang":"en","langName":"English","largeImg":"/blog/2023-10-23-kafka-connect-for-aws-part-4/featured_hu8acda52fcdf23bd4db3e9dcabda233b7_61820_500x0_resize_box_3.png","permalink":"/blog/2023-10-23-kafka-connect-for-aws-part-4/","series":[{"title":"Kafka Connect for AWS Services Integration","url":"/series/kafka-connect-for-aws-services-integration/"}],"smallImg":"/blog/2023-10-23-kafka-connect-for-aws-part-4/featured_hu8acda52fcdf23bd4db3e9dcabda233b7_61820_180x0_resize_box_3.png","tags":[{"title":"AWS","url":"/tags/aws/"},{"title":"Apache Kafka","url":"/tags/apache-kafka/"},{"title":"Kafka Connect","url":"/tags/kafka-connect/"},{"title":"OpenSearch","url":"/tags/opensearch/"},{"title":"Docker","url":"/tags/docker/"},{"title":"Docker Compose","url":"/tags/docker-compose/"}],"timestamp":1698019200,"title":"Kafka Connect for AWS Services Integration - Part 4 Develop Aiven OpenSearch Sink Connector"},{"categories":[{"title":"Data Streaming","url":"/categories/data-streaming/"}],"content":"Building Apache Flink Applications in Java is a course to introduce Apache Flink through a series of hands-on exercises, and it is provided by Confluent. Utilising the Flink DataStream API, the course develops three Flink applications that populate multiple source data sets, collect them into a standardised data set, and aggregate it to produce usage statistics. As part of learning the Flink DataStream API in Pyflink, I converted the Java apps into Python equivalent while performing the course exercises in Pyflink. This post summarises the progress of the conversion and shows the final output.\nArchitecture There are two airlines (SkyOne and Sunset) and they have their own flight data in different schemas. While the course ingests the source data into corresponding topics using a Flink application that makes use of the DataGen DataStream Connector, we use a Kafka producer application here because the DataGen connector is not available for python.\nThe flight importer job reads the messages from the source topics, standardises them into the flight data schema, and pushed into another Kafka topic, called flightdata. It is developed using Pyflink.\nThe usage statistics calculator sources the flightdata topic and calculates usage statistics over a one-minute window, which is grouped by email address. Moreover, while accessing the global state, it produces cumulative usage statistics, which carries information from one window to the next. It is developed using Pyflink as well.\nCourse Contents Below describes course contents. ✅ and ☑️ indicate exercises and course materials respectively. The lesson 3 covers how to set up Kafka and Flink clusters using Docker Compose. The Kafka producer app is created as the lesson 5 exercise. The final versions of the flight importer job and usage statistics calculator can be found as exercises of the lesson 16 and 20 respectively.\nApache Flink with Java - An Introduction Datastream Programming ✅ How to Start Flink and Get Setup (Exercise) Built Kafka and Flink clusters using Docker Bitnami images are used for the Kafka cluster - see this page for details. A custom Docker image (building-pyflink-apps:1.17.1) is created to install Python and the Pyflink package as well as to save dependent Jar files See the Dockerfile, and it can be built by docker build -t=building-pyflink-apps:1.17.1 . See the docker-compose.yml and the clusters can be started by docker-compose up -d ☑️ The Flink Job Lifecycle A minimal example of executing a Pyflink app is added. See course content(s) below s04_intro.py ✅ Running a Flink Job (Exercise) Pyflink doesn\u0026rsquo;t have the DataGen DataStream connector. Used a Kafka producer instead to create topics and send messages. 4 topics are created (skyone, sunset, flightdata and userstatistics) and messages are sent to the first two topics. See course content(s) below s05_data_gen.py Topics are created by a flag argument so add it if it is the first time running it. i.e. python src/s05_data_gen.py --create. Basically it deletes the topics if exits and creates them. Anatomy of a Stream Flink Data Sources ✅ Creating a Flink Data Source (Exercise) It reads from the skyone topic and prints the values. The values are deserialized as string in this exercise. This and all the other Pyflink applications can be executed locally or run in the Flink cluster. See the script for details. See course content(s) below s08_create_source.py Serializers \u0026amp; Deserializers ✅ Deserializing Messages in Flink (Exercise) The skyone message values are deserialized as Json string and they are returned as the named Row type. As the Flink type is not convenient for processing, it is converted into a Python object, specifically Data Classes. See course content(s) below s10_deserialization.py ☑️ Transforming Data in Flink Map, FlatMap, Filter and Reduce transformations are illustrated using built-in operators and process functions. See course content(s) below s11_transformation.py s11_process_function.py ✅ Flink Data Transformations (Exercise) The source data is transformed into the flight data. Later data from skyone and sunset will be converted into this schema for merging them. The transformation is performed in a function called define_workflow for being tested. This function will be updated gradually. See course content(s) below s12_transformation.py test_s12_transformation.py Expected to run testing scripts individually eg) pytest src/test_s12_transformation.py -svv Flink Data Sinks ✅ Creating a Flink Data Sink (Exercise) The converted data from skyone will be pushed into a Kafka topic (flightdata). Note that, as the Python Data Classes cannot be serialized, records are converted into the named Row type before being sent. See course content(s) below s14_sink.py ☑️ Creating Branching Data Streams in Flink Various branching methods are illustrated, which covers Union, CoProcessFunction, CoMapFunction, CoFlatMapFunction, and Side Outputs. See course content(s) below s15_branching.py ✅ Merging Flink Data Streams (Exercise) Records from the skyone and sunset topics are merged and sent into the flightdata topic after being converted into the flight data. See course content(s) below s16_merge.py test_s16_merge.py Windowing and Watermarks in Flink ✅ Aggregating Flink Data using Windowing (Exercise) Usage statistics (total flight duration and number of flights) are calculated by email address, and they are sent into the userstatistics topic. Note the transformation is stateless in a sense that aggregation is entirely within a one-minute tumbling window. See course content(s) below s18_aggregation.py test_s18_aggregation.py Working with Keyed State in Flink ✅ Managing State in Flink (Exercise) The transformation gets stateful so that usage statistics are continuously updated by accessing the state values. The reduce function includes a window function that allows you to access the global state. The window function takes the responsibility to keep updating the global state and to return updated values. See course content(s) below s20_manage_state.py test_s20_manage_state.py Closing Remarks Start Applications After creating the Kafka and Flink clusters using Docker Compose, we need to start the Python producer in one terminal. Then we can submit the other Pyflink applications in another terminal.\n1#### build docker image for Pyflink 2docker build -t=building-pyflink-apps:1.17.1 . 3 4#### create kafka and flink clusters and kafka-ui 5docker-compose up -d 6 7#### start kafka producer in one terminal 8python -m venv venv 9source venv/bin/activate 10# upgrade pip (optional) 11pip install pip --upgrade 12# install required packages 13pip install -r requirements-dev.txt 14## start with --create flag to create topics before sending messages 15python src/s05_data_gen.py --create 16 17#### submit pyflink apps in another terminal 18## flight importer 19docker exec jobmanager /opt/flink/bin/flink run \\ 20 --python /tmp/src/s16_merge.py \\ 21 --pyFiles file:///tmp/src/models.py,file:///tmp/src/utils.py \\ 22 -d 23 24## usage calculator 25docker exec jobmanager /opt/flink/bin/flink run \\ 26 --python /tmp/src/s20_manage_state.py \\ 27 --pyFiles file:///tmp/src/models.py,file:///tmp/src/utils.py \\ 28 -d We can check the Pyflink jobs are running on the Flink Dashboard via localhost:8081.\nAlso, we can check the Kafka topics on kafka-ui via localhost:8080.\nUnit Testing Four lessons have unit testing cases, and they are expected to run separately by specifying a testing script. For example, below shows running unit testing cases of the final usage statistics calculator job.\n","date":"October 19, 2023","img":"/blog/2023-10-19-build-pyflink-apps/featured.png","lang":"en","langName":"English","largeImg":"/blog/2023-10-19-build-pyflink-apps/featured_hubbcf607e326a86791dcaeb23a1bfd08e_154736_500x0_resize_box_3.png","permalink":"/blog/2023-10-19-build-pyflink-apps/","series":[],"smallImg":"/blog/2023-10-19-build-pyflink-apps/featured_hubbcf607e326a86791dcaeb23a1bfd08e_154736_180x0_resize_box_3.png","tags":[{"title":"Apache Kafka","url":"/tags/apache-kafka/"},{"title":"Apache Flink","url":"/tags/apache-flink/"},{"title":"PyFlink","url":"/tags/pyflink/"},{"title":"Docker","url":"/tags/docker/"},{"title":"Docker Compose","url":"/tags/docker-compose/"},{"title":"Python","url":"/tags/python/"}],"timestamp":1697673600,"title":"Building Apache Flink Applications in Python"},{"categories":[{"title":"General","url":"/categories/general/"}],"content":"I recently obtained the Certified Kubernetes Application Developer (CKAD) certification. CKAD has been developed by The Linux Foundation and the Cloud Native Computing Foundation (CNCF), to help expand the Kubernetes ecosystem through standardized training and certification. Specifically this certification is for Kubernetes engineers, cloud engineers and other IT professionals responsible for building, deploying, and configuring cloud native applications with Kubernetes. In this post, I will summarise how I prepared for the exam by reviewing three online courses and two practice tests that I went through.\nOnline Courses I took the following courses.\nUdemy Kubernetes Certified Application Developer (CKAD) with Tests Cloud Academy Certified Kubernetes Application Developer (CKAD) Exam Preparation A Cloud Guru Certified Kubernetes Application Developer (CKAD) Udemy I began with the Udemy course. The lectures are okay individually, and labs and practice tests are performed in an exam-like environment where questions are shown on the left side while the terminal is located on the right side. The course seems to had a major update and lots of lectures are located in the Updates for Sep 2021 Changes section. Those updated lectures are not according to the exam curriculum and I felt confused to figure out how they are related. Therefore, I\u0026rsquo;m not sure if the lectures are good collectively for those who are new to Kubernetes. Regardless its lightning labs and mock exams are quite useful. Besides, the course includes a 20% exam discount coupon, and it covers more than the course price.\nCloud Academy Because of the confusion, I decided to take the Cloud Academy course. Starting from a mini course titled an introduction to Kubernetes, it covers individual exam topics mostly via hands-on labs. This course was the most beneficial for me because it has a dedicated lecture for the Kubernetes command line tool (kubectl) and makes use of it in all labs and exam challenges. As you would have found out from other sources already, we can save a lot of time with kubectl, although the exam allows you to refer to the Kubernetes document site. The following lists some useful cases where the command line tool becomes beneficial significantly.\nCreating Kubenetes resources (pod, job, cron job, deployment, service \u0026hellip;) with basic configurations easily (or saving their YAML manifests for further updates), Saving quite some time of checking configuration objects via kubectl explain rather than searching the Kubenetes document, and Creating a dummy resource to see how specific configurations are applied e.g. for rolling update configuration of a deployment, we can create a dummy deployment and copy/paste from it Where you\u0026rsquo;re familiar with kubectl or not, you will be able to learn how to manage Kubenetes resources quickly and efficiently using it throughout the course, which I find one of the most important abilities for the exam. On the other hand, I think some parts of the course don\u0026rsquo;t specifically match the exam scope. For example, it has a full course of Helm, and it covers way too much by ending-up creating your own helm charts. You may skip it if you just focus on the exam. Instead, you can learn from the Introduction section of the Helm document.\nA Cloud Guru A friend of mine recommended the A Cloud Guru course and I took it as I had a good memory while I was preparing for my Kafka certification. The lectures are well-organised and matches the exam curriculum suitably. It also provides good study notes that keep key points and relevant document links. Although I enjoyed the lectures, I find quite a significant drawback of this course. The lectures and answers to hands-on labs/practice exams rely on YAML manifests, and I don\u0026rsquo;t think it is a good approach for exam preparation. Nonetheless, this concise and focused course can be beneficial if you\u0026rsquo;re familiar with the Kubernetes command line tool already.\nPractice Exams CKAD Exercises After finishing the courses, I searched practice exams and found CKAD exercises by Dimitris-Ilias Gkanatsios. Although the questions are grouped in the old curriculum, they do cover the current curriculum suitably as well. It also includes relevant Kubernetes document links, and you may go through them before or after attempting the questions if you\u0026rsquo;d like to learn more.\nKodeKloud Even after I completed the CKAD exercises, I still was not sure if I was prepared enough. Luckily the KodeKloud (folks who created the Udemy course) started a free week until the 1st of October, and I took the Ultimate Certified Kubernetes Application Developer (CKAD) Mock Exam Series. It has a total of 10 exams and each exam has 20 questions. Not every question is new, however, some of them are repeated in later exams. The questions are quite good as I was able to practice complicated scenarios and learn more especially Custom Resource Definition and Ingress.\nActual Exam I was given 16 questions that requires to create/update/fix Kubernetes resources. The final question was about creating a Docker image and saving it to a local folder, which is also a part of the curriculum. For me, the questions were not tricky as the instructions were quite specific. Overall I find the level of difficulty is similar to the CKAD exercises. Unlike what others mention, I had 30 minutes left after I attempted all questions, and I had a chance to review all of them starting from flagged ones.\nI hope you find this post useful and good luck with your exam!\n","date":"October 12, 2023","img":"/blog/2023-10-12-how-i-prepared-for-ckad/featured.png","lang":"en","langName":"English","largeImg":"/blog/2023-10-12-how-i-prepared-for-ckad/featured_hu6266fa892d616d56ee740677f3759115_5945_500x0_resize_box_3.png","permalink":"/blog/2023-10-12-how-i-prepared-for-ckad/","series":[],"smallImg":"/blog/2023-10-12-how-i-prepared-for-ckad/featured_hu6266fa892d616d56ee740677f3759115_5945_180x0_resize_box_3.png","tags":[{"title":"Kubernetes","url":"/tags/kubernetes/"},{"title":"Certification","url":"/tags/certification/"}],"timestamp":1697068800,"title":"How I Prepared for Certified Kubernetes Application Developer (CKAD)"},{"categories":[{"title":"Data Streaming","url":"/categories/data-streaming/"}],"content":"Real Time Streaming with Amazon Kinesis is an AWS workshop that helps users build a streaming analytics application on AWS. Incoming events are stored in a number of streams of the Amazon Kinesis Data Streams service, and various other AWS services and tools are used to process and analyse data.\nApache Kafka is a popular distributed event store and stream processing platform, and it stores incoming events in topics. As part of learning real time streaming analytics on AWS, we can rebuild the analytics applications by replacing the Kinesis streams with Kafka topics. As an introduction, this post compares the workshop architecture with the updated architecture of this series. The labs of the updated architecture will be implemented in subsequent posts.\nIntroduction (this post) Lab 1 Produce data to Kafka using Lambda Lab 2 Write data to Kafka from S3 using Flink Lab 3 Transform and write data to S3 from Kafka using Flink Lab 4 Clean, Aggregate, and Enrich Events with Flink Lab 5 Write data to DynamoDB using Kafka Connect Lab 6 Consume data from Kafka using Lambda [Update 2023-11-06] Initially I planned to deploy Pyflink applications on Amazon Managed Service for Apache Flink, but I changed the plan to use a local Flink cluster deployed on Docker. The main reasons are\nIt is not clear how to configure a Pyflink application for the managed service. For example, Apache Flink supports pluggable file systems and the required dependency (eg flink-s3-fs-hadoop-1.15.2.jar) should be placed under the plugins folder. However, the sample Pyflink applications from pyflink-getting-started and amazon-kinesis-data-analytics-blueprints either ignore the S3 jar file for deployment or package it together with other dependencies - none of them uses the S3 jar file as a plugin. I tried multiple different configurations, but all ended up with having an error whose code is CodeError.InvalidApplicationCode. I don\u0026rsquo;t have such an issue when I deploy the app on a local Flink cluster and I haven\u0026rsquo;t found a way to configure the app for the managed service as yet. The Pyflink app for Lab 4 requires the OpenSearch sink connector and the connector is available on 1.16.0+. However, the latest Flink version of the managed service is still 1.15.2 and the sink connector is not available on it. Normally the latest version of the managed service is behind two minor versions of the official release, but it seems to take a little longer to catch up at the moment - the version 1.18.0 was released a while ago. Workshop Architecture Lab 1 - Produce data to Kinesis Data Streams We will go through a couple of ways to write data to a Kinesis Data Stream using Amazon SDK and Amazon Kinesis Producer Library. Lab 2 - Write Data to a Kinesis Data Stream using Kinesis Data Analytics Studio Notebook We will use Zeppelin Notebook to read Taxi Ride data from S3 and insert into Kinesis Stream. Lab 3 - Lambda with Kinesis Data Firehose We will create a Kinesis stream and integrate with Amazon Kinesis Data Firehose delivery stream to write to a S3 bucket. We will also create a Lambda function that transforms the incoming events and then sends the transformed data to the Firehose Delivery Stream. Finally, the data in S3 will be queried by Amazon Athena. Lab 4 - Clean, Aggregate, and Enrich Events with Kinesis Data Analytics We will learn how to connect Kinesis Data Analytics Studio to your existing stream and clean, aggregate, and enrich the incoming events. The derived insights are finally persisted in Amazon OpenSearch Service, where they can be accessed and visualized using OpenSearch Dashboard. Lab 5 - Lambda Consumer for Kinesis Data Stream We will use a Lambda consumer to consume data from the Kinesis Data Stream. As part of the lab we will create the Lambda function to process records from the Kinesis Data Stream. Lab 6 - Consuming with Amazon KCL We will consume and process data with the Kinesis Client Library (KCL). The KCL takes care of many complex tasks associated with distributed processing and allows you to focus on the record processing logic. Architecture Based-on Kafka and Flink Lab 1 - Produce data to Kafka using Lambda We will create Kafka producers using an EventBridge schedule rule and Lambda producer function. The schedule rule is set to run every minute and has a configurable number of targets where each of them invokes the producer function. The producer function sends messages to a Kafka cluster on Amazon MSK. In this way we are able to generate events using multiple Lambda functions according to the desired volume of events. Lab 2 - Write data to Kafka from S3 using Flink We will develop a Pyflink application that reads Taxi Ride data from S3 and inserts into Kafka. As Apache Flink supports both stream and batch processing, we are able to process static data without an issue. This kind of exercise can be useful for data enrichment that joins static data into stream events. Lab 3 - Transform and write data to S3 from Kafka using Flink We will write Kafka messages to a S3 bucket using a Pyflink application. Although Kafka Connect supports simple data transformations by the single message transforms, they are quite limited compared to the scope that Apache Flink supports. Note that writing data to S3 allows us to build a data lake with real time data. Alternatively we would be able to use the managed data delivery of Amazon MSK, which loads data into Amazon S3 via Amazon Kinesis Data Firehose. This post sticks to a Pyflink application as it has potential to write data on open table formats such as Apache Iceberg and Apache Hudi. Lab 4 - Clean, Aggregate, and Enrich Events with Flink We will learn how to connect a Pyflink application to the existing Kafka topics and clean, aggregate, and enrich the incoming events. The derived insights are finally persisted in Amazon OpenSearch Service, where they can be accessed and visualised using OpenSearch Dashboard. Note that the OpenSearch Flink connector is supported on Apache Flink version 1.16+ where the latest supported version of Amazon Managed Flink is 1.15.2. Normally Amazon Managed Flink lags two minor versions behind and a newer version would be supported by the time when the lab is performed - The release of Apache Flink version 1.18 is expected at the end of September 2023. Lab 5 - Write data to DynamoDB using Kafka Connect We will learn how to write data into a DynamoDB table using Kafka Connect. Kafka Connect is a tool for scalably and reliably streaming data between Apache Kafka and other systems. Apache Camel provides a number of open source Kafka connectors that can be used to integrate AWS services. Lab 6 - Consume data from Kafka using Lambda We will consume and process data with a Lambda function. Lambda internally polls for new messages from Kafka topics and then synchronously invokes the target Lambda function. Lambda reads the messages in batches and provides these to your function as an event payload. ","date":"October 5, 2023","img":"/blog/2023-10-05-real-time-streaming-with-kafka-and-flink-1/featured.png","lang":"en","langName":"English","largeImg":"/blog/2023-10-05-real-time-streaming-with-kafka-and-flink-1/featured_hua45c2e579ce8f6bc0318d15d62654514_138141_500x0_resize_box_3.png","permalink":"/blog/2023-10-05-real-time-streaming-with-kafka-and-flink-1/","series":[{"title":"Real Time Streaming With Kafka and Flink","url":"/series/real-time-streaming-with-kafka-and-flink/"}],"smallImg":"/blog/2023-10-05-real-time-streaming-with-kafka-and-flink-1/featured_hua45c2e579ce8f6bc0318d15d62654514_138141_180x0_resize_box_3.png","tags":[{"title":"AWS","url":"/tags/aws/"},{"title":"Amazon MSK","url":"/tags/amazon-msk/"},{"title":"Amazon MSK Connect","url":"/tags/amazon-msk-connect/"},{"title":"AWS Lambda","url":"/tags/aws-lambda/"},{"title":"Amazon S3","url":"/tags/amazon-s3/"},{"title":"Amazon DyanmoDB","url":"/tags/amazon-dyanmodb/"},{"title":"Amazon Athena","url":"/tags/amazon-athena/"},{"title":"AWS Glue","url":"/tags/aws-glue/"},{"title":"Amazon OpenSearch Service","url":"/tags/amazon-opensearch-service/"},{"title":"Apache Kafka","url":"/tags/apache-kafka/"},{"title":"Kafka Connect","url":"/tags/kafka-connect/"},{"title":"Apache Flink","url":"/tags/apache-flink/"},{"title":"PyFlink","url":"/tags/pyflink/"},{"title":"Apache Camel","url":"/tags/apache-camel/"},{"title":"OpenSearch","url":"/tags/opensearch/"},{"title":"Docker","url":"/tags/docker/"},{"title":"Docker Compose","url":"/tags/docker-compose/"},{"title":"Python","url":"/tags/python/"}],"timestamp":1696464000,"title":"Real Time Streaming With Kafka and Flink - Introduction"},{"categories":[{"title":"Data Streaming","url":"/categories/data-streaming/"}],"content":"This series aims to help those who are new to Apache Flink and Amazon Managed Service for Apache Flink by re-implementing a simple fraud detection application that is discussed in an AWS workshop titled AWS Kafka and DynamoDB for real time fraud detection. In part 1, I demonstrated how to develop the application locally, and the app will be deployed via Amazon Managed Service for Apache Flink in this post.\nPart 1 Local Development Part 2 Deployment via AWS Managed Flink (this post) [Update 2023-08-30] Amazon Kinesis Data Analytics is renamed into Amazon Managed Service for Apache Flink. In this post, Kinesis Data Analytics (KDA) and Amazon Managed Service for Apache Flink will be used interchangeably.\nArchitecture There are two Python applications that send transaction and flagged account records into the corresponding topics - the transaction app sends records indefinitely in a loop. Note that, as the Kafka cluster is deployed in private subnets, a VPN server is used to generate records from the developer machine. Both the topics are consumed by a Flink application, and it filters the transactions from the flagged accounts followed by sending them into an output topic of flagged transactions. Finally, the flagged transaction records are sent into a DynamoDB table by the Camel DynamoDB sink connector in order to serve real-time requests from an API.\nInfrastructure The infrastructure resources are created using Terraform. The source can be found in the GitHub repository of this post.\nPreparation Flink Application and Kafka Connector Packages The Flink application has multiple jar dependencies as the Kafka cluster is authenticated via IAM. Therefore, the jar files have to be combined into a single Uber jar file because KDA does not allow you to specify multiple pipeline jar files. The details about how to create the custom jar file can be found in this post. Also, the Camel DynamoDB sink connector needs to be packaged into a zip file, and it can be performed after downloading the binaries from the Maven repository.\nThe following script (build.sh) creates the Flink app and Kafka connector packages. For the former, it builds the Uber Jar file, followed by downloading the kafka-python package, creating a zip file that can be used to deploy the Flink app via KDA. Note that, although the Flink app does not need the kafka-python package, it is added in order to check if --pyFiles option works.\n1# build.sh 2#!/usr/bin/env bash 3shopt -s extglob 4 5PKG_ALL=\u0026#34;${PKG_ALL:-yes}\u0026#34; 6SCRIPT_DIR=\u0026#34;$(cd $(dirname \u0026#34;$0\u0026#34;); pwd)\u0026#34; 7 8#### Steps to package the flink app 9# remove contents under $SRC_PATH (except for uber-jar-for-pyflink) and kda-package.zip file 10SRC_PATH=$SCRIPT_DIR/package 11rm -rf $SRC_PATH/!(uber-jar-for-pyflink) kda-package.zip 12 13## Generate Uber Jar for PyFlink app for MSK cluster with IAM authN 14echo \u0026#34;generate Uber jar for PyFlink app...\u0026#34; 15mkdir $SRC_PATH/lib 16mvn clean install -f $SRC_PATH/uber-jar-for-pyflink/pom.xml \\ 17 \u0026amp;\u0026amp; mv $SRC_PATH/uber-jar-for-pyflink/target/pyflink-getting-started-1.0.0.jar $SRC_PATH/lib \\ 18 \u0026amp;\u0026amp; rm -rf $SRC_PATH/uber-jar-for-pyflink/target 19 20## Install pip packages 21echo \u0026#34;install and zip pip packages...\u0026#34; 22pip install -r requirements.txt --target $SRC_PATH/site_packages 23 24if [ $PKG_ALL == \u0026#34;yes\u0026#34; ]; then 25 ## Package pyflink app 26 echo \u0026#34;package pyflink app\u0026#34; 27 zip -r kda-package.zip processor.py package/lib package/site_packages 28fi 29 30#### Steps to create the sink connector 31CONN_PATH=$SCRIPT_DIR/connectors 32rm -rf $CONN_PATH \u0026amp;\u0026amp; mkdir $CONN_PATH 33 34## Download camel dynamodb sink connector 35echo \u0026#34;download camel dynamodb sink connector...\u0026#34; 36CONNECTOR_SRC_DOWNLOAD_URL=https://repo.maven.apache.org/maven2/org/apache/camel/kafkaconnector/camel-aws-ddb-sink-kafka-connector/3.20.3/camel-aws-ddb-sink-kafka-connector-3.20.3-package.tar.gz 37 38## decompress and zip contents to create custom plugin of msk connect later 39curl -o $CONN_PATH/camel-aws-ddb-sink-kafka-connector.tar.gz $CONNECTOR_SRC_DOWNLOAD_URL \\ 40 \u0026amp;\u0026amp; tar -xvzf $CONN_PATH/camel-aws-ddb-sink-kafka-connector.tar.gz -C $CONN_PATH \\ 41 \u0026amp;\u0026amp; cd $CONN_PATH/camel-aws-ddb-sink-kafka-connector \\ 42 \u0026amp;\u0026amp; zip -r camel-aws-ddb-sink-kafka-connector.zip . \\ 43 \u0026amp;\u0026amp; mv camel-aws-ddb-sink-kafka-connector.zip $CONN_PATH \\ 44 \u0026amp;\u0026amp; rm $CONN_PATH/camel-aws-ddb-sink-kafka-connector.tar.gz Once completed, we can obtain the following zip files.\nKafka sink connector - connectors/camel-aws-ddb-sink-kafka-connector.zip Flink application - kda-package.zip Flink application - processor.py Pipeline jar file - package/lib/pyflink-getting-started-1.0.0.jar kafka-python package - package/site_packages/kafka Kafka Management App The Kpow CE is used for ease of monitoring Kafka topics and related resources. The bootstrap server address, security configuration for IAM authentication and AWS credentials are added as environment variables. See this post for details about Kafka management apps.\n1# docker-compose.yml 2version: \u0026#34;3\u0026#34; 3 4services: 5 kpow: 6 image: factorhouse/kpow-ce:91.2.1 7 container_name: kpow 8 ports: 9 - \u0026#34;3000:3000\u0026#34; 10 networks: 11 - appnet 12 environment: 13 AWS_ACCESS_KEY_ID: $AWS_ACCESS_KEY_ID 14 AWS_SECRET_ACCESS_KEY: $AWS_SECRET_ACCESS_KEY 15 AWS_SESSION_TOKEN: $AWS_SESSION_TOKEN 16 # MSK cluster 17 BOOTSTRAP: $BOOTSTRAP_SERVERS 18 SECURITY_PROTOCOL: SASL_SSL 19 SASL_MECHANISM: AWS_MSK_IAM 20 SASL_CLIENT_CALLBACK_HANDLER_CLASS: software.amazon.msk.auth.iam.IAMClientCallbackHandler 21 SASL_JAAS_CONFIG: software.amazon.msk.auth.iam.IAMLoginModule required; 22 # MSK connect 23 CONNECT_AWS_REGION: $AWS_DEFAULT_REGION 24 25networks: 26 appnet: 27 name: app-network VPC and VPN A VPC with 3 public and private subnets is created using the AWS VPC Terraform module (infra/vpc.tf). Also, a SoftEther VPN server is deployed in order to access the resources in the private subnets from the developer machine (infra/vpn.tf). It is particularly useful to monitor and manage the MSK cluster and Kafka topic locally. The details about how to configure the VPN server can be found in this post.\nMSK Cluster An MSK cluster with 2 brokers is created. The broker nodes are deployed with the kafka.m5.large instance type in private subnets and IAM authentication is used for the client authentication method. Finally, additional server configurations are added such as enabling auto creation of topics and topic deletion.\n1# infra/variable.tf 2locals { 3 ... 4 msk = { 5 version = \u0026#34;2.8.1\u0026#34; 6 instance_size = \u0026#34;kafka.m5.large\u0026#34; 7 ebs_volume_size = 20 8 log_retention_ms = 604800000 # 7 days 9 number_of_broker_nodes = 2 10 num_partitions = 2 11 default_replication_factor = 2 12 } 13 ... 14} 15# infra/msk.tf 16resource \u0026#34;aws_msk_cluster\u0026#34; \u0026#34;msk_data_cluster\u0026#34; { 17 cluster_name = \u0026#34;${local.name}-msk-cluster\u0026#34; 18 kafka_version = local.msk.version 19 number_of_broker_nodes = local.msk.number_of_broker_nodes 20 configuration_info { 21 arn = aws_msk_configuration.msk_config.arn 22 revision = aws_msk_configuration.msk_config.latest_revision 23 } 24 25 broker_node_group_info { 26 instance_type = local.msk.instance_size 27 client_subnets = slice(module.vpc.private_subnets, 0, local.msk.number_of_broker_nodes) 28 security_groups = [aws_security_group.msk.id] 29 storage_info { 30 ebs_storage_info { 31 volume_size = local.msk.ebs_volume_size 32 } 33 } 34 } 35 36 client_authentication { 37 sasl { 38 iam = true 39 } 40 } 41 42 logging_info { 43 broker_logs { 44 cloudwatch_logs { 45 enabled = true 46 log_group = aws_cloudwatch_log_group.msk_cluster_lg.name 47 } 48 s3 { 49 enabled = true 50 bucket = aws_s3_bucket.default_bucket.id 51 prefix = \u0026#34;logs/msk/cluster/\u0026#34; 52 } 53 } 54 } 55 56 tags = local.tags 57 58 depends_on = [aws_msk_configuration.msk_config] 59} 60 61resource \u0026#34;aws_msk_configuration\u0026#34; \u0026#34;msk_config\u0026#34; { 62 name = \u0026#34;${local.name}-msk-configuration\u0026#34; 63 64 kafka_versions = [local.msk.version] 65 66 server_properties = \u0026lt;\u0026lt;PROPERTIES 67 auto.create.topics.enable = true 68 delete.topic.enable = true 69 log.retention.ms = ${local.msk.log_retention_ms} 70 num.partitions = ${local.msk.num_partitions} 71 default.replication.factor = ${local.msk.default_replication_factor} 72 PROPERTIES 73} Security Group The security group of the MSK cluster allows all inbound traffic from itself and all outbound traffic into all IP addresses. The Kafka connectors will use the same security group and the former is necessary. Both the rules are configured too generously, however, we can limit the protocol and port ranges in production. Also, the security group has an additional inbound rule that permits it to connect on port 9098 from the security group of the Flink application.\n1resource \u0026#34;aws_security_group\u0026#34; \u0026#34;msk\u0026#34; { 2 name = \u0026#34;${local.name}-msk-sg\u0026#34; 3 vpc_id = module.vpc.vpc_id 4 5 lifecycle { 6 create_before_destroy = true 7 } 8 9 tags = local.tags 10} 11 12resource \u0026#34;aws_security_group_rule\u0026#34; \u0026#34;msk_self_inbound_all\u0026#34; { 13 type = \u0026#34;ingress\u0026#34; 14 description = \u0026#34;Allow ingress from itself - required for MSK Connect\u0026#34; 15 security_group_id = aws_security_group.msk.id 16 protocol = \u0026#34;-1\u0026#34; 17 from_port = \u0026#34;0\u0026#34; 18 to_port = \u0026#34;0\u0026#34; 19 source_security_group_id = aws_security_group.msk.id 20} 21 22resource \u0026#34;aws_security_group_rule\u0026#34; \u0026#34;msk_self_outbound_all\u0026#34; { 23 type = \u0026#34;egress\u0026#34; 24 description = \u0026#34;Allow outbound all\u0026#34; 25 security_group_id = aws_security_group.msk.id 26 protocol = \u0026#34;-1\u0026#34; 27 from_port = \u0026#34;0\u0026#34; 28 to_port = \u0026#34;0\u0026#34; 29 cidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;] 30} 31 32resource \u0026#34;aws_security_group_rule\u0026#34; \u0026#34;msk_kda_inbound\u0026#34; { 33 type = \u0026#34;ingress\u0026#34; 34 description = \u0026#34;Allow KDA access\u0026#34; 35 security_group_id = aws_security_group.msk.id 36 protocol = \u0026#34;tcp\u0026#34; 37 from_port = 9098 38 to_port = 9098 39 source_security_group_id = aws_security_group.kda_sg.id 40} DynamoDB Table The destination table is configured to have a composite primary key where transaction_id and transaction_date are the hash and range key respectively. It also has a global secondary index (GSI) where account_id and transaction_date constitute the primary key. The GSI is to facilitate querying by account id.\n1# infra/ddb.tf 2resource \u0026#34;aws_dynamodb_table\u0026#34; \u0026#34;transactions_table\u0026#34; { 3 name = \u0026#34;${local.name}-flagged-transactions\u0026#34; 4 billing_mode = \u0026#34;PROVISIONED\u0026#34; 5 read_capacity = 2 6 write_capacity = 2 7 hash_key = \u0026#34;transaction_id\u0026#34; 8 range_key = \u0026#34;transaction_date\u0026#34; 9 10 attribute { 11 name = \u0026#34;transaction_id\u0026#34; 12 type = \u0026#34;S\u0026#34; 13 } 14 15 attribute { 16 name = \u0026#34;account_id\u0026#34; 17 type = \u0026#34;N\u0026#34; 18 } 19 20 attribute { 21 name = \u0026#34;transaction_date\u0026#34; 22 type = \u0026#34;S\u0026#34; 23 } 24 25 global_secondary_index { 26 name = \u0026#34;account\u0026#34; 27 hash_key = \u0026#34;account_id\u0026#34; 28 range_key = \u0026#34;transaction_date\u0026#34; 29 write_capacity = 2 30 read_capacity = 2 31 projection_type = \u0026#34;ALL\u0026#34; 32 } 33 34 tags = local.tags 35} Flink Application The runtime environment and service execution role are required to create a Flink app. The latest supported Flink version (1.15.2) is specified for the former and an IAM role is created for the latter - it\u0026rsquo;ll be discussed more in a later section. Furthermore, we need to specify more configurations that are related to the Flink application and CloudWatch logging, and they will be covered below in detail as well.\n1# infra/variable.tf 2locals { 3 ... 4 kda = { 5 runtime_env = \u0026#34;FLINK-1_15\u0026#34; 6 package_name = \u0026#34;kda-package.zip\u0026#34; 7 consumer_0 = { 8 table_name = \u0026#34;flagged_accounts\u0026#34; 9 topic_name = \u0026#34;flagged-accounts\u0026#34; 10 } 11 consumer_1 = { 12 table_name = \u0026#34;transactions\u0026#34; 13 topic_name = \u0026#34;transactions\u0026#34; 14 } 15 producer_0 = { 16 table_name = \u0026#34;flagged_transactions\u0026#34; 17 topic_name = \u0026#34;flagged-transactions\u0026#34; 18 } 19 } 20 ... 21} 22 23resource \u0026#34;aws_kinesisanalyticsv2_application\u0026#34; \u0026#34;kda_app\u0026#34; { 24 name = \u0026#34;${local.name}-kda-app\u0026#34; 25 runtime_environment = local.kda.runtime_env 26 service_execution_role = aws_iam_role.kda_app_role.arn 27 28 ... 29} Application Configuration In the application configuration section, we can specify details of the application code, VPC, environment properties, and application itself.\nApplication Code Configuration The application package (kda-package.zip) is uploaded into the default S3 bucket using the aws_s3_object Terraform resource. Then it can be used as the code content by specifying the bucket and key names.\n1# infra/kda.tf 2resource \u0026#34;aws_kinesisanalyticsv2_application\u0026#34; \u0026#34;kda_app\u0026#34; { 3 4 ... 5 6 application_configuration { 7 application_code_configuration { 8 code_content { 9 s3_content_location { 10 bucket_arn = aws_s3_bucket.default_bucket.arn 11 file_key = aws_s3_object.kda_package[0].key 12 } 13 } 14 15 code_content_type = \u0026#34;ZIPFILE\u0026#34; 16 } 17 18 ... 19 20 } 21 22 ... 23 24} 25 26... 27 28 29resource \u0026#34;aws_s3_object\u0026#34; \u0026#34;kda_package\u0026#34; { 30 bucket = aws_s3_bucket.default_bucket.id 31 key = \u0026#34;packages/${local.kda.package_name}\u0026#34; 32 source = \u0026#34;${dirname(path.cwd)}/${local.kda.package_name}\u0026#34; 33 34 etag = filemd5(\u0026#34;${dirname(path.cwd)}/${local.kda.package_name}\u0026#34;) 35} VPC Configuration The app can be deployed in the private subnets as it doesn\u0026rsquo;t need to be connected from outside. Note that an outbound rule that permits connection on port 9098 is created in its security group because it should be able to access the Kafka brokers.\n1# infra/kda.tf 2resource \u0026#34;aws_kinesisanalyticsv2_application\u0026#34; \u0026#34;kda_app\u0026#34; { 3 4 ... 5 6 application_configuration { 7 8 ... 9 10 vpc_configuration { 11 security_group_ids = [aws_security_group.kda_sg.id] 12 subnet_ids = module.vpc.private_subnets 13 } 14 15 ... 16 17 } 18 19 ... 20 21} 22 23... 24 25resource \u0026#34;aws_security_group\u0026#34; \u0026#34;kda_sg\u0026#34; { 26 name = \u0026#34;${local.name}-kda-sg\u0026#34; 27 vpc_id = module.vpc.vpc_id 28 29 egress { 30 from_port = 9098 31 to_port = 9098 32 protocol = \u0026#34;tcp\u0026#34; 33 cidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;] 34 } 35 36 lifecycle { 37 create_before_destroy = true 38 } 39 40 tags = local.tags 41} Environment Properties In environment properties, we first add Flink CLI options in the kinesis.analytics.flink.run.options group. The values of the Pyflink app (python), pipeline jar (jarfile) and 3rd-party python package location (pyFiles) should match those in the application package (kda-package.zip). The other property groups are related to the Kafka source/sink table options, and they will be read by the application.\n1# infra/kda.tf 2resource \u0026#34;aws_kinesisanalyticsv2_application\u0026#34; \u0026#34;kda_app\u0026#34; { 3 4 ... 5 6 application_configuration { 7 8 ... 9 10 environment_properties { 11 property_group { 12 property_group_id = \u0026#34;kinesis.analytics.flink.run.options\u0026#34; 13 14 property_map = { 15 python = \u0026#34;processor.py\u0026#34; 16 jarfile = \u0026#34;package/lib/pyflink-getting-started-1.0.0.jar\u0026#34; 17 pyFiles = \u0026#34;package/site_packages/\u0026#34; 18 } 19 } 20 21 property_group { 22 property_group_id = \u0026#34;consumer.config.0\u0026#34; 23 24 property_map = { 25 \u0026#34;table.name\u0026#34; = local.kda.consumer_0.table_name 26 \u0026#34;topic.name\u0026#34; = local.kda.consumer_0.topic_name 27 \u0026#34;bootstrap.servers\u0026#34; = aws_msk_cluster.msk_data_cluster.bootstrap_brokers_sasl_iam 28 \u0026#34;startup.mode\u0026#34; = \u0026#34;earliest-offset\u0026#34; 29 } 30 } 31 32 property_group { 33 property_group_id = \u0026#34;consumer.config.1\u0026#34; 34 35 property_map = { 36 \u0026#34;table.name\u0026#34; = local.kda.consumer_1.table_name 37 \u0026#34;topic.name\u0026#34; = local.kda.consumer_1.topic_name 38 \u0026#34;bootstrap.servers\u0026#34; = aws_msk_cluster.msk_data_cluster.bootstrap_brokers_sasl_iam 39 \u0026#34;startup.mode\u0026#34; = \u0026#34;earliest-offset\u0026#34; 40 } 41 } 42 43 property_group { 44 property_group_id = \u0026#34;producer.config.0\u0026#34; 45 46 property_map = { 47 \u0026#34;table.name\u0026#34; = local.kda.producer_0.table_name 48 \u0026#34;topic.name\u0026#34; = local.kda.producer_0.topic_name 49 \u0026#34;bootstrap.servers\u0026#34; = aws_msk_cluster.msk_data_cluster.bootstrap_brokers_sasl_iam 50 } 51 } 52 } 53 54 ... 55 56 } 57 58 ... 59 60} Flink Application Configuration The Flink application configurations consist of the following.\nCheckpoints - Checkpoints are backups of application state that Managed Service for Apache Flink automatically creates periodically and uses to restore from faults. By default, the following values are configured. CheckpointingEnabled: true CheckpointInterval: 60000 MinPauseBetweenCheckpoints: 5000 Monitoring - The metrics level determines which metrics are created to CloudWatch - see this page for details. The supported values are APPLICATION, OPERATOR, PARALLELISM, and TASK. Here APPLICATION is selected as the metrics level value. Parallelism - We can configure the parallel execution of tasks and the allocation of resources to implement scaling. The parallelism indicates the initial number of parallel tasks that an application can perform while the parallelism_per_kpu is the number of parallel tasks that an application can perform per Kinesis Processing Unit (KPU). The application parallelism can be updated by enabling auto-scaling. 1# infra/kda.tf 2resource \u0026#34;aws_kinesisanalyticsv2_application\u0026#34; \u0026#34;kda_app\u0026#34; { 3 4 ... 5 6 application_configuration { 7 8 ... 9 10 flink_application_configuration { 11 checkpoint_configuration { 12 configuration_type = \u0026#34;DEFAULT\u0026#34; 13 } 14 15 monitoring_configuration { 16 configuration_type = \u0026#34;CUSTOM\u0026#34; 17 log_level = \u0026#34;INFO\u0026#34; 18 metrics_level = \u0026#34;APPLICATION\u0026#34; 19 } 20 21 parallelism_configuration { 22 configuration_type = \u0026#34;CUSTOM\u0026#34; 23 auto_scaling_enabled = true 24 parallelism = 1 25 parallelism_per_kpu = 1 26 } 27 } 28 } 29 30 ... 31 32} Cloudwatch Logging Options We can add a CloudWatch log stream ARN to the CloudWatch logging options. Note that, when I missed it at first, I saw a CloudWatch log group and log stream are created automatically, but logging was not enabled. It was only when I specified a custom log stream ARN that logging was enabled and log messages were ingested.\n1# infra/kda.tf 2resource \u0026#34;aws_kinesisanalyticsv2_application\u0026#34; \u0026#34;kda_app\u0026#34; { 3 4 ... 5 6 cloudwatch_logging_options { 7 log_stream_arn = aws_cloudwatch_log_stream.kda_ls.arn 8 } 9 10 ... 11 12} 13 14... 15 16resource \u0026#34;aws_cloudwatch_log_group\u0026#34; \u0026#34;kda_lg\u0026#34; { 17 name = \u0026#34;/${local.name}-kda-log-group\u0026#34; 18} 19 20resource \u0026#34;aws_cloudwatch_log_stream\u0026#34; \u0026#34;kda_ls\u0026#34; { 21 name = \u0026#34;${local.name}-kda-log-stream\u0026#34; 22 23 log_group_name = aws_cloudwatch_log_group.kda_lg.name 24} IAM Role The service execution role has the following permissions.\nFull access to CloudWatch, CloudWatch Log and Amazon Kinesis Data Analytics. It is given by AWS managed policies for logging, metrics generation etc. However, it is by no means recommended and should be updated according to the least privilege principle for production. 3 inline policies for connecting to the MSK cluster (kda-msk-access) in private subnets (kda-vpc-access) as well as giving access to the application package in S3 (kda-s3-access). 1# infra/kda.tf 2resource \u0026#34;aws_iam_role\u0026#34; \u0026#34;kda_app_role\u0026#34; { 3 name = \u0026#34;${local.name}-kda-role\u0026#34; 4 5 assume_role_policy = jsonencode({ 6 Version = \u0026#34;2012-10-17\u0026#34; 7 Statement = [ 8 { 9 Action = \u0026#34;sts:AssumeRole\u0026#34; 10 Effect = \u0026#34;Allow\u0026#34; 11 Sid = \u0026#34;\u0026#34; 12 Principal = { 13 Service = \u0026#34;kinesisanalytics.amazonaws.com\u0026#34; 14 } 15 }, 16 ] 17 }) 18 19 managed_policy_arns = [ 20 \u0026#34;arn:aws:iam::aws:policy/CloudWatchFullAccess\u0026#34;, 21 \u0026#34;arn:aws:iam::aws:policy/CloudWatchLogsFullAccess\u0026#34;, 22 \u0026#34;arn:aws:iam::aws:policy/AmazonKinesisAnalyticsFullAccess\u0026#34; 23 ] 24 25 inline_policy { 26 name = \u0026#34;kda-msk-access\u0026#34; 27 28 policy = jsonencode({ 29 Version = \u0026#34;2012-10-17\u0026#34; 30 Statement = [ 31 { 32 Sid = \u0026#34;PermissionOnCluster\u0026#34; 33 Action = [ 34 \u0026#34;kafka-cluster:Connect\u0026#34;, 35 \u0026#34;kafka-cluster:AlterCluster\u0026#34;, 36 \u0026#34;kafka-cluster:DescribeCluster\u0026#34; 37 ] 38 Effect = \u0026#34;Allow\u0026#34; 39 Resource = \u0026#34;arn:aws:kafka:${local.region}:${data.aws_caller_identity.current.account_id}:cluster/${local.name}-msk-cluster/*\u0026#34; 40 }, 41 { 42 Sid = \u0026#34;PermissionOnTopics\u0026#34; 43 Action = [ 44 \u0026#34;kafka-cluster:*Topic*\u0026#34;, 45 \u0026#34;kafka-cluster:WriteData\u0026#34;, 46 \u0026#34;kafka-cluster:ReadData\u0026#34; 47 ] 48 Effect = \u0026#34;Allow\u0026#34; 49 Resource = \u0026#34;arn:aws:kafka:${local.region}:${data.aws_caller_identity.current.account_id}:topic/${local.name}-msk-cluster/*\u0026#34; 50 }, 51 { 52 Sid = \u0026#34;PermissionOnGroups\u0026#34; 53 Action = [ 54 \u0026#34;kafka-cluster:AlterGroup\u0026#34;, 55 \u0026#34;kafka-cluster:DescribeGroup\u0026#34; 56 ] 57 Effect = \u0026#34;Allow\u0026#34; 58 Resource = \u0026#34;arn:aws:kafka:${local.region}:${data.aws_caller_identity.current.account_id}:group/${local.name}-msk-cluster/*\u0026#34; 59 } 60 ] 61 }) 62 } 63 64 inline_policy { 65 name = \u0026#34;kda-vpc-access\u0026#34; 66 # https://docs.aws.amazon.com/kinesisanalytics/latest/java/vpc-permissions.html 67 68 policy = jsonencode({ 69 Version = \u0026#34;2012-10-17\u0026#34; 70 Statement = [ 71 { 72 Sid = \u0026#34;VPCReadOnlyPermissions\u0026#34; 73 Action = [ 74 \u0026#34;ec2:DescribeVpcs\u0026#34;, 75 \u0026#34;ec2:DescribeSubnets\u0026#34;, 76 \u0026#34;ec2:DescribeSecurityGroups\u0026#34;, 77 \u0026#34;ec2:DescribeDhcpOptions\u0026#34; 78 ] 79 Effect = \u0026#34;Allow\u0026#34; 80 Resource = \u0026#34;*\u0026#34; 81 }, 82 { 83 Sid = \u0026#34;ENIReadWritePermissions\u0026#34; 84 Action = [ 85 \u0026#34;ec2:CreateNetworkInterface\u0026#34;, 86 \u0026#34;ec2:CreateNetworkInterfacePermission\u0026#34;, 87 \u0026#34;ec2:DescribeNetworkInterfaces\u0026#34;, 88 \u0026#34;ec2:DeleteNetworkInterface\u0026#34; 89 ] 90 Effect = \u0026#34;Allow\u0026#34; 91 Resource = \u0026#34;*\u0026#34; 92 } 93 94 ] 95 }) 96 } 97 98 inline_policy { 99 name = \u0026#34;kda-s3-access\u0026#34; 100 101 policy = jsonencode({ 102 Version = \u0026#34;2012-10-17\u0026#34; 103 Statement = [ 104 { 105 Sid = \u0026#34;ListObjectsInBucket\u0026#34; 106 Action = [\u0026#34;s3:ListBucket\u0026#34;] 107 Effect = \u0026#34;Allow\u0026#34; 108 Resource = \u0026#34;arn:aws:s3:::${aws_s3_bucket.default_bucket.id}\u0026#34; 109 }, 110 { 111 Sid = \u0026#34;AllObjectActions\u0026#34; 112 Action = [\u0026#34;s3:*Object\u0026#34;] 113 Effect = \u0026#34;Allow\u0026#34; 114 Resource = \u0026#34;arn:aws:s3:::${aws_s3_bucket.default_bucket.id}/*\u0026#34; 115 }, 116 ] 117 }) 118 } 119 120 tags = local.tags 121} Once deployed, we can see the application on AWS console, and it stays in the ready status.\nCamel DynamoDB Sink Connector The connector is configured to write messages from the flagged-transactions topic into the DynamoDB table created earlier. It requires to specify the table name, AWS region, operation, write capacity and whether to use the default credential provider - see the documentation for details. See this post for details about how to set up the sink connector.\n1# infra/msk-connect.tf 2resource \u0026#34;aws_mskconnect_connector\u0026#34; \u0026#34;camel_ddb_sink\u0026#34; { 3 name = \u0026#34;${local.name}-transactions-sink\u0026#34; 4 5 kafkaconnect_version = \u0026#34;2.7.1\u0026#34; 6 7 capacity { 8 provisioned_capacity { 9 mcu_count = 1 10 worker_count = 1 11 } 12 } 13 14 connector_configuration = { 15 # connector configuration 16 \u0026#34;connector.class\u0026#34; = \u0026#34;org.apache.camel.kafkaconnector.awsddbsink.CamelAwsddbsinkSinkConnector\u0026#34;, 17 \u0026#34;tasks.max\u0026#34; = \u0026#34;2\u0026#34;, 18 \u0026#34;key.converter\u0026#34; = \u0026#34;org.apache.kafka.connect.json.JsonConverter\u0026#34;, 19 \u0026#34;key.converter.schemas.enable\u0026#34; = false, 20 \u0026#34;value.converter\u0026#34; = \u0026#34;org.apache.kafka.connect.json.JsonConverter\u0026#34;, 21 \u0026#34;value.converter.schemas.enable\u0026#34; = false, 22 # camel ddb sink configuration 23 \u0026#34;topics\u0026#34; = local.kda.producer_0.topic_name, 24 \u0026#34;camel.kamelet.aws-ddb-sink.table\u0026#34; = aws_dynamodb_table.transactions_table.id, 25 \u0026#34;camel.kamelet.aws-ddb-sink.region\u0026#34; = local.region, 26 \u0026#34;camel.kamelet.aws-ddb-sink.operation\u0026#34; = \u0026#34;PutItem\u0026#34;, 27 \u0026#34;camel.kamelet.aws-ddb-sink.writeCapacity\u0026#34; = 1, 28 \u0026#34;camel.kamelet.aws-ddb-sink.useDefaultCredentialsProvider\u0026#34; = true, 29 \u0026#34;camel.sink.unmarshal\u0026#34; = \u0026#34;jackson\u0026#34; 30 } 31 32 kafka_cluster { 33 apache_kafka_cluster { 34 bootstrap_servers = aws_msk_cluster.msk_data_cluster.bootstrap_brokers_sasl_iam 35 36 vpc { 37 security_groups = [aws_security_group.msk.id] 38 subnets = module.vpc.private_subnets 39 } 40 } 41 } 42 43 kafka_cluster_client_authentication { 44 authentication_type = \u0026#34;IAM\u0026#34; 45 } 46 47 kafka_cluster_encryption_in_transit { 48 encryption_type = \u0026#34;TLS\u0026#34; 49 } 50 51 plugin { 52 custom_plugin { 53 arn = aws_mskconnect_custom_plugin.camel_ddb_sink.arn 54 revision = aws_mskconnect_custom_plugin.camel_ddb_sink.latest_revision 55 } 56 } 57 58 log_delivery { 59 worker_log_delivery { 60 cloudwatch_logs { 61 enabled = true 62 log_group = aws_cloudwatch_log_group.camel_ddb_sink.name 63 } 64 s3 { 65 enabled = true 66 bucket = aws_s3_bucket.default_bucket.id 67 prefix = \u0026#34;logs/msk/connect/camel-ddb-sink\u0026#34; 68 } 69 } 70 } 71 72 service_execution_role_arn = aws_iam_role.kafka_connector_role.arn 73} 74 75resource \u0026#34;aws_mskconnect_custom_plugin\u0026#34; \u0026#34;camel_ddb_sink\u0026#34; { 76 name = \u0026#34;${local.name}-camel-ddb-sink\u0026#34; 77 content_type = \u0026#34;ZIP\u0026#34; 78 79 location { 80 s3 { 81 bucket_arn = aws_s3_bucket.default_bucket.arn 82 file_key = aws_s3_object.camel_ddb_sink.key 83 } 84 } 85} 86 87resource \u0026#34;aws_s3_object\u0026#34; \u0026#34;camel_ddb_sink\u0026#34; { 88 bucket = aws_s3_bucket.default_bucket.id 89 key = \u0026#34;plugins/${local.msk_connect.package_name}\u0026#34; 90 source = \u0026#34;${dirname(path.cwd)}/connectors/${local.msk_connect.package_name}\u0026#34; 91 92 etag = filemd5(\u0026#34;${dirname(path.cwd)}/connectors/${local.msk_connect.package_name}\u0026#34;) 93} 94 95resource \u0026#34;aws_cloudwatch_log_group\u0026#34; \u0026#34;camel_ddb_sink\u0026#34; { 96 name = \u0026#34;/msk/connect/camel-ddb-sink\u0026#34; 97 98 retention_in_days = 1 99 100 tags = local.tags 101} The sink connector can be checked on AWS Console as shown below.\nRun Application We first need to create records in the source Kafka topics. It is performed by executing the data generator app (producer.py). See part 1 for details about the generator app and how to execute it. Note that we should connect to the VPN server in order to create records from the developer machine.\nOnce executed, we can check the source topics are created and messages are ingested.\nMonitoring on Flink Web UI We can run the Flink application on AWS console with the Run without snapshot option as we haven\u0026rsquo;t enabled snapshots.\nOnce the app is running, we can monitor it on the Flink Web UI available on AWS Console.\nIn the Overview section, it shows the available task slots, running jobs and completed jobs.\nWe can inspect an individual job in the Jobs menu. It shows key details about a job execution in Overview, Exceptions, TimeLine, Checkpoints and Configuration tabs.\nCloudWatch Logging The application log messages can be checked in the CloudWatch Console, and it gives additional capability to debug the application.\nApplication Output We can see details of all the topics in Kpow. The output topic (flagged-transactions) is created by the Flink application, and fraudulent transaction records are created in it.\nFinally, we can check the output records on the DynamoDB table items view. All account IDs end with odd numbers, and it indicates they are from flagged accounts.\nSummary This series aims to help those who are new to Apache Flink and Amazon Managed Service for Apache Flink by re-implementing a simple fraud detection application that is discussed in an AWS workshop titled AWS Kafka and DynamoDB for real time fraud detection. In part 1, I demonstrated how to develop the application locally, and the app was deployed via Amazon Managed Service for Apache Flink in this post.\n","date":"September 14, 2023","img":"/blog/2023-09-14-fraud-detection-part-2/featured.png","lang":"en","langName":"English","largeImg":"/blog/2023-09-14-fraud-detection-part-2/featured_hu6e12c35fe727128749446c930c7ac3f6_66221_500x0_resize_box_3.png","permalink":"/blog/2023-09-14-fraud-detection-part-2/","series":[{"title":"Kafka, Flink and DynamoDB for Real Time Fraud Detection","url":"/series/kafka-flink-and-dynamodb-for-real-time-fraud-detection/"}],"smallImg":"/blog/2023-09-14-fraud-detection-part-2/featured_hu6e12c35fe727128749446c930c7ac3f6_66221_180x0_resize_box_3.png","tags":[{"title":"Apache Flink","url":"/tags/apache-flink/"},{"title":"PyFlink","url":"/tags/pyflink/"},{"title":"Apache Kafka","url":"/tags/apache-kafka/"},{"title":"Kafka Connect","url":"/tags/kafka-connect/"},{"title":"Amazon DynamoDB","url":"/tags/amazon-dynamodb/"},{"title":"Amazon MSK","url":"/tags/amazon-msk/"},{"title":"Amazon MSK Connect","url":"/tags/amazon-msk-connect/"},{"title":"Amazon Managed Service for Apache Flink","url":"/tags/amazon-managed-service-for-apache-flink/"},{"title":"Amazon Managed Flink","url":"/tags/amazon-managed-flink/"},{"title":"Python","url":"/tags/python/"},{"title":"Fraud Detection","url":"/tags/fraud-detection/"}],"timestamp":1694649600,"title":"Kafka, Flink and DynamoDB for Real Time Fraud Detection - Part 2 Deployment via AWS Managed Flink"},{"categories":[{"title":"Data Streaming","url":"/categories/data-streaming/"}],"content":"In this series of posts, we discuss a Flink (Pyflink) application that reads/writes from/to Kafka topics. In the previous posts, I demonstrated a Pyflink app that targets a local Kafka cluster as well as a Kafka cluster on Amazon MSK. The app was executed in a virtual environment as well as in a local Flink cluster for improved monitoring. In this post, the app will be deployed via Amazon Managed Service for Apache Flink, which is the easiest option to run Flink applications on AWS.\nPart 1 Local Flink and Local Kafka Part 2 Local Flink and MSK Part 3 AWS Managed Flink and MSK (this post) [Update 2023-08-30] Amazon Kinesis Data Analytics is renamed into Amazon Managed Service for Apache Flink. In this post, Kinesis Data Analytics (KDA) and Amazon Managed Service for Apache Flink will be used interchangeably.\nArchitecture The Python source data generator sends random stock price records into a Kafka topic. The messages in the source topic are consumed by a Flink application, and it just writes those messages into a different sink topic. As the Kafka cluster is deployed in private subnets, a VPN server is used to generate records from the developer machine. This is the simplest application of the Pyflink getting started guide from AWS, and you may try other examples if interested.\nInfrastructure A Kafka cluster is created on Amazon MSK using Terraform, and the cluster is secured by IAM access control. Unlike the previous posts, the Pyflink app is deployed via Kinesis Data Analytics (KDA). The source can be found in the GitHub repository of this post.\nPreparation Application Package As discussed in part 2, the app has multiple jar dependencies, and they have to be combined into a single Uber jar file. This is because KDA does not allow you to specify multiple pipeline jar files. The details about how to create the custom jar file can be found in part 2.\nThe following script (build.sh) builds to create the Uber Jar file for this post, followed by downloading the kafka-python package and creating a zip file that can be used to deploy the Flink app via KDA. Although the Flink app does not need the kafka-python package, it is added in order to check if --pyFiles option works when deploying the app via KDA. The zip package file will be used for KDA deployment in this post.\n1# build.sh 2#!/usr/bin/env bash 3SCRIPT_DIR=\u0026#34;$(cd $(dirname \u0026#34;$0\u0026#34;); pwd)\u0026#34; 4SRC_PATH=$SCRIPT_DIR/package 5 6# remove contents under $SRC_PATH (except for uber-jar-for-pyflink) and kda-package.zip file 7shopt -s extglob 8rm -rf $SRC_PATH/!(uber-jar-for-pyflink) kda-package.zip 9 10## Generate Uber Jar for PyFlink app for MSK cluster with IAM authN 11echo \u0026#34;generate Uber jar for PyFlink app...\u0026#34; 12mkdir $SRC_PATH/lib 13mvn clean install -f $SRC_PATH/uber-jar-for-pyflink/pom.xml \\ 14 \u0026amp;\u0026amp; mv $SRC_PATH/uber-jar-for-pyflink/target/pyflink-getting-started-1.0.0.jar $SRC_PATH/lib \\ 15 \u0026amp;\u0026amp; rm -rf $SRC_PATH/uber-jar-for-pyflink/target 16 17## Install pip packages 18echo \u0026#34;install and zip pip packages...\u0026#34; 19pip install -r requirements.txt --target $SRC_PATH/site_packages 20 21## Package pyflink app 22echo \u0026#34;package pyflink app\u0026#34; 23zip -r kda-package.zip processor.py package/lib package/site_packages Once completed, we can check the following contents are included in the application package file.\nFlink application - processor.py Pipeline jar file - package/lib/pyflink-getting-started-1.0.0.jar kafka-python package - package/site_packages/kafka Kafka Management App The Kpow CE is used for ease of monitoring Kafka topics and related resources. The bootstrap server address, security configuration for IAM authentication and AWS credentials are added as environment variables. See this post for details about Kafka management apps.\n1# compose-ui.yml 2version: \u0026#34;3\u0026#34; 3 4services: 5 kpow: 6 image: factorhouse/kpow-ce:91.2.1 7 container_name: kpow 8 ports: 9 - \u0026#34;3000:3000\u0026#34; 10 networks: 11 - appnet 12 environment: 13 AWS_ACCESS_KEY_ID: $AWS_ACCESS_KEY_ID 14 AWS_SECRET_ACCESS_KEY: $AWS_SECRET_ACCESS_KEY 15 AWS_SESSION_TOKEN: $AWS_SESSION_TOKEN 16 # kafka cluster 17 BOOTSTRAP: $BOOTSTRAP_SERVERS 18 SECURITY_PROTOCOL: SASL_SSL 19 SASL_MECHANISM: AWS_MSK_IAM 20 SASL_CLIENT_CALLBACK_HANDLER_CLASS: software.amazon.msk.auth.iam.IAMClientCallbackHandler 21 SASL_JAAS_CONFIG: software.amazon.msk.auth.iam.IAMLoginModule required; 22 23networks: 24 appnet: 25 name: app-network VPC and VPN A VPC with 3 public and private subnets is created using the AWS VPC Terraform module (infra/vpc.tf). Also, a SoftEther VPN server is deployed in order to access the resources in the private subnets from the developer machine (infra/vpn.tf). It is particularly useful to monitor and manage the MSK cluster and Kafka topic locally. The details about how to configure the VPN server can be found in an earlier post.\nMSK Cluster An MSK cluster with 2 brokers is created. The broker nodes are deployed with the kafka.m5.large instance type in private subnets and IAM authentication is used for the client authentication method. Finally, additional server configurations are added such as enabling auto creation of topics and topic deletion. Note that the Flink application needs to have access to the Kafka brokers, and it is allowed by adding an inbound connection from the KDA app into the brokers on port 9098.\n1# infra/variable.tf 2locals { 3 ... 4 msk = { 5 version = \u0026#34;2.8.1\u0026#34; 6 instance_size = \u0026#34;kafka.m5.large\u0026#34; 7 ebs_volume_size = 20 8 log_retention_ms = 604800000 # 7 days 9 num_partitions = 2 10 default_replication_factor = 2 11 } 12 ... 13} 14# infra/msk.tf 15resource \u0026#34;aws_msk_cluster\u0026#34; \u0026#34;msk_data_cluster\u0026#34; { 16 cluster_name = \u0026#34;${local.name}-msk-cluster\u0026#34; 17 kafka_version = local.msk.version 18 number_of_broker_nodes = local.msk.number_of_broker_nodes 19 configuration_info { 20 arn = aws_msk_configuration.msk_config.arn 21 revision = aws_msk_configuration.msk_config.latest_revision 22 } 23 24 broker_node_group_info { 25 instance_type = local.msk.instance_size 26 client_subnets = slice(module.vpc.private_subnets, 0, local.msk.number_of_broker_nodes) 27 security_groups = [aws_security_group.msk.id] 28 storage_info { 29 ebs_storage_info { 30 volume_size = local.msk.ebs_volume_size 31 } 32 } 33 } 34 35 client_authentication { 36 sasl { 37 iam = true 38 } 39 } 40 41 logging_info { 42 broker_logs { 43 cloudwatch_logs { 44 enabled = true 45 log_group = aws_cloudwatch_log_group.msk_cluster_lg.name 46 } 47 s3 { 48 enabled = true 49 bucket = aws_s3_bucket.default_bucket.id 50 prefix = \u0026#34;logs/msk/cluster/\u0026#34; 51 } 52 } 53 } 54 55 tags = local.tags 56 57 depends_on = [aws_msk_configuration.msk_config] 58} 59 60resource \u0026#34;aws_msk_configuration\u0026#34; \u0026#34;msk_config\u0026#34; { 61 name = \u0026#34;${local.name}-msk-configuration\u0026#34; 62 63 kafka_versions = [local.msk.version] 64 65 server_properties = \u0026lt;\u0026lt;PROPERTIES 66 auto.create.topics.enable = true 67 delete.topic.enable = true 68 log.retention.ms = ${local.msk.log_retention_ms} 69 num.partitions = ${local.msk.num_partitions} 70 default.replication.factor = ${local.msk.default_replication_factor} 71 PROPERTIES 72} 73 74resource \u0026#34;aws_security_group\u0026#34; \u0026#34;msk\u0026#34; { 75 name = \u0026#34;${local.name}-msk-sg\u0026#34; 76 vpc_id = module.vpc.vpc_id 77 78 lifecycle { 79 create_before_destroy = true 80 } 81 82 tags = local.tags 83} 84 85... 86 87resource \u0026#34;aws_security_group_rule\u0026#34; \u0026#34;msk_kda_inbound\u0026#34; { 88 count = local.kda.to_create ? 1 : 0 89 type = \u0026#34;ingress\u0026#34; 90 description = \u0026#34;Allow KDA access\u0026#34; 91 security_group_id = aws_security_group.msk.id 92 protocol = \u0026#34;tcp\u0026#34; 93 from_port = 9098 94 to_port = 9098 95 source_security_group_id = aws_security_group.kda_sg[0].id 96} KDA Application The runtime environment and service execution role are required to create a Flink app. The latest supported Flink version (1.15.2) is specified for the former and an IAM role is created for the latter - it\u0026rsquo;ll be discussed more in a later section. Furthermore, we need to specify more configurations that are related to the Flink application and CloudWatch logging, and they will be covered below in detail as well.\n1# infra/variable.tf 2locals { 3 ... 4 kda = { 5 to_create = true 6 runtime_env = \u0026#34;FLINK-1_15\u0026#34; 7 package_name = \u0026#34;kda-package.zip\u0026#34; 8 } 9 ... 10} 11 12resource \u0026#34;aws_kinesisanalyticsv2_application\u0026#34; \u0026#34;kda_app\u0026#34; { 13 count = local.kda.to_create ? 1 : 0 14 15 name = \u0026#34;${local.name}-kda-app\u0026#34; 16 runtime_environment = local.kda.runtime_env # FLINK-1_15 17 service_execution_role = aws_iam_role.kda_app_role[0].arn 18 19 ... 20} Application Configuration In the application configuration section, we can specify details of the application code, VPC, environment properties, and application itself.\nApplication Code Configuration The application package (kda-package.zip) is uploaded into the default S3 bucket using the aws_s3_object Terraform resource. Then it can be used as the code content by specifying the bucket and key names.\n1# infra/kda.tf 2resource \u0026#34;aws_kinesisanalyticsv2_application\u0026#34; \u0026#34;kda_app\u0026#34; { 3 4 ... 5 6 application_configuration { 7 application_code_configuration { 8 code_content { 9 s3_content_location { 10 bucket_arn = aws_s3_bucket.default_bucket.arn 11 file_key = aws_s3_object.kda_package[0].key 12 } 13 } 14 15 code_content_type = \u0026#34;ZIPFILE\u0026#34; 16 } 17 18 ... 19 20 } 21 22 ... 23 24} 25 26... 27 28resource \u0026#34;aws_s3_object\u0026#34; \u0026#34;kda_package\u0026#34; { 29 count = local.kda.to_create ? 1 : 0 30 31 bucket = aws_s3_bucket.default_bucket.id 32 key = \u0026#34;package/${local.kda.package_name}\u0026#34; 33 source = \u0026#34;${dirname(path.cwd)}/${local.kda.package_name}\u0026#34; 34 35 etag = filemd5(\u0026#34;${dirname(path.cwd)}/${local.kda.package_name}\u0026#34;) 36} VPC Configuration The app can be deployed in the private subnets as it doesn\u0026rsquo;t need to be connected from outside. Note that an outbound rule that permits connection on port 9098 is created in its security group because it should be able to access the Kafka brokers.\n1# infra/kda.tf 2resource \u0026#34;aws_kinesisanalyticsv2_application\u0026#34; \u0026#34;kda_app\u0026#34; { 3 4 ... 5 6 application_configuration { 7 8 ... 9 10 vpc_configuration { 11 security_group_ids = [aws_security_group.kda_sg[0].id] 12 subnet_ids = module.vpc.private_subnets 13 } 14 15 ... 16 17 } 18 19 ... 20 21} 22 23... 24 25resource \u0026#34;aws_security_group\u0026#34; \u0026#34;kda_sg\u0026#34; { 26 count = local.kda.to_create ? 1 : 0 27 28 name = \u0026#34;${local.name}-kda-sg\u0026#34; 29 vpc_id = module.vpc.vpc_id 30 31 egress { 32 from_port = 9098 33 to_port = 9098 34 protocol = \u0026#34;tcp\u0026#34; 35 cidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;] 36 } 37 38 lifecycle { 39 create_before_destroy = true 40 } 41 42 tags = local.tags 43} Environment Properties In environment properties, we first add Flink CLI options in the kinesis.analytics.flink.run.options group. The values of the Pyflink app (python), pipeline jar (jarfile) and 3rd-party python package location (pyFiles) should match those in the application package (kda-package.zip). The other property groups are related to the Kafka source/sink table options, and they will be read by the application.\n1# infra/kda.tf 2resource \u0026#34;aws_kinesisanalyticsv2_application\u0026#34; \u0026#34;kda_app\u0026#34; { 3 4 ... 5 6 application_configuration { 7 8 ... 9 10 environment_properties { 11 property_group { 12 property_group_id = \u0026#34;kinesis.analytics.flink.run.options\u0026#34; 13 14 property_map = { 15 python = \u0026#34;processor.py\u0026#34; 16 jarfile = \u0026#34;package/lib/pyflink-getting-started-1.0.0.jar\u0026#34; 17 pyFiles = \u0026#34;package/site_packages/\u0026#34; 18 } 19 } 20 21 property_group { 22 property_group_id = \u0026#34;consumer.config.0\u0026#34; 23 24 property_map = { 25 \u0026#34;table.name\u0026#34; = \u0026#34;source_table\u0026#34; 26 \u0026#34;topic.name\u0026#34; = \u0026#34;stocks-in\u0026#34; 27 \u0026#34;bootstrap.servers\u0026#34; = aws_msk_cluster.msk_data_cluster.bootstrap_brokers_sasl_iam 28 \u0026#34;startup.mode\u0026#34; = \u0026#34;earliest-offset\u0026#34; 29 } 30 } 31 32 property_group { 33 property_group_id = \u0026#34;producer.config.0\u0026#34; 34 35 property_map = { 36 \u0026#34;table.name\u0026#34; = \u0026#34;sink_table\u0026#34; 37 \u0026#34;topic.name\u0026#34; = \u0026#34;stocks-out\u0026#34; 38 \u0026#34;bootstrap.servers\u0026#34; = aws_msk_cluster.msk_data_cluster.bootstrap_brokers_sasl_iam 39 } 40 } 41 } 42 43 ... 44 45 } 46 47 ... 48 49} Flink Application Configuration The Flink application configurations constitute of the following.\nCheckpoints - Checkpoints are backups of application state that Managed Service for Apache Flink automatically creates periodically and uses to restore from faults. By default, the following values are configured. CheckpointingEnabled: true CheckpointInterval: 60000 MinPauseBetweenCheckpoints: 5000 Monitoring - The metrics level determines which metrics are created to CloudWatch - see this page for details. The supported values are APPLICATION, OPERATOR, PARALLELISM, and TASK. Here APPLICATION is selected as the metrics level value. Parallelism - We can configure the parallel execution of tasks and the allocation of resources to implement scaling. The parallelism indicates the initial number of parallel tasks that an application can perform while the parallelism_per_kpu is the number of parallel tasks that an application can perform per Kinesis Processing Unit (KPU). The application parallelism can be updated by enabling auto-scaling. 1# infra/kda.tf 2resource \u0026#34;aws_kinesisanalyticsv2_application\u0026#34; \u0026#34;kda_app\u0026#34; { 3 4 ... 5 6 application_configuration { 7 8 ... 9 10 flink_application_configuration { 11 checkpoint_configuration { 12 configuration_type = \u0026#34;DEFAULT\u0026#34; 13 } 14 15 monitoring_configuration { 16 configuration_type = \u0026#34;CUSTOM\u0026#34; 17 log_level = \u0026#34;INFO\u0026#34; 18 metrics_level = \u0026#34;APPLICATION\u0026#34; 19 } 20 21 parallelism_configuration { 22 configuration_type = \u0026#34;CUSTOM\u0026#34; 23 auto_scaling_enabled = true 24 parallelism = 1 25 parallelism_per_kpu = 1 26 } 27 } 28 } 29 30 ... 31 32} Cloudwatch Logging Options We can add a CloudWatch log stream ARN to the CloudWatch logging options. Note that, when I missed it at first, I saw a CloudWatch log group and log stream are created automatically, but logging was not enabled. It was only when I specified a custom log stream ARN that logging was enabled and log messages were ingested.\n1# infra/kda.tf 2resource \u0026#34;aws_kinesisanalyticsv2_application\u0026#34; \u0026#34;kda_app\u0026#34; { 3 4 ... 5 6 cloudwatch_logging_options { 7 log_stream_arn = aws_cloudwatch_log_stream.kda_ls[0].arn 8 } 9 10 ... 11 12} 13 14... 15 16resource \u0026#34;aws_cloudwatch_log_group\u0026#34; \u0026#34;kda_lg\u0026#34; { 17 count = local.kda.to_create ? 1 : 0 18 19 name = \u0026#34;/${local.name}-kda-log-group\u0026#34; 20} 21 22resource \u0026#34;aws_cloudwatch_log_stream\u0026#34; \u0026#34;kda_ls\u0026#34; { 23 count = local.kda.to_create ? 1 : 0 24 25 name = \u0026#34;/${local.name}-kda-log-stream\u0026#34; 26 27 log_group_name = aws_cloudwatch_log_group.kda_lg[0].name 28} IAM Role The service execution role has the following permissions.\nFull access to CloudWatch, CloudWatch Log and Amazon Kinesis Data Analytics. It is given by AWS managed policies for logging, metrics generation etc. However, it is by no means recommended and should be updated according to the least privilege principle for production. 3 inline policies for connecting to the MSK cluster (kda-msk-access) in private subnets (kda-vpc-access) as well as giving access to the application package in S3 (kda-s3-access). 1# infra/kda.tf 2resource \u0026#34;aws_iam_role\u0026#34; \u0026#34;kda_app_role\u0026#34; { 3 count = local.kda.to_create ? 1 : 0 4 5 name = \u0026#34;${local.name}-kda-role\u0026#34; 6 7 assume_role_policy = jsonencode({ 8 Version = \u0026#34;2012-10-17\u0026#34; 9 Statement = [ 10 { 11 Action = \u0026#34;sts:AssumeRole\u0026#34; 12 Effect = \u0026#34;Allow\u0026#34; 13 Sid = \u0026#34;\u0026#34; 14 Principal = { 15 Service = \u0026#34;kinesisanalytics.amazonaws.com\u0026#34; 16 } 17 }, 18 ] 19 }) 20 21 managed_policy_arns = [ 22 \u0026#34;arn:aws:iam::aws:policy/CloudWatchFullAccess\u0026#34;, 23 \u0026#34;arn:aws:iam::aws:policy/CloudWatchLogsFullAccess\u0026#34;, 24 \u0026#34;arn:aws:iam::aws:policy/AmazonKinesisAnalyticsFullAccess\u0026#34; 25 ] 26 27 inline_policy { 28 name = \u0026#34;kda-msk-access\u0026#34; 29 30 policy = jsonencode({ 31 Version = \u0026#34;2012-10-17\u0026#34; 32 Statement = [ 33 { 34 Sid = \u0026#34;PermissionOnCluster\u0026#34; 35 Action = [ 36 \u0026#34;kafka-cluster:Connect\u0026#34;, 37 \u0026#34;kafka-cluster:AlterCluster\u0026#34;, 38 \u0026#34;kafka-cluster:DescribeCluster\u0026#34; 39 ] 40 Effect = \u0026#34;Allow\u0026#34; 41 Resource = \u0026#34;arn:aws:kafka:${local.region}:${data.aws_caller_identity.current.account_id}:cluster/${local.name}-msk-cluster/*\u0026#34; 42 }, 43 { 44 Sid = \u0026#34;PermissionOnTopics\u0026#34; 45 Action = [ 46 \u0026#34;kafka-cluster:*Topic*\u0026#34;, 47 \u0026#34;kafka-cluster:WriteData\u0026#34;, 48 \u0026#34;kafka-cluster:ReadData\u0026#34; 49 ] 50 Effect = \u0026#34;Allow\u0026#34; 51 Resource = \u0026#34;arn:aws:kafka:${local.region}:${data.aws_caller_identity.current.account_id}:topic/${local.name}-msk-cluster/*\u0026#34; 52 }, 53 { 54 Sid = \u0026#34;PermissionOnGroups\u0026#34; 55 Action = [ 56 \u0026#34;kafka-cluster:AlterGroup\u0026#34;, 57 \u0026#34;kafka-cluster:DescribeGroup\u0026#34; 58 ] 59 Effect = \u0026#34;Allow\u0026#34; 60 Resource = \u0026#34;arn:aws:kafka:${local.region}:${data.aws_caller_identity.current.account_id}:group/${local.name}-msk-cluster/*\u0026#34; 61 } 62 ] 63 }) 64 } 65 66 inline_policy { 67 name = \u0026#34;kda-vpc-access\u0026#34; 68 # https://docs.aws.amazon.com/kinesisanalytics/latest/java/vpc-permissions.html 69 70 policy = jsonencode({ 71 Version = \u0026#34;2012-10-17\u0026#34; 72 Statement = [ 73 { 74 Sid = \u0026#34;VPCReadOnlyPermissions\u0026#34; 75 Action = [ 76 \u0026#34;ec2:DescribeVpcs\u0026#34;, 77 \u0026#34;ec2:DescribeSubnets\u0026#34;, 78 \u0026#34;ec2:DescribeSecurityGroups\u0026#34;, 79 \u0026#34;ec2:DescribeDhcpOptions\u0026#34; 80 ] 81 Effect = \u0026#34;Allow\u0026#34; 82 Resource = \u0026#34;*\u0026#34; 83 }, 84 { 85 Sid = \u0026#34;ENIReadWritePermissions\u0026#34; 86 Action = [ 87 \u0026#34;ec2:CreateNetworkInterface\u0026#34;, 88 \u0026#34;ec2:CreateNetworkInterfacePermission\u0026#34;, 89 \u0026#34;ec2:DescribeNetworkInterfaces\u0026#34;, 90 \u0026#34;ec2:DeleteNetworkInterface\u0026#34; 91 ] 92 Effect = \u0026#34;Allow\u0026#34; 93 Resource = \u0026#34;*\u0026#34; 94 } 95 96 ] 97 }) 98 } 99 100 inline_policy { 101 name = \u0026#34;kda-s3-access\u0026#34; 102 103 policy = jsonencode({ 104 Version = \u0026#34;2012-10-17\u0026#34; 105 Statement = [ 106 { 107 Sid = \u0026#34;ListObjectsInBucket\u0026#34; 108 Action = [\u0026#34;s3:ListBucket\u0026#34;] 109 Effect = \u0026#34;Allow\u0026#34; 110 Resource = \u0026#34;arn:aws:s3:::${aws_s3_bucket.default_bucket.id}\u0026#34; 111 }, 112 { 113 Sid = \u0026#34;AllObjectActions\u0026#34; 114 Action = [\u0026#34;s3:*Object\u0026#34;] 115 Effect = \u0026#34;Allow\u0026#34; 116 Resource = \u0026#34;arn:aws:s3:::${aws_s3_bucket.default_bucket.id}/*\u0026#34; 117 }, 118 ] 119 }) 120 } 121 122 tags = local.tags 123} Once deployed, we can see the application on AWS console, and it stays in the ready status.\nRun Application We first need to create records in the source Kafka topic. It is done by executing the data generator app (producer.py). See part 2 for details about the generator app and how to execute it. Note that we should connect to the VPN server in order to create records from the developer machine.\nOnce executed, we can check the source topic is created and messages are ingested.\nMonitoring on Flink Web UI We can run the Flink application on AWS console with the Run without snapshot option as we haven\u0026rsquo;t enabled snapshots.\nOnce the app is running, we can monitor it on the Flink Web UI available on AWS Console.\nIn the Overview section, it shows the available task slots, running jobs and completed jobs.\nWe can inspect an individual job in the Jobs menu. It shows key details about a job execution in Overview, Exceptions, TimeLine, Checkpoints and Configuration tabs.\nCloudWatch Logging The application log messages can be checked in the CloudWatch Console, and it gives additional capability to debug the application.\nApplication Output We can see details of all the topics in Kpow. The total number of messages matches between the source and output topics but not within partitions.\nSummary In this series of posts, we discussed a Flink (Pyflink) application that reads/writes from/to Kafka topics. In the previous posts, I demonstrated a Pyflink app that targets a local Kafka cluster as well as a Kafka cluster on Amazon MSK. The app was executed in a virtual environment as well as in a local Flink cluster for improved monitoring. In this post, the app was deployed via Amazon Managed Service for Apache Flink, which is the easiest option to run Flink applications on AWS.\n","date":"September 4, 2023","img":"/blog/2023-09-04-getting-started-with-pyflink-on-aws-part-3/featured.png","lang":"en","langName":"English","largeImg":"/blog/2023-09-04-getting-started-with-pyflink-on-aws-part-3/featured_hue908d327a4e776f04c90ce3d9151d268_74618_500x0_resize_box_3.png","permalink":"/blog/2023-09-04-getting-started-with-pyflink-on-aws-part-3/","series":[{"title":"Getting Started With Pyflink on AWS","url":"/series/getting-started-with-pyflink-on-aws/"}],"smallImg":"/blog/2023-09-04-getting-started-with-pyflink-on-aws-part-3/featured_hue908d327a4e776f04c90ce3d9151d268_74618_180x0_resize_box_3.png","tags":[{"title":"Apache Flink","url":"/tags/apache-flink/"},{"title":"PyFlink","url":"/tags/pyflink/"},{"title":"Apache Kafka","url":"/tags/apache-kafka/"},{"title":"Amazon Managed Service for Apache Flink","url":"/tags/amazon-managed-service-for-apache-flink/"},{"title":"Amazon Managed Flink","url":"/tags/amazon-managed-flink/"},{"title":"Amazon MSK","url":"/tags/amazon-msk/"},{"title":"Python","url":"/tags/python/"},{"title":"Docker","url":"/tags/docker/"},{"title":"Docker Compose","url":"/tags/docker-compose/"}],"timestamp":1693785600,"title":"Getting Started With Pyflink on AWS - Part 3 AWS Managed Flink and MSK"},{"categories":[{"title":"Data Streaming","url":"/categories/data-streaming/"}],"content":"In this series of posts, we discuss a Flink (Pyflink) application that reads/writes from/to Kafka topics. In part 1, an app that targets a local Kafka cluster was created. In this post, we will update the app by connecting a Kafka cluster on Amazon MSK. The Kafka cluster is authenticated by IAM and the app has additional jar dependency. As Amazon Managed Service for Apache Flink does not allow you to specify multiple pipeline jar files, we have to build a custom Uber Jar that combines multiple jar files. Same as part 1, the app will be executed in a virtual environment as well as in a local Flink cluster for improved monitoring with the updated pipeline jar file.\nPart 1 Local Flink and Local Kafka Part 2 Local Flink and MSK (this post) Part 3 AWS Managed Flink and MSK [Update 2023-08-30] Amazon Kinesis Data Analytics is renamed into Amazon Managed Service for Apache Flink. In this post, Kinesis Data Analytics (KDA) and Amazon Managed Service for Apache Flink will be used interchangeably.\nArchitecture The Python source data generator sends random stock price records into a Kafka topic. The messages in the source topic are consumed by a Flink application, and it just writes those messages into a different sink topic. As the Kafka cluster is deployed in private subnets, it is accessed via a VPN server from the developer machine. This is the simplest application of the Pyflink getting started guide from AWS, and you may try other examples if interested.\nInfrastructure A Kafka cluster is created on Amazon MSK using Terraform, and the cluster is secured by IAM access control. Similar to part 1, the Python apps including the Flink app run in a virtual environment in the first trial. After that the Flink app is submitted to a local Flink cluster for improved monitoring. Same as part 1, the Flink cluster is created using Docker. The source can be found in the GitHub repository of this post.\nPreparation Flink Pipeline Jar The Flink application should be able to connect a Kafka cluster on Amazon MSK, and we used the Apache Kafka SQL Connector artifact (flink-sql-connector-kafka-1.15.2.jar) in part 1. The Kafka cluster is authenticated by IAM, however, it should be able to refer to the Amazon MSK Library for AWS Identity and Access Management (MSK IAM Auth). So far KDA does not allow you to specify multiple pipeline jar files, and we have to build a single Jar file (Uber Jar) that includes all the dependencies of the application. Moreover, as the MSK IAM Auth library is not compatible with the Apache Kafka SQL Connector due to shade relocation, we have to build the Jar file based on the Apache Kafka Connector instead. After some search, I found an example from the Blueprints: Kinesis Data Analytics for Apache Flink and was able to modify the POM file with necessary dependencies for this post. The modified POM file can be shown below, and it creates the Uber Jar file for this post - pyflink-getting-started-1.0.0.jar.\n1\u0026lt;!--package/uber-jar-for-pyflink/pom.xml--\u0026gt; 2\u0026lt;project xmlns=\u0026#34;http://maven.apache.org/POM/4.0.0\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; 3\txsi:schemaLocation=\u0026#34;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\u0026#34;\u0026gt; 4\t\u0026lt;modelVersion\u0026gt;4.0.0\u0026lt;/modelVersion\u0026gt; 5 6\t\u0026lt;groupId\u0026gt;com.amazonaws.services.kinesisanalytics\u0026lt;/groupId\u0026gt; 7\t\u0026lt;artifactId\u0026gt;pyflink-getting-started\u0026lt;/artifactId\u0026gt; 8\t\u0026lt;version\u0026gt;1.0.0\u0026lt;/version\u0026gt; 9\t\u0026lt;packaging\u0026gt;jar\u0026lt;/packaging\u0026gt; 10 11\t\u0026lt;name\u0026gt;Uber Jar for PyFlink App\u0026lt;/name\u0026gt; 12 13\t\u0026lt;properties\u0026gt; 14\t\u0026lt;project.build.sourceEncoding\u0026gt;UTF-8\u0026lt;/project.build.sourceEncoding\u0026gt; 15\t\u0026lt;flink.version\u0026gt;1.15.2\u0026lt;/flink.version\u0026gt; 16\t\u0026lt;target.java.version\u0026gt;1.11\u0026lt;/target.java.version\u0026gt; 17\t\u0026lt;jdk.version\u0026gt;11\u0026lt;/jdk.version\u0026gt; 18\t\u0026lt;scala.binary.version\u0026gt;2.12\u0026lt;/scala.binary.version\u0026gt; 19\t\u0026lt;kda.connectors.version\u0026gt;2.0.0\u0026lt;/kda.connectors.version\u0026gt; 20\t\u0026lt;kda.runtime.version\u0026gt;1.2.0\u0026lt;/kda.runtime.version\u0026gt; 21\t\u0026lt;kafka.clients.version\u0026gt;2.8.1\u0026lt;/kafka.clients.version\u0026gt; 22\t\u0026lt;log4j.version\u0026gt;2.17.1\u0026lt;/log4j.version\u0026gt; 23\t\u0026lt;aws-msk-iam-auth.version\u0026gt;1.1.7\u0026lt;/aws-msk-iam-auth.version\u0026gt; 24\t\u0026lt;/properties\u0026gt; 25 26\t\u0026lt;repositories\u0026gt; 27\t\u0026lt;repository\u0026gt; 28\t\u0026lt;id\u0026gt;apache.snapshots\u0026lt;/id\u0026gt; 29\t\u0026lt;name\u0026gt;Apache Development Snapshot Repository\u0026lt;/name\u0026gt; 30\t\u0026lt;url\u0026gt;https://repository.apache.org/content/repositories/snapshots/\u0026lt;/url\u0026gt; 31\t\u0026lt;releases\u0026gt; 32\t\u0026lt;enabled\u0026gt;false\u0026lt;/enabled\u0026gt; 33\t\u0026lt;/releases\u0026gt; 34\t\u0026lt;snapshots\u0026gt; 35\t\u0026lt;enabled\u0026gt;true\u0026lt;/enabled\u0026gt; 36\t\u0026lt;/snapshots\u0026gt; 37\t\u0026lt;/repository\u0026gt; 38\t\u0026lt;/repositories\u0026gt; 39 40\t\u0026lt;dependencies\u0026gt; 41 42\t\u0026lt;dependency\u0026gt; 43\t\u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; 44\t\u0026lt;artifactId\u0026gt;flink-connector-base\u0026lt;/artifactId\u0026gt; 45\t\u0026lt;version\u0026gt;${flink.version}\u0026lt;/version\u0026gt; 46\t\u0026lt;/dependency\u0026gt; 47 48\t\u0026lt;dependency\u0026gt; 49\t\u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; 50\t\u0026lt;artifactId\u0026gt;flink-connector-kafka\u0026lt;/artifactId\u0026gt; 51\t\u0026lt;version\u0026gt;${flink.version}\u0026lt;/version\u0026gt; 52\t\u0026lt;/dependency\u0026gt; 53 54\t\u0026lt;dependency\u0026gt; 55\t\u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; 56\t\u0026lt;artifactId\u0026gt;flink-connector-files\u0026lt;/artifactId\u0026gt; 57\t\u0026lt;version\u0026gt;${flink.version}\u0026lt;/version\u0026gt; 58\t\u0026lt;/dependency\u0026gt; 59 60\t\u0026lt;dependency\u0026gt; 61\t\u0026lt;groupId\u0026gt;org.apache.kafka\u0026lt;/groupId\u0026gt; 62\t\u0026lt;artifactId\u0026gt;kafka-clients\u0026lt;/artifactId\u0026gt; 63\t\u0026lt;version\u0026gt;${kafka.clients.version}\u0026lt;/version\u0026gt; 64\t\u0026lt;/dependency\u0026gt; 65 66\t\u0026lt;dependency\u0026gt; 67\t\u0026lt;groupId\u0026gt;software.amazon.msk\u0026lt;/groupId\u0026gt; 68\t\u0026lt;artifactId\u0026gt;aws-msk-iam-auth\u0026lt;/artifactId\u0026gt; 69\t\u0026lt;version\u0026gt;${aws-msk-iam-auth.version}\u0026lt;/version\u0026gt; 70\t\u0026lt;/dependency\u0026gt; 71 72\t\u0026lt;!-- Add logging framework, to produce console output when running in the IDE. --\u0026gt; 73\t\u0026lt;!-- These dependencies are excluded from the application JAR by default. --\u0026gt; 74\t\u0026lt;dependency\u0026gt; 75\t\u0026lt;groupId\u0026gt;org.apache.logging.log4j\u0026lt;/groupId\u0026gt; 76\t\u0026lt;artifactId\u0026gt;log4j-slf4j-impl\u0026lt;/artifactId\u0026gt; 77\t\u0026lt;version\u0026gt;${log4j.version}\u0026lt;/version\u0026gt; 78\t\u0026lt;scope\u0026gt;runtime\u0026lt;/scope\u0026gt; 79\t\u0026lt;/dependency\u0026gt; 80\t\u0026lt;dependency\u0026gt; 81\t\u0026lt;groupId\u0026gt;org.apache.logging.log4j\u0026lt;/groupId\u0026gt; 82\t\u0026lt;artifactId\u0026gt;log4j-api\u0026lt;/artifactId\u0026gt; 83\t\u0026lt;version\u0026gt;${log4j.version}\u0026lt;/version\u0026gt; 84\t\u0026lt;scope\u0026gt;runtime\u0026lt;/scope\u0026gt; 85\t\u0026lt;/dependency\u0026gt; 86\t\u0026lt;dependency\u0026gt; 87\t\u0026lt;groupId\u0026gt;org.apache.logging.log4j\u0026lt;/groupId\u0026gt; 88\t\u0026lt;artifactId\u0026gt;log4j-core\u0026lt;/artifactId\u0026gt; 89\t\u0026lt;version\u0026gt;${log4j.version}\u0026lt;/version\u0026gt; 90\t\u0026lt;scope\u0026gt;runtime\u0026lt;/scope\u0026gt; 91\t\u0026lt;/dependency\u0026gt; 92\t\u0026lt;/dependencies\u0026gt; 93 94\t\u0026lt;build\u0026gt; 95\t\u0026lt;plugins\u0026gt; 96 97\t\u0026lt;!-- Java Compiler --\u0026gt; 98\t\u0026lt;plugin\u0026gt; 99\t\u0026lt;groupId\u0026gt;org.apache.maven.plugins\u0026lt;/groupId\u0026gt; 100\t\u0026lt;artifactId\u0026gt;maven-compiler-plugin\u0026lt;/artifactId\u0026gt; 101\t\u0026lt;version\u0026gt;3.8.0\u0026lt;/version\u0026gt; 102\t\u0026lt;configuration\u0026gt; 103\t\u0026lt;source\u0026gt;${jdk.version}\u0026lt;/source\u0026gt; 104\t\u0026lt;target\u0026gt;${jdk.version}\u0026lt;/target\u0026gt; 105\t\u0026lt;/configuration\u0026gt; 106\t\u0026lt;/plugin\u0026gt; 107 108\t\u0026lt;!-- We use the maven-shade plugin to create a fat jar that contains all necessary dependencies. --\u0026gt; 109\t\u0026lt;!-- Change the value of \u0026lt;mainClass\u0026gt;...\u0026lt;/mainClass\u0026gt; if your program entry point changes. --\u0026gt; 110\t\u0026lt;plugin\u0026gt; 111\t\u0026lt;groupId\u0026gt;org.apache.maven.plugins\u0026lt;/groupId\u0026gt; 112\t\u0026lt;artifactId\u0026gt;maven-shade-plugin\u0026lt;/artifactId\u0026gt; 113\t\u0026lt;version\u0026gt;3.4.1\u0026lt;/version\u0026gt; 114\t\u0026lt;executions\u0026gt; 115\t\u0026lt;!-- Run shade goal on package phase --\u0026gt; 116\t\u0026lt;execution\u0026gt; 117\t\u0026lt;phase\u0026gt;package\u0026lt;/phase\u0026gt; 118\t\u0026lt;goals\u0026gt; 119\t\u0026lt;goal\u0026gt;shade\u0026lt;/goal\u0026gt; 120\t\u0026lt;/goals\u0026gt; 121\t\u0026lt;configuration\u0026gt; 122\t\u0026lt;artifactSet\u0026gt; 123\t\u0026lt;excludes\u0026gt; 124\t\u0026lt;exclude\u0026gt;org.apache.flink:force-shading\u0026lt;/exclude\u0026gt; 125\t\u0026lt;exclude\u0026gt;com.google.code.findbugs:jsr305\u0026lt;/exclude\u0026gt; 126\t\u0026lt;exclude\u0026gt;org.slf4j:*\u0026lt;/exclude\u0026gt; 127\t\u0026lt;exclude\u0026gt;org.apache.logging.log4j:*\u0026lt;/exclude\u0026gt; 128\t\u0026lt;/excludes\u0026gt; 129\t\u0026lt;/artifactSet\u0026gt; 130\t\u0026lt;filters\u0026gt; 131\t\u0026lt;filter\u0026gt; 132\t\u0026lt;!-- Do not copy the signatures in the META-INF folder. 133\tOtherwise, this might cause SecurityExceptions when using the JAR. --\u0026gt; 134\t\u0026lt;artifact\u0026gt;*:*\u0026lt;/artifact\u0026gt; 135\t\u0026lt;excludes\u0026gt; 136\t\u0026lt;exclude\u0026gt;META-INF/*.SF\u0026lt;/exclude\u0026gt; 137\t\u0026lt;exclude\u0026gt;META-INF/*.DSA\u0026lt;/exclude\u0026gt; 138\t\u0026lt;exclude\u0026gt;META-INF/*.RSA\u0026lt;/exclude\u0026gt; 139\t\u0026lt;/excludes\u0026gt; 140\t\u0026lt;/filter\u0026gt; 141\t\u0026lt;/filters\u0026gt; 142\t\u0026lt;transformers\u0026gt; 143\t\u0026lt;transformer implementation=\u0026#34;org.apache.maven.plugins.shade.resource.ServicesResourceTransformer\u0026#34;/\u0026gt; 144\t\u0026lt;/transformers\u0026gt; 145\t\u0026lt;/configuration\u0026gt; 146\t\u0026lt;/execution\u0026gt; 147\t\u0026lt;/executions\u0026gt; 148\t\u0026lt;/plugin\u0026gt; 149\t\u0026lt;/plugins\u0026gt; 150 151\t\u0026lt;pluginManagement\u0026gt; 152\t\u0026lt;plugins\u0026gt; 153 154\t\u0026lt;!-- This improves the out-of-the-box experience in Eclipse by resolving some warnings. --\u0026gt; 155\t\u0026lt;plugin\u0026gt; 156\t\u0026lt;groupId\u0026gt;org.eclipse.m2e\u0026lt;/groupId\u0026gt; 157\t\u0026lt;artifactId\u0026gt;lifecycle-mapping\u0026lt;/artifactId\u0026gt; 158\t\u0026lt;version\u0026gt;1.0.0\u0026lt;/version\u0026gt; 159\t\u0026lt;configuration\u0026gt; 160\t\u0026lt;lifecycleMappingMetadata\u0026gt; 161\t\u0026lt;pluginExecutions\u0026gt; 162\t\u0026lt;pluginExecution\u0026gt; 163\t\u0026lt;pluginExecutionFilter\u0026gt; 164\t\u0026lt;groupId\u0026gt;org.apache.maven.plugins\u0026lt;/groupId\u0026gt; 165\t\u0026lt;artifactId\u0026gt;maven-shade-plugin\u0026lt;/artifactId\u0026gt; 166\t\u0026lt;versionRange\u0026gt;[3.1.1,)\u0026lt;/versionRange\u0026gt; 167\t\u0026lt;goals\u0026gt; 168\t\u0026lt;goal\u0026gt;shade\u0026lt;/goal\u0026gt; 169\t\u0026lt;/goals\u0026gt; 170\t\u0026lt;/pluginExecutionFilter\u0026gt; 171\t\u0026lt;action\u0026gt; 172\t\u0026lt;ignore/\u0026gt; 173\t\u0026lt;/action\u0026gt; 174\t\u0026lt;/pluginExecution\u0026gt; 175\t\u0026lt;pluginExecution\u0026gt; 176\t\u0026lt;pluginExecutionFilter\u0026gt; 177\t\u0026lt;groupId\u0026gt;org.apache.maven.plugins\u0026lt;/groupId\u0026gt; 178\t\u0026lt;artifactId\u0026gt;maven-compiler-plugin\u0026lt;/artifactId\u0026gt; 179\t\u0026lt;versionRange\u0026gt;[3.1,)\u0026lt;/versionRange\u0026gt; 180\t\u0026lt;goals\u0026gt; 181\t\u0026lt;goal\u0026gt;testCompile\u0026lt;/goal\u0026gt; 182\t\u0026lt;goal\u0026gt;compile\u0026lt;/goal\u0026gt; 183\t\u0026lt;/goals\u0026gt; 184\t\u0026lt;/pluginExecutionFilter\u0026gt; 185\t\u0026lt;action\u0026gt; 186\t\u0026lt;ignore/\u0026gt; 187\t\u0026lt;/action\u0026gt; 188\t\u0026lt;/pluginExecution\u0026gt; 189\t\u0026lt;/pluginExecutions\u0026gt; 190\t\u0026lt;/lifecycleMappingMetadata\u0026gt; 191\t\u0026lt;/configuration\u0026gt; 192\t\u0026lt;/plugin\u0026gt; 193\t\u0026lt;/plugins\u0026gt; 194\t\u0026lt;/pluginManagement\u0026gt; 195\t\u0026lt;/build\u0026gt; 196\u0026lt;/project\u0026gt; The following script (build.sh) builds to create the Uber Jar file for this post, followed by downloading the kafka-python package and creating a zip file that can be used to deploy the Flink app via KDA. Although the Flink app does not need the package, it is added in order to check if --pyFiles option works when submitting the app to a Flink cluster or deploying via KDA. The zip package file will be used for KDA deployment in the next post.\n1# build.sh 2#!/usr/bin/env bash 3SCRIPT_DIR=\u0026#34;$(cd $(dirname \u0026#34;$0\u0026#34;); pwd)\u0026#34; 4SRC_PATH=$SCRIPT_DIR/package 5 6# remove contents under $SRC_PATH (except for uber-jar-for-pyflink) and kda-package.zip file 7shopt -s extglob 8rm -rf $SRC_PATH/!(uber-jar-for-pyflink) kda-package.zip 9 10## Generate Uber Jar for PyFlink app for MSK cluster with IAM authN 11echo \u0026#34;generate Uber jar for PyFlink app...\u0026#34; 12mkdir $SRC_PATH/lib 13mvn clean install -f $SRC_PATH/uber-jar-for-pyflink/pom.xml \\ 14 \u0026amp;\u0026amp; mv $SRC_PATH/uber-jar-for-pyflink/target/pyflink-getting-started-1.0.0.jar $SRC_PATH/lib \\ 15 \u0026amp;\u0026amp; rm -rf $SRC_PATH/uber-jar-for-pyflink/target 16 17## Install pip packages 18echo \u0026#34;install and zip pip packages...\u0026#34; 19pip install -r requirements.txt --target $SRC_PATH/site_packages 20 21## Package pyflink app 22echo \u0026#34;package pyflink app\u0026#34; 23zip -r kda-package.zip processor.py package/lib package/site_packages Once completed, the Uber Jar file and python package can be found in the lib and site_packages folders respectively as shown below.\nVPC and VPN A VPC with 3 public and private subnets is created using the AWS VPC Terraform module (infra/vpc.tf). Also, a SoftEther VPN server is deployed in order to access the resources in the private subnets from the developer machine (infra/vpn.tf). It is particularly useful to monitor and manage the MSK cluster and Kafka topic locally. The details about how to configure the VPN server can be found in an earlier post.\nMSK Cluster An MSK cluster with 2 brokers is created. The broker nodes are deployed with the kafka.m5.large instance type in private subnets and IAM authentication is used for the client authentication method. Finally, additional server configurations are added such as enabling auto creation of topics and topic deletion.\n1# infra/variable.tf 2locals { 3 ... 4 msk = { 5 version = \u0026#34;2.8.1\u0026#34; 6 instance_size = \u0026#34;kafka.m5.large\u0026#34; 7 ebs_volume_size = 20 8 log_retention_ms = 604800000 # 7 days 9 num_partitions = 2 10 default_replication_factor = 2 11 } 12 ... 13} 14# infra/msk.tf 15resource \u0026#34;aws_msk_cluster\u0026#34; \u0026#34;msk_data_cluster\u0026#34; { 16 cluster_name = \u0026#34;${local.name}-msk-cluster\u0026#34; 17 kafka_version = local.msk.version 18 number_of_broker_nodes = local.msk.number_of_broker_nodes 19 configuration_info { 20 arn = aws_msk_configuration.msk_config.arn 21 revision = aws_msk_configuration.msk_config.latest_revision 22 } 23 24 broker_node_group_info { 25 instance_type = local.msk.instance_size 26 client_subnets = slice(module.vpc.private_subnets, 0, local.msk.number_of_broker_nodes) 27 security_groups = [aws_security_group.msk.id] 28 storage_info { 29 ebs_storage_info { 30 volume_size = local.msk.ebs_volume_size 31 } 32 } 33 } 34 35 client_authentication { 36 sasl { 37 iam = true 38 } 39 } 40 41 logging_info { 42 broker_logs { 43 cloudwatch_logs { 44 enabled = true 45 log_group = aws_cloudwatch_log_group.msk_cluster_lg.name 46 } 47 s3 { 48 enabled = true 49 bucket = aws_s3_bucket.default_bucket.id 50 prefix = \u0026#34;logs/msk/cluster/\u0026#34; 51 } 52 } 53 } 54 55 tags = local.tags 56 57 depends_on = [aws_msk_configuration.msk_config] 58} 59 60resource \u0026#34;aws_msk_configuration\u0026#34; \u0026#34;msk_config\u0026#34; { 61 name = \u0026#34;${local.name}-msk-configuration\u0026#34; 62 63 kafka_versions = [local.msk.version] 64 65 server_properties = \u0026lt;\u0026lt;PROPERTIES 66 auto.create.topics.enable = true 67 delete.topic.enable = true 68 log.retention.ms = ${local.msk.log_retention_ms} 69 num.partitions = ${local.msk.num_partitions} 70 default.replication.factor = ${local.msk.default_replication_factor} 71 PROPERTIES 72} Kafka Management App The Kpow CE is used for ease of monitoring Kafka topics and related resources. The bootstrap server address, security configuration for IAM authentication and AWS credentials are added as environment variables. See this post for details about Kafka management apps.\n1# compose-ui.yml 2version: \u0026#34;3\u0026#34; 3 4services: 5 kpow: 6 image: factorhouse/kpow-ce:91.2.1 7 container_name: kpow 8 ports: 9 - \u0026#34;3000:3000\u0026#34; 10 networks: 11 - appnet 12 environment: 13 AWS_ACCESS_KEY_ID: $AWS_ACCESS_KEY_ID 14 AWS_SECRET_ACCESS_KEY: $AWS_SECRET_ACCESS_KEY 15 AWS_SESSION_TOKEN: $AWS_SESSION_TOKEN 16 # kafka cluster 17 BOOTSTRAP: $BOOTSTRAP_SERVERS 18 SECURITY_PROTOCOL: SASL_SSL 19 SASL_MECHANISM: AWS_MSK_IAM 20 SASL_CLIENT_CALLBACK_HANDLER_CLASS: software.amazon.msk.auth.iam.IAMClientCallbackHandler 21 SASL_JAAS_CONFIG: software.amazon.msk.auth.iam.IAMLoginModule required; 22 23networks: 24 appnet: 25 name: app-network Flink Cluster We can create a Flink cluster using the custom Docker image that we used in part 1. The cluster is made up of a single Job Manger and Task Manager, and the cluster runs in the Session Mode where one or more Flink applications can be submitted/executed simultaneously. See this page for details about how to create a Flink cluster using docker-compose.\nA set of environment variables are configured to adjust the application behaviour and to give permission to read/write messages from the Kafka cluster with IAM authentication. The RUNTIME_ENV is set to DOCKER, and it determines which pipeline jar and application property file to choose. Also, the BOOTSTRAP_SERVERS overrides the Kafka bootstrap server address value from the application property file. The bootstrap server address of the MSK cluster are referred from the host environment variable that has the same name. Finally, the current directory is volume-mapped into /etc/flink so that the application and related resources can be available in the Flink cluster.\nThe Flink cluster can be started by docker-compose -f compose-flink.yml up -d.\n1version: \u0026#34;3.5\u0026#34; 2 3services: 4 jobmanager: 5 image: pyflink:1.15.2-scala_2.12 6 container_name: jobmanager 7 command: jobmanager 8 ports: 9 - \u0026#34;8081:8081\u0026#34; 10 networks: 11 - flinknet 12 environment: 13 - | 14 FLINK_PROPERTIES= 15 jobmanager.rpc.address: jobmanager 16 state.backend: filesystem 17 state.checkpoints.dir: file:///tmp/flink-checkpoints 18 heartbeat.interval: 1000 19 heartbeat.timeout: 5000 20 rest.flamegraph.enabled: true 21 web.backpressure.refresh-interval: 10000 22 - RUNTIME_ENV=DOCKER 23 - BOOTSTRAP_SERVERS=$BOOTSTRAP_SERVERS 24 - AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID 25 - AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY 26 - AWS_SESSION_TOKEN=$AWS_SESSION_TOKEN 27 volumes: 28 - $PWD:/etc/flink 29 taskmanager: 30 image: pyflink:1.15.2-scala_2.12 31 container_name: taskmanager 32 command: taskmanager 33 networks: 34 - flinknet 35 volumes: 36 - flink_data:/tmp/ 37 - $PWD:/etc/flink 38 environment: 39 - | 40 FLINK_PROPERTIES= 41 jobmanager.rpc.address: jobmanager 42 taskmanager.numberOfTaskSlots: 3 43 state.backend: filesystem 44 state.checkpoints.dir: file:///tmp/flink-checkpoints 45 heartbeat.interval: 1000 46 heartbeat.timeout: 5000 47 - RUNTIME_ENV=DOCKER 48 - BOOTSTRAP_SERVERS=$BOOTSTRAP_SERVERS 49 - AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID 50 - AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY 51 - AWS_SESSION_TOKEN=$AWS_SESSION_TOKEN 52 depends_on: 53 - jobmanager 54 55networks: 56 flinknet: 57 name: flink-network 58 59volumes: 60 flink_data: 61 driver: local 62 name: flink_data Virtual Environment As mentioned earlier, all Python apps run in a virtual environment, and we have the following pip packages. We use the version 1.15.2 of the apache-flink package because it is the latest supported version by KDA. We also need the kafka-python package for source data generation. As the Kafka cluster is IAM-authenticated, a patched version is installed instead of the stable version. The pip packages can be installed by pip install -r requirements-dev.txt.\n1# kafka-python with IAM auth support - https://github.com/dpkp/kafka-python/pull/2255 2https://github.com/mattoberle/kafka-python/archive/7ff323727d99e0c33a68423300e7f88a9cf3f830.tar.gz 3 4# requirements-dev.txt 5-r requirements.txt 6apache-flink==1.15.2 7black==19.10b0 8pytest 9pytest-cov Application Source Data A single Python script is created to generate fake stock price records. The class for the stock record has the asdict, auto and create methods. The create method generates a list of records where each element is instantiated by the auto method. Those records are sent into the relevant Kafka topic after being converted into a dictionary by the asdict method.\nA Kafka producer is created as an attribute of the Producer class. The producer adds security configuration for IAM authentication when the bootstrap server address ends with 9098. The source records are sent into the relevant topic by the send method. Note that both the key and value of the messages are serialized as json.\nThe data generator can be started simply by python producer.py.\n1# producer.py 2import os 3import datetime 4import time 5import json 6import typing 7import random 8import logging 9import re 10import dataclasses 11 12from kafka import KafkaProducer 13 14logging.basicConfig( 15 level=logging.INFO, 16 format=\u0026#34;%(asctime)s.%(msecs)03d:%(levelname)s:%(name)s:%(message)s\u0026#34;, 17 datefmt=\u0026#34;%Y-%m-%d %H:%M:%S\u0026#34;, 18) 19 20datetime.datetime.now().strftime(\u0026#34;%Y-%m-%d %H:%M:%S.%f\u0026#34;) 21 22 23@dataclasses.dataclass 24class Stock: 25 event_time: str 26 ticker: str 27 price: float 28 29 def asdict(self): 30 return dataclasses.asdict(self) 31 32 @classmethod 33 def auto(cls, ticker: str): 34 # event_time = datetime.datetime.now().isoformat(timespec=\u0026#34;milliseconds\u0026#34;) 35 event_time = datetime.datetime.now().strftime(\u0026#34;%Y-%m-%d %H:%M:%S.%f\u0026#34;) 36 price = round(random.random() * 100, 2) 37 return cls(event_time, ticker, price) 38 39 @staticmethod 40 def create(): 41 tickers = \u0026#39;[\u0026#34;AAPL\u0026#34;, \u0026#34;ACN\u0026#34;, \u0026#34;ADBE\u0026#34;, \u0026#34;AMD\u0026#34;, \u0026#34;AVGO\u0026#34;, \u0026#34;CRM\u0026#34;, \u0026#34;CSCO\u0026#34;, \u0026#34;IBM\u0026#34;, \u0026#34;INTC\u0026#34;, \u0026#34;MA\u0026#34;, \u0026#34;MSFT\u0026#34;, \u0026#34;NVDA\u0026#34;, \u0026#34;ORCL\u0026#34;, \u0026#34;PYPL\u0026#34;, \u0026#34;QCOM\u0026#34;, \u0026#34;TXN\u0026#34;, \u0026#34;V\u0026#34;]\u0026#39; 42 return [Stock.auto(ticker) for ticker in json.loads(tickers)] 43 44 45class Producer: 46 def __init__(self, bootstrap_servers: list, topic: str): 47 self.bootstrap_servers = bootstrap_servers 48 self.topic = topic 49 self.producer = self.create() 50 51 def create(self): 52 params = { 53 \u0026#34;bootstrap_servers\u0026#34;: self.bootstrap_servers, 54 \u0026#34;key_serializer\u0026#34;: lambda v: json.dumps(v, default=self.serialize).encode(\u0026#34;utf-8\u0026#34;), 55 \u0026#34;value_serializer\u0026#34;: lambda v: json.dumps(v, default=self.serialize).encode(\u0026#34;utf-8\u0026#34;), 56 \u0026#34;api_version\u0026#34;: (2, 8, 1), 57 } 58 if re.search(\u0026#34;9098$\u0026#34;, self.bootstrap_servers[0]): 59 params = { 60 **params, 61 **{\u0026#34;security_protocol\u0026#34;: \u0026#34;SASL_SSL\u0026#34;, \u0026#34;sasl_mechanism\u0026#34;: \u0026#34;AWS_MSK_IAM\u0026#34;}, 62 } 63 return KafkaProducer(**params) 64 65 def send(self, stocks: typing.List[Stock]): 66 for stock in stocks: 67 try: 68 self.producer.send(self.topic, key={\u0026#34;ticker\u0026#34;: stock.ticker}, value=stock.asdict()) 69 except Exception as e: 70 raise RuntimeError(\u0026#34;fails to send a message\u0026#34;) from e 71 self.producer.flush() 72 73 def serialize(self, obj): 74 if isinstance(obj, datetime.datetime): 75 return obj.isoformat() 76 if isinstance(obj, datetime.date): 77 return str(obj) 78 return obj 79 80 81if __name__ == \u0026#34;__main__\u0026#34;: 82 producer = Producer( 83 bootstrap_servers=os.getenv(\u0026#34;BOOTSTRAP_SERVERS\u0026#34;, \u0026#34;localhost:29092\u0026#34;).split(\u0026#34;,\u0026#34;), 84 topic=os.getenv(\u0026#34;TOPIC_NAME\u0026#34;, \u0026#34;stocks-in\u0026#34;), 85 ) 86 max_run = int(os.getenv(\u0026#34;MAX_RUN\u0026#34;, \u0026#34;-1\u0026#34;)) 87 logging.info(f\u0026#34;max run - {max_run}\u0026#34;) 88 current_run = 0 89 while True: 90 current_run += 1 91 logging.info(f\u0026#34;current run - {current_run}\u0026#34;) 92 if current_run - max_run == 0: 93 logging.info(f\u0026#34;reached max run, finish\u0026#34;) 94 producer.producer.close() 95 break 96 producer.send(Stock.create()) 97 secs = random.randint(5, 10) 98 logging.info(f\u0026#34;messages sent... wait {secs} seconds\u0026#34;) 99 time.sleep(secs) Once we start the app, we can check the topic for the source data is created and messages are ingested.\nProcess Data The Flink application is built using the Table API. We have two Kafka topics - one for the source and the other for the sink. Simply put, we can manipulate the records of the topics as tables of unbounded real-time streams with the Table API. In order to read/write records from/to a Kafka topic where the cluster is IAM authenticated, we need to specify the custom Uber Jar that we created earlier - pyflink-getting-started-1.0.0.jar. Note we only need to configure the connector jar when we develop the app locally as the jar file will be specified by the --jarfile option when submitting it to a Flink cluster or deploying via KDA. We also need the application properties file (application_properties.json) in order to be comparable with KDA. The file contains the Flink runtime options in KDA as well as application specific properties. All the properties should be specified when deploying via KDA and, for local development, we keep them as a json file and only the application specific properties are used.\nThe tables for the source and output topics can be created using SQL with options that are related to the Kafka connector. Key options cover the connector name (connector), topic name (topic), bootstrap server address (properties.bootstrap.servers) and format (format). See the connector document for more details about the connector configuration. Note that the security options of the tables are updated for IAM authentication when the bootstrap server argument ends with 9098 using the inject_security_opts function. When it comes to inserting the source records into the output table, we can use either SQL or built-in add_insert method.\nIn the main method, we create all the source and sink tables after mapping relevant application properties. Then the output records are inserted into the output Kafka topic. Note that the output records are printed in the terminal additionally when the app is running locally for ease of checking them.\n1# processor.py 2import os 3import json 4import re 5import logging 6 7import kafka # check if --pyFiles works 8from pyflink.table import EnvironmentSettings, TableEnvironment 9 10logging.basicConfig( 11 level=logging.INFO, 12 format=\u0026#34;%(asctime)s.%(msecs)03d:%(levelname)s:%(name)s:%(message)s\u0026#34;, 13 datefmt=\u0026#34;%Y-%m-%d %H:%M:%S\u0026#34;, 14) 15 16RUNTIME_ENV = os.environ.get(\u0026#34;RUNTIME_ENV\u0026#34;, \u0026#34;KDA\u0026#34;) # KDA, DOCKER, LOCAL 17BOOTSTRAP_SERVERS = os.environ.get(\u0026#34;BOOTSTRAP_SERVERS\u0026#34;) # overwrite app config 18 19logging.info(f\u0026#34;runtime environment - {RUNTIME_ENV}...\u0026#34;) 20 21env_settings = EnvironmentSettings.in_streaming_mode() 22table_env = TableEnvironment.create(env_settings) 23 24APPLICATION_PROPERTIES_FILE_PATH = ( 25 \u0026#34;/etc/flink/application_properties.json\u0026#34; # on kda or docker-compose 26 if RUNTIME_ENV != \u0026#34;LOCAL\u0026#34; 27 else \u0026#34;application_properties.json\u0026#34; 28) 29 30if RUNTIME_ENV != \u0026#34;KDA\u0026#34;: 31 # on non-KDA, multiple jar files can be passed after being delimited by a semicolon 32 CURRENT_DIR = os.path.dirname(os.path.realpath(__file__)) 33 PIPELINE_JAR = \u0026#34;pyflink-getting-started-1.0.0.jar\u0026#34; 34 # PIPELINE_JAR = \u0026#34;uber-jar-for-pyflink-1.0.1.jar\u0026#34; 35 table_env.get_config().set( 36 \u0026#34;pipeline.jars\u0026#34;, f\u0026#34;file://{os.path.join(CURRENT_DIR, \u0026#39;package\u0026#39;, \u0026#39;lib\u0026#39;, PIPELINE_JAR)}\u0026#34; 37 ) 38logging.info(f\u0026#34;app properties file path - {APPLICATION_PROPERTIES_FILE_PATH}\u0026#34;) 39 40 41def get_application_properties(): 42 if os.path.isfile(APPLICATION_PROPERTIES_FILE_PATH): 43 with open(APPLICATION_PROPERTIES_FILE_PATH, \u0026#34;r\u0026#34;) as file: 44 contents = file.read() 45 properties = json.loads(contents) 46 return properties 47 else: 48 raise RuntimeError(f\u0026#34;A file at \u0026#39;{APPLICATION_PROPERTIES_FILE_PATH}\u0026#39; was not found\u0026#34;) 49 50 51def property_map(props: dict, property_group_id: str): 52 for prop in props: 53 if prop[\u0026#34;PropertyGroupId\u0026#34;] == property_group_id: 54 return prop[\u0026#34;PropertyMap\u0026#34;] 55 56 57def inject_security_opts(opts: dict, bootstrap_servers: str): 58 if re.search(\u0026#34;9098$\u0026#34;, bootstrap_servers): 59 opts = { 60 **opts, 61 **{ 62 \u0026#34;properties.security.protocol\u0026#34;: \u0026#34;SASL_SSL\u0026#34;, 63 \u0026#34;properties.sasl.mechanism\u0026#34;: \u0026#34;AWS_MSK_IAM\u0026#34;, 64 \u0026#34;properties.sasl.jaas.config\u0026#34;: \u0026#34;software.amazon.msk.auth.iam.IAMLoginModule required;\u0026#34;, 65 \u0026#34;properties.sasl.client.callback.handler.class\u0026#34;: \u0026#34;software.amazon.msk.auth.iam.IAMClientCallbackHandler\u0026#34;, 66 }, 67 } 68 return \u0026#34;, \u0026#34;.join({f\u0026#34;\u0026#39;{k}\u0026#39; = \u0026#39;{v}\u0026#39;\u0026#34; for k, v in opts.items()}) 69 70 71def create_source_table( 72 table_name: str, topic_name: str, bootstrap_servers: str, startup_mode: str 73): 74 opts = { 75 \u0026#34;connector\u0026#34;: \u0026#34;kafka\u0026#34;, 76 \u0026#34;topic\u0026#34;: topic_name, 77 \u0026#34;properties.bootstrap.servers\u0026#34;: bootstrap_servers, 78 \u0026#34;properties.group.id\u0026#34;: \u0026#34;soruce-group\u0026#34;, 79 \u0026#34;format\u0026#34;: \u0026#34;json\u0026#34;, 80 \u0026#34;scan.startup.mode\u0026#34;: startup_mode, 81 } 82 stmt = f\u0026#34;\u0026#34;\u0026#34; 83 CREATE TABLE {table_name} ( 84 event_time TIMESTAMP(3), 85 ticker VARCHAR(6), 86 price DOUBLE 87 ) 88 WITH ( 89 {inject_security_opts(opts, bootstrap_servers)} 90 ) 91 \u0026#34;\u0026#34;\u0026#34; 92 logging.info(\u0026#34;source table statement...\u0026#34;) 93 logging.info(stmt) 94 return stmt 95 96 97def create_sink_table(table_name: str, topic_name: str, bootstrap_servers: str): 98 opts = { 99 \u0026#34;connector\u0026#34;: \u0026#34;kafka\u0026#34;, 100 \u0026#34;topic\u0026#34;: topic_name, 101 \u0026#34;properties.bootstrap.servers\u0026#34;: bootstrap_servers, 102 \u0026#34;format\u0026#34;: \u0026#34;json\u0026#34;, 103 \u0026#34;key.format\u0026#34;: \u0026#34;json\u0026#34;, 104 \u0026#34;key.fields\u0026#34;: \u0026#34;ticker\u0026#34;, 105 \u0026#34;properties.allow.auto.create.topics\u0026#34;: \u0026#34;true\u0026#34;, 106 } 107 stmt = f\u0026#34;\u0026#34;\u0026#34; 108 CREATE TABLE {table_name} ( 109 event_time TIMESTAMP(3), 110 ticker VARCHAR(6), 111 price DOUBLE 112 ) 113 WITH ( 114 {inject_security_opts(opts, bootstrap_servers)} 115 ) 116 \u0026#34;\u0026#34;\u0026#34; 117 logging.info(\u0026#34;sint table statement...\u0026#34;) 118 logging.info(stmt) 119 return stmt 120 121 122def create_print_table(table_name: str): 123 return f\u0026#34;\u0026#34;\u0026#34; 124 CREATE TABLE {table_name} ( 125 event_time TIMESTAMP(3), 126 ticker VARCHAR(6), 127 price DOUBLE 128 ) 129 WITH ( 130 \u0026#39;connector\u0026#39; = \u0026#39;print\u0026#39; 131 ) 132 \u0026#34;\u0026#34;\u0026#34; 133 134 135def main(): 136 ## map consumer/producer properties 137 props = get_application_properties() 138 # consumer 139 consumer_property_group_key = \u0026#34;consumer.config.0\u0026#34; 140 consumer_properties = property_map(props, consumer_property_group_key) 141 consumer_table_name = consumer_properties[\u0026#34;table.name\u0026#34;] 142 consumer_topic_name = consumer_properties[\u0026#34;topic.name\u0026#34;] 143 consumer_bootstrap_servers = BOOTSTRAP_SERVERS or consumer_properties[\u0026#34;bootstrap.servers\u0026#34;] 144 consumer_startup_mode = consumer_properties[\u0026#34;startup.mode\u0026#34;] 145 # producer 146 producer_property_group_key = \u0026#34;producer.config.0\u0026#34; 147 producer_properties = property_map(props, producer_property_group_key) 148 producer_table_name = producer_properties[\u0026#34;table.name\u0026#34;] 149 producer_topic_name = producer_properties[\u0026#34;topic.name\u0026#34;] 150 producer_bootstrap_servers = BOOTSTRAP_SERVERS or producer_properties[\u0026#34;bootstrap.servers\u0026#34;] 151 # print 152 print_table_name = \u0026#34;sink_print\u0026#34; 153 ## create a souce table 154 table_env.execute_sql( 155 create_source_table( 156 consumer_table_name, 157 consumer_topic_name, 158 consumer_bootstrap_servers, 159 consumer_startup_mode, 160 ) 161 ) 162 ## create sink tables 163 table_env.execute_sql( 164 create_sink_table(producer_table_name, producer_topic_name, producer_bootstrap_servers) 165 ) 166 table_env.execute_sql(create_print_table(\u0026#34;sink_print\u0026#34;)) 167 ## insert into sink tables 168 if RUNTIME_ENV == \u0026#34;LOCAL\u0026#34;: 169 source_table = table_env.from_path(consumer_table_name) 170 statement_set = table_env.create_statement_set() 171 statement_set.add_insert(producer_table_name, source_table) 172 statement_set.add_insert(print_table_name, source_table) 173 statement_set.execute().wait() 174 else: 175 table_result = table_env.execute_sql( 176 f\u0026#34;INSERT INTO {producer_table_name} SELECT * FROM {consumer_table_name}\u0026#34; 177 ) 178 logging.info(table_result.get_job_client().get_job_status()) 179 180 181if __name__ == \u0026#34;__main__\u0026#34;: 182 main() 1// application_properties.json 2[ 3 { 4 \u0026#34;PropertyGroupId\u0026#34;: \u0026#34;kinesis.analytics.flink.run.options\u0026#34;, 5 \u0026#34;PropertyMap\u0026#34;: { 6 \u0026#34;python\u0026#34;: \u0026#34;processor.py\u0026#34;, 7 \u0026#34;jarfile\u0026#34;: \u0026#34;package/lib/pyflink-getting-started-1.0.0.jar\u0026#34;, 8 \u0026#34;pyFiles\u0026#34;: \u0026#34;package/site_packages/\u0026#34; 9 } 10 }, 11 { 12 \u0026#34;PropertyGroupId\u0026#34;: \u0026#34;consumer.config.0\u0026#34;, 13 \u0026#34;PropertyMap\u0026#34;: { 14 \u0026#34;table.name\u0026#34;: \u0026#34;source_table\u0026#34;, 15 \u0026#34;topic.name\u0026#34;: \u0026#34;stocks-in\u0026#34;, 16 \u0026#34;bootstrap.servers\u0026#34;: \u0026#34;localhost:29092\u0026#34;, 17 \u0026#34;startup.mode\u0026#34;: \u0026#34;earliest-offset\u0026#34; 18 } 19 }, 20 { 21 \u0026#34;PropertyGroupId\u0026#34;: \u0026#34;producer.config.0\u0026#34;, 22 \u0026#34;PropertyMap\u0026#34;: { 23 \u0026#34;table.name\u0026#34;: \u0026#34;sink_table\u0026#34;, 24 \u0026#34;topic.name\u0026#34;: \u0026#34;stocks-out\u0026#34;, 25 \u0026#34;bootstrap.servers\u0026#34;: \u0026#34;localhost:29092\u0026#34; 26 } 27 } 28] Run Locally We can run the app locally as following - RUNTIME_ENV=LOCAL python processor.py. The terminal on the right-hand side shows the output records of the Flink app while the left-hand side records logs of the producer app. We can see that the print output from the Flink app gets updated when new source records are sent into the source topic by the producer app.\nWe can also see details of all the topics in Kpow as shown below. The total number of messages matches between the source and output topics but not within partitions.\nRun in Flink Cluster The execution in a terminal is limited for monitoring, and we can inspect and understand what is happening inside Flink using the Flink Web UI. For this, we need to submit the app to the Flink cluster we created earlier. Typically, a Pyflink app can be submitted using the CLI interface by specifying the main application (\u0026ndash;python), Kafka connector artifact file (\u0026ndash;jarfile), and 3rd-party Python packages (\u0026ndash;pyFiles) if necessary. Once submitted, it shows the status with the job ID.\n1$ docker exec jobmanager /opt/flink/bin/flink run \\ 2 --python /etc/flink/processor.py \\ 3 --jarfile /etc/flink/package/lib/pyflink-getting-started-1.0.0.jar \\ 4 --pyFiles /etc/flink/package/site_packages/ \\ 5 -d 62023-08-08 04:21:48.198:INFO:root:runtime environment - DOCKER... 72023-08-08 04:21:49.187:INFO:root:app properties file path - /etc/flink/application_properties.json 82023-08-08 04:21:49.187:INFO:root:source table statement... 92023-08-08 04:21:49.187:INFO:root: 10 CREATE TABLE source_table ( 11 event_time TIMESTAMP(3), 12 ticker VARCHAR(6), 13 price DOUBLE 14 ) 15 WITH ( 16 \u0026#39;properties.sasl.jaas.config\u0026#39; = \u0026#39;software.amazon.msk.auth.iam.IAMLoginModule required;\u0026#39;, \u0026#39;scan.startup.mode\u0026#39; = \u0026#39;earliest-offset\u0026#39;, \u0026#39;properties.sasl.client.callback.handler.class\u0026#39; = \u0026#39;software.amazon.msk.auth.iam.IAMClientCallbackHandler\u0026#39;, \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39;, \u0026#39;properties.bootstrap.servers\u0026#39; = \u0026#39;b-1.kdagettingstarted.j92edp.c3.kafka.ap-southeast-2.amazonaws.com:9098,b-2.kdagettingstarted.j92edp.c3.kafka.ap-southeast-2.amazonaws.com:9098\u0026#39;, \u0026#39;properties.sasl.mechanism\u0026#39; = \u0026#39;AWS_MSK_IAM\u0026#39;, \u0026#39;format\u0026#39; = \u0026#39;json\u0026#39;, \u0026#39;properties.security.protocol\u0026#39; = \u0026#39;SASL_SSL\u0026#39;, \u0026#39;topic\u0026#39; = \u0026#39;stocks-in\u0026#39;, \u0026#39;properties.group.id\u0026#39; = \u0026#39;soruce-group\u0026#39; 17 ) 18 192023-08-08 04:21:49.301:INFO:root:sint table statement... 202023-08-08 04:21:49.301:INFO:root: 21 CREATE TABLE sink_table ( 22 event_time TIMESTAMP(3), 23 ticker VARCHAR(6), 24 price DOUBLE 25 ) 26 WITH ( 27 \u0026#39;topic\u0026#39; = \u0026#39;stocks-out\u0026#39;, \u0026#39;key.fields\u0026#39; = \u0026#39;ticker\u0026#39;, \u0026#39;properties.sasl.jaas.config\u0026#39; = \u0026#39;software.amazon.msk.auth.iam.IAMLoginModule required;\u0026#39;, \u0026#39;properties.sasl.client.callback.handler.class\u0026#39; = \u0026#39;software.amazon.msk.auth.iam.IAMClientCallbackHandler\u0026#39;, \u0026#39;key.format\u0026#39; = \u0026#39;json\u0026#39;, \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39;, \u0026#39;properties.bootstrap.servers\u0026#39; = \u0026#39;b-1.kdagettingstarted.j92edp.c3.kafka.ap-southeast-2.amazonaws.com:9098,b-2.kdagettingstarted.j92edp.c3.kafka.ap-southeast-2.amazonaws.com:9098\u0026#39;, \u0026#39;properties.allow.auto.create.topics\u0026#39; = \u0026#39;true\u0026#39;, \u0026#39;properties.sasl.mechanism\u0026#39; = \u0026#39;AWS_MSK_IAM\u0026#39;, \u0026#39;format\u0026#39; = \u0026#39;json\u0026#39;, \u0026#39;properties.security.protocol\u0026#39; = \u0026#39;SASL_SSL\u0026#39; 28 ) 29 30WARNING: An illegal reflective access operation has occurred 31WARNING: Illegal reflective access by org.apache.flink.api.java.ClosureCleaner (file:/opt/flink/lib/flink-dist-1.15.4.jar) to field java.lang.String.value 32WARNING: Please consider reporting this to the maintainers of org.apache.flink.api.java.ClosureCleaner 33WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations 34WARNING: All illegal access operations will be denied in a future release 35Job has been submitted with JobID 4827bc6fbafd8b628a26f1095dfc28f9 362023-08-08 04:21:56.327:INFO:root:java.util.concurrent.CompletableFuture@2da94b9a[Not completed] We can check the submitted job by listing all jobs as shown below.\n1$ docker exec jobmanager /opt/flink/bin/flink list 2Waiting for response... 3------------------ Running/Restarting Jobs ------------------- 408.08.2023 04:21:52 : 4827bc6fbafd8b628a26f1095dfc28f9 : insert-into_default_catalog.default_database.sink_table (RUNNING) 5-------------------------------------------------------------- 6No scheduled jobs. The Flink Web UI can be accessed on port 8081. In the Overview section, it shows the available task slots, running jobs and completed jobs.\nWe can inspect an individual job in the Jobs menu. It shows key details about a job execution in Overview, Exceptions, TimeLine, Checkpoints and Configuration tabs.\nWe can cancel a job on the web UI or using the CLI. Below shows how to cancel the job we submitted earlier using the CLI.\n1$ docker exec jobmanager /opt/flink/bin/flink cancel 4827bc6fbafd8b628a26f1095dfc28f9 2Cancelling job 4827bc6fbafd8b628a26f1095dfc28f9. 3Cancelled job 4827bc6fbafd8b628a26f1095dfc28f9. Summary In this post, we updated the Pyflink app developed in part 1 by connecting a Kafka cluster on Amazon MSK. The Kafka cluster is authenticated by IAM and the app has additional jar dependency. As Kinesis Data Analytics (KDA) does not allow you to specify multiple pipeline jar files, we had to build a custom Uber Jar that combines multiple jar files. Same as part 1, the app was executed in a virtual environment as well as in a local Flink cluster for improved monitoring with the updated pipeline jar file.\n","date":"August 28, 2023","img":"/blog/2023-08-28-getting-started-with-pyflink-on-aws-part-2/featured.png","lang":"en","langName":"English","largeImg":"/blog/2023-08-28-getting-started-with-pyflink-on-aws-part-2/featured_huff907a4f1fb46be4b7ff165486006a38_64005_500x0_resize_box_3.png","permalink":"/blog/2023-08-28-getting-started-with-pyflink-on-aws-part-2/","series":[{"title":"Getting Started With Pyflink on AWS","url":"/series/getting-started-with-pyflink-on-aws/"}],"smallImg":"/blog/2023-08-28-getting-started-with-pyflink-on-aws-part-2/featured_huff907a4f1fb46be4b7ff165486006a38_64005_180x0_resize_box_3.png","tags":[{"title":"Apache Flink","url":"/tags/apache-flink/"},{"title":"PyFlink","url":"/tags/pyflink/"},{"title":"Apache Kafka","url":"/tags/apache-kafka/"},{"title":"Amazon Managed Service for Apache Flink","url":"/tags/amazon-managed-service-for-apache-flink/"},{"title":"Amazon Managed Flink","url":"/tags/amazon-managed-flink/"},{"title":"Amazon MSK","url":"/tags/amazon-msk/"},{"title":"Python","url":"/tags/python/"},{"title":"Docker","url":"/tags/docker/"},{"title":"Docker Compose","url":"/tags/docker-compose/"}],"timestamp":1693180800,"title":"Getting Started With Pyflink on AWS - Part 2 Local Flink and MSK"},{"categories":[{"title":"Data Streaming","url":"/categories/data-streaming/"}],"content":"Apache Flink is an open-source, unified stream-processing and batch-processing framework. Its core is a distributed streaming data-flow engine that you can use to run real-time stream processing on high-throughput data sources. Currently, it is widely used to build applications for fraud/anomaly detection, rule-based alerting, business process monitoring, and continuous ETL to name a few. On AWS, we can deploy a Flink application via Amazon Kinesis Data Analytics (KDA), Amazon EMR and Amazon EKS. Among those, KDA is the easiest option as it provides the underlying infrastructure for your Apache Flink applications.\nFor those who are new to Flink (Pyflink) and KDA, AWS provides a good resource that guides how to develop a Flink application locally and deploy via KDA. The guide uses Amazon Kinesis Data Stream as a data source and demonstrates how to sink records into multiple destinations - Kinesis Data Stream, Kinesis Firehose Delivery Stream and S3. It can be found in this GitHub project.\nIn this series of posts, we will update one of the examples of the guide by changing the data source and sink into Apache Kafka topics. In part 1, we will discuss how to develop a Flink application that targets a local Kafka cluster. Furthermore, it will be executed in a virtual environment as well as in a local Flink cluster for improved monitoring. The Flink application will be amended to connect a Kafka cluster on Amazon MSK in part 2. The Kafka cluster will be authenticated by IAM access control, and the Flink app needs to change its configuration accordingly by creating a custom Uber Jar file. In part 3, the application will be deployed via KDA using an application package that is saved in S3. The application package is a zip file that includes the application script, custom Uber Jar file, and 3rd-party Python packages. The deployment will be made by Terraform.\nPart 1 Local Flink and Local Kafka (this post) Part 2 Local Flink and MSK Part 3 AWS Managed Flink and MSK [Update 2023-08-30] Amazon Kinesis Data Analytics is renamed into Amazon Managed Service for Apache Flink. In this post, Kinesis Data Analytics (KDA) and Amazon Managed Service for Apache Flink will be used interchangeably.\nArchitecture The Python source data generator (producer.py) sends random stock price records into a Kafka topic. The messages in the source topic are consumed by a Flink application, and it just writes those messages into a different sink topic. This is the simplest application of the AWS guide, and you may try other examples if interested.\nInfrastructure A Kafka cluster and management app (Kpow) are created using Docker while the Python apps including the Flink app run in a virtual environment in the first trial. After that the Flink app is submitted to a local Flink cluster for improved monitoring. The Flink cluster is also created using Docker. The source can be found in the GitHub repository of this post.\nPreparation As discussed later, the Flink application needs the Apache Kafka SQL Connector artifact (flink-sql-connector-kafka-1.15.2.jar) in order to connect a Kafka cluster. Also, the kafka-python package is downloaded to check if --pyFiles option works when submitting the app to a Flink cluster or deploying via KDA. They can be downloaded by executing the following script.\n1# build.sh 2#!/usr/bin/env bash 3SCRIPT_DIR=\u0026#34;$(cd $(dirname \u0026#34;$0\u0026#34;); pwd)\u0026#34; 4SRC_PATH=$SCRIPT_DIR/package 5rm -rf $SRC_PATH \u0026amp;\u0026amp; mkdir -p $SRC_PATH/lib 6 7## Download flink sql connector kafka 8echo \u0026#34;download flink sql connector kafka...\u0026#34; 9VERSION=1.15.2 10FILE_NAME=flink-sql-connector-kafka-$VERSION 11DOWNLOAD_URL=https://repo.maven.apache.org/maven2/org/apache/flink/flink-sql-connector-kafka/$VERSION/flink-sql-connector-kafka-$VERSION.jar 12curl -L -o $SRC_PATH/lib/$FILE_NAME.jar ${DOWNLOAD_URL} 13 14## Install pip packages 15echo \u0026#34;install and zip pip packages...\u0026#34; 16pip install -r requirements.txt --target $SRC_PATH/site_packages 17 18## Package pyflink app 19echo \u0026#34;package pyflink app\u0026#34; 20zip -r kda-package.zip processor.py package/lib package/site_packages Once downloaded, the Kafka SQL artifact and python package can be found in the lib and site_packages folders respectively as shown below.\nKafka Cluster A Kafka cluster with a single broker and zookeeper node is used in this post. The broker has two listeners and the port 9092 and 29092 are used for internal and external communication respectively. The default number of topic partitions is set to 2. Details about Kafka cluster setup can be found in this post.\nThe Kpow CE is used for ease of monitoring Kafka topics and related resources. The bootstrap server address is added as an environment variable. See this post for details about Kafka management apps.\nThe Kafka cluster can be started by docker-compose -f compose-kafka.yml up -d.\n1# compose-kafka.yml 2version: \u0026#34;3.5\u0026#34; 3 4services: 5 zookeeper: 6 image: bitnami/zookeeper:3.5 7 container_name: zookeeper 8 ports: 9 - \u0026#34;2181\u0026#34; 10 networks: 11 - kafkanet 12 environment: 13 - ALLOW_ANONYMOUS_LOGIN=yes 14 volumes: 15 - zookeeper_data:/bitnami/zookeeper 16 kafka-0: 17 image: bitnami/kafka:2.8.1 18 container_name: kafka-0 19 expose: 20 - 9092 21 ports: 22 - \u0026#34;29092:29092\u0026#34; 23 networks: 24 - kafkanet 25 environment: 26 - ALLOW_PLAINTEXT_LISTENER=yes 27 - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181 28 - KAFKA_CFG_BROKER_ID=0 29 - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT 30 - KAFKA_CFG_LISTENERS=INTERNAL://:9092,EXTERNAL://:29092 31 - KAFKA_CFG_ADVERTISED_LISTENERS=INTERNAL://kafka-0:9092,EXTERNAL://localhost:29092 32 - KAFKA_CFG_INTER_BROKER_LISTENER_NAME=INTERNAL 33 - KAFKA_CFG_NUM_PARTITIONS=2 34 volumes: 35 - kafka_0_data:/bitnami/kafka 36 depends_on: 37 - zookeeper 38 kpow: 39 image: factorhouse/kpow-ce:91.2.1 40 container_name: kpow 41 ports: 42 - \u0026#34;3000:3000\u0026#34; 43 networks: 44 - kafkanet 45 environment: 46 BOOTSTRAP: kafka-0:9092 47 depends_on: 48 - zookeeper 49 - kafka-0 50 51networks: 52 kafkanet: 53 name: kafka-network 54 55volumes: 56 zookeeper_data: 57 driver: local 58 name: zookeeper_data 59 kafka_0_data: 60 driver: local 61 name: kafka_0_data Flink Cluster In order to run a PyFlink application in a Flink cluster, we need to install Python and the apache-flink package additionally. We can create a custom Docker image based on the official Flink image. I chose the version 1.15.2 as it is the recommended Flink version by KDA. The custom image can be built by docker build -t pyflink:1.15.2-scala_2.12 ..\n1FROM flink:1.15.2-scala_2.12 2 3ARG PYTHON_VERSION 4ENV PYTHON_VERSION=${PYTHON_VERSION:-3.8.10} 5ARG FLINK_VERSION 6ENV FLINK_VERSION=${FLINK_VERSION:-1.15.2} 7 8# Currently only Python 3.6, 3.7 and 3.8 are supported officially. 9RUN apt-get update -y \u0026amp;\u0026amp; \\ 10 apt-get install -y build-essential libssl-dev zlib1g-dev libbz2-dev libffi-dev \u0026amp;\u0026amp; \\ 11 wget https://www.python.org/ftp/python/${PYTHON_VERSION}/Python-${PYTHON_VERSION}.tgz \u0026amp;\u0026amp; \\ 12 tar -xvf Python-${PYTHON_VERSION}.tgz \u0026amp;\u0026amp; \\ 13 cd Python-${PYTHON_VERSION} \u0026amp;\u0026amp; \\ 14 ./configure --without-tests --enable-shared \u0026amp;\u0026amp; \\ 15 make -j6 \u0026amp;\u0026amp; \\ 16 make install \u0026amp;\u0026amp; \\ 17 ldconfig /usr/local/lib \u0026amp;\u0026amp; \\ 18 cd .. \u0026amp;\u0026amp; rm -f Python-${PYTHON_VERSION}.tgz \u0026amp;\u0026amp; rm -rf Python-${PYTHON_VERSION} \u0026amp;\u0026amp; \\ 19 ln -s /usr/local/bin/python3 /usr/local/bin/python \u0026amp;\u0026amp; \\ 20 apt-get clean \u0026amp;\u0026amp; \\ 21 rm -rf /var/lib/apt/lists/* 22 23# install PyFlink 24RUN pip3 install apache-flink==${FLINK_VERSION} A Flink cluster is made up of a single Job Manger and Task Manager, and the cluster runs in the Session Mode where one or more Flink applications can be submitted/executed simultaneously. See this page for details about how to create a Flink cluster using docker-compose.\nTwo environment variables are configured to adjust the application behaviour. The RUNTIME_ENV is set to DOCKER, and it determines which pipeline jar and application property file to choose. Also, the BOOTSTRAP_SERVERS overrides the Kafka bootstrap server address value from the application property file. We will make use of it to configure the bootstrap server address dynamically for MSK in part 2. Finally, the current directory is volume-mapped into /etc/flink so that the application and related resources can be available in the Flink cluster.\nThe Flink cluster can be started by docker-compose -f compose-flink.yml up -d.\n1version: \u0026#34;3.5\u0026#34; 2 3services: 4 jobmanager: 5 image: pyflink:1.15.2-scala_2.12 6 container_name: jobmanager 7 command: jobmanager 8 ports: 9 - \u0026#34;8081:8081\u0026#34; 10 networks: 11 - kafkanet 12 environment: 13 - | 14 FLINK_PROPERTIES= 15 jobmanager.rpc.address: jobmanager 16 state.backend: filesystem 17 state.checkpoints.dir: file:///tmp/flink-checkpoints 18 heartbeat.interval: 1000 19 heartbeat.timeout: 5000 20 rest.flamegraph.enabled: true 21 web.backpressure.refresh-interval: 10000 22 - RUNTIME_ENV=DOCKER 23 - BOOTSTRAP_SERVERS=kafka-0:9092 24 volumes: 25 - $PWD:/etc/flink 26 taskmanager: 27 image: pyflink:1.15.2-scala_2.12 28 container_name: taskmanager 29 command: taskmanager 30 networks: 31 - kafkanet 32 volumes: 33 - flink_data:/tmp/ 34 - $PWD:/etc/flink 35 environment: 36 - | 37 FLINK_PROPERTIES= 38 jobmanager.rpc.address: jobmanager 39 taskmanager.numberOfTaskSlots: 3 40 state.backend: filesystem 41 state.checkpoints.dir: file:///tmp/flink-checkpoints 42 heartbeat.interval: 1000 43 heartbeat.timeout: 5000 44 - RUNTIME_ENV=DOCKER 45 - BOOTSTRAP_SERVERS=kafka-0:9092 46 depends_on: 47 - jobmanager 48 49networks: 50 kafkanet: 51 external: true 52 name: kafka-network 53 54volumes: 55 flink_data: 56 driver: local 57 name: flink_data Virtual Environment As mentioned earlier, all Python apps run in a virtual environment, and we have the following pip packages. We use the version 1.15.2 of the apache-flink package because it is the recommended version by KDA. We also need the kafka-python package for source data generation. The pip packages can be installed by pip install -r requirements-dev.txt.\n1# requirements.txt 2kafka-python==2.0.2 3 4# requirements-dev.txt 5-r requirements.txt 6apache-flink==1.15.2 7black==19.10b0 8pytest 9pytest-cov Application Source Data A single Python script is created to generate fake stock price records. The class for the stock record has the asdict, auto and create methods. The create method generates a list of records where each element is instantiated by the auto method. Those records are sent into the relevant Kafka topic after being converted into a dictionary by the asdict method.\nA Kafka producer is created as an attribute of the Producer class. The source records are sent into the relevant topic by the send method. Note that both the key and value of the messages are serialized as json.\nThe data generator can be started simply by python producer.py.\n1# producer.py 2import os 3import datetime 4import time 5import json 6import typing 7import random 8import logging 9import dataclasses 10 11from kafka import KafkaProducer 12 13logging.basicConfig( 14 level=logging.INFO, 15 format=\u0026#34;%(asctime)s.%(msecs)03d:%(levelname)s:%(name)s:%(message)s\u0026#34;, 16 datefmt=\u0026#34;%Y-%m-%d %H:%M:%S\u0026#34;, 17) 18 19datetime.datetime.now().strftime(\u0026#34;%Y-%m-%d %H:%M:%S.%f\u0026#34;) 20 21 22@dataclasses.dataclass 23class Stock: 24 event_time: str 25 ticker: str 26 price: float 27 28 def asdict(self): 29 return dataclasses.asdict(self) 30 31 @classmethod 32 def auto(cls, ticker: str): 33 # event_time = datetime.datetime.now().isoformat(timespec=\u0026#34;milliseconds\u0026#34;) 34 event_time = datetime.datetime.now().strftime(\u0026#34;%Y-%m-%d %H:%M:%S.%f\u0026#34;) 35 price = round(random.random() * 100, 2) 36 return cls(event_time, ticker, price) 37 38 @staticmethod 39 def create(): 40 tickers = \u0026#39;[\u0026#34;AAPL\u0026#34;, \u0026#34;ACN\u0026#34;, \u0026#34;ADBE\u0026#34;, \u0026#34;AMD\u0026#34;, \u0026#34;AVGO\u0026#34;, \u0026#34;CRM\u0026#34;, \u0026#34;CSCO\u0026#34;, \u0026#34;IBM\u0026#34;, \u0026#34;INTC\u0026#34;, \u0026#34;MA\u0026#34;, \u0026#34;MSFT\u0026#34;, \u0026#34;NVDA\u0026#34;, \u0026#34;ORCL\u0026#34;, \u0026#34;PYPL\u0026#34;, \u0026#34;QCOM\u0026#34;, \u0026#34;TXN\u0026#34;, \u0026#34;V\u0026#34;]\u0026#39; 41 return [Stock.auto(ticker) for ticker in json.loads(tickers)] 42 43 44class Producer: 45 def __init__(self, bootstrap_servers: list, topic: str): 46 self.bootstrap_servers = bootstrap_servers 47 self.topic = topic 48 self.producer = self.create() 49 50 def create(self): 51 return KafkaProducer( 52 bootstrap_servers=self.bootstrap_servers, 53 key_serializer=lambda v: json.dumps(v, default=self.serialize).encode(\u0026#34;utf-8\u0026#34;), 54 value_serializer=lambda v: json.dumps(v, default=self.serialize).encode(\u0026#34;utf-8\u0026#34;), 55 ) 56 57 def send(self, stocks: typing.List[Stock]): 58 for stock in stocks: 59 try: 60 self.producer.send(self.topic, key={\u0026#34;ticker\u0026#34;: stock.ticker}, value=stock.asdict()) 61 except Exception as e: 62 raise RuntimeError(\u0026#34;fails to send a message\u0026#34;) from e 63 self.producer.flush() 64 65 def serialize(self, obj): 66 if isinstance(obj, datetime.datetime): 67 return obj.isoformat() 68 if isinstance(obj, datetime.date): 69 return str(obj) 70 return obj 71 72 73if __name__ == \u0026#34;__main__\u0026#34;: 74 producer = Producer( 75 bootstrap_servers=os.getenv(\u0026#34;BOOTSTRAP_SERVERS\u0026#34;, \u0026#34;localhost:29092\u0026#34;).split(\u0026#34;,\u0026#34;), 76 topic=os.getenv(\u0026#34;TOPIC_NAME\u0026#34;, \u0026#34;stocks-in\u0026#34;), 77 ) 78 max_run = int(os.getenv(\u0026#34;MAX_RUN\u0026#34;, \u0026#34;-1\u0026#34;)) 79 logging.info(f\u0026#34;max run - {max_run}\u0026#34;) 80 current_run = 0 81 while True: 82 current_run += 1 83 logging.info(f\u0026#34;current run - {current_run}\u0026#34;) 84 if current_run - max_run == 0: 85 logging.info(f\u0026#34;reached max run, finish\u0026#34;) 86 producer.producer.close() 87 break 88 producer.send(Stock.create()) 89 secs = random.randint(5, 10) 90 logging.info(f\u0026#34;messages sent... wait {secs} seconds\u0026#34;) 91 time.sleep(secs) Once we start the app, we can check the topic for the source data is created and messages are ingested in Kpow.\nProcess Data The Flink application is built using the Table API. We have two Kafka topics - one for the source and the other for the sink. Simply put, we can manipulate the records of the topics as tables of unbounded real-time streams with the Table API. In order to read/write records from/to a Kafka topic, we need to specify the Apache Kafka SQL Connector artifact that we downloaded earlier as the pipeline jar. Note we only need to configure the connector jar when we develop the app locally as the jar file will be specified by the --jarfile option when submitting it to a Flink cluster or deploying via KDA. We also need the application properties file (application_properties.json) in order to be comparable with KDA. The file contains the Flink runtime options in KDA as well as application specific properties. All the properties should be specified when deploying via KDA and, for local development, we keep them as a json file and only the application specific properties are used.\nThe tables for the source and output topics can be created using SQL with options that are related to the Kafka connector. Key options cover the connector name (connector), topic name (topic), bootstrap server address (properties.bootstrap.servers) and format (format). See the connector document for more details about the connector configuration. When it comes to inserting the source records into the output table, we can use either SQL or built-in add_insert method.\nIn the main method, we create all the source and sink tables after mapping relevant application properties. Then the output records are inserted into the output Kafka topic. Note that the output records are printed in the terminal additionally when the app is running locally for ease of checking them.\n1# processor.py 2import os 3import json 4import re 5import logging 6 7import kafka # check if --pyFiles works 8from pyflink.table import EnvironmentSettings, TableEnvironment 9 10logging.basicConfig( 11 level=logging.INFO, 12 format=\u0026#34;%(asctime)s.%(msecs)03d:%(levelname)s:%(name)s:%(message)s\u0026#34;, 13 datefmt=\u0026#34;%Y-%m-%d %H:%M:%S\u0026#34;, 14) 15 16RUNTIME_ENV = os.environ.get(\u0026#34;RUNTIME_ENV\u0026#34;, \u0026#34;KDA\u0026#34;) # KDA, DOCKER, LOCAL 17BOOTSTRAP_SERVERS = os.environ.get(\u0026#34;BOOTSTRAP_SERVERS\u0026#34;) # overwrite app config 18 19logging.info(f\u0026#34;runtime environment - {RUNTIME_ENV}...\u0026#34;) 20 21env_settings = EnvironmentSettings.in_streaming_mode() 22table_env = TableEnvironment.create(env_settings) 23 24APPLICATION_PROPERTIES_FILE_PATH = ( 25 \u0026#34;/etc/flink/application_properties.json\u0026#34; # on kda or docker-compose 26 if RUNTIME_ENV != \u0026#34;LOCAL\u0026#34; 27 else \u0026#34;application_properties.json\u0026#34; 28) 29 30if RUNTIME_ENV != \u0026#34;KDA\u0026#34;: 31 # on non-KDA, multiple jar files can be passed after being delimited by a semicolon 32 CURRENT_DIR = os.path.dirname(os.path.realpath(__file__)) 33 PIPELINE_JAR = \u0026#34;flink-sql-connector-kafka-1.15.2.jar\u0026#34; 34 table_env.get_config().set( 35 \u0026#34;pipeline.jars\u0026#34;, f\u0026#34;file://{os.path.join(CURRENT_DIR, \u0026#39;package\u0026#39;, \u0026#39;lib\u0026#39;, PIPELINE_JAR)}\u0026#34; 36 ) 37logging.info(f\u0026#34;app properties file path - {APPLICATION_PROPERTIES_FILE_PATH}\u0026#34;) 38 39 40def get_application_properties(): 41 if os.path.isfile(APPLICATION_PROPERTIES_FILE_PATH): 42 with open(APPLICATION_PROPERTIES_FILE_PATH, \u0026#34;r\u0026#34;) as file: 43 contents = file.read() 44 properties = json.loads(contents) 45 return properties 46 else: 47 raise RuntimeError(f\u0026#34;A file at \u0026#39;{APPLICATION_PROPERTIES_FILE_PATH}\u0026#39; was not found\u0026#34;) 48 49 50def property_map(props: dict, property_group_id: str): 51 for prop in props: 52 if prop[\u0026#34;PropertyGroupId\u0026#34;] == property_group_id: 53 return prop[\u0026#34;PropertyMap\u0026#34;] 54 55 56def create_source_table( 57 table_name: str, topic_name: str, bootstrap_servers: str, startup_mode: str 58): 59 stmt = f\u0026#34;\u0026#34;\u0026#34; 60 CREATE TABLE {table_name} ( 61 event_time TIMESTAMP(3), 62 ticker VARCHAR(6), 63 price DOUBLE 64 ) 65 WITH ( 66 \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39;, 67 \u0026#39;topic\u0026#39; = \u0026#39;{topic_name}\u0026#39;, 68 \u0026#39;properties.bootstrap.servers\u0026#39; = \u0026#39;{bootstrap_servers}\u0026#39;, 69 \u0026#39;properties.group.id\u0026#39; = \u0026#39;source-group\u0026#39;, 70 \u0026#39;format\u0026#39; = \u0026#39;json\u0026#39;, 71 \u0026#39;scan.startup.mode\u0026#39; = \u0026#39;{startup_mode}\u0026#39; 72 ) 73 \u0026#34;\u0026#34;\u0026#34; 74 logging.info(\u0026#34;source table statement...\u0026#34;) 75 logging.info(stmt) 76 return stmt 77 78 79def create_sink_table(table_name: str, topic_name: str, bootstrap_servers: str): 80 stmt = f\u0026#34;\u0026#34;\u0026#34; 81 CREATE TABLE {table_name} ( 82 event_time TIMESTAMP(3), 83 ticker VARCHAR(6), 84 price DOUBLE 85 ) 86 WITH ( 87 \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39;, 88 \u0026#39;topic\u0026#39; = \u0026#39;{topic_name}\u0026#39;, 89 \u0026#39;properties.bootstrap.servers\u0026#39; = \u0026#39;{bootstrap_servers}\u0026#39;, 90 \u0026#39;format\u0026#39; = \u0026#39;json\u0026#39;, 91 \u0026#39;key.format\u0026#39; = \u0026#39;json\u0026#39;, 92 \u0026#39;key.fields\u0026#39; = \u0026#39;ticker\u0026#39;, 93 \u0026#39;properties.allow.auto.create.topics\u0026#39; = \u0026#39;true\u0026#39; 94 ) 95 \u0026#34;\u0026#34;\u0026#34; 96 logging.info(\u0026#34;sint table statement...\u0026#34;) 97 logging.info(stmt) 98 return stmt 99 100 101def create_print_table(table_name: str): 102 return f\u0026#34;\u0026#34;\u0026#34; 103 CREATE TABLE {table_name} ( 104 event_time TIMESTAMP(3), 105 ticker VARCHAR(6), 106 price DOUBLE 107 ) 108 WITH ( 109 \u0026#39;connector\u0026#39; = \u0026#39;print\u0026#39; 110 ) 111 \u0026#34;\u0026#34;\u0026#34; 112 113 114def main(): 115 ## map consumer/producer properties 116 props = get_application_properties() 117 # consumer 118 consumer_property_group_key = \u0026#34;consumer.config.0\u0026#34; 119 consumer_properties = property_map(props, consumer_property_group_key) 120 consumer_table_name = consumer_properties[\u0026#34;table.name\u0026#34;] 121 consumer_topic_name = consumer_properties[\u0026#34;topic.name\u0026#34;] 122 consumer_bootstrap_servers = BOOTSTRAP_SERVERS or consumer_properties[\u0026#34;bootstrap.servers\u0026#34;] 123 consumer_startup_mode = consumer_properties[\u0026#34;startup.mode\u0026#34;] 124 # producer 125 producer_property_group_key = \u0026#34;producer.config.0\u0026#34; 126 producer_properties = property_map(props, producer_property_group_key) 127 producer_table_name = producer_properties[\u0026#34;table.name\u0026#34;] 128 producer_topic_name = producer_properties[\u0026#34;topic.name\u0026#34;] 129 producer_bootstrap_servers = BOOTSTRAP_SERVERS or producer_properties[\u0026#34;bootstrap.servers\u0026#34;] 130 # print 131 print_table_name = \u0026#34;sink_print\u0026#34; 132 ## create a souce table 133 table_env.execute_sql( 134 create_source_table( 135 consumer_table_name, 136 consumer_topic_name, 137 consumer_bootstrap_servers, 138 consumer_startup_mode, 139 ) 140 ) 141 ## create sink tables 142 table_env.execute_sql( 143 create_sink_table(producer_table_name, producer_topic_name, producer_bootstrap_servers) 144 ) 145 table_env.execute_sql(create_print_table(\u0026#34;sink_print\u0026#34;)) 146 ## insert into sink tables 147 if RUNTIME_ENV == \u0026#34;LOCAL\u0026#34;: 148 source_table = table_env.from_path(consumer_table_name) 149 statement_set = table_env.create_statement_set() 150 statement_set.add_insert(producer_table_name, source_table) 151 statement_set.add_insert(print_table_name, source_table) 152 statement_set.execute().wait() 153 else: 154 table_result = table_env.execute_sql( 155 f\u0026#34;INSERT INTO {producer_table_name} SELECT * FROM {consumer_table_name}\u0026#34; 156 ) 157 logging.info(table_result.get_job_client().get_job_status()) 158 159 160if __name__ == \u0026#34;__main__\u0026#34;: 161 main() 1// application_properties.json 2[ 3 { 4 \u0026#34;PropertyGroupId\u0026#34;: \u0026#34;kinesis.analytics.flink.run.options\u0026#34;, 5 \u0026#34;PropertyMap\u0026#34;: { 6 \u0026#34;python\u0026#34;: \u0026#34;processor.py\u0026#34;, 7 \u0026#34;jarfile\u0026#34;: \u0026#34;package/lib/flink-sql-connector-kinesis-1.15.2.jar\u0026#34;, 8 \u0026#34;pyFiles\u0026#34;: \u0026#34;package/site_packages/\u0026#34; 9 } 10 }, 11 { 12 \u0026#34;PropertyGroupId\u0026#34;: \u0026#34;consumer.config.0\u0026#34;, 13 \u0026#34;PropertyMap\u0026#34;: { 14 \u0026#34;table.name\u0026#34;: \u0026#34;source_table\u0026#34;, 15 \u0026#34;topic.name\u0026#34;: \u0026#34;stocks-in\u0026#34;, 16 \u0026#34;bootstrap.servers\u0026#34;: \u0026#34;localhost:29092\u0026#34;, 17 \u0026#34;startup.mode\u0026#34;: \u0026#34;earliest-offset\u0026#34; 18 } 19 }, 20 { 21 \u0026#34;PropertyGroupId\u0026#34;: \u0026#34;producer.config.0\u0026#34;, 22 \u0026#34;PropertyMap\u0026#34;: { 23 \u0026#34;table.name\u0026#34;: \u0026#34;sink_table\u0026#34;, 24 \u0026#34;topic.name\u0026#34;: \u0026#34;stocks-out\u0026#34;, 25 \u0026#34;bootstrap.servers\u0026#34;: \u0026#34;localhost:29092\u0026#34; 26 } 27 } 28] Run Locally We can run the app locally as following - RUNTIME_ENV=LOCAL python processor.py. The terminal on the right-hand side shows the output records of the Flink app while the left-hand side records logs of the producer app. We can see that the print output from the Flink app gets updated when new source records are sent into the source topic by the producer app.\nWe can also see details of all the topics in Kpow as shown below. The total number of messages matches between the source and output topics but not within partitions.\nRun in Flink Cluster The execution in a terminal is limited for monitoring, and we can inspect and understand what is happening inside Flink using the Flink Web UI. For this, we need to submit the app to the Flink cluster we created earlier. Typically, a Pyflink app can be submitted using the CLI interface by specifying the main application (\u0026ndash;python), Kafka connector artifact file (\u0026ndash;jarfile), and 3rd-party Python packages (\u0026ndash;pyFiles) if necessary. Once submitted, it shows the status with the job ID.\n1$ docker exec jobmanager /opt/flink/bin/flink run \\ 2 --python /etc/flink/processor.py \\ 3 --jarfile /etc/flink/package/lib/flink-sql-connector-kafka-1.15.2.jar \\ 4 --pyFiles /etc/flink/package/site_packages/ \\ 5 -d 62023-08-08 02:07:13.220:INFO:root:runtime environment - DOCKER... 72023-08-08 02:07:14.341:INFO:root:app properties file path - /etc/flink/application_properties.json 82023-08-08 02:07:14.341:INFO:root:source table statement... 92023-08-08 02:07:14.341:INFO:root: 10 CREATE TABLE source_table ( 11 event_time TIMESTAMP(3), 12 ticker VARCHAR(6), 13 price DOUBLE 14 ) 15 WITH ( 16 \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39;, 17 \u0026#39;topic\u0026#39; = \u0026#39;stocks-in\u0026#39;, 18 \u0026#39;properties.bootstrap.servers\u0026#39; = \u0026#39;kafka-0:9092\u0026#39;, 19 \u0026#39;properties.group.id\u0026#39; = \u0026#39;source-group\u0026#39;, 20 \u0026#39;format\u0026#39; = \u0026#39;json\u0026#39;, 21 \u0026#39;scan.startup.mode\u0026#39; = \u0026#39;earliest-offset\u0026#39; 22 ) 23 242023-08-08 02:07:14.439:INFO:root:sint table statement... 252023-08-08 02:07:14.439:INFO:root: 26 CREATE TABLE sink_table ( 27 event_time TIMESTAMP(3), 28 ticker VARCHAR(6), 29 price DOUBLE 30 ) 31 WITH ( 32 \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39;, 33 \u0026#39;topic\u0026#39; = \u0026#39;stocks-out\u0026#39;, 34 \u0026#39;properties.bootstrap.servers\u0026#39; = \u0026#39;kafka-0:9092\u0026#39;, 35 \u0026#39;format\u0026#39; = \u0026#39;json\u0026#39;, 36 \u0026#39;key.format\u0026#39; = \u0026#39;json\u0026#39;, 37 \u0026#39;key.fields\u0026#39; = \u0026#39;ticker\u0026#39;, 38 \u0026#39;properties.allow.auto.create.topics\u0026#39; = \u0026#39;true\u0026#39; 39 ) 40 41WARNING: An illegal reflective access operation has occurred 42WARNING: Illegal reflective access by org.apache.flink.api.java.ClosureCleaner (file:/opt/flink/lib/flink-dist-1.15.4.jar) to field java.lang.String.value 43WARNING: Please consider reporting this to the maintainers of org.apache.flink.api.java.ClosureCleaner 44WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations 45WARNING: All illegal access operations will be denied in a future release 46Job has been submitted with JobID 02d83c46d646aa498a986c0a9335e276 472023-08-08 02:07:23.010:INFO:root:java.util.concurrent.CompletableFuture@34e729a3[Not completed] We can check the submitted job by listing all jobs as shown below.\n1$ docker exec jobmanager /opt/flink/bin/flink list 2Waiting for response... 3------------------ Running/Restarting Jobs ------------------- 408.08.2023 02:07:18 : 02d83c46d646aa498a986c0a9335e276 : insert-into_default_catalog.default_database.sink_table (RUNNING) 5-------------------------------------------------------------- 6No scheduled jobs. The Flink Web UI can be accessed on port 8081. In the Overview section, it shows the available task slots, running jobs and completed jobs.\nWe can inspect an individual job in the Jobs menu. It shows key details about a job execution in Overview, Exceptions, TimeLine, Checkpoints and Configuration tabs.\nWe can cancel a job on the web UI or using the CLI. Below shows how to cancel the job we submitted earlier using the CLI.\n1$ docker exec jobmanager /opt/flink/bin/flink cancel 02d83c46d646aa498a986c0a9335e276 2Cancelling job 02d83c46d646aa498a986c0a9335e276. 3Cancelled job 02d83c46d646aa498a986c0a9335e276. Summary Apache Flink is widely used for building real-time stream processing applications. On AWS, Kinesis Data Analytics (KDA) is the easiest option to develop a Flink app as it provides the underlying infrastructure. Updating a guide from AWS, this series of posts discuss how to develop and deploy a Flink (Pyflink) application via KDA where the data source and sink are Kafka topics. In part 1, the app was developed locally targeting a Kafka cluster created by Docker. Furthermore, it was executed in a virtual environment as well as in a local Flink cluster for improved monitoring.\n","date":"August 17, 2023","img":"/blog/2023-08-17-getting-started-with-pyflink-on-aws-part-1/featured.png","lang":"en","langName":"English","largeImg":"/blog/2023-08-17-getting-started-with-pyflink-on-aws-part-1/featured_hu6470c0101df93a224de5159e363e8665_55960_500x0_resize_box_3.png","permalink":"/blog/2023-08-17-getting-started-with-pyflink-on-aws-part-1/","series":[{"title":"Getting Started With Pyflink on AWS","url":"/series/getting-started-with-pyflink-on-aws/"}],"smallImg":"/blog/2023-08-17-getting-started-with-pyflink-on-aws-part-1/featured_hu6470c0101df93a224de5159e363e8665_55960_180x0_resize_box_3.png","tags":[{"title":"Apache Flink","url":"/tags/apache-flink/"},{"title":"PyFlink","url":"/tags/pyflink/"},{"title":"Apache Kafka","url":"/tags/apache-kafka/"},{"title":"Amazon Managed Service for Apache Flink","url":"/tags/amazon-managed-service-for-apache-flink/"},{"title":"Amazon Managed Flink","url":"/tags/amazon-managed-flink/"},{"title":"Amazon MSK","url":"/tags/amazon-msk/"},{"title":"Python","url":"/tags/python/"},{"title":"Docker","url":"/tags/docker/"},{"title":"Docker Compose","url":"/tags/docker-compose/"}],"timestamp":1692230400,"title":"Getting Started With Pyflink on AWS - Part 1 Local Flink and Local Kafka"},{"categories":[{"title":"Data Streaming","url":"/categories/data-streaming/"}],"content":"Apache Flink is an open-source, unified stream-processing and batch-processing framework. Its core is a distributed streaming data-flow engine that you can use to run real-time stream processing on high-throughput data sources. Currently, it is widely used to build applications for fraud/anomaly detection, rule-based alerting, business process monitoring, and continuous ETL to name a few. On AWS, we can deploy a Flink application via Amazon Kinesis Data Analytics (KDA), Amazon EMR and Amazon EKS. Among those, KDA is the easiest option as it provides the underlying infrastructure for your Apache Flink applications.\nThere are a number of AWS workshops and blog posts where we can learn Flink development on AWS and one of those is AWS Kafka and DynamoDB for real time fraud detection. While this workshop targets a Flink application on KDA, it would have been easier if it illustrated local development before moving into deployment via KDA. In this series of posts, we will re-implement the fraud detection application of the workshop for those who are new to Flink and KDA. Specifically the app will be developed locally using Docker in part 1, and it will be deployed via KDA in part 2.\nPart 1 Local Development (this post) Part 2 Deployment via AWS Managed Flink [Update 2023-08-30] Amazon Kinesis Data Analytics is renamed into Amazon Managed Service for Apache Flink. In this post, Kinesis Data Analytics (KDA) and Amazon Managed Service for Apache Flink will be used interchangeably.\nArchitecture There are two Python applications that send transaction and flagged account records into the corresponding topics - the transaction app sends records indefinitely in a loop. Both the topics are consumed by a Flink application, and it filters the transactions from the flagged accounts followed by sending them into an output topic of flagged transactions. Finally, the flagged transaction records are sent into a DynamoDB table by the Camel DynamoDB sink connector in order to serve real-time requests from an API.\nInfrastructure The Kafka cluster, Kafka connect and management app (kpow) are created using Docker while the python apps including the Flink app run in a virtual environment. The source can be found in the GitHub repository of this post.\nPreparation As discussed later, the Flink application needs the Kafka connector artifact (flink-sql-connector-kafka-1.15.2.jar) in order to connect a Kafka cluster. Also, the source of the Camel DynamoDB sink connector should be available in the Kafka connect service. They can be downloaded by executing the following script.\n1# build.sh 2#!/usr/bin/env bash 3PKG_ALL=\u0026#34;${PKG_ALL:-no}\u0026#34; 4 5SCRIPT_DIR=\u0026#34;$(cd $(dirname \u0026#34;$0\u0026#34;); pwd)\u0026#34; 6 7#### Steps to package the flink app 8SRC_PATH=$SCRIPT_DIR/package 9rm -rf $SRC_PATH \u0026amp;\u0026amp; mkdir -p $SRC_PATH/lib 10 11## Download flink sql connector kafka 12echo \u0026#34;download flink sql connector kafka...\u0026#34; 13VERSION=1.15.2 14FILE_NAME=flink-sql-connector-kafka-$VERSION 15FLINK_SRC_DOWNLOAD_URL=https://repo.maven.apache.org/maven2/org/apache/flink/flink-sql-connector-kafka/$VERSION/flink-sql-connector-kafka-$VERSION.jar 16curl -L -o $SRC_PATH/lib/$FILE_NAME.jar ${FLINK_SRC_DOWNLOAD_URL} 17 18## Install pip packages 19echo \u0026#34;install and zip pip packages...\u0026#34; 20pip3 install -r requirements.txt --target $SRC_PATH/site_packages 21 22if [ $PKG_ALL == \u0026#34;yes\u0026#34; ]; then 23 ## Package pyflink app 24 echo \u0026#34;package pyflink app\u0026#34; 25 zip -r kda-package.zip processor.py package/lib package/site_packages 26fi 27 28#### Steps to create the sink connector 29CONN_PATH=$SCRIPT_DIR/connectors 30rm -rf $CONN_PATH \u0026amp;\u0026amp; mkdir $CONN_PATH 31 32## Download camel dynamodb sink connector 33echo \u0026#34;download camel dynamodb sink connector...\u0026#34; 34CONNECTOR_SRC_DOWNLOAD_URL=https://repo.maven.apache.org/maven2/org/apache/camel/kafkaconnector/camel-aws-ddb-sink-kafka-connector/3.20.3/camel-aws-ddb-sink-kafka-connector-3.20.3-package.tar.gz 35 36## decompress and zip contents to create custom plugin of msk connect later 37curl -o $CONN_PATH/camel-aws-ddb-sink-kafka-connector.tar.gz $CONNECTOR_SRC_DOWNLOAD_URL \\ 38 \u0026amp;\u0026amp; tar -xvzf $CONN_PATH/camel-aws-ddb-sink-kafka-connector.tar.gz -C $CONN_PATH \\ 39 \u0026amp;\u0026amp; cd $CONN_PATH/camel-aws-ddb-sink-kafka-connector \\ 40 \u0026amp;\u0026amp; zip -r camel-aws-ddb-sink-kafka-connector.zip . \\ 41 \u0026amp;\u0026amp; mv camel-aws-ddb-sink-kafka-connector.zip $CONN_PATH \\ 42 \u0026amp;\u0026amp; rm $CONN_PATH/camel-aws-ddb-sink-kafka-connector.tar.gz Once downloaded, they can be found in the corresponding folders as shown below. Although the Flink app doesn\u0026rsquo;t need the kafka-python package, it is included in the site_packages folder in order to check if --pyFiles option works in KDA - it\u0026rsquo;ll be checked in part 2.\nKafka and Related Services A Kafka cluster with a single broker and zookeeper node is used in this post. The broker has two listeners and the port 9092 and 29092 are used for internal and external communication respectively. The default number of topic partitions is set to 2. Details about Kafka cluster setup can be found in this post.\nA Kafka Connect service is configured to run in a distributed mode. The connect properties file and the source of the Camel DynamoDB sink connector are volume-mapped. Also AWS credentials are added to environment variables as it needs permission to put items into a DynamoDB table. Details about Kafka connect setup can be found in this post.\nFinally, the Kpow CE is used for ease of monitoring Kafka topics and related resources. The bootstrap server address and connect REST URL are added as environment variables. See this post for details about Kafka management apps.\n1# docker-compose.yml 2version: \u0026#34;3.5\u0026#34; 3 4services: 5 zookeeper: 6 image: bitnami/zookeeper:3.5 7 container_name: zookeeper 8 ports: 9 - \u0026#34;2181\u0026#34; 10 networks: 11 - kafkanet 12 environment: 13 - ALLOW_ANONYMOUS_LOGIN=yes 14 volumes: 15 - zookeeper_data:/bitnami/zookeeper 16 kafka-0: 17 image: bitnami/kafka:2.8.1 18 container_name: kafka-0 19 expose: 20 - 9092 21 ports: 22 - \u0026#34;29092:29092\u0026#34; 23 networks: 24 - kafkanet 25 environment: 26 - ALLOW_PLAINTEXT_LISTENER=yes 27 - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181 28 - KAFKA_CFG_BROKER_ID=0 29 - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT 30 - KAFKA_CFG_LISTENERS=INTERNAL://:9092,EXTERNAL://:29092 31 - KAFKA_CFG_ADVERTISED_LISTENERS=INTERNAL://kafka-0:9092,EXTERNAL://localhost:29092 32 - KAFKA_CFG_INTER_BROKER_LISTENER_NAME=INTERNAL 33 - KAFKA_CFG_NUM_PARTITIONS=2 34 volumes: 35 - kafka_0_data:/bitnami/kafka 36 depends_on: 37 - zookeeper 38 kafka-connect: 39 image: bitnami/kafka:2.8.1 40 container_name: connect 41 command: \u0026gt; 42 /opt/bitnami/kafka/bin/connect-distributed.sh 43 /opt/bitnami/kafka/config/connect-distributed.properties 44 ports: 45 - \u0026#34;8083:8083\u0026#34; 46 networks: 47 - kafkanet 48 environment: 49 AWS_ACCESS_KEY_ID: $AWS_ACCESS_KEY_ID 50 AWS_SECRET_ACCESS_KEY: $AWS_SECRET_ACCESS_KEY 51 AWS_SESSION_TOKEN: $AWS_SESSION_TOKEN 52 volumes: 53 - \u0026#34;./configs/connect-distributed.properties:/opt/bitnami/kafka/config/connect-distributed.properties\u0026#34; 54 - \u0026#34;./connectors/camel-aws-ddb-sink-kafka-connector:/opt/connectors/camel-aws-ddb-sink-kafka-connector\u0026#34; 55 depends_on: 56 - zookeeper 57 - kafka-0 58 kpow: 59 image: factorhouse/kpow-ce:91.2.1 60 container_name: kpow 61 ports: 62 - \u0026#34;3000:3000\u0026#34; 63 networks: 64 - kafkanet 65 environment: 66 BOOTSTRAP: kafka-0:9092 67 CONNECT_REST_URL: http://kafka-connect:8083 68 depends_on: 69 - zookeeper 70 - kafka-0 71 - kafka-connect 72 73networks: 74 kafkanet: 75 name: kafka-network 76 77volumes: 78 zookeeper_data: 79 driver: local 80 name: zookeeper_data 81 kafka_0_data: 82 driver: local 83 name: kafka_0_data DynamoDB Table The destination table is named flagged-transactions, and it has the primary key where transaction_id and transaction_date are the hash and range key respectively. It also has a global secondary index (GSI) where account_id and transaction_date constitute the primary key. The purpose of the GSI is for ease of querying transactions by account ID. The table can be created using the AWS CLI as shown below.\n1aws dynamodb create-table \\ 2 --cli-input-json file://configs/ddb.json 1// configs/ddb.json 2{ 3 \u0026#34;TableName\u0026#34;: \u0026#34;flagged-transactions\u0026#34;, 4 \u0026#34;KeySchema\u0026#34;: [ 5 { \u0026#34;AttributeName\u0026#34;: \u0026#34;transaction_id\u0026#34;, \u0026#34;KeyType\u0026#34;: \u0026#34;HASH\u0026#34; }, 6 { \u0026#34;AttributeName\u0026#34;: \u0026#34;transaction_date\u0026#34;, \u0026#34;KeyType\u0026#34;: \u0026#34;RANGE\u0026#34; } 7 ], 8 \u0026#34;AttributeDefinitions\u0026#34;: [ 9 { \u0026#34;AttributeName\u0026#34;: \u0026#34;transaction_id\u0026#34;, \u0026#34;AttributeType\u0026#34;: \u0026#34;S\u0026#34; }, 10 { \u0026#34;AttributeName\u0026#34;: \u0026#34;account_id\u0026#34;, \u0026#34;AttributeType\u0026#34;: \u0026#34;N\u0026#34; }, 11 { \u0026#34;AttributeName\u0026#34;: \u0026#34;transaction_date\u0026#34;, \u0026#34;AttributeType\u0026#34;: \u0026#34;S\u0026#34; } 12 ], 13 \u0026#34;ProvisionedThroughput\u0026#34;: { 14 \u0026#34;ReadCapacityUnits\u0026#34;: 2, 15 \u0026#34;WriteCapacityUnits\u0026#34;: 2 16 }, 17 \u0026#34;GlobalSecondaryIndexes\u0026#34;: [ 18 { 19 \u0026#34;IndexName\u0026#34;: \u0026#34;account\u0026#34;, 20 \u0026#34;KeySchema\u0026#34;: [ 21 { \u0026#34;AttributeName\u0026#34;: \u0026#34;account_id\u0026#34;, \u0026#34;KeyType\u0026#34;: \u0026#34;HASH\u0026#34; }, 22 { \u0026#34;AttributeName\u0026#34;: \u0026#34;transaction_date\u0026#34;, \u0026#34;KeyType\u0026#34;: \u0026#34;RANGE\u0026#34; } 23 ], 24 \u0026#34;Projection\u0026#34;: { \u0026#34;ProjectionType\u0026#34;: \u0026#34;ALL\u0026#34; }, 25 \u0026#34;ProvisionedThroughput\u0026#34;: { 26 \u0026#34;ReadCapacityUnits\u0026#34;: 2, 27 \u0026#34;WriteCapacityUnits\u0026#34;: 2 28 } 29 } 30 ] 31} Virtual Environment As mentioned earlier, all Python apps run in a virtual environment, and we need the following pip packages. We use the version 1.15.2 of the apache-flink package because it is the latest supported version by KDA. We also need the kafka-python package for source data generation. The pip packages can be installed by pip install -r requirements-dev.txt.\n1# requirements.txt 2kafka-python==2.0.2 3 4# requirements-dev.txt 5-r requirements.txt 6apache-flink==1.15.2 7black==19.10b0 8pytest 9pytest-cov Application Source Data A single Python script is created for the transaction and flagged account apps. They generate and send source data into Kafka topics. Each of the classes for the flagged account and transaction has the asdict, auto and create methods. The create method generates a list of records where each element is instantiated by the auto method. Those records are sent into the relevant Kafka topic after being converted into a dictionary by the asdict method.\nA Kafka producer is created as an attribute of the Producer class. The source records are sent into the relevant topic by the send method. Note that both the key and value of the messages are serialized as json.\nWhether to send flagged account or transaction records is determined by an environment variable called DATA_TYPE. We can run those apps as shown below.\nflagged account - DATE_TYPE=account python producer.py transaction - DATE_TYPE=transaction python producer.py 1# producer.py 2import os 3import datetime 4import time 5import json 6import typing 7import random 8import logging 9import dataclasses 10 11from kafka import KafkaProducer 12 13logging.basicConfig( 14 level=logging.INFO, 15 format=\u0026#34;%(asctime)s.%(msecs)03d:%(levelname)s:%(name)s:%(message)s\u0026#34;, 16 datefmt=\u0026#34;%Y-%m-%d %H:%M:%S\u0026#34;, 17) 18 19 20@dataclasses.dataclass 21class FlagAccount: 22 account_id: int 23 flag_date: str 24 25 def asdict(self): 26 return dataclasses.asdict(self) 27 28 @classmethod 29 def auto(cls, account_id: int): 30 flag_date = datetime.datetime.now().strftime(\u0026#34;%Y-%m-%d %H:%M:%S.%f\u0026#34;) 31 return cls(account_id, flag_date) 32 33 @staticmethod 34 def create(): 35 return [FlagAccount.auto(account_id) for account_id in range(1000000001, 1000000010, 2)] 36 37 38@dataclasses.dataclass 39class Transaction: 40 account_id: int 41 customer_id: str 42 merchant_type: str 43 transaction_id: str 44 transaction_type: str 45 transaction_amount: float 46 transaction_date: str 47 48 def asdict(self): 49 return dataclasses.asdict(self) 50 51 @classmethod 52 def auto(cls): 53 account_id = random.randint(1000000001, 1000000010) 54 customer_id = f\u0026#34;C{str(account_id)[::-1]}\u0026#34; 55 merchant_type = random.choice([\u0026#34;Online\u0026#34;, \u0026#34;In Store\u0026#34;]) 56 transaction_id = \u0026#34;\u0026#34;.join(random.choice(\u0026#34;0123456789ABCDEF\u0026#34;) for i in range(16)) 57 transaction_type = random.choice( 58 [ 59 \u0026#34;Grocery_Store\u0026#34;, 60 \u0026#34;Gas_Station\u0026#34;, 61 \u0026#34;Shopping_Mall\u0026#34;, 62 \u0026#34;City_Services\u0026#34;, 63 \u0026#34;HealthCare_Service\u0026#34;, 64 \u0026#34;Food and Beverage\u0026#34;, 65 \u0026#34;Others\u0026#34;, 66 ] 67 ) 68 transaction_amount = round(random.randint(100, 10000) * random.random(), 2) 69 transaction_date = datetime.datetime.now().strftime(\u0026#34;%Y-%m-%d %H:%M:%S.%f\u0026#34;) 70 return cls( 71 account_id, 72 customer_id, 73 merchant_type, 74 transaction_id, 75 transaction_type, 76 transaction_amount, 77 transaction_date, 78 ) 79 80 @staticmethod 81 def create(num: int): 82 return [Transaction.auto() for _ in range(num)] 83 84 85class Producer: 86 def __init__(self, bootstrap_servers: list, account_topic: str, transaction_topic: str): 87 self.bootstrap_servers = bootstrap_servers 88 self.account_topic = account_topic 89 self.transaction_topic = transaction_topic 90 self.producer = self.create() 91 92 def create(self): 93 return KafkaProducer( 94 bootstrap_servers=self.bootstrap_servers, 95 key_serializer=lambda v: json.dumps(v, default=self.serialize).encode(\u0026#34;utf-8\u0026#34;), 96 value_serializer=lambda v: json.dumps(v, default=self.serialize).encode(\u0026#34;utf-8\u0026#34;), 97 api_version=(2, 8, 1), 98 ) 99 100 def send(self, records: typing.Union[typing.List[FlagAccount], typing.List[Transaction]]): 101 for record in records: 102 try: 103 key = {\u0026#34;account_id\u0026#34;: record.account_id} 104 topic = self.account_topic 105 if hasattr(record, \u0026#34;transaction_id\u0026#34;): 106 key[\u0026#34;transaction_id\u0026#34;] = record.transaction_id 107 topic = self.transaction_topic 108 self.producer.send(topic=topic, key=key, value=record.asdict()) 109 except Exception as e: 110 raise RuntimeError(\u0026#34;fails to send a message\u0026#34;) from e 111 self.producer.flush() 112 113 def serialize(self, obj): 114 if isinstance(obj, datetime.datetime): 115 return obj.isoformat() 116 if isinstance(obj, datetime.date): 117 return str(obj) 118 return obj 119 120 121if __name__ == \u0026#34;__main__\u0026#34;: 122 producer = Producer( 123 bootstrap_servers=os.getenv(\u0026#34;BOOTSTRAP_SERVERS\u0026#34;, \u0026#34;localhost:29092\u0026#34;).split(\u0026#34;,\u0026#34;), 124 account_topic=os.getenv(\u0026#34;CUSTOMER_TOPIC_NAME\u0026#34;, \u0026#34;flagged-accounts\u0026#34;), 125 transaction_topic=os.getenv(\u0026#34;TRANSACTION_TOPIC_NAME\u0026#34;, \u0026#34;transactions\u0026#34;), 126 ) 127 if os.getenv(\u0026#34;DATE_TYPE\u0026#34;, \u0026#34;account\u0026#34;) == \u0026#34;account\u0026#34;: 128 producer.send(FlagAccount.create()) 129 producer.producer.close() 130 else: 131 max_run = int(os.getenv(\u0026#34;MAX_RUN\u0026#34;, \u0026#34;-1\u0026#34;)) 132 logging.info(f\u0026#34;max run - {max_run}\u0026#34;) 133 current_run = 0 134 while True: 135 current_run += 1 136 logging.info(f\u0026#34;current run - {current_run}\u0026#34;) 137 if current_run - max_run == 0: 138 logging.info(f\u0026#34;reached max run, finish\u0026#34;) 139 producer.producer.close() 140 break 141 producer.send(Transaction.create(5)) 142 secs = random.randint(2, 5) 143 logging.info(f\u0026#34;messages sent... wait {secs} seconds\u0026#34;) 144 time.sleep(secs) Once we start the apps, we can check the topics for the source data are created and messages are ingested in Kpow.\nOutput Data The Flink application is built using the Table API. We have two Kafka source topics and one output topic. Simply put, we can query the records of the topics as tables of unbounded real-time streams with the Table API. In order to read/write records from/to a Kafka topic, we need to specify the Kafka connector artifact that we downloaded earlier as the pipeline jar. Note we only need to configure the connector jar when we develop the app locally as the jar file will be specified by the --jarfile option for KDA. We also need the application properties file (application_properties.json) in order to be comparable with KDA. The file contains the Flink runtime options in KDA as well as application specific properties. All the properties should be specified when deploying via KDA and, for local development, we keep them as a json file and only the application specific properties are used.\nThe tables for the source and output topics can be created using SQL with options that are related to the Kafka connector. Key options cover the connector name (connector), topic name (topic), bootstrap server address (properties.bootstrap.servers) and format (format). See the connector document for more details about the connector configuration. When it comes to inserting flagged transaction records into the output topic, we use a function written in SQL as well - insert_into_stmt.\nIn the main method, we create all the source and sink tables after mapping relevant application properties. Then the output records are inserted into the output Kafka topic. Note that the output records are printed in the terminal additionally when the app is running locally for ease of checking them. We can run the app as following - RUNTIME_ENV=LOCAL python processor.py\n1# processor.py 2import os 3import json 4import logging 5 6import kafka # check if --pyFiles works 7from pyflink.table import EnvironmentSettings, TableEnvironment 8 9logging.basicConfig( 10 level=logging.INFO, 11 format=\u0026#34;%(asctime)s.%(msecs)03d:%(levelname)s:%(name)s:%(message)s\u0026#34;, 12 datefmt=\u0026#34;%Y-%m-%d %H:%M:%S\u0026#34;, 13) 14 15RUNTIME_ENV = os.environ.get(\u0026#34;RUNTIME_ENV\u0026#34;, \u0026#34;KDA\u0026#34;) # KDA, LOCAL 16logging.info(f\u0026#34;runtime environment - {RUNTIME_ENV}...\u0026#34;) 17 18env_settings = EnvironmentSettings.in_streaming_mode() 19table_env = TableEnvironment.create(env_settings) 20 21APPLICATION_PROPERTIES_FILE_PATH = ( 22 \u0026#34;/etc/flink/application_properties.json\u0026#34; # on kda or docker-compose 23 if RUNTIME_ENV != \u0026#34;LOCAL\u0026#34; 24 else \u0026#34;application_properties.json\u0026#34; 25) 26 27if RUNTIME_ENV != \u0026#34;KDA\u0026#34;: 28 # on non-KDA, multiple jar files can be passed after being delimited by a semicolon 29 CURRENT_DIR = os.path.dirname(os.path.realpath(__file__)) 30 PIPELINE_JAR = \u0026#34;flink-sql-connector-kafka-1.15.2.jar\u0026#34; 31 table_env.get_config().set( 32 \u0026#34;pipeline.jars\u0026#34;, f\u0026#34;file://{os.path.join(CURRENT_DIR, \u0026#39;package\u0026#39;, \u0026#39;lib\u0026#39;, PIPELINE_JAR)}\u0026#34; 33 ) 34logging.info(f\u0026#34;app properties file path - {APPLICATION_PROPERTIES_FILE_PATH}\u0026#34;) 35 36def get_application_properties(): 37 if os.path.isfile(APPLICATION_PROPERTIES_FILE_PATH): 38 with open(APPLICATION_PROPERTIES_FILE_PATH, \u0026#34;r\u0026#34;) as file: 39 contents = file.read() 40 properties = json.loads(contents) 41 return properties 42 else: 43 raise RuntimeError(f\u0026#34;A file at \u0026#39;{APPLICATION_PROPERTIES_FILE_PATH}\u0026#39; was not found\u0026#34;) 44 45def property_map(props: dict, property_group_id: str): 46 for prop in props: 47 if prop[\u0026#34;PropertyGroupId\u0026#34;] == property_group_id: 48 return prop[\u0026#34;PropertyMap\u0026#34;] 49 50def create_flagged_account_source_table( 51 table_name: str, topic_name: str, bootstrap_servers: str, startup_mode: str 52): 53 stmt = f\u0026#34;\u0026#34;\u0026#34; 54 CREATE TABLE {table_name} ( 55 account_id BIGINT, 56 flag_date TIMESTAMP(3) 57 ) 58 WITH ( 59 \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39;, 60 \u0026#39;topic\u0026#39; = \u0026#39;{topic_name}\u0026#39;, 61 \u0026#39;properties.bootstrap.servers\u0026#39; = \u0026#39;{bootstrap_servers}\u0026#39;, 62 \u0026#39;properties.group.id\u0026#39; = \u0026#39;flagged-account-source-group\u0026#39;, 63 \u0026#39;format\u0026#39; = \u0026#39;json\u0026#39;, 64 \u0026#39;scan.startup.mode\u0026#39; = \u0026#39;{startup_mode}\u0026#39; 65 ) 66 \u0026#34;\u0026#34;\u0026#34; 67 logging.info(\u0026#34;flagged account source table statement...\u0026#34;) 68 logging.info(stmt) 69 return stmt 70 71def create_transaction_source_table( 72 table_name: str, topic_name: str, bootstrap_servers: str, startup_mode: str 73): 74 stmt = f\u0026#34;\u0026#34;\u0026#34; 75 CREATE TABLE {table_name} ( 76 account_id BIGINT, 77 customer_id VARCHAR(15), 78 merchant_type VARCHAR(8), 79 transaction_id VARCHAR(16), 80 transaction_type VARCHAR(20), 81 transaction_amount DECIMAL(10,2), 82 transaction_date TIMESTAMP(3) 83 ) 84 WITH ( 85 \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39;, 86 \u0026#39;topic\u0026#39; = \u0026#39;{topic_name}\u0026#39;, 87 \u0026#39;properties.bootstrap.servers\u0026#39; = \u0026#39;{bootstrap_servers}\u0026#39;, 88 \u0026#39;properties.group.id\u0026#39; = \u0026#39;transaction-source-group\u0026#39;, 89 \u0026#39;format\u0026#39; = \u0026#39;json\u0026#39;, 90 \u0026#39;scan.startup.mode\u0026#39; = \u0026#39;{startup_mode}\u0026#39; 91 ) 92 \u0026#34;\u0026#34;\u0026#34; 93 logging.info(\u0026#34;transaction source table statement...\u0026#34;) 94 logging.info(stmt) 95 return stmt 96 97def create_flagged_transaction_sink_table(table_name: str, topic_name: str, bootstrap_servers: str): 98 stmt = f\u0026#34;\u0026#34;\u0026#34; 99 CREATE TABLE {table_name} ( 100 account_id BIGINT, 101 customer_id VARCHAR(15), 102 merchant_type VARCHAR(8), 103 transaction_id VARCHAR(16), 104 transaction_type VARCHAR(20), 105 transaction_amount DECIMAL(10,2), 106 transaction_date TIMESTAMP(3) 107 ) 108 WITH ( 109 \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39;, 110 \u0026#39;topic\u0026#39; = \u0026#39;{topic_name}\u0026#39;, 111 \u0026#39;properties.bootstrap.servers\u0026#39; = \u0026#39;{bootstrap_servers}\u0026#39;, 112 \u0026#39;format\u0026#39; = \u0026#39;json\u0026#39;, 113 \u0026#39;key.format\u0026#39; = \u0026#39;json\u0026#39;, 114 \u0026#39;key.fields\u0026#39; = \u0026#39;account_id;transaction_id\u0026#39;, 115 \u0026#39;properties.allow.auto.create.topics\u0026#39; = \u0026#39;true\u0026#39; 116 ) 117 \u0026#34;\u0026#34;\u0026#34; 118 logging.info(\u0026#34;transaction sink table statement...\u0026#34;) 119 logging.info(stmt) 120 return stmt 121 122def create_print_table(table_name: str): 123 return f\u0026#34;\u0026#34;\u0026#34; 124 CREATE TABLE {table_name} ( 125 account_id BIGINT, 126 customer_id VARCHAR(15), 127 merchant_type VARCHAR(8), 128 transaction_id VARCHAR(16), 129 transaction_type VARCHAR(20), 130 transaction_amount DECIMAL(10,2), 131 transaction_date TIMESTAMP(3) 132 ) 133 WITH ( 134 \u0026#39;connector\u0026#39; = \u0026#39;print\u0026#39; 135 ) 136 \u0026#34;\u0026#34;\u0026#34; 137 138def insert_into_stmt(insert_from_tbl: str, compare_with_tbl: str, insert_into_tbl: str): 139 return f\u0026#34;\u0026#34;\u0026#34; 140 INSERT INTO {insert_into_tbl} 141 SELECT l.* 142 FROM {insert_from_tbl} AS l 143 JOIN {compare_with_tbl} AS r 144 ON l.account_id = r.account_id 145 AND l.transaction_date \u0026gt; r.flag_date 146 \u0026#34;\u0026#34;\u0026#34; 147 148def main(): 149 ## map consumer/producer properties 150 props = get_application_properties() 151 # consumer for flagged account 152 consumer_0_property_group_key = \u0026#34;consumer.config.0\u0026#34; 153 consumer_0_properties = property_map(props, consumer_0_property_group_key) 154 consumer_0_table_name = consumer_0_properties[\u0026#34;table.name\u0026#34;] 155 consumer_0_topic_name = consumer_0_properties[\u0026#34;topic.name\u0026#34;] 156 consumer_0_bootstrap_servers = consumer_0_properties[\u0026#34;bootstrap.servers\u0026#34;] 157 consumer_0_startup_mode = consumer_0_properties[\u0026#34;startup.mode\u0026#34;] 158 # consumer for transactions 159 consumer_1_property_group_key = \u0026#34;consumer.config.1\u0026#34; 160 consumer_1_properties = property_map(props, consumer_1_property_group_key) 161 consumer_1_table_name = consumer_1_properties[\u0026#34;table.name\u0026#34;] 162 consumer_1_topic_name = consumer_1_properties[\u0026#34;topic.name\u0026#34;] 163 consumer_1_bootstrap_servers = consumer_1_properties[\u0026#34;bootstrap.servers\u0026#34;] 164 consumer_1_startup_mode = consumer_1_properties[\u0026#34;startup.mode\u0026#34;] 165 # producer 166 producer_0_property_group_key = \u0026#34;producer.config.0\u0026#34; 167 producer_0_properties = property_map(props, producer_0_property_group_key) 168 producer_0_table_name = producer_0_properties[\u0026#34;table.name\u0026#34;] 169 producer_0_topic_name = producer_0_properties[\u0026#34;topic.name\u0026#34;] 170 producer_0_bootstrap_servers = producer_0_properties[\u0026#34;bootstrap.servers\u0026#34;] 171 # print 172 print_table_name = \u0026#34;sink_print\u0026#34; 173 ## create the source table for flagged accounts 174 table_env.execute_sql( 175 create_flagged_account_source_table( 176 consumer_0_table_name, 177 consumer_0_topic_name, 178 consumer_0_bootstrap_servers, 179 consumer_0_startup_mode, 180 ) 181 ) 182 table_env.from_path(consumer_0_table_name).print_schema() 183 ## create the source table for transactions 184 table_env.execute_sql( 185 create_transaction_source_table( 186 consumer_1_table_name, 187 consumer_1_topic_name, 188 consumer_1_bootstrap_servers, 189 consumer_1_startup_mode, 190 ) 191 ) 192 table_env.from_path(consumer_1_table_name).print_schema() 193 ## create sink table for flagged accounts 194 table_env.execute_sql( 195 create_flagged_transaction_sink_table( 196 producer_0_table_name, producer_0_topic_name, producer_0_bootstrap_servers 197 ) 198 ) 199 table_env.from_path(producer_0_table_name).print_schema() 200 table_env.execute_sql(create_print_table(\u0026#34;sink_print\u0026#34;)) 201 ## insert into sink tables 202 if RUNTIME_ENV == \u0026#34;LOCAL\u0026#34;: 203 statement_set = table_env.create_statement_set() 204 statement_set.add_insert_sql( 205 insert_into_stmt(consumer_1_table_name, consumer_0_table_name, producer_0_table_name) 206 ) 207 statement_set.add_insert_sql( 208 insert_into_stmt(consumer_1_table_name, consumer_0_table_name, print_table_name) 209 ) 210 statement_set.execute().wait() 211 else: 212 table_result = table_env.execute_sql( 213 insert_into_stmt(consumer_1_table_name, consumer_0_table_name, producer_0_table_name) 214 ) 215 logging.info(table_result.get_job_client().get_job_status()) 216 217 218if __name__ == \u0026#34;__main__\u0026#34;: 219 main() 1// application_properties.json 2[ 3 { 4 \u0026#34;PropertyGroupId\u0026#34;: \u0026#34;kinesis.analytics.flink.run.options\u0026#34;, 5 \u0026#34;PropertyMap\u0026#34;: { 6 \u0026#34;python\u0026#34;: \u0026#34;processor.py\u0026#34;, 7 \u0026#34;jarfile\u0026#34;: \u0026#34;package/lib/flink-sql-connector-kinesis-1.15.2.jar\u0026#34;, 8 \u0026#34;pyFiles\u0026#34;: \u0026#34;package/site_packages/\u0026#34; 9 } 10 }, 11 { 12 \u0026#34;PropertyGroupId\u0026#34;: \u0026#34;consumer.config.0\u0026#34;, 13 \u0026#34;PropertyMap\u0026#34;: { 14 \u0026#34;table.name\u0026#34;: \u0026#34;flagged_accounts\u0026#34;, 15 \u0026#34;topic.name\u0026#34;: \u0026#34;flagged-accounts\u0026#34;, 16 \u0026#34;bootstrap.servers\u0026#34;: \u0026#34;localhost:29092\u0026#34;, 17 \u0026#34;startup.mode\u0026#34;: \u0026#34;earliest-offset\u0026#34; 18 } 19 }, 20 { 21 \u0026#34;PropertyGroupId\u0026#34;: \u0026#34;consumer.config.1\u0026#34;, 22 \u0026#34;PropertyMap\u0026#34;: { 23 \u0026#34;table.name\u0026#34;: \u0026#34;transactions\u0026#34;, 24 \u0026#34;topic.name\u0026#34;: \u0026#34;transactions\u0026#34;, 25 \u0026#34;bootstrap.servers\u0026#34;: \u0026#34;localhost:29092\u0026#34;, 26 \u0026#34;startup.mode\u0026#34;: \u0026#34;earliest-offset\u0026#34; 27 } 28 }, 29 { 30 \u0026#34;PropertyGroupId\u0026#34;: \u0026#34;producer.config.0\u0026#34;, 31 \u0026#34;PropertyMap\u0026#34;: { 32 \u0026#34;table.name\u0026#34;: \u0026#34;flagged_transactions\u0026#34;, 33 \u0026#34;topic.name\u0026#34;: \u0026#34;flagged-transactions\u0026#34;, 34 \u0026#34;bootstrap.servers\u0026#34;: \u0026#34;localhost:29092\u0026#34; 35 } 36 } 37] The terminal on the right-hand side shows the output records of the Flink app while the left-hand side records logs of the transaction app. We see that the account IDs end with all odd numbers, which matches transactions from flagged accounts.\nWe can also see details of all the topics in Kpow as shown below.\nSink Output Data Kafka Connect provides a REST API that manages connectors, and we can create a connector programmatically using it. The REST endpoint requires a JSON payload that includes connector configurations.\n1$ curl -i -X POST -H \u0026#34;Accept:application/json\u0026#34; -H \u0026#34;Content-Type:application/json\u0026#34; \\ 2 http://localhost:8083/connectors/ -d @configs/sink.json The connector is configured to write messages from the flagged-transactions topic into the DynamoDB table created earlier. It requires to specify the table name, AWS region, operation, write capacity and whether to use the default credential provider - see the documentation for details. Note that, if you don\u0026rsquo;t use the default credential provider, you have to specify the access key id and secret access key. Note further that, although the current LTS version is v3.18.2, the default credential provider option didn\u0026rsquo;t work for me, and I was recommended to use v3.20.3 instead. Finally, the camel.sink.unmarshal option is to convert data from the internal java.util.HashMap type into the required java.io.InputStream type. Without this configuration, the connector fails with org.apache.camel.NoTypeConversionAvailableException error.\n1// configs/sink.json 2{ 3 \u0026#34;name\u0026#34;: \u0026#34;transactions-sink\u0026#34;, 4 \u0026#34;config\u0026#34;: { 5 \u0026#34;connector.class\u0026#34;: \u0026#34;org.apache.camel.kafkaconnector.awsddbsink.CamelAwsddbsinkSinkConnector\u0026#34;, 6 \u0026#34;tasks.max\u0026#34;: \u0026#34;2\u0026#34;, 7 \u0026#34;key.converter\u0026#34;: \u0026#34;org.apache.kafka.connect.json.JsonConverter\u0026#34;, 8 \u0026#34;key.converter.schemas.enable\u0026#34;: false, 9 \u0026#34;value.converter\u0026#34;: \u0026#34;org.apache.kafka.connect.json.JsonConverter\u0026#34;, 10 \u0026#34;value.converter.schemas.enable\u0026#34;: false, 11 \u0026#34;topics\u0026#34;: \u0026#34;flagged-transactions\u0026#34;, 12 13 \u0026#34;camel.kamelet.aws-ddb-sink.table\u0026#34;: \u0026#34;flagged-transactions\u0026#34;, 14 \u0026#34;camel.kamelet.aws-ddb-sink.region\u0026#34;: \u0026#34;ap-southeast-2\u0026#34;, 15 \u0026#34;camel.kamelet.aws-ddb-sink.operation\u0026#34;: \u0026#34;PutItem\u0026#34;, 16 \u0026#34;camel.kamelet.aws-ddb-sink.writeCapacity\u0026#34;: 1, 17 \u0026#34;camel.kamelet.aws-ddb-sink.useDefaultCredentialsProvider\u0026#34;: true, 18 \u0026#34;camel.sink.unmarshal\u0026#34;: \u0026#34;jackson\u0026#34; 19 } 20} Below shows the sink connector details on Kpow.\nWe can check the ingested records on the DynamoDB table items view. Below shows a list of scanned records.\nSummary Apache Flink is widely used for building real-time stream processing applications. On AWS, Kinesis Data Analytics (KDA) is the easiest option to develop a Flink app as it provides the underlying infrastructure. Re-implementing a solution from an AWS workshop, this series of posts discuss how to develop and deploy a fraud detection app using Kafka, Flink and DynamoDB. In this post, we covered local development using Docker, and deployment via KDA will be discussed in part 2.\n","date":"August 10, 2023","img":"/blog/2023-08-10-fraud-detection-part-1/featured.png","lang":"en","langName":"English","largeImg":"/blog/2023-08-10-fraud-detection-part-1/featured_hu0e6881a0b5c39bbfe5c0b72138bca889_72929_500x0_resize_box_3.png","permalink":"/blog/2023-08-10-fraud-detection-part-1/","series":[{"title":"Kafka, Flink and DynamoDB for Real Time Fraud Detection","url":"/series/kafka-flink-and-dynamodb-for-real-time-fraud-detection/"}],"smallImg":"/blog/2023-08-10-fraud-detection-part-1/featured_hu0e6881a0b5c39bbfe5c0b72138bca889_72929_180x0_resize_box_3.png","tags":[{"title":"Apache Flink","url":"/tags/apache-flink/"},{"title":"PyFlink","url":"/tags/pyflink/"},{"title":"Apache Kafka","url":"/tags/apache-kafka/"},{"title":"Kafka Connect","url":"/tags/kafka-connect/"},{"title":"Amazon DynamoDB","url":"/tags/amazon-dynamodb/"},{"title":"Python","url":"/tags/python/"},{"title":"Docker","url":"/tags/docker/"},{"title":"Docker Compose","url":"/tags/docker-compose/"},{"title":"Fraud Detection","url":"/tags/fraud-detection/"}],"timestamp":1691625600,"title":"Kafka, Flink and DynamoDB for Real Time Fraud Detection - Part 1 Local Development"},{"categories":[{"title":"Apache Kafka","url":"/categories/apache-kafka/"}],"content":"In the previous posts, we discussed how to implement client authentication by TLS (SSL or TLS/SSL) and SASL authentication. One of the key benefits of client authentication is achieving user access control. Kafka ships with a pluggable, out-of-the box authorization framework, which is configured with the authorizer.class.name property in the server configuration and stores Access Control Lists (ACLs) in the cluster metadata (either Zookeeper or the KRaft metadata log). In this post, we will discuss how to configure Kafka authorization with Java and Python client examples while SASL is kept for client authentication.\nPart 1 Cluster Setup Part 2 Management App Part 3 Kafka Connect Part 4 Producer and Consumer Part 5 Glue Schema Registry Part 6 Kafka Connect with Glue Schema Registry Part 7 Producer and Consumer with Glue Schema Registry Part 8 SSL Encryption Part 9 SSL Authentication Part 10 SASL Authentication Part 11 Kafka Authorization (this post) Certificate Setup As we will leave Kafka communication to remain encrypted, we need to keep the components for SSL encryption. The details can be found in Part 8, and those components can be generated by generate.sh. Once we execute the script, the following files are created.\n1$ tree certificate-authority keystore truststore pem 2certificate-authority 3├── ca-cert 4└── ca-key 5keystore 6├── kafka-0.server.keystore.jks 7├── kafka-1.server.keystore.jks 8└── kafka-2.server.keystore.jks 9truststore 10└── kafka.truststore.jks 11pem 12└── ca-root.pem Kafka Cluster Update As discussed in Part 10, authentication should be enabled on the Zookeeper node for SASL authentication. Moreover, it is important to secure it for authorization because ACLs are stored in it. Therefore, I enabled authentication and specified user credentials. The credentials will be referred in the Client context of the Java Authentication and Authorization Service(JAAS) configuration file (kafka_jaas.conf). The details about the configuration file can be found below.\nWhen it comes to Kafka broker configurations, we should add the SASL_SSL listener to the broker configuration and the port 9094 is reserved for it. Both the Keystore and Truststore files are specified in the broker configuration for SSL. The former is to send the broker certificate to clients while the latter is necessary because a Kafka broker can be a client of other brokers. While SASL supports multiple mechanisms, we enabled the Salted Challenge Response Authentication Mechanism (SCRAM) by specifying SCRAM-SHA-256 in the following environment variables.\nKAFKA_CFG_SASL_ENABLED_MECHANISMS KAFKA_CFG_SASL_MECHANISM_INTER_BROKER_PROTOCOL. For authorization, AclAuthorizer is specified as the authorizer class name, which uses Zookeeper to persist ACLs. A super user named superuser is created. As the name suggests, super users are those who are allowed to execute operations without checking ACLs. Finally, it is configured that anyone is allowed to access resources when no ACL is found (allow.everyone.if.no.acl.found). This is enabled to create the super user after the Kafka cluster gets started. However, it is not recommended in production environemnt.\nThe changes made to the first Kafka broker are shown below, and the same updates are made to the other brokers. The source can be found in the GitHub repository of this post, and the cluster can be started by docker-compose -f compose-kafka.yml up -d.\n1# kafka-dev-with-docker/part-11/compose-kafka.yml 2version: \u0026#34;3.5\u0026#34; 3 4services: 5 zookeeper: 6 image: bitnami/zookeeper:3.5 7 container_name: zookeeper 8 ports: 9 - \u0026#34;2181\u0026#34; 10 networks: 11 - kafkanet 12 environment: 13 - ZOO_ENABLE_AUTH=yes 14 - ZOO_SERVER_USERS=admin 15 - ZOO_SERVER_PASSWORDS=password 16 volumes: 17 - zookeeper_data:/bitnami/zookeeper 18 kafka-0: 19 image: bitnami/kafka:2.8.1 20 container_name: kafka-0 21 expose: 22 - 9092 23 - 9093 24 - 9094 25 ports: 26 - \u0026#34;29092:29092\u0026#34; 27 networks: 28 - kafkanet 29 environment: 30 - ALLOW_PLAINTEXT_LISTENER=yes 31 - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181 32 - KAFKA_CFG_BROKER_ID=0 33 - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=INTERNAL:PLAINTEXT,SSL:SSL,SASL_SSL:SASL_SSL,EXTERNAL:PLAINTEXT 34 - KAFKA_CFG_LISTENERS=INTERNAL://:9092,SSL://:9093,SASL_SSL://:9094,EXTERNAL://:29092 35 - KAFKA_CFG_ADVERTISED_LISTENERS=INTERNAL://kafka-0:9092,SSL://kafka-0:9093,SASL_SSL://kafka-0:9094,EXTERNAL://localhost:29092 36 - KAFKA_CFG_INTER_BROKER_LISTENER_NAME=SSL 37 - KAFKA_CFG_SSL_KEYSTORE_LOCATION=/opt/bitnami/kafka/config/certs/kafka.keystore.jks 38 - KAFKA_CFG_SSL_KEYSTORE_PASSWORD=supersecret 39 - KAFKA_CFG_SSL_KEY_PASSWORD=supersecret 40 - KAFKA_CFG_SSL_TRUSTSTORE_LOCATION=/opt/bitnami/kafka/config/certs/kafka.truststore.jks 41 - KAFKA_CFG_SSL_TRUSTSTORE_PASSWORD=supersecret 42 - KAFKA_CFG_SASL_ENABLED_MECHANISMS=SCRAM-SHA-256 43 - KAFKA_CFG_SASL_MECHANISM_INTER_BROKER_PROTOCOL=SCRAM-SHA-256 44 - KAFKA_CFG_AUTHORIZER_CLASS_NAME=kafka.security.authorizer.AclAuthorizer 45 - KAFKA_CFG_SUPER_USERS=User:superuser 46 - KAFKA_CFG_ALLOW_EVERYONE_IF_NO_ACL_FOUND=true 47 volumes: 48 - kafka_0_data:/bitnami/kafka 49 - ./keystore/kafka-0.server.keystore.jks:/opt/bitnami/kafka/config/certs/kafka.keystore.jks:ro 50 - ./truststore/kafka.truststore.jks:/opt/bitnami/kafka/config/certs/kafka.truststore.jks:ro 51 - ./kafka_jaas.conf:/opt/bitnami/kafka/config/kafka_jaas.conf:ro 52 - ./client.properties:/opt/bitnami/kafka/config/client.properties:ro 53 - ./command.properties:/opt/bitnami/kafka/config/command.properties:ro 54 - ./superuser.properties:/opt/bitnami/kafka/config/superuser.properties:ro 55 depends_on: 56 - zookeeper 57 58... 59 60networks: 61 kafkanet: 62 name: kafka-network 63 64... As mentioned earlier, the broker needs a JAAS configuration file, and it should include 2 contexts - KafkaServer and Client. The former is required for inter-broker communication while the latter is for accessing the Zookeeper node. As SASL is not enabled for inter-broker communication, dummy credentials are added for the KafkaServer context while the Zookeeper user credentials are kept in the Client context. The credentials are those that are specified by the following environment variables in the Zookeeper node - ZOO_SERVER_USERS and ZOO_SERVER_PASSWORDS.\n1# kafka-dev-with-docker/part-11/kafka_jaas.conf 2KafkaServer { 3 org.apache.kafka.common.security.scram.ScramLoginModule required 4 username=\u0026#34;_\u0026#34; 5 password=\u0026#34;_\u0026#34;; 6}; 7 8Client { 9 org.apache.kafka.common.security.plain.PlainLoginModule required 10 username=\u0026#34;admin\u0026#34; 11 password=\u0026#34;password\u0026#34;; 12}; Examples For SSL encryption, Java and non-Java clients need different configurations. The former can use the Keystore file of the Truststore directly while the latter needs corresponding details in a PEM file. The Kafka CLI and Kafka-UI will be taken as Java client examples while Python producer/consumer will be used to illustrate non-Java clients.\nFor client authentication, we will create a total of 4 SCRAM users. At first we will create the super user. Then the super user will create the 3 client users as well as their permissions.\nUser Creation The SCRAM super user can be created by using either the PLAINTEXT or SSL listener within a broker container. Here we will use the SSL listener with the following configuration.\n1# kafka-dev-with-docker/part-11/command.properties 2security.protocol=SSL 3ssl.truststore.location=/opt/bitnami/kafka/config/certs/kafka.truststore.jks 4ssl.truststore.password=supersecret Once the super user is created, the client users will be created via the SASL_SSL listener using the following properties.\n1# kafka-dev-with-docker/part-11/superuser.properties 2sasl.mechanism=SCRAM-SHA-256 3sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required username=\u0026#34;superuser\u0026#34; password=\u0026#34;password\u0026#34;; 4security.protocol=SASL_SSL 5ssl.truststore.location=/opt/bitnami/kafka/config/certs/kafka.truststore.jks 6ssl.truststore.password=supersecret Below shows details of creating users. There is no user by default, and the SCRAM super user as well as the 3 client users are created. The client users are named client, producer and consumer.\n1$ docker exec -it kafka-0 bash 2I have no name!@b28e71a2ae2c:/$ cd /opt/bitnami/kafka/bin/ 3## describe (list) all users (via SSH) - no user exists 4I have no name!@b28e71a2ae2c:/opt/bitnami/kafka/bin$ ./kafka-configs.sh --bootstrap-server kafka-1:9093 --describe \\ 5 --entity-type users --command-config /opt/bitnami/kafka/config/command.properties 6 7## create superuser via (via SSH) 8I have no name!@b28e71a2ae2c:/opt/bitnami/kafka/bin$ ./kafka-configs.sh --bootstrap-server kafka-1:9093 --alter \\ 9 --add-config \u0026#39;SCRAM-SHA-256=[iterations=8192,password=password]\u0026#39; \\ 10 --entity-type users --entity-name superuser \\ 11 --command-config /opt/bitnami/kafka/config/command.properties 12# Completed updating config for user superuser. 13 14## create users for Kafka client (via SASL_SSL as superuser) 15I have no name!@b28e71a2ae2c:/opt/bitnami/kafka/bin$ for USER in \u0026#34;client\u0026#34; \u0026#34;producer\u0026#34; \u0026#34;consumer\u0026#34;; do 16 ./kafka-configs.sh --bootstrap-server kafka-1:9094 --alter \\ 17 --add-config \u0026#39;SCRAM-SHA-256=[iterations=8192,password=password]\u0026#39; \\ 18 --entity-type users --entity-name $USER \\ 19 --command-config /opt/bitnami/kafka/config/superuser.properties 20done 21# Completed updating config for user client. 22# Completed updating config for user producer. 23# Completed updating config for user consumer. 24 25## check if all users exist (via SASL_SSL as superuser) 26I have no name!@b28e71a2ae2c:/opt/bitnami/kafka/bin$ ./kafka-configs.sh --bootstrap-server kafka-1:9094 --describe \\ 27 --entity-type users --command-config /opt/bitnami/kafka/config/superuser.properties 28# SCRAM credential configs for user-principal \u0026#39;client\u0026#39; are SCRAM-SHA-256=iterations=8192 29# SCRAM credential configs for user-principal \u0026#39;consumer\u0026#39; are SCRAM-SHA-256=iterations=8192 30# SCRAM credential configs for user-principal \u0026#39;producer\u0026#39; are SCRAM-SHA-256=iterations=8192 31# SCRAM credential configs for user-principal \u0026#39;superuser\u0026#39; are SCRAM-SHA-256=iterations=8192 ACL Creation The user named client is authorized to perform all operations on a topic named inventory. This user will be used to demonstrate how to produce and consume messages using Kafka CLI.\n1## create ACL for inventory topic. The user \u0026#39;client\u0026#39; has permission on all operations 2I have no name!@b28e71a2ae2c:/opt/bitnami/kafka/bin$ ./kafka-acls.sh --bootstrap-server kafka-1:9094 --add \\ 3 --allow-principal User:client --operation All --group \u0026#39;*\u0026#39; \\ 4 --topic inventory --command-config /opt/bitnami/kafka/config/superuser.properties 5# Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=inventory, patternType=LITERAL)`: 6# (principal=User:client, host=*, operation=ALL, permissionType=ALLOW) 7 8# Adding ACLs for resource `ResourcePattern(resourceType=GROUP, name=*, patternType=LITERAL)`: 9# (principal=User:client, host=*, operation=ALL, permissionType=ALLOW) 10 11# Current ACLs for resource `ResourcePattern(resourceType=GROUP, name=*, patternType=LITERAL)`: 12# (principal=User:client, host=*, operation=ALL, permissionType=ALLOW) 13 14# Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=inventory, patternType=LITERAL)`: 15# (principal=User:client, host=*, operation=ALL, permissionType=ALLOW) 16 17I have no name!@b28e71a2ae2c:/opt/bitnami/kafka/bin$ ./kafka-acls.sh --bootstrap-server kafka-1:9094 --list \\ 18 --topic inventory --command-config /opt/bitnami/kafka/config/superuser.properties 19# Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=inventory, patternType=LITERAL)`: 20# (principal=User:client, host=*, operation=ALL, permissionType=ALLOW) The Kafka CLI supports to create canned ACLs that are specific to a producer or consumer. As we have separate Python producer and consumer apps, separate ACLs are created according to their roles.\n1I have no name!@b28e71a2ae2c:/opt/bitnami/kafka/bin$ ./kafka-acls.sh --bootstrap-server kafka-1:9094 --add \\ 2 --allow-principal User:producer --producer \\ 3 --topic orders --command-config /opt/bitnami/kafka/config/superuser.properties 4# Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=orders, patternType=LITERAL)`: 5# (principal=User:producer, host=*, operation=WRITE, permissionType=ALLOW) 6# (principal=User:producer, host=*, operation=DESCRIBE, permissionType=ALLOW) 7# (principal=User:producer, host=*, operation=CREATE, permissionType=ALLOW) 8 9# Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=orders, patternType=LITERAL)`: 10# (principal=User:producer, host=*, operation=WRITE, permissionType=ALLOW) 11# (principal=User:producer, host=*, operation=CREATE, permissionType=ALLOW) 12# (principal=User:producer, host=*, operation=DESCRIBE, permissionType=ALLOW) 13 14I have no name!@b28e71a2ae2c:/opt/bitnami/kafka/bin$ ./kafka-acls.sh --bootstrap-server kafka-1:9094 --add \\ 15 --allow-principal User:consumer --consumer --group \u0026#39;*\u0026#39; \\ 16 --topic orders --command-config /opt/bitnami/kafka/config/superuser.properties 17# Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=orders, patternType=LITERAL)`: 18# (principal=User:consumer, host=*, operation=READ, permissionType=ALLOW) 19# (principal=User:consumer, host=*, operation=DESCRIBE, permissionType=ALLOW) 20 21# Adding ACLs for resource `ResourcePattern(resourceType=GROUP, name=*, patternType=LITERAL)`: 22# (principal=User:consumer, host=*, operation=READ, permissionType=ALLOW) 23 24# Current ACLs for resource `ResourcePattern(resourceType=GROUP, name=*, patternType=LITERAL)`: 25# (principal=User:consumer, host=*, operation=READ, permissionType=ALLOW) 26# (principal=User:client, host=*, operation=ALL, permissionType=ALLOW) 27 28# Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=orders, patternType=LITERAL)`: 29# (principal=User:producer, host=*, operation=CREATE, permissionType=ALLOW) 30# (principal=User:producer, host=*, operation=DESCRIBE, permissionType=ALLOW) 31# (principal=User:consumer, host=*, operation=DESCRIBE, permissionType=ALLOW) 32# (principal=User:producer, host=*, operation=WRITE, permissionType=ALLOW) 33# (principal=User:consumer, host=*, operation=READ, permissionType=ALLOW) 34 35I have no name!@b28e71a2ae2c:/opt/bitnami/kafka/bin$ ./kafka-acls.sh --bootstrap-server kafka-1:9094 --list \\ 36 --topic orders --command-config /opt/bitnami/kafka/config/superuser.properties 37# Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=orders, patternType=LITERAL)`: 38# (principal=User:producer, host=*, operation=CREATE, permissionType=ALLOW) 39# (principal=User:producer, host=*, operation=DESCRIBE, permissionType=ALLOW) 40# (principal=User:consumer, host=*, operation=DESCRIBE, permissionType=ALLOW) 41# (principal=User:producer, host=*, operation=WRITE, permissionType=ALLOW) 42# (principal=User:consumer, host=*, operation=READ, permissionType=ALLOW) Kafka CLI The following configuration is necessary to use the SASL_SSL listener. Firstly the security protocol is set to be SASL_SSL. Next the location of the Truststore file and the password to access it are specified for SSL encryption. Finally, the SASL mechanism and corresponding JAAS configuration are added for client authentication.\n1# kafka-dev-with-docker/part-11/client.properties 2sasl.mechanism=SCRAM-SHA-256 3sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required username=\u0026#34;client\u0026#34; password=\u0026#34;password\u0026#34;; 4security.protocol=SASL_SSL 5ssl.truststore.location=/opt/bitnami/kafka/config/certs/kafka.truststore.jks 6ssl.truststore.password=supersecret Below shows a producer example. It produces messages to a topic named inventory successfully via the SASL_SSL listener. Note the client configuration file (client.properties) is specified in the producer configuration, and it is available via volume-mapping.\n1## producer 2$ docker exec -it kafka-0 bash 3I have no name!@b28e71a2ae2c:/$ cd /opt/bitnami/kafka/bin/ 4I have no name!@b28e71a2ae2c:/opt/bitnami/kafka/bin$ ./kafka-console-producer.sh --bootstrap-server kafka-0:9093 \\ 5 --topic inventory --producer.config /opt/bitnami/kafka/config/client.properties 6\u0026gt;product: apples, quantity: 5 7\u0026gt;product: lemons, quantity: 7 Once messages are created, we can check it by a consumer. We can execute a consumer in a separate console.\n1## consumer 2$ docker exec -it kafka-0 bash 3I have no name!@b28e71a2ae2c:/$ cd /opt/bitnami/kafka/bin/ 4I have no name!@b28e71a2ae2c:/opt/bitnami/kafka/bin$ ./kafka-console-consumer.sh --bootstrap-server kafka-0:9093 \\ 5 --topic inventory --consumer.config /opt/bitnami/kafka/config/client.properties --from-beginning 6# [2023-06-21 01:30:01,890] WARN [Consumer clientId=consumer-console-consumer-94700-1, groupId=console-consumer-94700] Error while fetching metadata with correlation id 2 : {inventory=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient) 7product: apples, quantity: 5 8product: lemons, quantity: 7 Python Client We will run the Python producer and consumer apps using docker-compose. At startup, each of them installs required packages and executes its corresponding app script. As it shares the same network to the Kafka cluster, we can take the service names (e.g. kafka-0) on port 9094 as Kafka bootstrap servers. As shown below, we will need the certificate of the CA (ca-root.pem) and it will be available via volume-mapping. Also, the relevant SCRAM user credentials are added to environment variables. The apps can be started by docker-compose -f compose-apps.yml up -d.\n1# kafka-dev-with-docker/part-11/compose-apps.yml 2version: \u0026#34;3.5\u0026#34; 3 4services: 5 producer: 6 image: bitnami/python:3.9 7 container_name: producer 8 command: \u0026#34;sh -c \u0026#39;pip install -r requirements.txt \u0026amp;\u0026amp; python producer.py\u0026#39;\u0026#34; 9 networks: 10 - kafkanet 11 environment: 12 BOOTSTRAP_SERVERS: kafka-0:9094,kafka-1:9094,kafka-2:9094 13 TOPIC_NAME: orders 14 TZ: Australia/Sydney 15 SASL_USERNAME: producer 16 SASL_PASSWORD: password 17 volumes: 18 - .:/app 19 consumer: 20 image: bitnami/python:3.9 21 container_name: consumer 22 command: \u0026#34;sh -c \u0026#39;pip install -r requirements.txt \u0026amp;\u0026amp; python consumer.py\u0026#39;\u0026#34; 23 networks: 24 - kafkanet 25 environment: 26 BOOTSTRAP_SERVERS: kafka-0:9094,kafka-1:9094,kafka-2:9094 27 TOPIC_NAME: orders 28 GROUP_ID: orders-group 29 TZ: Australia/Sydney 30 SASL_USERNAME: consumer 31 SASL_PASSWORD: password 32 volumes: 33 - .:/app 34 35networks: 36 kafkanet: 37 external: true 38 name: kafka-network Producer The same producer app discussed in Part 4 is used here. The following arguments are added to access the SASL_SSL listener.\nsecurity_protocol - Protocol used to communicate with brokers. ssl_check_hostname - Flag to configure whether SSL handshake should verify that the certificate matches the broker\u0026rsquo;s hostname. ssl_cafile - Optional filename of CA (certificate) file to use in certificate verification. sasl_mechanism - Authentication mechanism when security_protocol is configured for SASL_PLAINTEXT or SASL_SSL. sasl_plain_username - Username for SASL PLAIN and SCRAM authentication. Required if sasl_mechanism is PLAIN or one of the SCRAM mechanisms. sasl_plain_password - Password for SASL PLAIN and SCRAM authentication. Required if sasl_mechanism is PLAIN or one of the SCRAM mechanisms. 1# kafka-dev-with-docker/part-11/producer.py 2... 3 4class Producer: 5 def __init__(self, bootstrap_servers: list, topic: str): 6 self.bootstrap_servers = bootstrap_servers 7 self.topic = topic 8 self.producer = self.create() 9 10 def create(self): 11 return KafkaProducer( 12 bootstrap_servers=self.bootstrap_servers, 13 security_protocol=\u0026#34;SASL_SSL\u0026#34;, 14 ssl_check_hostname=True, 15 ssl_cafile=\u0026#34;pem/ca-root.pem\u0026#34;, 16 sasl_mechanism=\u0026#34;SCRAM-SHA-256\u0026#34;, 17 sasl_plain_username=os.environ[\u0026#34;SASL_USERNAME\u0026#34;], 18 sasl_plain_password=os.environ[\u0026#34;SASL_PASSWORD\u0026#34;], 19 value_serializer=lambda v: json.dumps(v, default=self.serialize).encode(\u0026#34;utf-8\u0026#34;), 20 key_serializer=lambda v: json.dumps(v, default=self.serialize).encode(\u0026#34;utf-8\u0026#34;), 21 ) 22 23 def send(self, orders: typing.List[Order]): 24 for order in orders: 25 try: 26 self.producer.send( 27 self.topic, key={\u0026#34;order_id\u0026#34;: order.order_id}, value=order.asdict() 28 ) 29 except Exception as e: 30 raise RuntimeError(\u0026#34;fails to send a message\u0026#34;) from e 31 self.producer.flush() 32 33... 34 35if __name__ == \u0026#34;__main__\u0026#34;: 36 producer = Producer( 37 bootstrap_servers=os.getenv(\u0026#34;BOOTSTRAP_SERVERS\u0026#34;, \u0026#34;localhost:29092\u0026#34;).split(\u0026#34;,\u0026#34;), 38 topic=os.getenv(\u0026#34;TOPIC_NAME\u0026#34;, \u0026#34;orders\u0026#34;), 39 ) 40 max_run = int(os.getenv(\u0026#34;MAX_RUN\u0026#34;, \u0026#34;-1\u0026#34;)) 41 logging.info(f\u0026#34;max run - {max_run}\u0026#34;) 42 current_run = 0 43 while True: 44 current_run += 1 45 logging.info(f\u0026#34;current run - {current_run}\u0026#34;) 46 if current_run \u0026gt; max_run and max_run \u0026gt;= 0: 47 logging.info(f\u0026#34;exceeds max run, finish\u0026#34;) 48 producer.producer.close() 49 break 50 producer.send(Order.auto().create(100)) 51 time.sleep(1) In the container log, we can check SSH Handshake and client authentication are performed successfully.\n1INFO:kafka.conn:\u0026lt;BrokerConnection node_id=bootstrap-0 host=kafka-1:9094 \u0026lt;handshake\u0026gt; [IPv4 (\u0026#39;192.168.0.3\u0026#39;, 9094)]\u0026gt;: Loading SSL CA from pem/ca-root.pem 2INFO:kafka.conn:\u0026lt;BrokerConnection node_id=bootstrap-0 host=kafka-1:9094 \u0026lt;authenticating\u0026gt; [IPv4 (\u0026#39;192.168.0.3\u0026#39;, 9094)]\u0026gt;: Authenticated as producer via SCRAM-SHA-256 3INFO:kafka.conn:\u0026lt;BrokerConnection node_id=bootstrap-0 host=kafka-1:9094 \u0026lt;authenticating\u0026gt; [IPv4 (\u0026#39;192.168.0.3\u0026#39;, 9094)]\u0026gt;: Connection complete. 4INFO:root:max run - -1 5INFO:root:current run - 1 6... 7INFO:root:current run - 2 Consumer The same consumer app in Part 4 is used here as well. As the producer app, the following arguments are added - security_protocol, ssl_check_hostname, ssl_cafile, sasl_mechanism, sasl_plain_username and sasl_plain_password.\n1# kafka-dev-with-docker/part-11/consumer.py 2... 3 4class Consumer: 5 def __init__(self, bootstrap_servers: list, topics: list, group_id: str) -\u0026gt; None: 6 self.bootstrap_servers = bootstrap_servers 7 self.topics = topics 8 self.group_id = group_id 9 self.consumer = self.create() 10 11 def create(self): 12 return KafkaConsumer( 13 *self.topics, 14 bootstrap_servers=self.bootstrap_servers, 15 security_protocol=\u0026#34;SASL_SSL\u0026#34;, 16 ssl_check_hostname=True, 17 ssl_cafile=\u0026#34;pem/ca-root.pem\u0026#34;, 18 sasl_mechanism=\u0026#34;SCRAM-SHA-256\u0026#34;, 19 sasl_plain_username=os.environ[\u0026#34;SASL_USERNAME\u0026#34;], 20 sasl_plain_password=os.environ[\u0026#34;SASL_PASSWORD\u0026#34;], 21 auto_offset_reset=\u0026#34;earliest\u0026#34;, 22 enable_auto_commit=True, 23 group_id=self.group_id, 24 key_deserializer=lambda v: v.decode(\u0026#34;utf-8\u0026#34;), 25 value_deserializer=lambda v: v.decode(\u0026#34;utf-8\u0026#34;), 26 ) 27 28 def process(self): 29 try: 30 while True: 31 msg = self.consumer.poll(timeout_ms=1000) 32 if msg is None: 33 continue 34 self.print_info(msg) 35 time.sleep(1) 36 except KafkaError as error: 37 logging.error(error) 38 39 def print_info(self, msg: dict): 40 for t, v in msg.items(): 41 for r in v: 42 logging.info( 43 f\u0026#34;key={r.key}, value={r.value}, topic={t.topic}, partition={t.partition}, offset={r.offset}, ts={r.timestamp}\u0026#34; 44 ) 45 46 47if __name__ == \u0026#34;__main__\u0026#34;: 48 consumer = Consumer( 49 bootstrap_servers=os.getenv(\u0026#34;BOOTSTRAP_SERVERS\u0026#34;, \u0026#34;localhost:29092\u0026#34;).split(\u0026#34;,\u0026#34;), 50 topics=os.getenv(\u0026#34;TOPIC_NAME\u0026#34;, \u0026#34;orders\u0026#34;).split(\u0026#34;,\u0026#34;), 51 group_id=os.getenv(\u0026#34;GROUP_ID\u0026#34;, \u0026#34;orders-group\u0026#34;), 52 ) 53 consumer.process() We can also check messages are consumed after SSH Handshake and client authentication are succeeded in the container log.\n1INFO:kafka.coordinator:Elected group leader -- performing partition assignments using range 2INFO:kafka.coordinator:Successfully joined group orders-group with generation 1 3INFO:kafka.consumer.subscription_state:Updated partition assignment: [TopicPartition(topic=\u0026#39;orders\u0026#39;, partition=0)] 4INFO:kafka.coordinator.consumer:Setting newly assigned partitions {TopicPartition(topic=\u0026#39;orders\u0026#39;, partition=0)} for group orders-group 5... 6INFO:kafka.conn:\u0026lt;BrokerConnection node_id=2 host=kafka-2:9094 \u0026lt;connecting\u0026gt; [IPv4 (\u0026#39;192.168.0.5\u0026#39;, 9094)]\u0026gt;: connecting to kafka-2:9094 [(\u0026#39;192.168.0.5\u0026#39;, 9094) IPv4] 7INFO:kafka.conn:\u0026lt;BrokerConnection node_id=2 host=kafka-2:9094 \u0026lt;handshake\u0026gt; [IPv4 (\u0026#39;192.168.0.5\u0026#39;, 9094)]\u0026gt;: Loading SSL CA from pem/ca-root.pem 8INFO:kafka.conn:\u0026lt;BrokerConnection node_id=2 host=kafka-2:9094 \u0026lt;authenticating\u0026gt; [IPv4 (\u0026#39;192.168.0.5\u0026#39;, 9094)]\u0026gt;: Authenticated as consumer via SCRAM-SHA-256 9INFO:kafka.conn:\u0026lt;BrokerConnection node_id=2 host=kafka-2:9094 \u0026lt;authenticating\u0026gt; [IPv4 (\u0026#39;192.168.0.5\u0026#39;, 9094)]\u0026gt;: Connection complete. 10INFO:root:key={\u0026#34;order_id\u0026#34;: \u0026#34;7de9132b-c71e-4739-a2f8-7b6aed7ce8c9\u0026#34;}, value={\u0026#34;order_id\u0026#34;: \u0026#34;7de9132b-c71e-4739-a2f8-7b6aed7ce8c9\u0026#34;, \u0026#34;ordered_at\u0026#34;: \u0026#34;2023-06-21T03:13:19.363325\u0026#34;, \u0026#34;user_id\u0026#34;: \u0026#34;017\u0026#34;, \u0026#34;order_items\u0026#34;: [{\u0026#34;product_id\u0026#34;: 553, \u0026#34;quantity\u0026#34;: 8}]}, topic=orders, partition=0, offset=0, ts=1687317199370 11INFO:root:key={\u0026#34;order_id\u0026#34;: \u0026#34;f222065e-489c-4ecd-b864-88163e800c79\u0026#34;}, value={\u0026#34;order_id\u0026#34;: \u0026#34;f222065e-489c-4ecd-b864-88163e800c79\u0026#34;, \u0026#34;ordered_at\u0026#34;: \u0026#34;2023-06-21T03:13:19.363402\u0026#34;, \u0026#34;user_id\u0026#34;: \u0026#34;023\u0026#34;, \u0026#34;order_items\u0026#34;: [{\u0026#34;product_id\u0026#34;: 417, \u0026#34;quantity\u0026#34;: 10}, {\u0026#34;product_id\u0026#34;: 554, \u0026#34;quantity\u0026#34;: 1}, {\u0026#34;product_id\u0026#34;: 942, \u0026#34;quantity\u0026#34;: 6}]}, topic=orders, partition=0, offset=1, ts=1687317199371 Kafka-UI Kafka-UI is also a Java client, and it accepts the Keystore file of the Kafka Truststore (kafka.truststore.jks). We can specify the file and password to access it as environment variables for SSL encryption. For client authentication, we need to add the SASL mechanism and corresponding JAAS configuration to environment variables. Note that the super user credentials are added to the configuration but it is not recommended in production environment. The app can be started by docker-compose -f compose-ui.yml up -d.\n1# kafka-dev-with-docker/part-11/compose-ui.yml 2version: \u0026#34;3.5\u0026#34; 3 4services: 5 kafka-ui: 6 image: provectuslabs/kafka-ui:master 7 container_name: kafka-ui 8 ports: 9 - \u0026#34;8080:8080\u0026#34; 10 networks: 11 - kafkanet 12 environment: 13 KAFKA_CLUSTERS_0_NAME: local 14 KAFKA_CLUSTERS_0_PROPERTIES_SECURITY_PROTOCOL: SASL_SSL 15 KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka-0:9094,kafka-1:9094,kafka-2:9094 16 KAFKA_CLUSTERS_0_PROPERTIES_SASL_MECHANISM: SCRAM-SHA-256 17 KAFKA_CLUSTERS_0_PROPERTIES_PROTOCOL: SASL 18 KAFKA_CLUSTERS_0_PROPERTIES_SASL_JAAS_CONFIG: org.apache.kafka.common.security.scram.ScramLoginModule required username=\u0026#34;superuser\u0026#34; password=\u0026#34;password\u0026#34;; 19 KAFKA_CLUSTERS_0_ZOOKEEPER: zookeeper:2181 20 KAFKA_CLUSTERS_0_SSL_TRUSTSTORELOCATION: /kafka.truststore.jks 21 KAFKA_CLUSTERS_0_SSL_TRUSTSTOREPASSWORD: supersecret 22 volumes: 23 - ./truststore/kafka.truststore.jks:/kafka.truststore.jks:ro 24 25networks: 26 kafkanet: 27 external: true 28 name: kafka-network Summary In the previous posts, we discussed how to implement client authentication by TLS (SSL or TLS/SSL) and SASL authentication. One of the key benefits of client authentication is achieving user access control. In this post, we discussed how to configure Kafka authorization with Java and Python client examples while SASL is kept for client authentication.\n","date":"July 20, 2023","img":"/blog/2023-07-20-kafka-development-with-docker-part-11/featured.png","lang":"en","langName":"English","largeImg":"/blog/2023-07-20-kafka-development-with-docker-part-11/featured_hu02926410fee69d2a9d233fecc3582cc8_458848_500x0_resize_box_3.png","permalink":"/blog/2023-07-20-kafka-development-with-docker-part-11/","series":[{"title":"Kafka Development With Docker","url":"/series/kafka-development-with-docker/"}],"smallImg":"/blog/2023-07-20-kafka-development-with-docker-part-11/featured_hu02926410fee69d2a9d233fecc3582cc8_458848_180x0_resize_box_3.png","tags":[{"title":"Apache Kafka","url":"/tags/apache-kafka/"},{"title":"Security","url":"/tags/security/"},{"title":"Python","url":"/tags/python/"},{"title":"Docker","url":"/tags/docker/"},{"title":"Docker Compose","url":"/tags/docker-compose/"}],"timestamp":1689811200,"title":"Kafka Development With Docker - Part 11 Kafka Authorization"},{"categories":[{"title":"Apache Kafka","url":"/categories/apache-kafka/"}],"content":"In the previous post, we discussed TLS (SSL or TLS/SSL) authentication to improve security. It enforces two-way verification where a client certificate is verified by Kafka brokers. Client authentication can also be enabled by Simple Authentication and Security Layer (SASL), and we will discuss how to implement SASL authentication with Java and Python client examples in this post.\nPart 1 Cluster Setup Part 2 Management App Part 3 Kafka Connect Part 4 Producer and Consumer Part 5 Glue Schema Registry Part 6 Kafka Connect with Glue Schema Registry Part 7 Producer and Consumer with Glue Schema Registry Part 8 SSL Encryption Part 9 SSL Authentication Part 10 SASL Authentication (this post) Part 11 Kafka Authorization Certificate Setup As we will leave Kafka communication to remain encrypted, we need to keep the components for SSL encryption. The details can be found in Part 8, and those components can be generated by generate.sh. Once we execute the script, the following files are created.\n1$ tree certificate-authority keystore truststore pem 2certificate-authority 3├── ca-cert 4└── ca-key 5keystore 6├── kafka-0.server.keystore.jks 7├── kafka-1.server.keystore.jks 8└── kafka-2.server.keystore.jks 9truststore 10└── kafka.truststore.jks 11pem 12└── ca-root.pem Kafka Cluster Update I was planning to keep the Zookeeper node as simple as possible because the focus is discussing client authentication. Without enabling authentication in the Zookeeper node, however, I encountered an error that indicates SASL authentication failed using login context \u0026lsquo;Client\u0026rsquo;. Therefore, I had to enable authentication and specify user credentials. The credentials will be referred in the Client context of the Java Authentication and Authorization Service(JAAS) configuration file (kafka_jaas.conf). The details about the configuration file can be found below.\nWhen it comes to Kafka broker configurations, we should add the SASL_SSL listener to the broker configuration and the port 9094 is reserved for it. Both the Keystore and Truststore files are specified in the broker configuration for SSL. The former is to send the broker certificate to clients while the latter is necessary because a Kafka broker can be a client of other brokers. While SASL supports multiple mechanisms, we enabled the Salted Challenge Response Authentication Mechanism (SCRAM) by specifying SCRAM-SHA-256 in the following environment variables.\nKAFKA_CFG_SASL_ENABLED_MECHANISMS KAFKA_CFG_SASL_MECHANISM_INTER_BROKER_PROTOCOL. The changes made to the first Kafka broker are shown below, and the same updates are made to the other brokers. The source can be found in the GitHub repository of this post, and the cluster can be started by docker-compose -f compose-kafka.yml up -d.\n1# kafka-dev-with-docker/part-10/compose-kafka.yml 2version: \u0026#34;3.5\u0026#34; 3 4services: 5 zookeeper: 6 image: bitnami/zookeeper:3.5 7 container_name: zookeeper 8 ports: 9 - \u0026#34;2181\u0026#34; 10 networks: 11 - kafkanet 12 environment: 13 - ZOO_ENABLE_AUTH=yes 14 - ZOO_SERVER_USERS=admin 15 - ZOO_SERVER_PASSWORDS=password 16 volumes: 17 - zookeeper_data:/bitnami/zookeeper 18 kafka-0: 19 image: bitnami/kafka:2.8.1 20 container_name: kafka-0 21 expose: 22 - 9092 23 - 9093 24 - 9094 25 ports: 26 - \u0026#34;29092:29092\u0026#34; 27 networks: 28 - kafkanet 29 environment: 30 - ALLOW_PLAINTEXT_LISTENER=yes 31 - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181 32 - KAFKA_CFG_BROKER_ID=0 33 - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=INTERNAL:PLAINTEXT,SSL:SSL,SASL_SSL:SASL_SSL,EXTERNAL:PLAINTEXT 34 - KAFKA_CFG_LISTENERS=INTERNAL://:9092,SSL://:9093,SASL_SSL://:9094,EXTERNAL://:29092 35 - KAFKA_CFG_ADVERTISED_LISTENERS=INTERNAL://kafka-0:9092,SSL://kafka-0:9093,SASL_SSL://kafka-0:9094,EXTERNAL://localhost:29092 36 - KAFKA_CFG_INTER_BROKER_LISTENER_NAME=SSL 37 - KAFKA_CFG_SSL_KEYSTORE_LOCATION=/opt/bitnami/kafka/config/certs/kafka.keystore.jks 38 - KAFKA_CFG_SSL_KEYSTORE_PASSWORD=supersecret 39 - KAFKA_CFG_SSL_KEY_PASSWORD=supersecret 40 - KAFKA_CFG_SSL_TRUSTSTORE_LOCATION=/opt/bitnami/kafka/config/certs/kafka.truststore.jks 41 - KAFKA_CFG_SSL_TRUSTSTORE_PASSWORD=supersecret 42 - KAFKA_CFG_SASL_ENABLED_MECHANISMS=SCRAM-SHA-256 43 - KAFKA_CFG_SASL_MECHANISM_INTER_BROKER_PROTOCOL=SCRAM-SHA-256 44 volumes: 45 - kafka_0_data:/bitnami/kafka 46 - ./keystore/kafka-0.server.keystore.jks:/opt/bitnami/kafka/config/certs/kafka.keystore.jks:ro 47 - ./truststore/kafka.truststore.jks:/opt/bitnami/kafka/config/certs/kafka.truststore.jks:ro 48 - ./kafka_jaas.conf:/opt/bitnami/kafka/config/kafka_jaas.conf:ro 49 - ./client.properties:/opt/bitnami/kafka/config/client.properties:ro 50 - ./command.properties:/opt/bitnami/kafka/config/command.properties:ro 51 depends_on: 52 - zookeeper 53 54... 55 56networks: 57 kafkanet: 58 name: kafka-network 59 60... As mentioned earlier, the broker needs a JAAS configuration file, and it should include 2 contexts - KafkaServer and Client. The former is required for inter-broker communication while the latter is for accessing the Zookeeper node. As SASL is not enabled for inter-broker communication, dummy credentials are added for the KafkaServer context while the Zookeeper user credentials are kept in the Client context. The credentials are those that are specified by the following environment variables in the Zookeeper node - ZOO_SERVER_USERS and ZOO_SERVER_PASSWORDS.\n1# kafka-dev-with-docker/part-10/kafka_jaas.conf 2KafkaServer { 3 org.apache.kafka.common.security.scram.ScramLoginModule required 4 username=\u0026#34;_\u0026#34; 5 password=\u0026#34;_\u0026#34;; 6}; 7 8Client { 9 org.apache.kafka.common.security.plain.PlainLoginModule required 10 username=\u0026#34;admin\u0026#34; 11 password=\u0026#34;password\u0026#34;; 12}; Examples For SSL encryption, Java and non-Java clients need different configurations. The former can use the Keystore file of the Truststore directly while the latter needs corresponding details in a PEM file. The Kafka CLI and Kafka-UI will be taken as Java client examples while Python producer/consumer will be used to illustrate non-Java clients.\nFor client authentication, we will create a single SCRAM user named client and authentication will be made by that user for all examples.\nUser Creation The SCRAM user can be created by using either the PLAINTEXT or SSL listener within a broker container. Here we will use the SSL listener with the following configuration.\n1# kafka-dev-with-docker/part-10/command.properties 2security.protocol=SSL 3ssl.truststore.location=/opt/bitnami/kafka/config/certs/kafka.truststore.jks 4ssl.truststore.password=supersecret There is no user by default, and we can create the SCRAM user by specifying the entity name (client) and SCRAM password (password) as shown below.\n1$ docker exec -it kafka-0 bash 2I have no name!@ab0c55c36b22:/$ cd /opt/bitnami/kafka/bin/ 3## describe (list) all users - no user exists 4I have no name!@ab0c55c36b22:/opt/bitnami/kafka/bin$ ./kafka-configs.sh --bootstrap-server kafka-1:9093 --describe \\ 5 --entity-type users --command-config /opt/bitnami/kafka/config/command.properties 6 7## create a SCRAM user \u0026#39;client\u0026#39; 8I have no name!@ab0c55c36b22:/opt/bitnami/kafka/bin$ ./kafka-configs.sh --bootstrap-server kafka-1:9093 --alter \\ 9 --add-config \u0026#39;SCRAM-SHA-256=[iterations=8192,password=password]\u0026#39; \\ 10 --entity-type users --entity-name client \\ 11 --command-config /opt/bitnami/kafka/config/command.properties 12# Completed updating config for user client. 13 14## check if the new user exists 15I have no name!@ab0c55c36b22:/opt/bitnami/kafka/bin$ ./kafka-configs.sh --bootstrap-server kafka-1:9093 --describe \\ 16 --entity-type users --command-config /opt/bitnami/kafka/config/command.properties 17# SCRAM credential configs for user-principal \u0026#39;client\u0026#39; are SCRAM-SHA-256=iterations=8192 Kafka CLI The following configuration is necessary to use the SASL_SSL listener. Firstly the security protocol is set to be SASL_SSL. Next the location of the Truststore file and the password to access it are specified for SSL encryption. Finally, the SASL mechanism and corresponding JAAS configuration are added for client authentication.\n1# kafka-dev-with-docker/part-10/client.properties 2sasl.mechanism=SCRAM-SHA-256 3sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required username=\u0026#34;client\u0026#34; password=\u0026#34;password\u0026#34;; 4security.protocol=SASL_SSL 5ssl.truststore.location=/opt/bitnami/kafka/config/certs/kafka.truststore.jks 6ssl.truststore.password=supersecret Below shows a producer example. It produces messages to a topic named inventory successfully via the SASL_SSL listener. Note the client configuration file (client.properties) is specified in the producer configuration, and it is available via volume-mapping.\n1## producer 2$ docker exec -it kafka-0 bash 3I have no name!@ab0c55c36b22:/$ cd /opt/bitnami/kafka/bin/ 4I have no name!@ab0c55c36b22:/opt/bitnami/kafka/bin$ ./kafka-console-producer.sh --bootstrap-server kafka-0:9093 \\ 5 --topic inventory --producer.config /opt/bitnami/kafka/config/client.properties 6\u0026gt;product: apples, quantity: 5 7\u0026gt;product: lemons, quantity: 7 Once messages are created, we can check it by a consumer. We can execute a consumer in a separate console.\n1## consumer 2$ docker exec -it kafka-0 bash 3I have no name!@ab0c55c36b22:/$ cd /opt/bitnami/kafka/bin/ 4I have no name!@ab0c55c36b22:/opt/bitnami/kafka/bin$ ./kafka-console-consumer.sh --bootstrap-server kafka-0:9093 \\ 5 --topic inventory --consumer.config /opt/bitnami/kafka/config/client.properties --from-beginning 6# [2023-06-21 01:30:01,890] WARN [Consumer clientId=consumer-console-consumer-94700-1, groupId=console-consumer-94700] Error while fetching metadata with correlation id 2 : {inventory=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient) 7product: apples, quantity: 5 8product: lemons, quantity: 7 Python Client We will run the Python producer and consumer apps using docker-compose. At startup, each of them installs required packages and executes its corresponding app script. As it shares the same network to the Kafka cluster, we can take the service names (e.g. kafka-0) on port 9094 as Kafka bootstrap servers. As shown below, we will need the certificate of the CA (ca-root.pem) and it will be available via volume-mapping. Also, the SCRAM user credentials are added to environment variables. The apps can be started by docker-compose -f compose-apps.yml up -d.\n1# kafka-dev-with-docker/part-10/compose-apps.yml 2version: \u0026#34;3.5\u0026#34; 3 4services: 5 producer: 6 image: bitnami/python:3.9 7 container_name: producer 8 command: \u0026#34;sh -c \u0026#39;pip install -r requirements.txt \u0026amp;\u0026amp; python producer.py\u0026#39;\u0026#34; 9 networks: 10 - kafkanet 11 environment: 12 BOOTSTRAP_SERVERS: kafka-0:9094,kafka-1:9094,kafka-2:9094 13 TOPIC_NAME: orders 14 TZ: Australia/Sydney 15 SASL_USERNAME: client 16 SASL_PASSWORD: password 17 volumes: 18 - .:/app 19 consumer: 20 image: bitnami/python:3.9 21 container_name: consumer 22 command: \u0026#34;sh -c \u0026#39;pip install -r requirements.txt \u0026amp;\u0026amp; python consumer.py\u0026#39;\u0026#34; 23 networks: 24 - kafkanet 25 environment: 26 BOOTSTRAP_SERVERS: kafka-0:9094,kafka-1:9094,kafka-2:9094 27 TOPIC_NAME: orders 28 GROUP_ID: orders-group 29 TZ: Australia/Sydney 30 SASL_USERNAME: client 31 SASL_PASSWORD: password 32 volumes: 33 - .:/app 34 35networks: 36 kafkanet: 37 external: true 38 name: kafka-network Producer The same producer app discussed in Part 4 is used here. The following arguments are added to access the SASL_SSL listener.\nsecurity_protocol - Protocol used to communicate with brokers. ssl_check_hostname - Flag to configure whether SSL handshake should verify that the certificate matches the broker\u0026rsquo;s hostname. ssl_cafile - Optional filename of CA (certificate) file to use in certificate verification. sasl_mechanism - Authentication mechanism when security_protocol is configured for SASL_PLAINTEXT or SASL_SSL. sasl_plain_username - Username for SASL PLAIN and SCRAM authentication. Required if sasl_mechanism is PLAIN or one of the SCRAM mechanisms. sasl_plain_password - Password for SASL PLAIN and SCRAM authentication. Required if sasl_mechanism is PLAIN or one of the SCRAM mechanisms. 1# kafka-dev-with-docker/part-10/producer.py 2... 3 4class Producer: 5 def __init__(self, bootstrap_servers: list, topic: str): 6 self.bootstrap_servers = bootstrap_servers 7 self.topic = topic 8 self.producer = self.create() 9 10 def create(self): 11 return KafkaProducer( 12 bootstrap_servers=self.bootstrap_servers, 13 security_protocol=\u0026#34;SASL_SSL\u0026#34;, 14 ssl_check_hostname=True, 15 ssl_cafile=\u0026#34;pem/ca-root.pem\u0026#34;, 16 sasl_mechanism=\u0026#34;SCRAM-SHA-256\u0026#34;, 17 sasl_plain_username=os.environ[\u0026#34;SASL_USERNAME\u0026#34;], 18 sasl_plain_password=os.environ[\u0026#34;SASL_PASSWORD\u0026#34;], 19 value_serializer=lambda v: json.dumps(v, default=self.serialize).encode(\u0026#34;utf-8\u0026#34;), 20 key_serializer=lambda v: json.dumps(v, default=self.serialize).encode(\u0026#34;utf-8\u0026#34;), 21 ) 22 23 def send(self, orders: typing.List[Order]): 24 for order in orders: 25 try: 26 self.producer.send( 27 self.topic, key={\u0026#34;order_id\u0026#34;: order.order_id}, value=order.asdict() 28 ) 29 except Exception as e: 30 raise RuntimeError(\u0026#34;fails to send a message\u0026#34;) from e 31 self.producer.flush() 32 33... 34 35if __name__ == \u0026#34;__main__\u0026#34;: 36 producer = Producer( 37 bootstrap_servers=os.getenv(\u0026#34;BOOTSTRAP_SERVERS\u0026#34;, \u0026#34;localhost:29092\u0026#34;).split(\u0026#34;,\u0026#34;), 38 topic=os.getenv(\u0026#34;TOPIC_NAME\u0026#34;, \u0026#34;orders\u0026#34;), 39 ) 40 max_run = int(os.getenv(\u0026#34;MAX_RUN\u0026#34;, \u0026#34;-1\u0026#34;)) 41 logging.info(f\u0026#34;max run - {max_run}\u0026#34;) 42 current_run = 0 43 while True: 44 current_run += 1 45 logging.info(f\u0026#34;current run - {current_run}\u0026#34;) 46 if current_run \u0026gt; max_run and max_run \u0026gt;= 0: 47 logging.info(f\u0026#34;exceeds max run, finish\u0026#34;) 48 producer.producer.close() 49 break 50 producer.send(Order.auto().create(100)) 51 time.sleep(1) In the container log, we can check SSH Handshake and client authentication are performed successfully.\n1INFO:kafka.conn:\u0026lt;BrokerConnection node_id=bootstrap-2 host=kafka-2:9094 \u0026lt;handshake\u0026gt; [IPv4 (\u0026#39;172.31.0.4\u0026#39;, 9094)]\u0026gt;: Loading SSL CA from pem/ca-root.pem 2INFO:kafka.conn:\u0026lt;BrokerConnection node_id=bootstrap-2 host=kafka-2:9094 \u0026lt;authenticating\u0026gt; [IPv4 (\u0026#39;172.31.0.4\u0026#39;, 9094)]\u0026gt;: Authenticated as client via SCRAM-SHA-256 3INFO:kafka.conn:\u0026lt;BrokerConnection node_id=bootstrap-2 host=kafka-2:9094 \u0026lt;authenticating\u0026gt; [IPv4 (\u0026#39;172.31.0.4\u0026#39;, 9094)]\u0026gt;: Connection complete. 4INFO:root:max run - -1 5INFO:root:current run - 1 6... 7INFO:root:current run - 2 Consumer The same consumer app in Part 4 is used here as well. As the producer app, the following arguments are added - security_protocol, ssl_check_hostname, ssl_cafile, sasl_mechanism, sasl_plain_username and sasl_plain_password.\n1# kafka-dev-with-docker/part-10/consumer.py 2... 3 4class Consumer: 5 def __init__(self, bootstrap_servers: list, topics: list, group_id: str) -\u0026gt; None: 6 self.bootstrap_servers = bootstrap_servers 7 self.topics = topics 8 self.group_id = group_id 9 self.consumer = self.create() 10 11 def create(self): 12 return KafkaConsumer( 13 *self.topics, 14 bootstrap_servers=self.bootstrap_servers, 15 security_protocol=\u0026#34;SASL_SSL\u0026#34;, 16 ssl_check_hostname=True, 17 ssl_cafile=\u0026#34;pem/ca-root.pem\u0026#34;, 18 sasl_mechanism=\u0026#34;SCRAM-SHA-256\u0026#34;, 19 sasl_plain_username=os.environ[\u0026#34;SASL_USERNAME\u0026#34;], 20 sasl_plain_password=os.environ[\u0026#34;SASL_PASSWORD\u0026#34;], 21 auto_offset_reset=\u0026#34;earliest\u0026#34;, 22 enable_auto_commit=True, 23 group_id=self.group_id, 24 key_deserializer=lambda v: v.decode(\u0026#34;utf-8\u0026#34;), 25 value_deserializer=lambda v: v.decode(\u0026#34;utf-8\u0026#34;), 26 ) 27 28 def process(self): 29 try: 30 while True: 31 msg = self.consumer.poll(timeout_ms=1000) 32 if msg is None: 33 continue 34 self.print_info(msg) 35 time.sleep(1) 36 except KafkaError as error: 37 logging.error(error) 38 39 def print_info(self, msg: dict): 40 for t, v in msg.items(): 41 for r in v: 42 logging.info( 43 f\u0026#34;key={r.key}, value={r.value}, topic={t.topic}, partition={t.partition}, offset={r.offset}, ts={r.timestamp}\u0026#34; 44 ) 45 46 47if __name__ == \u0026#34;__main__\u0026#34;: 48 consumer = Consumer( 49 bootstrap_servers=os.getenv(\u0026#34;BOOTSTRAP_SERVERS\u0026#34;, \u0026#34;localhost:29092\u0026#34;).split(\u0026#34;,\u0026#34;), 50 topics=os.getenv(\u0026#34;TOPIC_NAME\u0026#34;, \u0026#34;orders\u0026#34;).split(\u0026#34;,\u0026#34;), 51 group_id=os.getenv(\u0026#34;GROUP_ID\u0026#34;, \u0026#34;orders-group\u0026#34;), 52 ) 53 consumer.process() We can also check messages are consumed after SSH Handshake and client authentication are succeeded in the container log.\n1... 2INFO:kafka.coordinator:Elected group leader -- performing partition assignments using range 3INFO:kafka.coordinator:Successfully joined group orders-group with generation 1 4INFO:kafka.consumer.subscription_state:Updated partition assignment: [TopicPartition(topic=\u0026#39;orders\u0026#39;, partition=0)] 5INFO:kafka.coordinator.consumer:Setting newly assigned partitions {TopicPartition(topic=\u0026#39;orders\u0026#39;, partition=0)} for group orders-group 6INFO:kafka.conn:\u0026lt;BrokerConnection node_id=0 host=kafka-0:9094 \u0026lt;connecting\u0026gt; [IPv4 (\u0026#39;172.31.0.5\u0026#39;, 9094)]\u0026gt;: connecting to kafka-0:9094 [(\u0026#39;172.31.0.5\u0026#39;, 9094) IPv4] 7INFO:kafka.conn:\u0026lt;BrokerConnection node_id=0 host=kafka-0:9094 \u0026lt;handshake\u0026gt; [IPv4 (\u0026#39;172.31.0.5\u0026#39;, 9094)]\u0026gt;: Loading SSL CA from pem/ca-root.pem 8INFO:kafka.conn:\u0026lt;BrokerConnection node_id=0 host=kafka-0:9094 \u0026lt;authenticating\u0026gt; [IPv4 (\u0026#39;172.31.0.5\u0026#39;, 9094)]\u0026gt;: Authenticated as client via SCRAM-SHA-256 9INFO:kafka.conn:\u0026lt;BrokerConnection node_id=0 host=kafka-0:9094 \u0026lt;authenticating\u0026gt; [IPv4 (\u0026#39;172.31.0.5\u0026#39;, 9094)]\u0026gt;: Connection complete. 10INFO:kafka.conn:\u0026lt;BrokerConnection node_id=bootstrap-0 host=kafka-2:9094 \u0026lt;connected\u0026gt; [IPv4 (\u0026#39;172.31.0.4\u0026#39;, 9094)]\u0026gt;: Closing connection. 11INFO:root:key={\u0026#34;order_id\u0026#34;: \u0026#34;715720d6-cf21-4c87-ba05-28660109aa73\u0026#34;}, value={\u0026#34;order_id\u0026#34;: \u0026#34;715720d6-cf21-4c87-ba05-28660109aa73\u0026#34;, \u0026#34;ordered_at\u0026#34;: \u0026#34;2023-06-21T01:39:51.291932\u0026#34;, \u0026#34;user_id\u0026#34;: \u0026#34;016\u0026#34;, \u0026#34;order_items\u0026#34;: [{\u0026#34;product_id\u0026#34;: 956, \u0026#34;quantity\u0026#34;: 7}]}, topic=orders, partition=0, offset=0, ts=1687311592501 12INFO:root:key={\u0026#34;order_id\u0026#34;: \u0026#34;1d5ece29-ab03-4f62-b88a-bc2242e5e839\u0026#34;}, value={\u0026#34;order_id\u0026#34;: \u0026#34;1d5ece29-ab03-4f62-b88a-bc2242e5e839\u0026#34;, \u0026#34;ordered_at\u0026#34;: \u0026#34;2023-06-21T01:39:51.292015\u0026#34;, \u0026#34;user_id\u0026#34;: \u0026#34;003\u0026#34;, \u0026#34;order_items\u0026#34;: [{\u0026#34;product_id\u0026#34;: 880, \u0026#34;quantity\u0026#34;: 5}, {\u0026#34;product_id\u0026#34;: 257, \u0026#34;quantity\u0026#34;: 5}]}, topic=orders, partition=0, offset=1, ts=1687311592501 Kafka-UI Kafka-UI is also a Java client, and it accepts the Keystore file of the Kafka Truststore (kafka.truststore.jks). We can specify the file and password to access it as environment variables for SSL encryption. For client authentication, we need to add the SASL mechanism and corresponding JAAS configuration to environment variables. The app can be started by docker-compose -f compose-ui.yml up -d.\n1# kafka-dev-with-docker/part-10/compose-ui.yml 2version: \u0026#34;3.5\u0026#34; 3 4services: 5 kafka-ui: 6 image: provectuslabs/kafka-ui:master 7 container_name: kafka-ui 8 ports: 9 - \u0026#34;8080:8080\u0026#34; 10 networks: 11 - kafkanet 12 environment: 13 KAFKA_CLUSTERS_0_NAME: local 14 KAFKA_CLUSTERS_0_PROPERTIES_SECURITY_PROTOCOL: SASL_SSL 15 KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka-0:9094,kafka-1:9094,kafka-2:9094 16 KAFKA_CLUSTERS_0_PROPERTIES_SASL_MECHANISM: SCRAM-SHA-256 17 KAFKA_CLUSTERS_0_PROPERTIES_PROTOCOL: SASL 18 KAFKA_CLUSTERS_0_PROPERTIES_SASL_JAAS_CONFIG: org.apache.kafka.common.security.scram.ScramLoginModule required username=\u0026#34;client\u0026#34; password=\u0026#34;password\u0026#34;; 19 KAFKA_CLUSTERS_0_ZOOKEEPER: zookeeper:2181 20 KAFKA_CLUSTERS_0_SSL_TRUSTSTORELOCATION: /kafka.truststore.jks 21 KAFKA_CLUSTERS_0_SSL_TRUSTSTOREPASSWORD: supersecret 22 volumes: 23 - ./truststore/kafka.truststore.jks:/kafka.truststore.jks:ro 24 25networks: 26 kafkanet: 27 external: true 28 name: kafka-network Once started, we can check the messages of the orders topic successfully.\nSummary In the previous post, we discussed TLS (SSL or TLS/SSL) authentication to improve security. It enforces two-way verification where a client certificate is verified by Kafka brokers. Client authentication can also be enabled by Simple Authentication and Security Layer (SASL), and we discussed how to implement SASL authentication with Java and Python client examples in this post.\n","date":"July 13, 2023","img":"/blog/2023-07-13-kafka-development-with-docker-part-10/featured.png","lang":"en","langName":"English","largeImg":"/blog/2023-07-13-kafka-development-with-docker-part-10/featured_hu35df27459c2871526cb88101a926a14d_471947_500x0_resize_box_3.png","permalink":"/blog/2023-07-13-kafka-development-with-docker-part-10/","series":[{"title":"Kafka Development With Docker","url":"/series/kafka-development-with-docker/"}],"smallImg":"/blog/2023-07-13-kafka-development-with-docker-part-10/featured_hu35df27459c2871526cb88101a926a14d_471947_180x0_resize_box_3.png","tags":[{"title":"Apache Kafka","url":"/tags/apache-kafka/"},{"title":"Security","url":"/tags/security/"},{"title":"Python","url":"/tags/python/"},{"title":"Docker","url":"/tags/docker/"},{"title":"Docker Compose","url":"/tags/docker-compose/"}],"timestamp":1689206400,"title":"Kafka Development With Docker - Part 10 SASL Authentication"},{"categories":[{"title":"Apache Kafka","url":"/categories/apache-kafka/"}],"content":"In the previous post, we discussed how to configure TLS (SSL or TLS/SSL) encryption with Java and Python client examples. SSL encryption is a one-way verification process where a server certificate is verified by a client via SSL Handshake. To improve security, we can add client authentication either by enforcing two-way verification where a client certificate is verified by Kafka brokers (SSL authentication). Or we can choose a separate authentication mechanism, which is typically Simple Authentication and Security Layer (SASL). In this post, we will discuss how to implement SSL authentication with Java and Python client examples while SASL authentication is covered in the next post.\nPart 1 Cluster Setup Part 2 Management App Part 3 Kafka Connect Part 4 Producer and Consumer Part 5 Glue Schema Registry Part 6 Kafka Connect with Glue Schema Registry Part 7 Producer and Consumer with Glue Schema Registry Part 8 SSL Encryption Part 9 SSL Authentication (this post) Part 10 SASL Authentication Part 11 Kafka Authorization Certificate Setup Below shows an overview of certificate setup and SSL authentication. Compared to SSL encryption, we need an additional Keystore for the client and the client certificate should be verified by Kafka brokers. It is from Apache Kafka Series - Kafka Security | SSL SASL Kerberos ACL by Stephane Maarek and Gerd Koenig (LINK).\nSSL authentication is a two-way verification process where both the server and client verify the certificate of their counterpart via SSL Handshake. The following components are required for setting-up certificates.\nCertificate Authority (CA) - CA is responsible for signing certificates. We\u0026rsquo;ll be using our own CA rather than relying upon an external trusted CA. Two files will be created for the CA - private key (ca-key) and certificate (ca-cert). Keystore - Keystore stores the identity of each machine (Kafka broker or logical client), and the certificate of a machine is signed by the CA. As the CA\u0026rsquo;s certificate is imported into the Truststore of a Kafka client, the machine\u0026rsquo;s certificate is also trusted and verified during SSL Handshake. Note that each machine requires to have its own Keystore. As we have 3 Kafka brokers, 3 Java Keystore files will be created and each of the file names begins with the host name e.g. kafka-0.server.keystore.jks. Also we will keep a single Keystore for all Kafka clients for simplicity - kafka.client.keystore.jks. Truststore - Truststore stores one or more certificates that a Kafka client should trust. Note that importing a certificate of a CA means the client should trust all other certificates that are signed by that certificate, which is called the chain of trust. We\u0026rsquo;ll have a single Java Keystore file for the Truststore named kafka.truststore.jks, and it will be shared by all Kafka brokers and clients. The following script generates the components mentioned above. It begins with creating the files for the CA. Then it generates the Keystore of each Kafka broker and client followed by producing the Truststore of Kafka clients. Note that the host names of all Kafka brokers and client should be added to the Kafka host file (kafka-hosts.txt) so that their Keystore files are generated recursively. Note also that non-Java clients require PEM (Privacy Enhanced Mail) files rather than Java Keystore files. Therefore, the following files are created and they will be used by the Python clients below.\nca-root.pem - CA file to use in certificate veriication client-certificate.pem - File that contains client certificate, as well as any CA certificates needed to establish the certificate\u0026rsquo;s authenticity client-private-key.pem - File that contains client private key The source can be found in the GitHub repository of this post.\n1# kafka-dev-with-docker/part-09/generate.sh 2#!/usr/bin/env bash 3 4set -eu 5 6CN=\u0026#34;${CN:-kafka-admin}\u0026#34; 7PASSWORD=\u0026#34;${PASSWORD:-supersecret}\u0026#34; 8TO_GENERATE_PEM=\u0026#34;${CITY:-yes}\u0026#34; 9 10VALIDITY_IN_DAYS=3650 11CA_WORKING_DIRECTORY=\u0026#34;certificate-authority\u0026#34; 12TRUSTSTORE_WORKING_DIRECTORY=\u0026#34;truststore\u0026#34; 13KEYSTORE_WORKING_DIRECTORY=\u0026#34;keystore\u0026#34; 14PEM_WORKING_DIRECTORY=\u0026#34;pem\u0026#34; 15CA_KEY_FILE=\u0026#34;ca-key\u0026#34; 16CA_CERT_FILE=\u0026#34;ca-cert\u0026#34; 17DEFAULT_TRUSTSTORE_FILE=\u0026#34;kafka.truststore.jks\u0026#34; 18KEYSTORE_SIGN_REQUEST=\u0026#34;cert-file\u0026#34; 19KEYSTORE_SIGN_REQUEST_SRL=\u0026#34;ca-cert.srl\u0026#34; 20KEYSTORE_SIGNED_CERT=\u0026#34;cert-signed\u0026#34; 21KAFKA_HOSTS_FILE=\u0026#34;kafka-hosts.txt\u0026#34; 22 23if [ ! -f \u0026#34;$KAFKA_HOSTS_FILE\u0026#34; ]; then 24 echo \u0026#34;\u0026#39;$KAFKA_HOSTS_FILE\u0026#39; does not exists. Create this file\u0026#34; 25 exit 1 26fi 27 28echo \u0026#34;Welcome to the Kafka SSL certificate authority, key store and trust store generator script.\u0026#34; 29 30echo 31echo \u0026#34;First we will create our own certificate authority\u0026#34; 32echo \u0026#34; Two files will be created if not existing:\u0026#34; 33echo \u0026#34; - $CA_WORKING_DIRECTORY/$CA_KEY_FILE -- the private key used later to sign certificates\u0026#34; 34echo \u0026#34; - $CA_WORKING_DIRECTORY/$CA_CERT_FILE -- the certificate that will be stored in the trust store\u0026#34; 35echo \u0026#34; and serve as the certificate authority (CA).\u0026#34; 36if [ -f \u0026#34;$CA_WORKING_DIRECTORY/$CA_KEY_FILE\u0026#34; ] \u0026amp;\u0026amp; [ -f \u0026#34;$CA_WORKING_DIRECTORY/$CA_CERT_FILE\u0026#34; ]; then 37 echo \u0026#34;Use existing $CA_WORKING_DIRECTORY/$CA_KEY_FILE and $CA_WORKING_DIRECTORY/$CA_CERT_FILE ...\u0026#34; 38else 39 rm -rf $CA_WORKING_DIRECTORY \u0026amp;\u0026amp; mkdir $CA_WORKING_DIRECTORY 40 echo 41 echo \u0026#34;Generate $CA_WORKING_DIRECTORY/$CA_KEY_FILE and $CA_WORKING_DIRECTORY/$CA_CERT_FILE ...\u0026#34; 42 echo 43 openssl req -new -newkey rsa:4096 -days $VALIDITY_IN_DAYS -x509 -subj \u0026#34;/CN=$CN\u0026#34; \\ 44 -keyout $CA_WORKING_DIRECTORY/$CA_KEY_FILE -out $CA_WORKING_DIRECTORY/$CA_CERT_FILE -nodes 45fi 46 47echo 48echo \u0026#34;A keystore will be generated for each host in $KAFKA_HOSTS_FILE as each broker and logical client needs its own keystore\u0026#34; 49echo 50echo \u0026#34; NOTE: currently in Kafka, the Common Name (CN) does not need to be the FQDN of\u0026#34; 51echo \u0026#34; this host. However, at some point, this may change. As such, make the CN\u0026#34; 52echo \u0026#34; the FQDN. Some operating systems call the CN prompt \u0026#39;first / last name\u0026#39;\u0026#34; 53echo \u0026#34; To learn more about CNs and FQDNs, read:\u0026#34; 54echo \u0026#34; https://docs.oracle.com/javase/7/docs/api/javax/net/ssl/X509ExtendedTrustManager.html\u0026#34; 55rm -rf $KEYSTORE_WORKING_DIRECTORY \u0026amp;\u0026amp; mkdir $KEYSTORE_WORKING_DIRECTORY 56while read -r KAFKA_HOST || [ -n \u0026#34;$KAFKA_HOST\u0026#34; ]; do 57 if [[ $KAFKA_HOST =~ ^kafka-[0-9]+$ ]]; then 58 SUFFIX=\u0026#34;server\u0026#34; 59 DNAME=\u0026#34;CN=$KAFKA_HOST\u0026#34; 60 else 61 SUFFIX=\u0026#34;client\u0026#34; 62 DNAME=\u0026#34;CN=client\u0026#34; 63 fi 64 KEY_STORE_FILE_NAME=\u0026#34;$KAFKA_HOST.$SUFFIX.keystore.jks\u0026#34; 65 echo 66 echo \u0026#34;\u0026#39;$KEYSTORE_WORKING_DIRECTORY/$KEY_STORE_FILE_NAME\u0026#39; will contain a key pair and a self-signed certificate.\u0026#34; 67 keytool -genkey -keystore $KEYSTORE_WORKING_DIRECTORY/\u0026#34;$KEY_STORE_FILE_NAME\u0026#34; \\ 68 -alias localhost -validity $VALIDITY_IN_DAYS -keyalg RSA \\ 69 -noprompt -dname $DNAME -keypass $PASSWORD -storepass $PASSWORD 70 71 echo 72 echo \u0026#34;Now a certificate signing request will be made to the keystore.\u0026#34; 73 keytool -certreq -keystore $KEYSTORE_WORKING_DIRECTORY/\u0026#34;$KEY_STORE_FILE_NAME\u0026#34; \\ 74 -alias localhost -file $KEYSTORE_SIGN_REQUEST -keypass $PASSWORD -storepass $PASSWORD 75 76 echo 77 echo \u0026#34;Now the private key of the certificate authority (CA) will sign the keystore\u0026#39;s certificate.\u0026#34; 78 openssl x509 -req -CA $CA_WORKING_DIRECTORY/$CA_CERT_FILE \\ 79 -CAkey $CA_WORKING_DIRECTORY/$CA_KEY_FILE \\ 80 -in $KEYSTORE_SIGN_REQUEST -out $KEYSTORE_SIGNED_CERT \\ 81 -days $VALIDITY_IN_DAYS -CAcreateserial 82 # creates $CA_WORKING_DIRECTORY/$KEYSTORE_SIGN_REQUEST_SRL which is never used or needed. 83 84 echo 85 echo \u0026#34;Now the CA will be imported into the keystore.\u0026#34; 86 keytool -keystore $KEYSTORE_WORKING_DIRECTORY/\u0026#34;$KEY_STORE_FILE_NAME\u0026#34; -alias CARoot \\ 87 -import -file $CA_WORKING_DIRECTORY/$CA_CERT_FILE -keypass $PASSWORD -storepass $PASSWORD -noprompt 88 89 echo 90 echo \u0026#34;Now the keystore\u0026#39;s signed certificate will be imported back into the keystore.\u0026#34; 91 keytool -keystore $KEYSTORE_WORKING_DIRECTORY/\u0026#34;$KEY_STORE_FILE_NAME\u0026#34; -alias localhost \\ 92 -import -file $KEYSTORE_SIGNED_CERT -keypass $PASSWORD -storepass $PASSWORD 93 94 echo 95 echo \u0026#34;Complete keystore generation!\u0026#34; 96 echo 97 echo \u0026#34;Deleting intermediate files. They are:\u0026#34; 98 echo \u0026#34; - \u0026#39;$CA_WORKING_DIRECTORY/$KEYSTORE_SIGN_REQUEST_SRL\u0026#39;: CA serial number\u0026#34; 99 echo \u0026#34; - \u0026#39;$KEYSTORE_SIGN_REQUEST\u0026#39;: the keystore\u0026#39;s certificate signing request\u0026#34; 100 echo \u0026#34; - \u0026#39;$KEYSTORE_SIGNED_CERT\u0026#39;: the keystore\u0026#39;s certificate, signed by the CA, and stored back\u0026#34; 101 echo \u0026#34; into the keystore\u0026#34; 102 rm -f $CA_WORKING_DIRECTORY/$KEYSTORE_SIGN_REQUEST_SRL $KEYSTORE_SIGN_REQUEST $KEYSTORE_SIGNED_CERT 103done \u0026lt; \u0026#34;$KAFKA_HOSTS_FILE\u0026#34; 104 105echo 106echo \u0026#34;Now the trust store will be generated from the certificate.\u0026#34; 107rm -rf $TRUSTSTORE_WORKING_DIRECTORY \u0026amp;\u0026amp; mkdir $TRUSTSTORE_WORKING_DIRECTORY 108keytool -keystore $TRUSTSTORE_WORKING_DIRECTORY/$DEFAULT_TRUSTSTORE_FILE \\ 109 -alias CARoot -import -file $CA_WORKING_DIRECTORY/$CA_CERT_FILE \\ 110 -noprompt -dname \u0026#34;CN=$CN\u0026#34; -keypass $PASSWORD -storepass $PASSWORD 111 112if [ $TO_GENERATE_PEM == \u0026#34;yes\u0026#34; ]; then 113 echo 114 echo \u0026#34;The following files for SSL configuration will be created for a non-java client\u0026#34; 115 echo \u0026#34; $PEM_WORKING_DIRECTORY/ca-root.pem: CA file to use in certificate veriication (ssl_cafile)\u0026#34; 116 echo \u0026#34; $PEM_WORKING_DIRECTORY/client-certificate.pem: File that contains client certificate, as well as\u0026#34; 117 echo \u0026#34; any ca certificates needed to establish the certificate\u0026#39;s authenticity (ssl_certfile)\u0026#34; 118 echo \u0026#34; $PEM_WORKING_DIRECTORY/client-private-key.pem: File that contains client private key (ssl_keyfile)\u0026#34; 119 rm -rf $PEM_WORKING_DIRECTORY \u0026amp;\u0026amp; mkdir $PEM_WORKING_DIRECTORY 120 121 keytool -exportcert -alias CARoot -keystore $KEYSTORE_WORKING_DIRECTORY/kafka.client.keystore.jks \\ 122 -rfc -file $PEM_WORKING_DIRECTORY/ca-root.pem -storepass $PASSWORD 123 124 keytool -exportcert -alias localhost -keystore $KEYSTORE_WORKING_DIRECTORY/kafka.client.keystore.jks \\ 125 -rfc -file $PEM_WORKING_DIRECTORY/client-certificate.pem -storepass $PASSWORD 126 127 keytool -importkeystore -srcalias localhost -srckeystore $KEYSTORE_WORKING_DIRECTORY/kafka.client.keystore.jks \\ 128 -destkeystore cert_and_key.p12 -deststoretype PKCS12 -srcstorepass $PASSWORD -deststorepass $PASSWORD 129 openssl pkcs12 -in cert_and_key.p12 -nocerts -nodes -password pass:$PASSWORD \\ 130 | awk \u0026#39;/-----BEGIN PRIVATE KEY-----/,/-----END PRIVATE KEY-----/\u0026#39; \u0026gt; $PEM_WORKING_DIRECTORY/client-private-key.pem 131 rm -f cert_and_key.p12 132fi The script generates the following files listed below.\n1$ tree certificate-authority keystore truststore pem 2certificate-authority 3├── ca-cert 4└── ca-key 5keystore 6├── kafka-0.server.keystore.jks 7├── kafka-1.server.keystore.jks 8├── kafka-2.server.keystore.jks 9└── kafka.client.keystore.jks 10truststore 11└── kafka.truststore.jks 12pem 13├── ca-root.pem 14├── client-certificate.pem 15└── client-private-key.pem Kafka Broker Update We should add the SSL listener to the broker configuration and the port 9093 is reserved for it. Both the Keystore and Truststore files are specified in the broker configuration. The former is to send the broker certificate to clients while the latter is necessary because a Kafka broker can be a client of other brokers. Also, we should make SSL client authentication to be required by updating the KAFKA_CFG_SSL_CLIENT_AUTH environment variable. The changes made to the first Kafka broker are shown below, and the same updates are made to the other brokers. The cluster can be started by docker-compose -f compose-kafka.yml up -d.\n1# kafka-dev-with-docker/part-09/compose-kafka.yml 2version: \u0026#34;3.5\u0026#34; 3 4services: 5... 6 7 kafka-0: 8 image: bitnami/kafka:2.8.1 9 container_name: kafka-0 10 expose: 11 - 9092 12 - 9093 13 ports: 14 - \u0026#34;29092:29092\u0026#34; 15 networks: 16 - kafkanet 17 environment: 18 - ALLOW_PLAINTEXT_LISTENER=yes 19 - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181 20 - KAFKA_CFG_BROKER_ID=0 21 - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=INTERNAL:PLAINTEXT,SSL:SSL,EXTERNAL:PLAINTEXT 22 - KAFKA_CFG_LISTENERS=INTERNAL://:9092,SSL://:9093,EXTERNAL://:29092 23 - KAFKA_CFG_ADVERTISED_LISTENERS=INTERNAL://kafka-0:9092,SSL://kafka-0:9093,EXTERNAL://localhost:29092 24 - KAFKA_CFG_INTER_BROKER_LISTENER_NAME=SSL 25 - KAFKA_CFG_SSL_KEYSTORE_LOCATION=/opt/bitnami/kafka/config/certs/kafka.keystore.jks 26 - KAFKA_CFG_SSL_KEYSTORE_PASSWORD=supersecret 27 - KAFKA_CFG_SSL_KEY_PASSWORD=supersecret 28 - KAFKA_CFG_SSL_TRUSTSTORE_LOCATION=/opt/bitnami/kafka/config/certs/kafka.truststore.jks 29 - KAFKA_CFG_SSL_TRUSTSTORE_PASSWORD=supersecret 30 - KAFKA_CFG_SSL_CLIENT_AUTH=required 31 volumes: 32 - kafka_0_data:/bitnami/kafka 33 - ./keystore/kafka-0.server.keystore.jks:/opt/bitnami/kafka/config/certs/kafka.keystore.jks:ro 34 - ./keystore/kafka.client.keystore.jks:/opt/bitnami/kafka/config/certs/kafka.client.keystore.jks:ro 35 - ./truststore/kafka.truststore.jks:/opt/bitnami/kafka/config/certs/kafka.truststore.jks:ro 36 - ./client.properties:/opt/bitnami/kafka/config/client.properties:ro 37 depends_on: 38 - zookeeper 39 40... 41 42networks: 43 kafkanet: 44 name: kafka-network 45 46... Examples Java and non-Java clients need different configurations. The former can use Java Keystore files directly while the latter needs corresponding details in PEM files. The Kafka CLI and Kafka-UI will be taken as Java client examples while Python producer/consumer will be used to illustrate non-Java clients.\nKafka CLI The following configuration is necessary to use the SSL listener. It includes the security protocol and details about the Keystore and Truststore.\n1# kafka-dev-with-docker/part-09/client.properties 2security.protocol=SSL 3ssl.truststore.location=/opt/bitnami/kafka/config/certs/kafka.truststore.jks 4ssl.truststore.password=supersecret 5ssl.keystore.location=/opt/bitnami/kafka/config/certs/kafka.client.keystore.jks 6ssl.keystore.password=supersecret 7ssl.key.password=supersecret Below shows a producer example. It creates a topic named inventory and produces messages using corresponding scripts. Note the client configuration file (client.properties) is specified in configurations, and it is available via volume-mapping.\n1## producer example 2$ docker exec -it kafka-1 bash 3I have no name!@be871da96c09:/$ cd /opt/bitnami/kafka/bin/ 4 5## create a topic 6I have no name!@be871da96c09:/opt/bitnami/kafka/bin$ ./kafka-topics.sh --bootstrap-server kafka-0:9093 \\ 7 --create --topic inventory --partitions 3 --replication-factor 3 \\ 8 --command-config /opt/bitnami/kafka/config/client.properties 9# Created topic inventory. 10 11## produce messages 12I have no name!@be871da96c09:/opt/bitnami/kafka/bin$ ./kafka-console-producer.sh --bootstrap-server kafka-0:9093 \\ 13 --topic inventory --producer.config /opt/bitnami/kafka/config/client.properties 14\u0026gt;product: apples, quantity: 5 15\u0026gt;product: lemons, quantity: 7 Once messages are created, we can check it by a consumer. We can execute a consumer in a separate console.\n1## consumer example 2$ docker exec -it kafka-1 bash 3I have no name!@be871da96c09:/$ cd /opt/bitnami/kafka/bin/ 4 5## consume messages 6I have no name!@be871da96c09:/opt/bitnami/kafka/bin$ ./kafka-console-consumer.sh --bootstrap-server kafka-0:9093 \\ 7 --topic inventory --consumer.config /opt/bitnami/kafka/config/client.properties --from-beginning 8product: apples, quantity: 5 9product: lemons, quantity: 7 Python Client We will run the Python producer and consumer apps using docker-compose. At startup, each of them installs required packages and executes its corresponding app script. As it shares the same network to the Kafka cluster, we can take the service names (e.g. kafka-0) on port 9093 as Kafka bootstrap servers. As shown below, we will need multiple PEM files, and they will be available via volume-mapping. The apps can be started by docker-compose -f compose-apps.yml up -d.\n1# kafka-dev-with-docker/part-09/compose-apps.yml 2version: \u0026#34;3.5\u0026#34; 3 4services: 5 producer: 6 image: bitnami/python:3.9 7 container_name: producer 8 command: \u0026#34;sh -c \u0026#39;pip install -r requirements.txt \u0026amp;\u0026amp; python producer.py\u0026#39;\u0026#34; 9 networks: 10 - kafkanet 11 environment: 12 BOOTSTRAP_SERVERS: kafka-0:9093,kafka-1:9093,kafka-2:9093 13 TOPIC_NAME: orders 14 TZ: Australia/Sydney 15 volumes: 16 - .:/app 17 consumer: 18 image: bitnami/python:3.9 19 container_name: consumer 20 command: \u0026#34;sh -c \u0026#39;pip install -r requirements.txt \u0026amp;\u0026amp; python consumer.py\u0026#39;\u0026#34; 21 networks: 22 - kafkanet 23 environment: 24 BOOTSTRAP_SERVERS: kafka-0:9093,kafka-1:9093,kafka-2:9093 25 TOPIC_NAME: orders 26 GROUP_ID: orders-group 27 TZ: Australia/Sydney 28 volumes: 29 - .:/app 30 31networks: 32 kafkanet: 33 external: true 34 name: kafka-network Producer The same producer app discussed in Part 4 is used here. The following arguments are added to access the SSL listener.\nsecurity_protocol - Protocol used to communicate with brokers. ssl_check_hostname - Flag to configure whether SSL handshake should verify that the certificate matches the broker\u0026rsquo;s hostname. ssl_cafile - Optional filename of CA (certificate) file to use in certificate verification. ssl_certfile - Optional filename that contains client certificate, as well as any CA certificates needed to establish the certificate\u0026rsquo;s authenticity. ssl_keyfile - Optional filename that contains the client private key. 1# kafka-dev-with-docker/part-09/producer.py 2... 3 4class Producer: 5 def __init__(self, bootstrap_servers: list, topic: str): 6 self.bootstrap_servers = bootstrap_servers 7 self.topic = topic 8 self.producer = self.create() 9 10 def create(self): 11 return KafkaProducer( 12 bootstrap_servers=self.bootstrap_servers, 13 security_protocol=\u0026#34;SSL\u0026#34;, 14 ssl_check_hostname=True, 15 ssl_cafile=\u0026#34;pem/ca-root.pem\u0026#34;, 16 ssl_certfile=\u0026#34;pem/client-certificate.pem\u0026#34;, 17 ssl_keyfile=\u0026#34;pem/client-private-key.pem\u0026#34;, 18 value_serializer=lambda v: json.dumps(v, default=self.serialize).encode(\u0026#34;utf-8\u0026#34;), 19 key_serializer=lambda v: json.dumps(v, default=self.serialize).encode(\u0026#34;utf-8\u0026#34;), 20 ) 21 22 def send(self, orders: typing.List[Order]): 23 for order in orders: 24 try: 25 self.producer.send( 26 self.topic, key={\u0026#34;order_id\u0026#34;: order.order_id}, value=order.asdict() 27 ) 28 except Exception as e: 29 raise RuntimeError(\u0026#34;fails to send a message\u0026#34;) from e 30 self.producer.flush() 31 32... 33 34if __name__ == \u0026#34;__main__\u0026#34;: 35 producer = Producer( 36 bootstrap_servers=os.getenv(\u0026#34;BOOTSTRAP_SERVERS\u0026#34;, \u0026#34;localhost:29092\u0026#34;).split(\u0026#34;,\u0026#34;), 37 topic=os.getenv(\u0026#34;TOPIC_NAME\u0026#34;, \u0026#34;orders\u0026#34;), 38 ) 39 max_run = int(os.getenv(\u0026#34;MAX_RUN\u0026#34;, \u0026#34;-1\u0026#34;)) 40 logging.info(f\u0026#34;max run - {max_run}\u0026#34;) 41 current_run = 0 42 while True: 43 current_run += 1 44 logging.info(f\u0026#34;current run - {current_run}\u0026#34;) 45 if current_run \u0026gt; max_run and max_run \u0026gt;= 0: 46 logging.info(f\u0026#34;exceeds max run, finish\u0026#34;) 47 producer.producer.close() 48 break 49 producer.send(Order.auto().create(100)) 50 time.sleep(1) In the container log, we can check SSH Handshake is performed successfully by loading the PEM files.\n1INFO:kafka.conn:\u0026lt;BrokerConnection node_id=bootstrap-0 host=kafka-2:9093 \u0026lt;connecting\u0026gt; [IPv4 (\u0026#39;172.24.0.5\u0026#39;, 9093)]\u0026gt;: connecting to kafka-2:9093 [(\u0026#39;172.24.0.5\u0026#39;, 9093) IPv4] 2INFO:kafka.conn:Probing node bootstrap-0 broker version 3INFO:kafka.conn:\u0026lt;BrokerConnection node_id=bootstrap-0 host=kafka-2:9093 \u0026lt;handshake\u0026gt; [IPv4 (\u0026#39;172.24.0.5\u0026#39;, 9093)]\u0026gt;: Loading SSL CA from pem/ca-root.pem 4INFO:kafka.conn:\u0026lt;BrokerConnection node_id=bootstrap-0 host=kafka-2:9093 \u0026lt;handshake\u0026gt; [IPv4 (\u0026#39;172.24.0.5\u0026#39;, 9093)]\u0026gt;: Loading SSL Cert from pem/client-certificate.pem 5INFO:kafka.conn:\u0026lt;BrokerConnection node_id=bootstrap-0 host=kafka-2:9093 \u0026lt;handshake\u0026gt; [IPv4 (\u0026#39;172.24.0.5\u0026#39;, 9093)]\u0026gt;: Loading SSL Key from pem/client-private-key.pem 6INFO:kafka.conn:\u0026lt;BrokerConnection node_id=bootstrap-0 host=kafka-2:9093 \u0026lt;handshake\u0026gt; [IPv4 (\u0026#39;172.24.0.5\u0026#39;, 9093)]\u0026gt;: Connection complete. 7INFO:root:max run - -1 8INFO:root:current run - 1 9... 10INFO:root:current run - 2 Consumer The same consumer app in Part 4 is used here as well. As the producer app, the following arguments are added - security_protocol, ssl_check_hostname, ssl_cafile, ssl_certfile and ssl_keyfile.\n1# kafka-dev-with-docker/part-09/consumer.py 2... 3 4class Consumer: 5 def __init__(self, bootstrap_servers: list, topics: list, group_id: str) -\u0026gt; None: 6 self.bootstrap_servers = bootstrap_servers 7 self.topics = topics 8 self.group_id = group_id 9 self.consumer = self.create() 10 11 def create(self): 12 return KafkaConsumer( 13 *self.topics, 14 bootstrap_servers=self.bootstrap_servers, 15 security_protocol=\u0026#34;SSL\u0026#34;, 16 ssl_check_hostname=True, 17 ssl_cafile=\u0026#34;pem/ca-root.pem\u0026#34;, 18 ssl_certfile=\u0026#34;pem/client-certificate.pem\u0026#34;, 19 ssl_keyfile=\u0026#34;pem/client-private-key.pem\u0026#34;, 20 auto_offset_reset=\u0026#34;earliest\u0026#34;, 21 enable_auto_commit=True, 22 group_id=self.group_id, 23 key_deserializer=lambda v: v.decode(\u0026#34;utf-8\u0026#34;), 24 value_deserializer=lambda v: v.decode(\u0026#34;utf-8\u0026#34;), 25 ) 26 27 def process(self): 28 try: 29 while True: 30 msg = self.consumer.poll(timeout_ms=1000) 31 if msg is None: 32 continue 33 self.print_info(msg) 34 time.sleep(1) 35 except KafkaError as error: 36 logging.error(error) 37 38 def print_info(self, msg: dict): 39 for t, v in msg.items(): 40 for r in v: 41 logging.info( 42 f\u0026#34;key={r.key}, value={r.value}, topic={t.topic}, partition={t.partition}, offset={r.offset}, ts={r.timestamp}\u0026#34; 43 ) 44 45 46if __name__ == \u0026#34;__main__\u0026#34;: 47 consumer = Consumer( 48 bootstrap_servers=os.getenv(\u0026#34;BOOTSTRAP_SERVERS\u0026#34;, \u0026#34;localhost:29092\u0026#34;).split(\u0026#34;,\u0026#34;), 49 topics=os.getenv(\u0026#34;TOPIC_NAME\u0026#34;, \u0026#34;orders\u0026#34;).split(\u0026#34;,\u0026#34;), 50 group_id=os.getenv(\u0026#34;GROUP_ID\u0026#34;, \u0026#34;orders-group\u0026#34;), 51 ) 52 consumer.process() We can also check messages are consumed after SSH Handshake is succeeded in the container log.\n1... 2INFO:kafka.conn:\u0026lt;BrokerConnection node_id=2 host=kafka-2:9093 \u0026lt;connecting\u0026gt; [IPv4 (\u0026#39;172.24.0.5\u0026#39;, 9093)]\u0026gt;: connecting to kafka-2:9093 [(\u0026#39;172.24.0.5\u0026#39;, 9093) IPv4] 3INFO:kafka.conn:\u0026lt;BrokerConnection node_id=2 host=kafka-2:9093 \u0026lt;handshake\u0026gt; [IPv4 (\u0026#39;172.24.0.5\u0026#39;, 9093)]\u0026gt;: Loading SSL CA from pem/ca-root.pem 4INFO:kafka.conn:\u0026lt;BrokerConnection node_id=2 host=kafka-2:9093 \u0026lt;handshake\u0026gt; [IPv4 (\u0026#39;172.24.0.5\u0026#39;, 9093)]\u0026gt;: Loading SSL Cert from pem/client-certificate.pem 5INFO:kafka.conn:\u0026lt;BrokerConnection node_id=2 host=kafka-2:9093 \u0026lt;handshake\u0026gt; [IPv4 (\u0026#39;172.24.0.5\u0026#39;, 9093)]\u0026gt;: Loading SSL Key from pem/client-private-key.pem 6INFO:kafka.conn:\u0026lt;BrokerConnection node_id=2 host=kafka-2:9093 \u0026lt;handshake\u0026gt; [IPv4 (\u0026#39;172.24.0.5\u0026#39;, 9093)]\u0026gt;: Connection complete. 7INFO:kafka.cluster:Group coordinator for orders-group is BrokerMetadata(nodeId=\u0026#39;coordinator-0\u0026#39;, host=\u0026#39;kafka-0\u0026#39;, port=9093, rack=None) 8INFO:kafka.coordinator:Discovered coordinator coordinator-0 for group orders-group 9WARNING:kafka.coordinator:Marking the coordinator dead (node coordinator-0) for group orders-group: Node Disconnected. 10INFO:kafka.conn:\u0026lt;BrokerConnection node_id=coordinator-0 host=kafka-0:9093 \u0026lt;connecting\u0026gt; [IPv4 (\u0026#39;172.24.0.3\u0026#39;, 9093)]\u0026gt;: connecting to kafka-0:9093 [(\u0026#39;172.24.0.3\u0026#39;, 9093) IPv4] 11INFO:kafka.cluster:Group coordinator for orders-group is BrokerMetadata(nodeId=\u0026#39;coordinator-0\u0026#39;, host=\u0026#39;kafka-0\u0026#39;, port=9093, rack=None) 12INFO:kafka.coordinator:Discovered coordinator coordinator-0 for group orders-group 13INFO:kafka.conn:\u0026lt;BrokerConnection node_id=coordinator-0 host=kafka-0:9093 \u0026lt;handshake\u0026gt; [IPv4 (\u0026#39;172.24.0.3\u0026#39;, 9093)]\u0026gt;: Connection complete. 14INFO:kafka.coordinator:(Re-)joining group orders-group 15INFO:kafka.coordinator:Elected group leader -- performing partition assignments using range 16INFO:kafka.coordinator:Successfully joined group orders-group with generation 1 17INFO:kafka.consumer.subscription_state:Updated partition assignment: [TopicPartition(topic=\u0026#39;orders\u0026#39;, partition=0)] 18INFO:kafka.coordinator.consumer:Setting newly assigned partitions {TopicPartition(topic=\u0026#39;orders\u0026#39;, partition=0)} for group orders-group 19... 20INFO:root:key={\u0026#34;order_id\u0026#34;: \u0026#34;e2253de4-7c44-4cf1-b45d-7091a0dd1f23\u0026#34;}, value={\u0026#34;order_id\u0026#34;: \u0026#34;e2253de4-7c44-4cf1-b45d-7091a0dd1f23\u0026#34;, \u0026#34;ordered_at\u0026#34;: \u0026#34;2023-06-20T21:38:18.524398\u0026#34;, \u0026#34;user_id\u0026#34;: \u0026#34;053\u0026#34;, \u0026#34;order_items\u0026#34;: [{\u0026#34;product_id\u0026#34;: 279, \u0026#34;quantity\u0026#34;: 1}]}, topic=orders, partition=0, offset=0, ts=1687297098839 21INFO:root:key={\u0026#34;order_id\u0026#34;: \u0026#34;f522db30-f2a1-4b43-8233-a2b36b4f3f95\u0026#34;}, value={\u0026#34;order_id\u0026#34;: \u0026#34;f522db30-f2a1-4b43-8233-a2b36b4f3f95\u0026#34;, \u0026#34;ordered_at\u0026#34;: \u0026#34;2023-06-20T21:38:18.524430\u0026#34;, \u0026#34;user_id\u0026#34;: \u0026#34;038\u0026#34;, \u0026#34;order_items\u0026#34;: [{\u0026#34;product_id\u0026#34;: 456, \u0026#34;quantity\u0026#34;: 3}]}, topic=orders, partition=0, offset=1, ts=1687297098840 Kafka-UI Kafka-UI is also a Java client, and it accepts the Keystore files of the Kafka Truststore (kafka.truststore.jks) and client Keystore (kafka.client.keystore.jks). We can specify the files and passwords to access those as environment variables. The app can be started by docker-compose -f compose-ui.yml up -d.\n1# kafka-dev-with-docker/part-09/compose-ui.yml 2version: \u0026#34;3.5\u0026#34; 3 4services: 5 kafka-ui: 6 image: provectuslabs/kafka-ui:master 7 container_name: kafka-ui 8 ports: 9 - \u0026#34;8080:8080\u0026#34; 10 networks: 11 - kafkanet 12 environment: 13 KAFKA_CLUSTERS_0_NAME: local 14 KAFKA_CLUSTERS_0_PROPERTIES_SECURITY_PROTOCOL: SSL 15 KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka-0:9093,kafka-1:9093,kafka-2:9093 16 KAFKA_CLUSTERS_0_ZOOKEEPER: zookeeper:2181 17 KAFKA_CLUSTERS_0_PROPERTIES_SSL_KEYSTORE_LOCATION: /kafka.client.keystore.jks 18 KAFKA_CLUSTERS_0_PROPERTIES_SSL_KEYSTORE_PASSWORD: supersecret 19 KAFKA_CLUSTERS_0_SSL_TRUSTSTORELOCATION: /kafka.truststore.jks 20 KAFKA_CLUSTERS_0_SSL_TRUSTSTOREPASSWORD: supersecret 21 volumes: 22 - ./truststore/kafka.truststore.jks:/kafka.truststore.jks:ro 23 - ./keystore/kafka.client.keystore.jks:/kafka.client.keystore.jks:ro 24 25networks: 26 kafkanet: 27 external: true 28 name: kafka-network Summary To improve security, we can extend TLS (SSL or TLS/SSL) encryption either by enforcing two-way verification where a client certificate is verified by Kafka brokers (SSL authentication). Or we can choose a separate authentication mechanism, which is typically Simple Authentication and Security Layer (SASL). In this post, we discussed how to implement SSL authentication with Java and Python client examples while SASL authentication is covered in the next post.\n","date":"July 6, 2023","img":"/blog/2023-07-06-kafka-development-with-docker-part-9/featured.png","lang":"en","langName":"English","largeImg":"/blog/2023-07-06-kafka-development-with-docker-part-9/featured_hu46b705891e566c5f4e1fa5fb958154c6_471471_500x0_resize_box_3.png","permalink":"/blog/2023-07-06-kafka-development-with-docker-part-9/","series":[{"title":"Kafka Development With Docker","url":"/series/kafka-development-with-docker/"}],"smallImg":"/blog/2023-07-06-kafka-development-with-docker-part-9/featured_hu46b705891e566c5f4e1fa5fb958154c6_471471_180x0_resize_box_3.png","tags":[{"title":"Apache Kafka","url":"/tags/apache-kafka/"},{"title":"Security","url":"/tags/security/"},{"title":"Python","url":"/tags/python/"},{"title":"Docker","url":"/tags/docker/"},{"title":"Docker Compose","url":"/tags/docker-compose/"}],"timestamp":1688601600,"title":"Kafka Development With Docker - Part 9 SSL Authentication"},{"categories":[{"title":"Data Streaming","url":"/categories/data-streaming/"}],"content":"As part of investigating how to utilize Kafka Connect effectively for AWS services integration, I demonstrated how to develop the Camel DynamoDB sink connector using Docker in Part 2. Fake order data was generated using the MSK Data Generator source connector, and the sink connector was configured to consume the topic messages to ingest them into a DynamoDB table. In this post, I will illustrate how to deploy the data ingestion applications using Amazon MSK and MSK Connect.\nPart 1 Introduction Part 2 Develop Camel DynamoDB Sink Connector Part 3 Deploy Camel DynamoDB Sink Connector (this post) Part 4 Develop Aiven OpenSearch Sink Connector Part 5 Deploy Aiven OpenSearch Sink Connector Infrastructure A VPC with 3 public and private subnets is created using the AWS VPC Terraform module (vpc.tf). Also, a SoftEther VPN server is deployed in order to access the resources in the private subnets from the developer machine (vpn.tf). It is particularly useful to monitor and manage the MSK cluster and Kafka topic locally. The details about how to configure the VPN server can be found in an earlier post. The source can be found in the GitHub repository of this post.\nMSK An MSK cluster with 3 brokers is created. The broker nodes are deployed with the kafka.m5.large instance type in private subnets and IAM authentication is used for the client authentication method. Finally, additional server configurations are added such as enabling auto creation of topics and topic deletion.\n1# kafka-connect-for-aws/part-03/variable.tf 2locals { 3 ... 4 msk = { 5 version = \u0026#34;2.8.1\u0026#34; 6 instance_size = \u0026#34;kafka.m5.large\u0026#34; 7 ebs_volume_size = 20 8 log_retention_ms = 604800000 # 7 days 9 num_partitions = 3 10 default_replication_factor = 3 11 } 12 ... 13} 14# kafka-connect-for-aws/part-03/msk.tf 15resource \u0026#34;aws_msk_cluster\u0026#34; \u0026#34;msk_data_cluster\u0026#34; { 16 cluster_name = \u0026#34;${local.name}-msk-cluster\u0026#34; 17 kafka_version = local.msk.version 18 number_of_broker_nodes = length(module.vpc.private_subnets) 19 configuration_info { 20 arn = aws_msk_configuration.msk_config.arn 21 revision = aws_msk_configuration.msk_config.latest_revision 22 } 23 24 broker_node_group_info { 25 instance_type = local.msk.instance_size 26 client_subnets = module.vpc.private_subnets 27 security_groups = [aws_security_group.msk.id] 28 storage_info { 29 ebs_storage_info { 30 volume_size = local.msk.ebs_volume_size 31 } 32 } 33 } 34 35 client_authentication { 36 sasl { 37 iam = true 38 } 39 } 40 41 logging_info { 42 broker_logs { 43 cloudwatch_logs { 44 enabled = true 45 log_group = aws_cloudwatch_log_group.msk_cluster_lg.name 46 } 47 s3 { 48 enabled = true 49 bucket = aws_s3_bucket.default_bucket.id 50 prefix = \u0026#34;logs/msk/cluster/\u0026#34; 51 } 52 } 53 } 54 55 tags = local.tags 56 57 depends_on = [aws_msk_configuration.msk_config] 58} 59 60resource \u0026#34;aws_msk_configuration\u0026#34; \u0026#34;msk_config\u0026#34; { 61 name = \u0026#34;${local.name}-msk-configuration\u0026#34; 62 63 kafka_versions = [local.msk.version] 64 65 server_properties = \u0026lt;\u0026lt;PROPERTIES 66 auto.create.topics.enable = true 67 delete.topic.enable = true 68 log.retention.ms = ${local.msk.log_retention_ms} 69 num.partitions = ${local.msk.num_partitions} 70 default.replication.factor = ${local.msk.default_replication_factor} 71 PROPERTIES 72} Security Group The security group for the MSK cluster allows all inbound traffic from itself and all outbound traffic into all IP addresses. The Kafka connectors will use the same security group and the former is necessary. Both the rules are configured too generously, and we can limit the protocol and port ranges in production. The last inbound rule is for VPN access.\n1# kafka-connect-for-aws/part-03/msk.tf 2resource \u0026#34;aws_security_group\u0026#34; \u0026#34;msk\u0026#34; { 3 name = \u0026#34;${local.name}-msk-sg\u0026#34; 4 vpc_id = module.vpc.vpc_id 5 6 lifecycle { 7 create_before_destroy = true 8 } 9 10 tags = local.tags 11} 12 13resource \u0026#34;aws_security_group_rule\u0026#34; \u0026#34;msk_self_inbound_all\u0026#34; { 14 type = \u0026#34;ingress\u0026#34; 15 description = \u0026#34;Allow ingress from itself - required for MSK Connect\u0026#34; 16 security_group_id = aws_security_group.msk.id 17 protocol = \u0026#34;-1\u0026#34; 18 from_port = \u0026#34;0\u0026#34; 19 to_port = \u0026#34;0\u0026#34; 20 source_security_group_id = aws_security_group.msk.id 21} 22 23resource \u0026#34;aws_security_group_rule\u0026#34; \u0026#34;msk_self_outbound_all\u0026#34; { 24 type = \u0026#34;egress\u0026#34; 25 description = \u0026#34;Allow outbound all\u0026#34; 26 security_group_id = aws_security_group.msk.id 27 protocol = \u0026#34;-1\u0026#34; 28 from_port = \u0026#34;0\u0026#34; 29 to_port = \u0026#34;0\u0026#34; 30 cidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;] 31} 32 33resource \u0026#34;aws_security_group_rule\u0026#34; \u0026#34;msk_vpn_inbound\u0026#34; { 34 count = local.vpn.to_create ? 1 : 0 35 type = \u0026#34;ingress\u0026#34; 36 description = \u0026#34;Allow VPN access\u0026#34; 37 security_group_id = aws_security_group.msk.id 38 protocol = \u0026#34;tcp\u0026#34; 39 from_port = 9098 40 to_port = 9098 41 source_security_group_id = aws_security_group.vpn[0].id 42} DynamoDB The destination table is named connect-for-aws-orders (${local.name}-orders), and it has the primary key where order_id and ordered_at are the hash and range key respectively. It also has a global secondary index where customer_id and ordered_at constitute the primary key. Note that ordered_at is not generated by the source connector as the Java faker library doesn\u0026rsquo;t have a method to generate a current timestamp. As illustrated below it\u0026rsquo;ll be created by the sink connector using SMTs. The table can be created using as shown below.\n1# kafka-connect-for-aws/part-03/ddb.tf 2resource \u0026#34;aws_dynamodb_table\u0026#34; \u0026#34;orders_table\u0026#34; { 3 name = \u0026#34;${local.name}-orders\u0026#34; 4 billing_mode = \u0026#34;PROVISIONED\u0026#34; 5 read_capacity = 1 6 write_capacity = 1 7 hash_key = \u0026#34;order_id\u0026#34; 8 range_key = \u0026#34;ordered_at\u0026#34; 9 10 attribute { 11 name = \u0026#34;order_id\u0026#34; 12 type = \u0026#34;S\u0026#34; 13 } 14 15 attribute { 16 name = \u0026#34;customer_id\u0026#34; 17 type = \u0026#34;S\u0026#34; 18 } 19 20 attribute { 21 name = \u0026#34;ordered_at\u0026#34; 22 type = \u0026#34;S\u0026#34; 23 } 24 25 global_secondary_index { 26 name = \u0026#34;customer\u0026#34; 27 hash_key = \u0026#34;customer_id\u0026#34; 28 range_key = \u0026#34;ordered_at\u0026#34; 29 write_capacity = 1 30 read_capacity = 1 31 projection_type = \u0026#34;ALL\u0026#34; 32 } 33 34 tags = local.tags 35} Kafka Management App A Kafka management app can be a good companion for development as it helps monitor and manage resources on an easy-to-use user interface. We\u0026rsquo;ll use Kpow Community Edition (CE) in this post. It allows you to manage one Kafka Cluster, one Schema Registry, and one Connect Cluster, with the UI supporting a single user session at a time. In the following compose file, we added connection details of the MSK cluster and MSK Connect.\n1# kafka-connect-for-aws/part-03/docker-compose.yml 2version: \u0026#34;3\u0026#34; 3 4services: 5 kpow: 6 image: factorhouse/kpow-ce:91.2.1 7 container_name: kpow 8 ports: 9 - \u0026#34;3000:3000\u0026#34; 10 networks: 11 - kafkanet 12 environment: 13 AWS_ACCESS_KEY_ID: $AWS_ACCESS_KEY_ID 14 AWS_SECRET_ACCESS_KEY: $AWS_SECRET_ACCESS_KEY 15 AWS_SESSION_TOKEN: $AWS_SESSION_TOKEN 16 # broker details 17 BOOTSTRAP: $BOOTSTRAP_SERVERS 18 # client authentication 19 SECURITY_PROTOCOL: SASL_SSL 20 SASL_MECHANISM: AWS_MSK_IAM 21 SASL_JAAS_CONFIG: software.amazon.msk.auth.iam.IAMLoginModule required; 22 SASL_CLIENT_CALLBACK_HANDLER_CLASS: software.amazon.msk.auth.iam.IAMClientCallbackHandler 23 # MSK connect 24 CONNECT_AWS_REGION: ap-southeast-2 25 26networks: 27 kafkanet: 28 name: kafka-network Data Ingestion Pipeline Connector Source Download Before we deploy the connectors, their sources need to be downloaded into the ./connectors path so that they can be saved into S3 followed by being created as custom plugins. The MSK Data Generator is a single Jar file, and it can be kept it as is. On the other hand, the Camel DynamoDB sink connector is an archive file, and the contents should be compressed as the zip format.\n1# kafka-connect-for-aws/part-03/download.sh 2#!/usr/bin/env bash 3SCRIPT_DIR=\u0026#34;$(cd $(dirname \u0026#34;$0\u0026#34;); pwd)\u0026#34; 4 5SRC_PATH=${SCRIPT_DIR}/connectors 6rm -rf ${SRC_PATH} \u0026amp;\u0026amp; mkdir ${SRC_PATH} 7 8## MSK Data Generator Souce Connector 9echo \u0026#34;downloading msk data generator...\u0026#34; 10DOWNLOAD_URL=https://github.com/awslabs/amazon-msk-data-generator/releases/download/v0.4.0/msk-data-generator-0.4-jar-with-dependencies.jar 11 12curl -L -o ${SRC_PATH}/msk-data-generator.jar ${DOWNLOAD_URL} 13 14## Download camel dynamodb sink connector 15echo \u0026#34;download camel dynamodb sink connector...\u0026#34; 16DOWNLOAD_URL=https://repo.maven.apache.org/maven2/org/apache/camel/kafkaconnector/camel-aws-ddb-sink-kafka-connector/3.20.3/camel-aws-ddb-sink-kafka-connector-3.20.3-package.tar.gz 17 18# decompress and zip contents to create custom plugin of msk connect later 19curl -o ${SRC_PATH}/camel-aws-ddb-sink-kafka-connector.tar.gz ${DOWNLOAD_URL} \\ 20 \u0026amp;\u0026amp; tar -xvzf ${SRC_PATH}/camel-aws-ddb-sink-kafka-connector.tar.gz -C ${SRC_PATH} \\ 21 \u0026amp;\u0026amp; cd ${SRC_PATH}/camel-aws-ddb-sink-kafka-connector \\ 22 \u0026amp;\u0026amp; zip -r camel-aws-ddb-sink-kafka-connector.zip . \\ 23 \u0026amp;\u0026amp; mv camel-aws-ddb-sink-kafka-connector.zip ${SRC_PATH} \\ 24 \u0026amp;\u0026amp; rm ${SRC_PATH}/camel-aws-ddb-sink-kafka-connector.tar.gz Below shows the connector sources that can be used to create custom plugins.\n1$ tree connectors -I \u0026#39;camel-aws-ddb-sink-kafka-connector|docs\u0026#39; 2connectors 3├── camel-aws-ddb-sink-kafka-connector.zip 4└── msk-data-generator.jar Connector IAM Role For simplicity, a single IAM role will be used for both the source and sink connectors. The custom managed policy has permission on MSK cluster resources (cluster, topic and group). It also has permission on S3 bucket and CloudWatch Log for logging. Also, an AWS managed policy for DynamoDB (AmazonDynamoDBFullAccess) is attached for the sink connector.\n1# kafka-connect-for-aws/part-03/msk-connect.tf 2resource \u0026#34;aws_iam_role\u0026#34; \u0026#34;kafka_connector_role\u0026#34; { 3 name = \u0026#34;${local.name}-connector-role\u0026#34; 4 5 assume_role_policy = jsonencode({ 6 Version = \u0026#34;2012-10-17\u0026#34; 7 Statement = [ 8 { 9 Action = \u0026#34;sts:AssumeRole\u0026#34; 10 Effect = \u0026#34;Allow\u0026#34; 11 Sid = \u0026#34;\u0026#34; 12 Principal = { 13 Service = \u0026#34;kafkaconnect.amazonaws.com\u0026#34; 14 } 15 }, 16 ] 17 }) 18 managed_policy_arns = [ 19 \u0026#34;arn:aws:iam::aws:policy/AmazonDynamoDBFullAccess\u0026#34;, 20 aws_iam_policy.kafka_connector_policy.arn 21 ] 22} 23 24resource \u0026#34;aws_iam_policy\u0026#34; \u0026#34;kafka_connector_policy\u0026#34; { 25 name = \u0026#34;${local.name}-connector-policy\u0026#34; 26 27 policy = jsonencode({ 28 Version = \u0026#34;2012-10-17\u0026#34; 29 Statement = [ 30 { 31 Sid = \u0026#34;PermissionOnCluster\u0026#34; 32 Action = [ 33 \u0026#34;kafka-cluster:Connect\u0026#34;, 34 \u0026#34;kafka-cluster:AlterCluster\u0026#34;, 35 \u0026#34;kafka-cluster:DescribeCluster\u0026#34; 36 ] 37 Effect = \u0026#34;Allow\u0026#34; 38 Resource = \u0026#34;arn:aws:kafka:${local.region}:${data.aws_caller_identity.current.account_id}:cluster/${local.name}-msk-cluster/*\u0026#34; 39 }, 40 { 41 Sid = \u0026#34;PermissionOnTopics\u0026#34; 42 Action = [ 43 \u0026#34;kafka-cluster:*Topic*\u0026#34;, 44 \u0026#34;kafka-cluster:WriteData\u0026#34;, 45 \u0026#34;kafka-cluster:ReadData\u0026#34; 46 ] 47 Effect = \u0026#34;Allow\u0026#34; 48 Resource = \u0026#34;arn:aws:kafka:${local.region}:${data.aws_caller_identity.current.account_id}:topic/${local.name}-msk-cluster/*\u0026#34; 49 }, 50 { 51 Sid = \u0026#34;PermissionOnGroups\u0026#34; 52 Action = [ 53 \u0026#34;kafka-cluster:AlterGroup\u0026#34;, 54 \u0026#34;kafka-cluster:DescribeGroup\u0026#34; 55 ] 56 Effect = \u0026#34;Allow\u0026#34; 57 Resource = \u0026#34;arn:aws:kafka:${local.region}:${data.aws_caller_identity.current.account_id}:group/${local.name}-msk-cluster/*\u0026#34; 58 }, 59 { 60 Sid = \u0026#34;PermissionOnDataBucket\u0026#34; 61 Action = [ 62 \u0026#34;s3:ListBucket\u0026#34;, 63 \u0026#34;s3:*Object\u0026#34; 64 ] 65 Effect = \u0026#34;Allow\u0026#34; 66 Resource = [ 67 \u0026#34;${aws_s3_bucket.default_bucket.arn}\u0026#34;, 68 \u0026#34;${aws_s3_bucket.default_bucket.arn}/*\u0026#34; 69 ] 70 }, 71 { 72 Sid = \u0026#34;LoggingPermission\u0026#34; 73 Action = [ 74 \u0026#34;logs:CreateLogStream\u0026#34;, 75 \u0026#34;logs:CreateLogGroup\u0026#34;, 76 \u0026#34;logs:PutLogEvents\u0026#34; 77 ] 78 Effect = \u0026#34;Allow\u0026#34; 79 Resource = \u0026#34;*\u0026#34; 80 }, 81 ] 82 }) 83} Source Connector The connector source will be uploaded into S3 and a custom plugin is created with it. Then the source connector will be created using the custom plugin.\nIn connector configuration, the connector class (connector.class) is required for any connector and I set it for the MSK Data Generator. Also, a single worker is allocated to the connector (tasks.max). As mentioned earlier, the converter-related properties are overridden. Specifically, the key converter is set to the string converter as the key of the topic is set to be primitive values (genkp). Also, schemas are not enabled for both the key and value.\nThose properties in the middle are specific to the source connector. Basically it sends messages to a topic named order. The key is marked as to-replace as it will be replaced with the order_id attribute of the value - see below. The value has order_id, product_id, quantity, customer_id and customer_name attributes, and they are generated by the Java faker library.\nIt can be easier to manage messages if the same order ID is shared with the key and value. We can achieve it using single message transforms (SMTs). Specifically I used two transforms - ValueToKey and ExtractField to achieve it. As the name suggests, the former copies the order_id value into the key. The latter is used additionally because the key is set to have primitive string values. Finally, the last transform (Cast) is to change the quantity value into integer.\n1# kafka-connect-for-aws/part-03/msk-connect.tf 2resource \u0026#34;aws_mskconnect_connector\u0026#34; \u0026#34;msk_data_generator\u0026#34; { 3 name = \u0026#34;${local.name}-order-source\u0026#34; 4 5 kafkaconnect_version = \u0026#34;2.7.1\u0026#34; 6 7 capacity { 8 provisioned_capacity { 9 mcu_count = 1 10 worker_count = 1 11 } 12 } 13 14 connector_configuration = { 15 # connector configuration 16 \u0026#34;connector.class\u0026#34; = \u0026#34;com.amazonaws.mskdatagen.GeneratorSourceConnector\u0026#34;, 17 \u0026#34;tasks.max\u0026#34; = \u0026#34;1\u0026#34;, 18 \u0026#34;key.converter\u0026#34; = \u0026#34;org.apache.kafka.connect.storage.StringConverter\u0026#34;, 19 \u0026#34;key.converter.schemas.enable\u0026#34; = false, 20 \u0026#34;value.converter\u0026#34; = \u0026#34;org.apache.kafka.connect.json.JsonConverter\u0026#34;, 21 \u0026#34;value.converter.schemas.enable\u0026#34; = false, 22 # msk data generator configuration 23 \u0026#34;genkp.order.with\u0026#34; = \u0026#34;to-replace\u0026#34;, 24 \u0026#34;genv.order.order_id.with\u0026#34; = \u0026#34;#{Internet.uuid}\u0026#34;, 25 \u0026#34;genv.order.product_id.with\u0026#34; = \u0026#34;#{Code.isbn10}\u0026#34;, 26 \u0026#34;genv.order.quantity.with\u0026#34; = \u0026#34;#{number.number_between \u0026#39;1\u0026#39;,\u0026#39;5\u0026#39;}\u0026#34;, 27 \u0026#34;genv.order.customer_id.with\u0026#34; = \u0026#34;#{number.number_between \u0026#39;100\u0026#39;,\u0026#39;199\u0026#39;}\u0026#34;, 28 \u0026#34;genv.order.customer_name.with\u0026#34; = \u0026#34;#{Name.full_name}\u0026#34;, 29 \u0026#34;global.throttle.ms\u0026#34; = \u0026#34;500\u0026#34;, 30 \u0026#34;global.history.records.max\u0026#34; = \u0026#34;1000\u0026#34;, 31 # single message transforms 32 \u0026#34;transforms\u0026#34; = \u0026#34;copyIdToKey,extractKeyFromStruct,cast\u0026#34;, 33 \u0026#34;transforms.copyIdToKey.type\u0026#34; = \u0026#34;org.apache.kafka.connect.transforms.ValueToKey\u0026#34;, 34 \u0026#34;transforms.copyIdToKey.fields\u0026#34; = \u0026#34;order_id\u0026#34;, 35 \u0026#34;transforms.extractKeyFromStruct.type\u0026#34; = \u0026#34;org.apache.kafka.connect.transforms.ExtractField$Key\u0026#34;, 36 \u0026#34;transforms.extractKeyFromStruct.field\u0026#34; = \u0026#34;order_id\u0026#34;, 37 \u0026#34;transforms.cast.type\u0026#34; = \u0026#34;org.apache.kafka.connect.transforms.Cast$Value\u0026#34;, 38 \u0026#34;transforms.cast.spec\u0026#34; = \u0026#34;quantity:int8\u0026#34; 39 } 40 41 kafka_cluster { 42 apache_kafka_cluster { 43 bootstrap_servers = aws_msk_cluster.msk_data_cluster.bootstrap_brokers_sasl_iam 44 45 vpc { 46 security_groups = [aws_security_group.msk.id] 47 subnets = module.vpc.private_subnets 48 } 49 } 50 } 51 52 kafka_cluster_client_authentication { 53 authentication_type = \u0026#34;IAM\u0026#34; 54 } 55 56 kafka_cluster_encryption_in_transit { 57 encryption_type = \u0026#34;TLS\u0026#34; 58 } 59 60 plugin { 61 custom_plugin { 62 arn = aws_mskconnect_custom_plugin.msk_data_generator.arn 63 revision = aws_mskconnect_custom_plugin.msk_data_generator.latest_revision 64 } 65 } 66 67 log_delivery { 68 worker_log_delivery { 69 cloudwatch_logs { 70 enabled = true 71 log_group = aws_cloudwatch_log_group.msk_data_generator.name 72 } 73 s3 { 74 enabled = true 75 bucket = aws_s3_bucket.default_bucket.id 76 prefix = \u0026#34;logs/msk/connect/msk-data-generator\u0026#34; 77 } 78 } 79 } 80 81 service_execution_role_arn = aws_iam_role.kafka_connector_role.arn 82} 83 84resource \u0026#34;aws_mskconnect_custom_plugin\u0026#34; \u0026#34;msk_data_generator\u0026#34; { 85 name = \u0026#34;${local.name}-msk-data-generator\u0026#34; 86 content_type = \u0026#34;JAR\u0026#34; 87 88 location { 89 s3 { 90 bucket_arn = aws_s3_bucket.default_bucket.arn 91 file_key = aws_s3_object.msk_data_generator.key 92 } 93 } 94} 95 96resource \u0026#34;aws_s3_object\u0026#34; \u0026#34;msk_data_generator\u0026#34; { 97 bucket = aws_s3_bucket.default_bucket.id 98 key = \u0026#34;plugins/msk-data-generator.jar\u0026#34; 99 source = \u0026#34;connectors/msk-data-generator.jar\u0026#34; 100 101 etag = filemd5(\u0026#34;connectors/msk-data-generator.jar\u0026#34;) 102} 103 104resource \u0026#34;aws_cloudwatch_log_group\u0026#34; \u0026#34;msk_data_generator\u0026#34; { 105 name = \u0026#34;/msk/connect/msk-data-generator\u0026#34; 106 107 retention_in_days = 1 108 109 tags = local.tags 110} We can check the details of the connector on AWS Console as shown below.\nKafka Topic As configured, the source connector ingests messages to the order topic, and we can check it on kpow.\nWe can browse individual messages in the Inspect tab in the Data menu.\nSink Connector The connector is configured to write messages from the order topic into the DynamoDB table created earlier. It requires to specify the table name, AWS region, operation, write capacity and whether to use the default credential provider - see the documentation for details. Note that, if you don\u0026rsquo;t use the default credential provider, you have to specify the access key id and secret access key. Note further that, although the current LTS version is v3.18.2, the default credential provider option didn\u0026rsquo;t work for me, and I was recommended to use v3.20.3 instead. Finally, the camel.sink.unmarshal option is to convert data from the internal java.util.HashMap type into the required java.io.InputStream type. Without this configuration, the connector fails with org.apache.camel.NoTypeConversionAvailableException error.\nAlthough the destination table has ordered_at as the range key, it is not created by the source connector because the Java faker library doesn\u0026rsquo;t have a method to generate a current timestamp. Therefore, it is created by the sink connector using two SMTs - InsertField and TimestampConverter. Specifically they add a timestamp value to the order_at attribute, format the value as yyyy-MM-dd HH:mm:ss:SSS, and convert its type into string.\n1# kafka-connect-for-aws/part-03/msk-connect.tf 2resource \u0026#34;aws_mskconnect_connector\u0026#34; \u0026#34;camel_ddb_sink\u0026#34; { 3 name = \u0026#34;${local.name}-order-sink\u0026#34; 4 5 kafkaconnect_version = \u0026#34;2.7.1\u0026#34; 6 7 capacity { 8 provisioned_capacity { 9 mcu_count = 1 10 worker_count = 1 11 } 12 } 13 14 connector_configuration = { 15 # connector configuration 16 \u0026#34;connector.class\u0026#34; = \u0026#34;org.apache.camel.kafkaconnector.awsddbsink.CamelAwsddbsinkSinkConnector\u0026#34;, 17 \u0026#34;tasks.max\u0026#34; = \u0026#34;1\u0026#34;, 18 \u0026#34;key.converter\u0026#34; = \u0026#34;org.apache.kafka.connect.storage.StringConverter\u0026#34;, 19 \u0026#34;key.converter.schemas.enable\u0026#34; = false, 20 \u0026#34;value.converter\u0026#34; = \u0026#34;org.apache.kafka.connect.json.JsonConverter\u0026#34;, 21 \u0026#34;value.converter.schemas.enable\u0026#34; = false, 22 # camel ddb sink configuration 23 \u0026#34;topics\u0026#34; = \u0026#34;order\u0026#34;, 24 \u0026#34;camel.kamelet.aws-ddb-sink.table\u0026#34; = aws_dynamodb_table.orders_table.id, 25 \u0026#34;camel.kamelet.aws-ddb-sink.region\u0026#34; = local.region, 26 \u0026#34;camel.kamelet.aws-ddb-sink.operation\u0026#34; = \u0026#34;PutItem\u0026#34;, 27 \u0026#34;camel.kamelet.aws-ddb-sink.writeCapacity\u0026#34; = 1, 28 \u0026#34;camel.kamelet.aws-ddb-sink.useDefaultCredentialsProvider\u0026#34; = true, 29 \u0026#34;camel.sink.unmarshal\u0026#34; = \u0026#34;jackson\u0026#34;, 30 # single message transforms 31 \u0026#34;transforms\u0026#34; = \u0026#34;insertTS,formatTS\u0026#34;, 32 \u0026#34;transforms.insertTS.type\u0026#34; = \u0026#34;org.apache.kafka.connect.transforms.InsertField$Value\u0026#34;, 33 \u0026#34;transforms.insertTS.timestamp.field\u0026#34; = \u0026#34;ordered_at\u0026#34;, 34 \u0026#34;transforms.formatTS.type\u0026#34; = \u0026#34;org.apache.kafka.connect.transforms.TimestampConverter$Value\u0026#34;, 35 \u0026#34;transforms.formatTS.format\u0026#34; = \u0026#34;yyyy-MM-dd HH:mm:ss:SSS\u0026#34;, 36 \u0026#34;transforms.formatTS.field\u0026#34; = \u0026#34;ordered_at\u0026#34;, 37 \u0026#34;transforms.formatTS.target.type\u0026#34; = \u0026#34;string\u0026#34; 38 } 39 40 kafka_cluster { 41 apache_kafka_cluster { 42 bootstrap_servers = aws_msk_cluster.msk_data_cluster.bootstrap_brokers_sasl_iam 43 44 vpc { 45 security_groups = [aws_security_group.msk.id] 46 subnets = module.vpc.private_subnets 47 } 48 } 49 } 50 51 kafka_cluster_client_authentication { 52 authentication_type = \u0026#34;IAM\u0026#34; 53 } 54 55 kafka_cluster_encryption_in_transit { 56 encryption_type = \u0026#34;TLS\u0026#34; 57 } 58 59 plugin { 60 custom_plugin { 61 arn = aws_mskconnect_custom_plugin.camel_ddb_sink.arn 62 revision = aws_mskconnect_custom_plugin.camel_ddb_sink.latest_revision 63 } 64 } 65 66 log_delivery { 67 worker_log_delivery { 68 cloudwatch_logs { 69 enabled = true 70 log_group = aws_cloudwatch_log_group.camel_ddb_sink.name 71 } 72 s3 { 73 enabled = true 74 bucket = aws_s3_bucket.default_bucket.id 75 prefix = \u0026#34;logs/msk/connect/camel-ddb-sink\u0026#34; 76 } 77 } 78 } 79 80 service_execution_role_arn = aws_iam_role.kafka_connector_role.arn 81 82 depends_on = [ 83 aws_mskconnect_connector.msk_data_generator 84 ] 85} 86 87resource \u0026#34;aws_mskconnect_custom_plugin\u0026#34; \u0026#34;camel_ddb_sink\u0026#34; { 88 name = \u0026#34;${local.name}-camel-ddb-sink\u0026#34; 89 content_type = \u0026#34;ZIP\u0026#34; 90 91 location { 92 s3 { 93 bucket_arn = aws_s3_bucket.default_bucket.arn 94 file_key = aws_s3_object.camel_ddb_sink.key 95 } 96 } 97} 98 99resource \u0026#34;aws_s3_object\u0026#34; \u0026#34;camel_ddb_sink\u0026#34; { 100 bucket = aws_s3_bucket.default_bucket.id 101 key = \u0026#34;plugins/camel-aws-ddb-sink-kafka-connector.zip\u0026#34; 102 source = \u0026#34;connectors/camel-aws-ddb-sink-kafka-connector.zip\u0026#34; 103 104 etag = filemd5(\u0026#34;connectors/camel-aws-ddb-sink-kafka-connector.zip\u0026#34;) 105} 106 107resource \u0026#34;aws_cloudwatch_log_group\u0026#34; \u0026#34;camel_ddb_sink\u0026#34; { 108 name = \u0026#34;/msk/connect/camel-ddb-sink\u0026#34; 109 110 retention_in_days = 1 111 112 tags = local.tags 113} The sink connector can be checked on AWS Console as shown below.\nDynamoDB Destination We can check the ingested records on the DynamoDB table items view. Below shows a list of scanned records. As expected, it has the order_id, ordered_at and other attributes.\nWe can also obtain an individual Json record by clicking an order_id value as shown below.\nSummary As part of investigating how to utilize Kafka Connect effectively for AWS services integration, I demonstrated how to develop the Camel DynamoDB sink connector using Docker in Part 2. Fake order data was generated using the MSK Data Generator source connector, and the sink connector was configured to consume the topic messages to ingest them into a DynamoDB table. In this post, I illustrated how to deploy the data ingestion applications using Amazon MSK and MSK Connect.\n","date":"July 3, 2023","img":"/blog/2023-07-03-kafka-connect-for-aws-part-3/featured.png","lang":"en","langName":"English","largeImg":"/blog/2023-07-03-kafka-connect-for-aws-part-3/featured_hu135e43c86b5f4f289cf9de4c4f37c397_76240_500x0_resize_box_3.png","permalink":"/blog/2023-07-03-kafka-connect-for-aws-part-3/","series":[{"title":"Kafka Connect for AWS Services Integration","url":"/series/kafka-connect-for-aws-services-integration/"}],"smallImg":"/blog/2023-07-03-kafka-connect-for-aws-part-3/featured_hu135e43c86b5f4f289cf9de4c4f37c397_76240_180x0_resize_box_3.png","tags":[{"title":"AWS","url":"/tags/aws/"},{"title":"Amazon MSK","url":"/tags/amazon-msk/"},{"title":"Amazon MSK Connect","url":"/tags/amazon-msk-connect/"},{"title":"Apache Kafka","url":"/tags/apache-kafka/"},{"title":"Kafka Connect","url":"/tags/kafka-connect/"},{"title":"Apache Camel","url":"/tags/apache-camel/"},{"title":"Amazon DynamoDB","url":"/tags/amazon-dynamodb/"}],"timestamp":1688342400,"title":"Kafka Connect for AWS Services Integration - Part 3 Deploy Camel DynamoDB Sink Connector"},{"categories":[{"title":"Apache Kafka","url":"/categories/apache-kafka/"}],"content":"By default, Apache Kafka communicates in PLAINTEXT, which means that all data is sent without being encrypted. To secure communication, we can configure Kafka clients and other components to use Transport Layer Security (TLS) encryption. Note that TLS is also referred to Secure Sockets Layer (SSL) or TLS/SSL. SSL is the predecessor of TLS, and has been deprecated since June 2015. However, it is used in configuration and code instead of TLS for historical reasons. In this post, SSL, TLS and TLS/SSL will be used interchangeably. SSL encryption is a one-way verification process where a server certificate is verified by a client via SSL Handshake. Moreover, we can improve security by adding client authentication. For example, we can enforce two-way verification so that a client certificate is verified by Kafka brokers as well (SSL Authentication). Alternatively we can choose a separate authentication mechanism and typically Simple Authentication and Security Layer (SASL) is used (SASL Authentication). In this post, we will discuss how to configure SSL encryption with Java and Python client examples while SSL and SASL client authentication will be covered in later posts.\nPart 1 Cluster Setup Part 2 Management App Part 3 Kafka Connect Part 4 Producer and Consumer Part 5 Glue Schema Registry Part 6 Kafka Connect with Glue Schema Registry Part 7 Producer and Consumer with Glue Schema Registry Part 8 SSL Encryption (this post) Part 9 SSL Authentication Part 10 SASL Authentication Part 11 Kafka Authorization Certificate Setup Below shows an overview of certificate setup and SSL Handshake. It is from Apache Kafka Series - Kafka Security | SSL SASL Kerberos ACL by Stephane Maarek and Gerd Koenig (LINK).\nSSL encryption is a one-way verification process where a server certificate is verified by a client via SSL Handshake. The following components are required for setting-up certificates.\nCertificate Authority (CA) - CA is responsible for signing certificates. We\u0026rsquo;ll be using our own CA rather than relying upon an external trusted CA. Two files will be created for the CA - private key (ca-key) and certificate (ca-cert). Keystore - Keystore stores the identity of each machine (Kafka broker or logical client), and the certificate of a machine is signed by the CA. As the CA\u0026rsquo;s certificate is imported into the Truststore of a Kafka client, the machine\u0026rsquo;s certificate is also trusted and verified during SSL Handshake. Note that each machine requires to have its own Keystore. As we have 3 Kafka brokers, 3 Java Keystore files will be created and each of the file names begins with the host name e.g. kafka-0.server.keystore.jks. Truststore - Truststore stores one or more certificates that a Kafka client should trust. Note that importing a certificate of a CA means the client should trust all other certificates that are signed by that certificate, which is called the chain of trust. We\u0026rsquo;ll have a single Java Keystore file for the Truststore named kafka.truststore.jks, and it will be shared by all Kafka brokers and clients. The following script generates the components mentioned above. It begins with creating the files for the CA followed by generating the Keystore of each Kafka broker and the Truststore of Kafka clients. Note that the host names of all Kafka brokers should be added to the Kafka host file (kafka-hosts.txt) so that their Keystore files are generated recursively. Note also that it ends up producing the CA certificate file in the PEM (Privacy Enhanced Mail) format as it is required by a non-Java client - ca-root.pem. The PEM file will be used by the Python clients below. The source can be found in the GitHub repository of this post.\n1# kafka-dev-with-docker/part-08/generate.sh 2#!/usr/bin/env bash 3 4set -eu 5 6CN=\u0026#34;${CN:-kafka-admin}\u0026#34; 7PASSWORD=\u0026#34;${PASSWORD:-supersecret}\u0026#34; 8TO_GENERATE_PEM=\u0026#34;${CITY:-yes}\u0026#34; 9 10VALIDITY_IN_DAYS=3650 11CA_WORKING_DIRECTORY=\u0026#34;certificate-authority\u0026#34; 12TRUSTSTORE_WORKING_DIRECTORY=\u0026#34;truststore\u0026#34; 13KEYSTORE_WORKING_DIRECTORY=\u0026#34;keystore\u0026#34; 14PEM_WORKING_DIRECTORY=\u0026#34;pem\u0026#34; 15CA_KEY_FILE=\u0026#34;ca-key\u0026#34; 16CA_CERT_FILE=\u0026#34;ca-cert\u0026#34; 17DEFAULT_TRUSTSTORE_FILE=\u0026#34;kafka.truststore.jks\u0026#34; 18KEYSTORE_SIGN_REQUEST=\u0026#34;cert-file\u0026#34; 19KEYSTORE_SIGN_REQUEST_SRL=\u0026#34;ca-cert.srl\u0026#34; 20KEYSTORE_SIGNED_CERT=\u0026#34;cert-signed\u0026#34; 21KAFKA_HOSTS_FILE=\u0026#34;kafka-hosts.txt\u0026#34; 22 23if [ ! -f \u0026#34;$KAFKA_HOSTS_FILE\u0026#34; ]; then 24 echo \u0026#34;\u0026#39;$KAFKA_HOSTS_FILE\u0026#39; does not exists. Create this file\u0026#34; 25 exit 1 26fi 27 28echo \u0026#34;Welcome to the Kafka SSL certificate authority, key store and trust store generator script.\u0026#34; 29 30echo 31echo \u0026#34;First we will create our own certificate authority\u0026#34; 32echo \u0026#34; Two files will be created if not existing:\u0026#34; 33echo \u0026#34; - $CA_WORKING_DIRECTORY/$CA_KEY_FILE -- the private key used later to sign certificates\u0026#34; 34echo \u0026#34; - $CA_WORKING_DIRECTORY/$CA_CERT_FILE -- the certificate that will be stored in the trust store\u0026#34; 35echo \u0026#34; and serve as the certificate authority (CA).\u0026#34; 36if [ -f \u0026#34;$CA_WORKING_DIRECTORY/$CA_KEY_FILE\u0026#34; ] \u0026amp;\u0026amp; [ -f \u0026#34;$CA_WORKING_DIRECTORY/$CA_CERT_FILE\u0026#34; ]; then 37 echo \u0026#34;Use existing $CA_WORKING_DIRECTORY/$CA_KEY_FILE and $CA_WORKING_DIRECTORY/$CA_CERT_FILE ...\u0026#34; 38else 39 rm -rf $CA_WORKING_DIRECTORY \u0026amp;\u0026amp; mkdir $CA_WORKING_DIRECTORY 40 echo 41 echo \u0026#34;Generate $CA_WORKING_DIRECTORY/$CA_KEY_FILE and $CA_WORKING_DIRECTORY/$CA_CERT_FILE ...\u0026#34; 42 echo 43 openssl req -new -newkey rsa:4096 -days $VALIDITY_IN_DAYS -x509 -subj \u0026#34;/CN=$CN\u0026#34; \\ 44 -keyout $CA_WORKING_DIRECTORY/$CA_KEY_FILE -out $CA_WORKING_DIRECTORY/$CA_CERT_FILE -nodes 45fi 46 47echo 48echo \u0026#34;A keystore will be generated for each host in $KAFKA_HOSTS_FILE as each broker and logical client needs its own keystore\u0026#34; 49echo 50echo \u0026#34; NOTE: currently in Kafka, the Common Name (CN) does not need to be the FQDN of\u0026#34; 51echo \u0026#34; this host. However, at some point, this may change. As such, make the CN\u0026#34; 52echo \u0026#34; the FQDN. Some operating systems call the CN prompt \u0026#39;first / last name\u0026#39;\u0026#34; 53echo \u0026#34; To learn more about CNs and FQDNs, read:\u0026#34; 54echo \u0026#34; https://docs.oracle.com/javase/7/docs/api/javax/net/ssl/X509ExtendedTrustManager.html\u0026#34; 55rm -rf $KEYSTORE_WORKING_DIRECTORY \u0026amp;\u0026amp; mkdir $KEYSTORE_WORKING_DIRECTORY 56while read -r KAFKA_HOST || [ -n \u0026#34;$KAFKA_HOST\u0026#34; ]; do 57 KEY_STORE_FILE_NAME=\u0026#34;$KAFKA_HOST.server.keystore.jks\u0026#34; 58 echo 59 echo \u0026#34;\u0026#39;$KEYSTORE_WORKING_DIRECTORY/$KEY_STORE_FILE_NAME\u0026#39; will contain a key pair and a self-signed certificate.\u0026#34; 60 keytool -genkey -keystore $KEYSTORE_WORKING_DIRECTORY/\u0026#34;$KEY_STORE_FILE_NAME\u0026#34; \\ 61 -alias localhost -validity $VALIDITY_IN_DAYS -keyalg RSA \\ 62 -noprompt -dname \u0026#34;CN=$KAFKA_HOST\u0026#34; -keypass $PASSWORD -storepass $PASSWORD 63 64 echo 65 echo \u0026#34;Now a certificate signing request will be made to the keystore.\u0026#34; 66 keytool -certreq -keystore $KEYSTORE_WORKING_DIRECTORY/\u0026#34;$KEY_STORE_FILE_NAME\u0026#34; \\ 67 -alias localhost -file $KEYSTORE_SIGN_REQUEST -keypass $PASSWORD -storepass $PASSWORD 68 69 echo 70 echo \u0026#34;Now the private key of the certificate authority (CA) will sign the keystore\u0026#39;s certificate.\u0026#34; 71 openssl x509 -req -CA $CA_WORKING_DIRECTORY/$CA_CERT_FILE \\ 72 -CAkey $CA_WORKING_DIRECTORY/$CA_KEY_FILE \\ 73 -in $KEYSTORE_SIGN_REQUEST -out $KEYSTORE_SIGNED_CERT \\ 74 -days $VALIDITY_IN_DAYS -CAcreateserial 75 # creates $CA_WORKING_DIRECTORY/$KEYSTORE_SIGN_REQUEST_SRL which is never used or needed. 76 77 echo 78 echo \u0026#34;Now the CA will be imported into the keystore.\u0026#34; 79 keytool -keystore $KEYSTORE_WORKING_DIRECTORY/\u0026#34;$KEY_STORE_FILE_NAME\u0026#34; -alias CARoot \\ 80 -import -file $CA_WORKING_DIRECTORY/$CA_CERT_FILE -keypass $PASSWORD -storepass $PASSWORD -noprompt 81 82 echo 83 echo \u0026#34;Now the keystore\u0026#39;s signed certificate will be imported back into the keystore.\u0026#34; 84 keytool -keystore $KEYSTORE_WORKING_DIRECTORY/\u0026#34;$KEY_STORE_FILE_NAME\u0026#34; -alias localhost \\ 85 -import -file $KEYSTORE_SIGNED_CERT -keypass $PASSWORD -storepass $PASSWORD 86 87 echo 88 echo \u0026#34;Complete keystore generation!\u0026#34; 89 echo 90 echo \u0026#34;Deleting intermediate files. They are:\u0026#34; 91 echo \u0026#34; - \u0026#39;$CA_WORKING_DIRECTORY/$KEYSTORE_SIGN_REQUEST_SRL\u0026#39;: CA serial number\u0026#34; 92 echo \u0026#34; - \u0026#39;$KEYSTORE_SIGN_REQUEST\u0026#39;: the keystore\u0026#39;s certificate signing request\u0026#34; 93 echo \u0026#34; - \u0026#39;$KEYSTORE_SIGNED_CERT\u0026#39;: the keystore\u0026#39;s certificate, signed by the CA, and stored back\u0026#34; 94 echo \u0026#34; into the keystore\u0026#34; 95 rm -f $CA_WORKING_DIRECTORY/$KEYSTORE_SIGN_REQUEST_SRL $KEYSTORE_SIGN_REQUEST $KEYSTORE_SIGNED_CERT 96done \u0026lt; \u0026#34;$KAFKA_HOSTS_FILE\u0026#34; 97 98echo 99echo \u0026#34;Now the trust store will be generated from the certificate.\u0026#34; 100rm -rf $TRUSTSTORE_WORKING_DIRECTORY \u0026amp;\u0026amp; mkdir $TRUSTSTORE_WORKING_DIRECTORY 101keytool -keystore $TRUSTSTORE_WORKING_DIRECTORY/$DEFAULT_TRUSTSTORE_FILE \\ 102 -alias CARoot -import -file $CA_WORKING_DIRECTORY/$CA_CERT_FILE \\ 103 -noprompt -dname \u0026#34;CN=$CN\u0026#34; -keypass $PASSWORD -storepass $PASSWORD 104 105if [ $TO_GENERATE_PEM == \u0026#34;yes\u0026#34; ]; then 106 echo 107 echo \u0026#34;The following files for SSL configuration will be created for a non-java client\u0026#34; 108 echo \u0026#34; $PEM_WORKING_DIRECTORY/ca-root.pem: CA file to use in certificate veriication\u0026#34; 109 rm -rf $PEM_WORKING_DIRECTORY \u0026amp;\u0026amp; mkdir $PEM_WORKING_DIRECTORY 110 111 keytool -exportcert -alias CARoot -keystore $TRUSTSTORE_WORKING_DIRECTORY/$DEFAULT_TRUSTSTORE_FILE \\ 112 -rfc -file $PEM_WORKING_DIRECTORY/ca-root.pem -storepass $PASSWORD 113fi The script generates the following files listed below.\n1$ tree certificate-authority keystore truststore pem 2certificate-authority 3├── ca-cert 4└── ca-key 5keystore 6├── kafka-0.server.keystore.jks 7├── kafka-1.server.keystore.jks 8└── kafka-2.server.keystore.jks 9truststore 10└── kafka.truststore.jks 11pem 12└── ca-root.pem Kafka Broker Update We should add the SSL listener to the broker configuration and the port 9093 is reserved for it. Both the Keystore and Truststore files are specified in the broker configuration. The former is to send the broker certificate to clients while the latter is necessary because a Kafka broker can be a client of other brokers. The changes made to the first Kafka broker are shown below, and the same updates are made to the other brokers. The cluster can be started by docker-compose -f compose-kafka.yml up -d.\n1# kafka-dev-with-docker/part-08/compose-kafka.yml 2version: \u0026#34;3.5\u0026#34; 3 4services: 5... 6 7 kafka-0: 8 image: bitnami/kafka:2.8.1 9 container_name: kafka-0 10 expose: 11 - 9092 12 - 9093 13 ports: 14 - \u0026#34;29092:29092\u0026#34; 15 networks: 16 - kafkanet 17 environment: 18 - ALLOW_PLAINTEXT_LISTENER=yes 19 - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181 20 - KAFKA_CFG_BROKER_ID=0 21 - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=INTERNAL:PLAINTEXT,SSL:SSL,EXTERNAL:PLAINTEXT 22 - KAFKA_CFG_LISTENERS=INTERNAL://:9092,SSL://:9093,EXTERNAL://:29092 23 - KAFKA_CFG_ADVERTISED_LISTENERS=INTERNAL://kafka-0:9092,SSL://kafka-0:9093,EXTERNAL://localhost:29092 24 - KAFKA_CFG_INTER_BROKER_LISTENER_NAME=SSL 25 - KAFKA_CFG_SSL_KEYSTORE_LOCATION=/opt/bitnami/kafka/config/certs/kafka.keystore.jks 26 - KAFKA_CFG_SSL_KEYSTORE_PASSWORD=supersecret 27 - KAFKA_CFG_SSL_KEY_PASSWORD=supersecret 28 - KAFKA_CFG_SSL_TRUSTSTORE_LOCATION=/opt/bitnami/kafka/config/certs/kafka.truststore.jks 29 - KAFKA_CFG_SSL_TRUSTSTORE_PASSWORD=supersecret 30 volumes: 31 - kafka_0_data:/bitnami/kafka 32 - ./keystore/kafka-0.server.keystore.jks:/opt/bitnami/kafka/config/certs/kafka.keystore.jks:ro 33 - ./truststore/kafka.truststore.jks:/opt/bitnami/kafka/config/certs/kafka.truststore.jks:ro 34 - ./client.properties:/opt/bitnami/kafka/config/client.properties:ro 35 depends_on: 36 - zookeeper 37 38... 39 40networks: 41 kafkanet: 42 name: kafka-network 43 44... Examples Java and non-Java clients need different configurations. The former can use the Keystore file of the Truststore directly while the latter needs corresponding details in a PEM file. The Kafka CLI and Kafka-UI will be taken as Java client examples while Python producer/consumer will be used to illustrate non-Java clients.\nKafka CLI The following configuration is necessary to use the SSL listener. It includes the security protocol, the location of the Truststore file and the password to access it.\n1# kafka-dev-with-docker/part-08/client.properties 2security.protocol=SSL 3ssl.truststore.location=/opt/bitnami/kafka/config/certs/kafka.truststore.jks 4ssl.truststore.password=supersecret Below shows a producer example. It creates a topic named inventory and produces messages using corresponding scripts. Note the client configuration file (client.properties) is specified in configurations, and it is available via volume-mapping.\n1## producer example 2$ docker exec -it kafka-1 bash 3I have no name!@07d1ca934530:/$ cd /opt/bitnami/kafka/bin/ 4 5## create a topic 6I have no name!@07d1ca934530:/opt/bitnami/kafka/bin$ ./kafka-topics.sh --bootstrap-server kafka-0:9093 \\ 7 --create --topic inventory --partitions 3 --replication-factor 3 \\ 8 --command-config /opt/bitnami/kafka/config/client.properties 9# Created topic inventory. 10 11## produce messages 12I have no name!@07d1ca934530:/opt/bitnami/kafka/bin$ ./kafka-console-producer.sh --bootstrap-server kafka-0:9093 \\ 13 --topic inventory --producer.config /opt/bitnami/kafka/config/client.properties 14\u0026gt;product: apples, quantity: 5 15\u0026gt;product: lemons, quantity: 7 Once messages are created, we can check it by a consumer. We can execute a consumer in a separate console.\n1## consumer example 2$ docker exec -it kafka-1 bash 3I have no name!@07d1ca934530:/$ cd /opt/bitnami/kafka/bin/ 4 5## consume messages 6I have no name!@07d1ca934530:/opt/bitnami/kafka/bin$ ./kafka-console-consumer.sh --bootstrap-server kafka-0:9093 \\ 7 --topic inventory --consumer.config /opt/bitnami/kafka/config/client.properties --from-beginning 8product: apples, quantity: 5 9product: lemons, quantity: 7 Python Client We will run the Python producer and consumer apps using docker-compose. At startup, each of them installs required packages and executes its corresponding app script. As it shares the same network to the Kafka cluster, we can take the service names (e.g. kafka-0) on port 9093 as Kafka bootstrap servers. As shown below, we will need the certificate of the CA (ca-root.pem) and it will be available via volume-mapping. The apps can be started by docker-compose -f compose-apps.yml up -d.\n1# kafka-dev-with-docker/part-08/compose-apps.yml 2version: \u0026#34;3.5\u0026#34; 3 4services: 5 producer: 6 image: bitnami/python:3.9 7 container_name: producer 8 command: \u0026#34;sh -c \u0026#39;pip install -r requirements.txt \u0026amp;\u0026amp; python producer.py\u0026#39;\u0026#34; 9 networks: 10 - kafkanet 11 environment: 12 BOOTSTRAP_SERVERS: kafka-0:9093,kafka-1:9093,kafka-2:9093 13 TOPIC_NAME: orders 14 TZ: Australia/Sydney 15 volumes: 16 - .:/app 17 consumer: 18 image: bitnami/python:3.9 19 container_name: consumer 20 command: \u0026#34;sh -c \u0026#39;pip install -r requirements.txt \u0026amp;\u0026amp; python consumer.py\u0026#39;\u0026#34; 21 networks: 22 - kafkanet 23 environment: 24 BOOTSTRAP_SERVERS: kafka-0:9093,kafka-1:9093,kafka-2:9093 25 TOPIC_NAME: orders 26 GROUP_ID: orders-group 27 TZ: Australia/Sydney 28 volumes: 29 - .:/app 30 31networks: 32 kafkanet: 33 external: true 34 name: kafka-network Producer The same producer app discussed in Part 4 is used here. The following arguments are added to access the SSL listener.\nsecurity_protocol - Protocol used to communicate with brokers. ssl_check_hostname - Flag to configure whether SSL handshake should verify that the certificate matches the broker\u0026rsquo;s hostname. ssl_cafile - Optional filename of CA (certificate) file to use in certificate verification. 1# kafka-dev-with-docker/part-08/producer.py 2... 3 4class Producer: 5 def __init__(self, bootstrap_servers: list, topic: str): 6 self.bootstrap_servers = bootstrap_servers 7 self.topic = topic 8 self.producer = self.create() 9 10 def create(self): 11 return KafkaProducer( 12 bootstrap_servers=self.bootstrap_servers, 13 security_protocol=\u0026#34;SSL\u0026#34;, 14 ssl_check_hostname=True, 15 ssl_cafile=\u0026#34;pem/ca-root.pem\u0026#34;, 16 value_serializer=lambda v: json.dumps(v, default=self.serialize).encode(\u0026#34;utf-8\u0026#34;), 17 key_serializer=lambda v: json.dumps(v, default=self.serialize).encode(\u0026#34;utf-8\u0026#34;), 18 ) 19 20 def send(self, orders: typing.List[Order]): 21 for order in orders: 22 try: 23 self.producer.send( 24 self.topic, key={\u0026#34;order_id\u0026#34;: order.order_id}, value=order.asdict() 25 ) 26 except Exception as e: 27 raise RuntimeError(\u0026#34;fails to send a message\u0026#34;) from e 28 self.producer.flush() 29 30... 31 32if __name__ == \u0026#34;__main__\u0026#34;: 33 producer = Producer( 34 bootstrap_servers=os.getenv(\u0026#34;BOOTSTRAP_SERVERS\u0026#34;, \u0026#34;localhost:29092\u0026#34;).split(\u0026#34;,\u0026#34;), 35 topic=os.getenv(\u0026#34;TOPIC_NAME\u0026#34;, \u0026#34;orders\u0026#34;), 36 ) 37 max_run = int(os.getenv(\u0026#34;MAX_RUN\u0026#34;, \u0026#34;-1\u0026#34;)) 38 logging.info(f\u0026#34;max run - {max_run}\u0026#34;) 39 current_run = 0 40 while True: 41 current_run += 1 42 logging.info(f\u0026#34;current run - {current_run}\u0026#34;) 43 if current_run \u0026gt; max_run and max_run \u0026gt;= 0: 44 logging.info(f\u0026#34;exceeds max run, finish\u0026#34;) 45 producer.producer.close() 46 break 47 producer.send(Order.auto().create(100)) 48 time.sleep(1) In the container log, we can check SSH Handshake is performed successfully by loading the CA certificate file.\n1INFO:kafka.conn:\u0026lt;BrokerConnection node_id=bootstrap-1 host=kafka-0:9093 \u0026lt;connecting\u0026gt; [IPv4 (\u0026#39;172.20.0.3\u0026#39;, 9093)]\u0026gt;: connecting to kafka-0:9093 [(\u0026#39;172.20.0.3\u0026#39;, 9093) IPv4] 2INFO:kafka.conn:Probing node bootstrap-1 broker version 3INFO:kafka.conn:\u0026lt;BrokerConnection node_id=bootstrap-1 host=kafka-0:9093 \u0026lt;handshake\u0026gt; [IPv4 (\u0026#39;172.20.0.3\u0026#39;, 9093)]\u0026gt;: Loading SSL CA from pem/ca-root.pem 4INFO:kafka.conn:\u0026lt;BrokerConnection node_id=bootstrap-1 host=kafka-0:9093 \u0026lt;handshake\u0026gt; [IPv4 (\u0026#39;172.20.0.3\u0026#39;, 9093)]\u0026gt;: Connection complete. 5INFO:root:max run - -1 6INFO:root:current run - 1 7... 8INFO:root:current run - 2 Consumer The same consumer app in Part 4 is used here as well. As the producer app, the following arguments are added - security_protocol, ssl_check_hostname and ssl_cafile.\n1# kafka-dev-with-docker/part-08/consumer.py 2... 3 4class Consumer: 5 def __init__(self, bootstrap_servers: list, topics: list, group_id: str) -\u0026gt; None: 6 self.bootstrap_servers = bootstrap_servers 7 self.topics = topics 8 self.group_id = group_id 9 self.consumer = self.create() 10 11 def create(self): 12 return KafkaConsumer( 13 *self.topics, 14 bootstrap_servers=self.bootstrap_servers, 15 security_protocol=\u0026#34;SSL\u0026#34;, 16 ssl_check_hostname=True, 17 ssl_cafile=\u0026#34;pem/ca-root.pem\u0026#34;, 18 auto_offset_reset=\u0026#34;earliest\u0026#34;, 19 enable_auto_commit=True, 20 group_id=self.group_id, 21 key_deserializer=lambda v: v.decode(\u0026#34;utf-8\u0026#34;), 22 value_deserializer=lambda v: v.decode(\u0026#34;utf-8\u0026#34;), 23 ) 24 25 def process(self): 26 try: 27 while True: 28 msg = self.consumer.poll(timeout_ms=1000) 29 if msg is None: 30 continue 31 self.print_info(msg) 32 time.sleep(1) 33 except KafkaError as error: 34 logging.error(error) 35 36 def print_info(self, msg: dict): 37 for t, v in msg.items(): 38 for r in v: 39 logging.info( 40 f\u0026#34;key={r.key}, value={r.value}, topic={t.topic}, partition={t.partition}, offset={r.offset}, ts={r.timestamp}\u0026#34; 41 ) 42 43 44if __name__ == \u0026#34;__main__\u0026#34;: 45 consumer = Consumer( 46 bootstrap_servers=os.getenv(\u0026#34;BOOTSTRAP_SERVERS\u0026#34;, \u0026#34;localhost:29092\u0026#34;).split(\u0026#34;,\u0026#34;), 47 topics=os.getenv(\u0026#34;TOPIC_NAME\u0026#34;, \u0026#34;orders\u0026#34;).split(\u0026#34;,\u0026#34;), 48 group_id=os.getenv(\u0026#34;GROUP_ID\u0026#34;, \u0026#34;orders-group\u0026#34;), 49 ) 50 consumer.process() We can also check messages are consumed after SSH Handshake is succeeded in the container log.\n1... 2INFO:kafka.conn:\u0026lt;BrokerConnection node_id=0 host=kafka-0:9093 \u0026lt;connecting\u0026gt; [IPv4 (\u0026#39;172.20.0.3\u0026#39;, 9093)]\u0026gt;: connecting to kafka-0:9093 [(\u0026#39;172.20.0.3\u0026#39;, 9093) IPv4] 3INFO:kafka.conn:\u0026lt;BrokerConnection node_id=0 host=kafka-0:9093 \u0026lt;handshake\u0026gt; [IPv4 (\u0026#39;172.20.0.3\u0026#39;, 9093)]\u0026gt;: Loading SSL CA from pem/ca-root.pem 4INFO:kafka.conn:\u0026lt;BrokerConnection node_id=0 host=kafka-0:9093 \u0026lt;handshake\u0026gt; [IPv4 (\u0026#39;172.20.0.3\u0026#39;, 9093)]\u0026gt;: Connection complete. 5INFO:kafka.cluster:Group coordinator for orders-group is BrokerMetadata(nodeId=\u0026#39;coordinator-0\u0026#39;, host=\u0026#39;kafka-0\u0026#39;, port=9093, rack=None) 6INFO:kafka.coordinator:Discovered coordinator coordinator-0 for group orders-group 7WARNING:kafka.coordinator:Marking the coordinator dead (node coordinator-0) for group orders-group: Node Disconnected. 8INFO:kafka.conn:\u0026lt;BrokerConnection node_id=coordinator-0 host=kafka-0:9093 \u0026lt;connecting\u0026gt; [IPv4 (\u0026#39;172.20.0.3\u0026#39;, 9093)]\u0026gt;: connecting to kafka-0:9093 [(\u0026#39;172.20.0.3\u0026#39;, 9093) IPv4] 9INFO:kafka.cluster:Group coordinator for orders-group is BrokerMetadata(nodeId=\u0026#39;coordinator-0\u0026#39;, host=\u0026#39;kafka-0\u0026#39;, port=9093, rack=None) 10INFO:kafka.coordinator:Discovered coordinator coordinator-0 for group orders-group 11INFO:kafka.conn:\u0026lt;BrokerConnection node_id=coordinator-0 host=kafka-0:9093 \u0026lt;handshake\u0026gt; [IPv4 (\u0026#39;172.20.0.3\u0026#39;, 9093)]\u0026gt;: Connection complete. 12INFO:kafka.coordinator:(Re-)joining group orders-group 13INFO:kafka.coordinator:Elected group leader -- performing partition assignments using range 14INFO:kafka.coordinator:Successfully joined group orders-group with generation 3 15INFO:kafka.consumer.subscription_state:Updated partition assignment: [TopicPartition(topic=\u0026#39;orders\u0026#39;, partition=0)] 16INFO:kafka.coordinator.consumer:Setting newly assigned partitions {TopicPartition(topic=\u0026#39;orders\u0026#39;, partition=0)} for group orders-group 17... 18INFO:root:key={\u0026#34;order_id\u0026#34;: \u0026#34;6f642267-0497-4e63-8989-45e29e768351\u0026#34;}, value={\u0026#34;order_id\u0026#34;: \u0026#34;6f642267-0497-4e63-8989-45e29e768351\u0026#34;, \u0026#34;ordered_at\u0026#34;: \u0026#34;2023-06-20T20:26:45.635986\u0026#34;, \u0026#34;user_id\u0026#34;: \u0026#34;003\u0026#34;, \u0026#34;order_items\u0026#34;: [{\u0026#34;product_id\u0026#34;: 1000, \u0026#34;quantity\u0026#34;: 2}, {\u0026#34;product_id\u0026#34;: 541, \u0026#34;quantity\u0026#34;: 10}, {\u0026#34;product_id\u0026#34;: 431, \u0026#34;quantity\u0026#34;: 10}, {\u0026#34;product_id\u0026#34;: 770, \u0026#34;quantity\u0026#34;: 7}]}, topic=orders, partition=0, offset=10700, ts=1687292805638 19INFO:root:key={\u0026#34;order_id\u0026#34;: \u0026#34;1d5a92bc-75e0-46e9-a334-43e03e408ea0\u0026#34;}, value={\u0026#34;order_id\u0026#34;: \u0026#34;1d5a92bc-75e0-46e9-a334-43e03e408ea0\u0026#34;, \u0026#34;ordered_at\u0026#34;: \u0026#34;2023-06-20T20:26:45.636034\u0026#34;, \u0026#34;user_id\u0026#34;: \u0026#34;032\u0026#34;, \u0026#34;order_items\u0026#34;: [{\u0026#34;product_id\u0026#34;: 404, \u0026#34;quantity\u0026#34;: 7}, {\u0026#34;product_id\u0026#34;: 932, \u0026#34;quantity\u0026#34;: 8}]}, topic=orders, partition=0, offset=10701, ts=1687292805638 Kafka-UI Kafka-UI is also a Java client, and it accepts the Keystore file of the Kafka Truststore (kafka.truststore.jks). We can specify the file and password to access it as environment variables. The app can be started by docker-compose -f compose-ui.yml up -d.\n1# kafka-dev-with-docker/part-08/compose-ui.yml 2version: \u0026#34;3.5\u0026#34; 3 4services: 5 kafka-ui: 6 image: provectuslabs/kafka-ui:master 7 container_name: kafka-ui 8 ports: 9 - \u0026#34;8080:8080\u0026#34; 10 networks: 11 - kafkanet 12 environment: 13 KAFKA_CLUSTERS_0_NAME: local 14 KAFKA_CLUSTERS_0_PROPERTIES_SECURITY_PROTOCOL: SSL 15 KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka-0:9093,kafka-1:9093,kafka-2:9093 16 KAFKA_CLUSTERS_0_ZOOKEEPER: zookeeper:2181 17 KAFKA_CLUSTERS_0_SSL_TRUSTSTORELOCATION: /kafka.truststore.jks 18 KAFKA_CLUSTERS_0_SSL_TRUSTSTOREPASSWORD: supersecret 19 volumes: 20 - ./truststore/kafka.truststore.jks:/kafka.truststore.jks:ro 21 22networks: 23 kafkanet: 24 external: true 25 name: kafka-network Once started, we can check the messages of the orders topic successfully.\nSummary By default, Apache Kafka communicates in PLAINTEXT, and we can configure Kafka clients and other components to use TLS (SSL or TLS/SSL) encryption to secure communication. In this post, we discussed how to configure SSL encryption with Java and Python client examples.\n","date":"June 29, 2023","img":"/blog/2023-06-29-kafka-development-with-docker-part-8/featured.png","lang":"en","langName":"English","largeImg":"/blog/2023-06-29-kafka-development-with-docker-part-8/featured_hu5ba48e27b04577c6841598e1e7862406_469311_500x0_resize_box_3.png","permalink":"/blog/2023-06-29-kafka-development-with-docker-part-8/","series":[{"title":"Kafka Development With Docker","url":"/series/kafka-development-with-docker/"}],"smallImg":"/blog/2023-06-29-kafka-development-with-docker-part-8/featured_hu5ba48e27b04577c6841598e1e7862406_469311_180x0_resize_box_3.png","tags":[{"title":"Apache Kafka","url":"/tags/apache-kafka/"},{"title":"Security","url":"/tags/security/"},{"title":"Python","url":"/tags/python/"},{"title":"Docker","url":"/tags/docker/"},{"title":"Docker Compose","url":"/tags/docker-compose/"}],"timestamp":1687996800,"title":"Kafka Development With Docker - Part 8 SSL Encryption"},{"categories":[{"title":"Apache Kafka","url":"/categories/apache-kafka/"}],"content":"In Part 4, we developed Kafka producer and consumer applications using the kafka-python package. The Kafka messages are serialized as Json, but are not associated with a schema as there was not an integrated schema registry. Later we discussed how producers and consumers to Kafka topics can use schemas to ensure data consistency and compatibility as schemas evolve in Part 5. In this post, I\u0026rsquo;ll demonstrate how to enhance the existing applications by integrating AWS Glue Schema Registry.\nPart 1 Cluster Setup Part 2 Management App Part 3 Kafka Connect Part 4 Producer and Consumer Part 5 Glue Schema Registry Part 6 Kafka Connect with Glue Schema Registry Part 7 Producer and Consumer with Glue Schema Registry (this post) Part 8 SSL Encryption Part 9 SSL Authentication Part 10 SASL Authentication Part 11 Kafka Authorization Producer Fake order data is generated using the Faker package and the dataclasses_avroschema package is used to automatically generate the Avro schema according to its attributes. A mixin class called InjectCompatMixin is injected into the Order class, which specifies a schema compatibility mode into the generated schema. The auto() class method is used to generate an order record by instantiating the class.\nThe aws-glue-schema-registry package is used serialize order records. It provides the KafkaSerializer class that validates, registers and serializes the relevant records. It supports Json and Avro schemas, and we can add it to the value_serializer argument of the KafkaProducer class. By default, the schemas are named as \u0026lt;topic\u0026gt;-key and \u0026lt;topic\u0026gt;-value and it can be changed by updating the schema_naming_strategy argument. Note that, when sending a message, the value should be a tuple of data and schema.\nThe producer application can be run simply by python producer.py. Note that, as the producer runs outside the Docker network, the host name of the external listener (localhost:29092) should be used as the bootstrap server address. The source can be found in the GitHub repository of this post.\n1# kafka-pocs/kafka-dev-with-docker/part-07/producer.py 2import os 3import datetime 4import time 5import json 6import typing 7import logging 8import dataclasses 9import enum 10 11from faker import Faker 12from dataclasses_avroschema import AvroModel 13import boto3 14import botocore.exceptions 15from kafka import KafkaProducer 16from aws_schema_registry import SchemaRegistryClient 17from aws_schema_registry.avro import AvroSchema 18from aws_schema_registry.adapter.kafka import KafkaSerializer 19 20logging.basicConfig(level=logging.INFO) 21 22 23class Compatibility(enum.Enum): 24 NONE = \u0026#34;NONE\u0026#34; 25 DISABLED = \u0026#34;DISABLED\u0026#34; 26 BACKWARD = \u0026#34;BACKWARD\u0026#34; 27 BACKWARD_ALL = \u0026#34;BACKWARD_ALL\u0026#34; 28 FORWARD = \u0026#34;FORWARD\u0026#34; 29 FORWARD_ALL = \u0026#34;FORWARD_ALL\u0026#34; 30 FULL = \u0026#34;FULL\u0026#34; 31 FULL_ALL = \u0026#34;FULL_ALL\u0026#34; 32 33 34class InjectCompatMixin: 35 @classmethod 36 def updated_avro_schema_to_python(cls, compat: Compatibility = Compatibility.BACKWARD): 37 schema = cls.avro_schema_to_python() 38 schema[\u0026#34;compatibility\u0026#34;] = compat.value 39 return schema 40 41 @classmethod 42 def updated_avro_schema(cls, compat: Compatibility = Compatibility.BACKWARD): 43 schema = cls.updated_avro_schema_to_python(compat) 44 return json.dumps(schema) 45 46 47@dataclasses.dataclass 48class OrderItem(AvroModel): 49 product_id: int 50 quantity: int 51 52 53@dataclasses.dataclass 54class Order(AvroModel, InjectCompatMixin): 55 \u0026#34;Online fake order item\u0026#34; 56 order_id: str 57 ordered_at: datetime.datetime 58 user_id: str 59 order_items: typing.List[OrderItem] 60 61 class Meta: 62 namespace = \u0026#34;Order V1\u0026#34; 63 64 def asdict(self): 65 return dataclasses.asdict(self) 66 67 @classmethod 68 def auto(cls, fake: Faker = Faker()): 69 user_id = str(fake.random_int(1, 100)).zfill(3) 70 order_items = [ 71 OrderItem(fake.random_int(1, 1000), fake.random_int(1, 10)) 72 for _ in range(fake.random_int(1, 4)) 73 ] 74 return cls(fake.uuid4(), datetime.datetime.utcnow(), user_id, order_items) 75 76 def create(self, num: int): 77 return [self.auto() for _ in range(num)] 78 79 80class Producer: 81 def __init__(self, bootstrap_servers: list, topic: str, registry: str): 82 self.bootstrap_servers = bootstrap_servers 83 self.topic = topic 84 self.registry = registry 85 self.glue_client = boto3.client( 86 \u0026#34;glue\u0026#34;, region_name=os.getenv(\u0026#34;AWS_DEFAULT_REGION\u0026#34;, \u0026#34;ap-southeast-2\u0026#34;) 87 ) 88 self.producer = self.create() 89 90 @property 91 def serializer(self): 92 client = SchemaRegistryClient(self.glue_client, registry_name=self.registry) 93 return KafkaSerializer(client) 94 95 def create(self): 96 return KafkaProducer( 97 bootstrap_servers=self.bootstrap_servers, 98 key_serializer=lambda v: json.dumps(v, default=self.serialize).encode(\u0026#34;utf-8\u0026#34;), 99 value_serializer=self.serializer, 100 ) 101 102 def send(self, orders: typing.List[Order], schema: AvroSchema): 103 if not self.check_registry(): 104 print(f\u0026#34;registry not found, create {self.registry}\u0026#34;) 105 self.create_registry() 106 107 for order in orders: 108 try: 109 self.producer.send( 110 self.topic, key={\u0026#34;order_id\u0026#34;: order.order_id}, value=(order.asdict(), schema) 111 ) 112 except Exception as e: 113 raise RuntimeError(\u0026#34;fails to send a message\u0026#34;) from e 114 self.producer.flush() 115 116 def serialize(self, obj): 117 if isinstance(obj, datetime.datetime): 118 return obj.isoformat() 119 if isinstance(obj, datetime.date): 120 return str(obj) 121 return obj 122 123 def check_registry(self): 124 try: 125 self.glue_client.get_registry(RegistryId={\u0026#34;RegistryName\u0026#34;: self.registry}) 126 return True 127 except botocore.exceptions.ClientError as e: 128 if e.response[\u0026#34;Error\u0026#34;][\u0026#34;Code\u0026#34;] == \u0026#34;EntityNotFoundException\u0026#34;: 129 return False 130 else: 131 raise e 132 133 def create_registry(self): 134 try: 135 self.glue_client.create_registry(RegistryName=self.registry) 136 return True 137 except botocore.exceptions.ClientError as e: 138 if e.response[\u0026#34;Error\u0026#34;][\u0026#34;Code\u0026#34;] == \u0026#34;AlreadyExistsException\u0026#34;: 139 return True 140 else: 141 raise e 142 143 144if __name__ == \u0026#34;__main__\u0026#34;: 145 producer = Producer( 146 bootstrap_servers=os.getenv(\u0026#34;BOOTSTRAP_SERVERS\u0026#34;, \u0026#34;localhost:29092\u0026#34;).split(\u0026#34;,\u0026#34;), 147 topic=os.getenv(\u0026#34;TOPIC_NAME\u0026#34;, \u0026#34;orders\u0026#34;), 148 registry=os.getenv(\u0026#34;REGISTRY_NAME\u0026#34;, \u0026#34;online-order\u0026#34;), 149 ) 150 max_run = int(os.getenv(\u0026#34;MAX_RUN\u0026#34;, \u0026#34;-1\u0026#34;)) 151 logging.info(f\u0026#34;max run - {max_run}\u0026#34;) 152 current_run = 0 153 while True: 154 current_run += 1 155 logging.info(f\u0026#34;current run - {current_run}\u0026#34;) 156 if current_run \u0026gt; max_run and max_run \u0026gt;= 0: 157 logging.info(f\u0026#34;exceeds max run, finish\u0026#34;) 158 producer.producer.close() 159 break 160 orders = Order.auto().create(1) 161 schema = AvroSchema(Order.updated_avro_schema(Compatibility.BACKWARD)) 162 producer.send(Order.auto().create(100), schema) 163 time.sleep(1) The generated schema of the Order class can be found below.\n1{ 2 \u0026#34;doc\u0026#34;: \u0026#34;Online fake order item\u0026#34;, 3 \u0026#34;namespace\u0026#34;: \u0026#34;Order V1\u0026#34;, 4 \u0026#34;name\u0026#34;: \u0026#34;Order\u0026#34;, 5 \u0026#34;compatibility\u0026#34;: \u0026#34;BACKWARD\u0026#34;, 6 \u0026#34;type\u0026#34;: \u0026#34;record\u0026#34;, 7 \u0026#34;fields\u0026#34;: [ 8 { 9 \u0026#34;name\u0026#34;: \u0026#34;order_id\u0026#34;, 10 \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; 11 }, 12 { 13 \u0026#34;name\u0026#34;: \u0026#34;ordered_at\u0026#34;, 14 \u0026#34;type\u0026#34;: { 15 \u0026#34;type\u0026#34;: \u0026#34;long\u0026#34;, 16 \u0026#34;logicalType\u0026#34;: \u0026#34;timestamp-millis\u0026#34; 17 } 18 }, 19 { 20 \u0026#34;name\u0026#34;: \u0026#34;user_id\u0026#34;, 21 \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; 22 }, 23 { 24 \u0026#34;name\u0026#34;: \u0026#34;order_items\u0026#34;, 25 \u0026#34;type\u0026#34;: { 26 \u0026#34;type\u0026#34;: \u0026#34;array\u0026#34;, 27 \u0026#34;items\u0026#34;: { 28 \u0026#34;type\u0026#34;: \u0026#34;record\u0026#34;, 29 \u0026#34;name\u0026#34;: \u0026#34;OrderItem\u0026#34;, 30 \u0026#34;fields\u0026#34;: [ 31 { 32 \u0026#34;name\u0026#34;: \u0026#34;product_id\u0026#34;, 33 \u0026#34;type\u0026#34;: \u0026#34;long\u0026#34; 34 }, 35 { 36 \u0026#34;name\u0026#34;: \u0026#34;quantity\u0026#34;, 37 \u0026#34;type\u0026#34;: \u0026#34;long\u0026#34; 38 } 39 ] 40 }, 41 \u0026#34;name\u0026#34;: \u0026#34;order_item\u0026#34; 42 } 43 } 44 ] 45} Below shows an example order record.\n1{ 2\t\u0026#34;order_id\u0026#34;: \u0026#34;00328584-7db3-4cfb-a5b7-de2c7eed7f43\u0026#34;, 3\t\u0026#34;ordered_at\u0026#34;: \u0026#34;2023-05-23T08:30:52.461000\u0026#34;, 4\t\u0026#34;user_id\u0026#34;: \u0026#34;010\u0026#34;, 5\t\u0026#34;order_items\u0026#34;: [ 6\t{ 7\t\u0026#34;product_id\u0026#34;: 213, 8\t\u0026#34;quantity\u0026#34;: 5 9\t}, 10\t{ 11\t\u0026#34;product_id\u0026#34;: 486, 12\t\u0026#34;quantity\u0026#34;: 3 13\t} 14\t] 15} Consumer The Consumer class instantiates the KafkaConsumer class in the create method. The main consumer configuration values are provided by the constructor arguments: Kafka bootstrap server addresses (bootstrap_servers), topic names (topics) and consumer group ID (group_id). Note that the aws-glue-schema-registry package provides the KafkaDeserializer class that deserializes messages according to the corresponding schema version, and we should use it as the value_deserializer. The process() method of the class polls messages and logs details of consumer records.\n1# kafka-pocs/kafka-dev-with-docker/part-07/consumer.py 2import os 3import time 4import logging 5 6from kafka import KafkaConsumer 7from kafka.errors import KafkaError 8import boto3 9from aws_schema_registry import SchemaRegistryClient, DataAndSchema 10from aws_schema_registry.adapter.kafka import KafkaDeserializer 11 12logging.basicConfig(level=logging.INFO) 13 14 15class Consumer: 16 def __init__(self, bootstrap_servers: list, topics: list, group_id: str, registry: str) -\u0026gt; None: 17 self.bootstrap_servers = bootstrap_servers 18 self.topics = topics 19 self.group_id = group_id 20 self.registry = registry 21 self.glue_client = boto3.client( 22 \u0026#34;glue\u0026#34;, region_name=os.getenv(\u0026#34;AWS_DEFAULT_REGION\u0026#34;, \u0026#34;ap-southeast-2\u0026#34;) 23 ) 24 self.consumer = self.create() 25 26 @property 27 def deserializer(self): 28 client = SchemaRegistryClient(self.glue_client, registry_name=self.registry) 29 return KafkaDeserializer(client) 30 31 def create(self): 32 return KafkaConsumer( 33 *self.topics, 34 bootstrap_servers=self.bootstrap_servers, 35 auto_offset_reset=\u0026#34;earliest\u0026#34;, 36 enable_auto_commit=True, 37 group_id=self.group_id, 38 key_deserializer=lambda v: v.decode(\u0026#34;utf-8\u0026#34;), 39 value_deserializer=self.deserializer, 40 ) 41 42 def process(self): 43 try: 44 while True: 45 msg = self.consumer.poll(timeout_ms=1000) 46 if msg is None: 47 continue 48 self.print_info(msg) 49 time.sleep(1) 50 except KafkaError as error: 51 logging.error(error) 52 53 def print_info(self, msg: dict): 54 for t, v in msg.items(): 55 for r in v: 56 value: DataAndSchema = r.value 57 logging.info( 58 f\u0026#34;key={r.key}, value={value.data}, topic={t.topic}, partition={t.partition}, offset={r.offset}, ts={r.timestamp}\u0026#34; 59 ) 60 61 62if __name__ == \u0026#34;__main__\u0026#34;: 63 consumer = Consumer( 64 bootstrap_servers=os.getenv(\u0026#34;BOOTSTRAP_SERVERS\u0026#34;, \u0026#34;localhost:29092\u0026#34;).split(\u0026#34;,\u0026#34;), 65 topics=os.getenv(\u0026#34;TOPIC_NAME\u0026#34;, \u0026#34;orders\u0026#34;).split(\u0026#34;,\u0026#34;), 66 group_id=os.getenv(\u0026#34;GROUP_ID\u0026#34;, \u0026#34;orders-group\u0026#34;), 67 registry=os.getenv(\u0026#34;REGISTRY_NAME\u0026#34;, \u0026#34;online-order\u0026#34;), 68 ) 69 consumer.process() Consumer Services The default number of partitions is set to be 3 in the compose-kafka.yml. A docker compose file is created for the consumer in order to deploy multiple instances of the app using the scale option. As the service uses the same docker network (kafkanet), we can take the service names of the brokers (e.g. kafka-0) on port 9092. Once started, it installs required packages and starts the consumer.\n1# kafka-pocs/kafka-dev-with-docker/part-07/compose-consumer.yml 2version: \u0026#34;3.5\u0026#34; 3 4services: 5 app: 6 image: bitnami/python:3.9 7 command: \u0026#34;sh -c \u0026#39;pip install -r requirements.txt \u0026amp;\u0026amp; python consumer.py\u0026#39;\u0026#34; 8 networks: 9 - kafkanet 10 environment: 11 AWS_ACCESS_KEY_ID: $AWS_ACCESS_KEY_ID 12 AWS_SECRET_ACCESS_KEY: $AWS_SECRET_ACCESS_KEY 13 AWS_SESSION_TOKEN: $AWS_SESSION_TOKEN 14 BOOTSTRAP_SERVERS: kafka-0:9092,kafka-1:9092,kafka-2:9092 15 TOPIC_NAME: orders 16 GROUP_ID: orders-group 17 REGISTRY_NAME: online-order 18 TZ: Australia/Sydney 19 volumes: 20 - ./consumer.py:/app/consumer.py 21 - ./requirements.txt:/app/requirements.txt 22 23networks: 24 kafkanet: 25 external: true 26 name: kafka-network Kafka Management App We should configure additional details in environment variables in order to integrate Glue Schema Registry. While both apps provide serializers/deserializers, kpow supports to manage schemas to some extent as well.\nFor kafka-ui, we can add one or more serialization plugins. I added the Glue registry serializer as a plugin and named it online-order. It requires the plugin binary file path, class name, registry name and AWS region name. Another key configuration values are the key and value schema templates values, which are used for finding schema names. Only the value schema template is updated as it is different from the default value. Note that the template values are applicable for producing messages on the UI. Therefore, we can leave them commented out if we don\u0026rsquo;t want to produce messages on it. Finally, the Glue registry serializer binary should be downloaded as it is volume-mapped in the compose file. It can be downloaded from the project repository - see download.sh.\nThe configuration of kpow is simpler as it only requires the registry ARN and AWS region. Note that the app fails to start if the registry doesn\u0026rsquo;t exit. I created the registry named online-order before starting it.\n1# /kafka-dev-with-docker/part-07/compose-ui.yml 2version: \u0026#34;3.5\u0026#34; 3 4services: 5 kafka-ui: 6 image: provectuslabs/kafka-ui:master 7 container_name: kafka-ui 8 ports: 9 - \u0026#34;8080:8080\u0026#34; 10 networks: 11 - kafkanet 12 environment: 13 AWS_ACCESS_KEY_ID: $AWS_ACCESS_KEY_ID 14 AWS_SECRET_ACCESS_KEY: $AWS_SECRET_ACCESS_KEY 15 AWS_SESSION_TOKEN: $AWS_SESSION_TOKEN 16 # kafka cluster 17 KAFKA_CLUSTERS_0_NAME: local 18 KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka-0:9092,kafka-1:9092,kafka-2:9092 19 KAFKA_CLUSTERS_0_ZOOKEEPER: zookeeper:2181 20 # glue schema registry serde 21 KAFKA_CLUSTERS_0_SERDE_0_NAME: online-order 22 KAFKA_CLUSTERS_0_SERDE_0_FILEPATH: /glue-serde/kafkaui-glue-serde-v1.0.3-jar-with-dependencies.jar 23 KAFKA_CLUSTERS_0_SERDE_0_CLASSNAME: com.provectus.kafka.ui.serdes.glue.GlueSerde 24 KAFKA_CLUSTERS_0_SERDE_0_PROPERTIES_REGION: $AWS_DEFAULT_REGION #required 25 KAFKA_CLUSTERS_0_SERDE_0_PROPERTIES_REGISTRY: online-order #required, name of Glue Schema Registry 26 # template that will be used to find schema name for topic key. Optional, default is null (not set). 27 # KAFKA_CLUSTERS_0_SERDE_0_PROPERTIES_KEYSCHEMANAMETEMPLATE: \u0026#34;%s-key\u0026#34; 28 # template that will be used to find schema name for topic value. Optional, default is \u0026#39;%s\u0026#39; 29 KAFKA_CLUSTERS_0_SERDE_0_PROPERTIES_VALUESCHEMANAMETEMPLATE: \u0026#34;%s-value\u0026#34; 30 volumes: 31 - ./kafkaui-glue-serde-v1.0.3-jar-with-dependencies.jar:/glue-serde/kafkaui-glue-serde-v1.0.3-jar-with-dependencies.jar 32 kpow: 33 image: factorhouse/kpow-ce:91.2.1 34 container_name: kpow 35 ports: 36 - \u0026#34;3000:3000\u0026#34; 37 networks: 38 - kafkanet 39 environment: 40 AWS_ACCESS_KEY_ID: $AWS_ACCESS_KEY_ID 41 AWS_SECRET_ACCESS_KEY: $AWS_SECRET_ACCESS_KEY 42 AWS_SESSION_TOKEN: $AWS_SESSION_TOKEN 43 # kafka cluster 44 BOOTSTRAP: kafka-0:9092,kafka-1:9092,kafka-2:9092 45 # glue schema registry 46 SCHEMA_REGISTRY_ARN: $SCHEMA_REGISTRY_ARN 47 SCHEMA_REGISTRY_REGION: $AWS_DEFAULT_REGION 48 49networks: 50 kafkanet: 51 external: true 52 name: kafka-network Start Applications We can run the Kafka cluster by docker-compose -f compose-kafka.yml up -d and the producer by python producer.py. As mentioned earlier, we\u0026rsquo;ll deploy 3 instances of the consumer, and they can be deployed with the scale option as shown below. We can check the running consumer instances using the ps command of Docker Compose.\n1$ docker-compose -f compose-consumer.yml up -d --scale app=3 2$ docker-compose -f compose-consumer.yml ps 3 Name Command State Ports 4----------------------------------------------------------------- 5part-07_app_1 sh -c pip install -r requi ... Up 8000/tcp 6part-07_app_2 sh -c pip install -r requi ... Up 8000/tcp 7part-07_app_3 sh -c pip install -r requi ... Up 8000/tcp Each instance of the consumer subscribes to its own topic partition, and we can check that in container logs. Below shows the last 10 log entries of one of the instances. It shows it polls messages from partition 0 only.\n1$ docker logs -f --tail 10 part-07_app_1 2 3INFO:root:key={\u0026#34;order_id\u0026#34;: \u0026#34;91db5fac-8e6b-46d5-a8e1-3df911fa9c60\u0026#34;}, value={\u0026#39;order_id\u0026#39;: \u0026#39;91db5fac-8e6b-46d5-a8e1-3df911fa9c60\u0026#39;, \u0026#39;ordered_at\u0026#39;: datetime.datetime(2023, 5, 22, 12, 30, 52, 459000, tzinfo=datetime.timezone.utc), \u0026#39;user_id\u0026#39;: \u0026#39;066\u0026#39;, \u0026#39;order_items\u0026#39;: [{\u0026#39;product_id\u0026#39;: 110, \u0026#39;quantity\u0026#39;: 7}, {\u0026#39;product_id\u0026#39;: 599, \u0026#39;quantity\u0026#39;: 4}, {\u0026#39;product_id\u0026#39;: 142, \u0026#39;quantity\u0026#39;: 3}, {\u0026#39;product_id\u0026#39;: 923, \u0026#39;quantity\u0026#39;: 7}]}, topic=orders, partition=0, offset=4886, ts=1684794652579 4INFO:root:key={\u0026#34;order_id\u0026#34;: \u0026#34;260c0ccf-29d1-4cef-88a9-7fc00618616e\u0026#34;}, value={\u0026#39;order_id\u0026#39;: \u0026#39;260c0ccf-29d1-4cef-88a9-7fc00618616e\u0026#39;, \u0026#39;ordered_at\u0026#39;: datetime.datetime(2023, 5, 22, 12, 30, 52, 459000, tzinfo=datetime.timezone.utc), \u0026#39;user_id\u0026#39;: \u0026#39;070\u0026#39;, \u0026#39;order_items\u0026#39;: [{\u0026#39;product_id\u0026#39;: 709, \u0026#39;quantity\u0026#39;: 6}, {\u0026#39;product_id\u0026#39;: 523, \u0026#39;quantity\u0026#39;: 4}, {\u0026#39;product_id\u0026#39;: 895, \u0026#39;quantity\u0026#39;: 4}, {\u0026#39;product_id\u0026#39;: 944, \u0026#39;quantity\u0026#39;: 2}]}, topic=orders, partition=0, offset=4887, ts=1684794652583 5INFO:root:key={\u0026#34;order_id\u0026#34;: \u0026#34;5ec8c9a1-5ad6-40f1-b0a5-09e7a060adb7\u0026#34;}, value={\u0026#39;order_id\u0026#39;: \u0026#39;5ec8c9a1-5ad6-40f1-b0a5-09e7a060adb7\u0026#39;, \u0026#39;ordered_at\u0026#39;: datetime.datetime(2023, 5, 22, 12, 30, 52, 459000, tzinfo=datetime.timezone.utc), \u0026#39;user_id\u0026#39;: \u0026#39;027\u0026#39;, \u0026#39;order_items\u0026#39;: [{\u0026#39;product_id\u0026#39;: 401, \u0026#39;quantity\u0026#39;: 7}]}, topic=orders, partition=0, offset=4888, ts=1684794652589 6INFO:root:key={\u0026#34;order_id\u0026#34;: \u0026#34;1d58572f-18d3-4181-9c35-5d179e1fc322\u0026#34;}, value={\u0026#39;order_id\u0026#39;: \u0026#39;1d58572f-18d3-4181-9c35-5d179e1fc322\u0026#39;, \u0026#39;ordered_at\u0026#39;: datetime.datetime(2023, 5, 22, 12, 30, 52, 460000, tzinfo=datetime.timezone.utc), \u0026#39;user_id\u0026#39;: \u0026#39;076\u0026#39;, \u0026#39;order_items\u0026#39;: [{\u0026#39;product_id\u0026#39;: 30, \u0026#39;quantity\u0026#39;: 4}, {\u0026#39;product_id\u0026#39;: 230, \u0026#39;quantity\u0026#39;: 3}, {\u0026#39;product_id\u0026#39;: 351, \u0026#39;quantity\u0026#39;: 7}]}, topic=orders, partition=0, offset=4889, ts=1684794652609 7INFO:root:key={\u0026#34;order_id\u0026#34;: \u0026#34;13fbd9c3-87e8-4d25-aec6-00c0a897e2f2\u0026#34;}, value={\u0026#39;order_id\u0026#39;: \u0026#39;13fbd9c3-87e8-4d25-aec6-00c0a897e2f2\u0026#39;, \u0026#39;ordered_at\u0026#39;: datetime.datetime(2023, 5, 22, 12, 30, 52, 461000, tzinfo=datetime.timezone.utc), \u0026#39;user_id\u0026#39;: \u0026#39;078\u0026#39;, \u0026#39;order_items\u0026#39;: [{\u0026#39;product_id\u0026#39;: 617, \u0026#39;quantity\u0026#39;: 6}]}, topic=orders, partition=0, offset=4890, ts=1684794652612 8INFO:root:key={\u0026#34;order_id\u0026#34;: \u0026#34;00328584-7db3-4cfb-a5b7-de2c7eed7f43\u0026#34;}, value={\u0026#39;order_id\u0026#39;: \u0026#39;00328584-7db3-4cfb-a5b7-de2c7eed7f43\u0026#39;, \u0026#39;ordered_at\u0026#39;: datetime.datetime(2023, 5, 22, 12, 30, 52, 461000, tzinfo=datetime.timezone.utc), \u0026#39;user_id\u0026#39;: \u0026#39;010\u0026#39;, \u0026#39;order_items\u0026#39;: [{\u0026#39;product_id\u0026#39;: 213, \u0026#39;quantity\u0026#39;: 5}, {\u0026#39;product_id\u0026#39;: 486, \u0026#39;quantity\u0026#39;: 3}]}, topic=orders, partition=0, offset=4891, ts=1684794652612 9INFO:root:key={\u0026#34;order_id\u0026#34;: \u0026#34;2b45ef3c-4061-4e24-ace9-a897878eb5a4\u0026#34;}, value={\u0026#39;order_id\u0026#39;: \u0026#39;2b45ef3c-4061-4e24-ace9-a897878eb5a4\u0026#39;, \u0026#39;ordered_at\u0026#39;: datetime.datetime(2023, 5, 22, 12, 30, 52, 461000, tzinfo=datetime.timezone.utc), \u0026#39;user_id\u0026#39;: \u0026#39;061\u0026#39;, \u0026#39;order_items\u0026#39;: [{\u0026#39;product_id\u0026#39;: 240, \u0026#39;quantity\u0026#39;: 5}, {\u0026#39;product_id\u0026#39;: 585, \u0026#39;quantity\u0026#39;: 5}, {\u0026#39;product_id\u0026#39;: 356, \u0026#39;quantity\u0026#39;: 9}, {\u0026#39;product_id\u0026#39;: 408, \u0026#39;quantity\u0026#39;: 2}]}, topic=orders, partition=0, offset=4892, ts=1684794652613 10INFO:root:key={\u0026#34;order_id\u0026#34;: \u0026#34;293f6008-b3c5-41b0-a37b-df90c04f8e0c\u0026#34;}, value={\u0026#39;order_id\u0026#39;: \u0026#39;293f6008-b3c5-41b0-a37b-df90c04f8e0c\u0026#39;, \u0026#39;ordered_at\u0026#39;: datetime.datetime(2023, 5, 22, 12, 30, 52, 461000, tzinfo=datetime.timezone.utc), \u0026#39;user_id\u0026#39;: \u0026#39;066\u0026#39;, \u0026#39;order_items\u0026#39;: [{\u0026#39;product_id\u0026#39;: 96, \u0026#39;quantity\u0026#39;: 5}, {\u0026#39;product_id\u0026#39;: 359, \u0026#39;quantity\u0026#39;: 2}, {\u0026#39;product_id\u0026#39;: 682, \u0026#39;quantity\u0026#39;: 9}]}, topic=orders, partition=0, offset=4893, ts=1684794652614 11INFO:root:key={\u0026#34;order_id\u0026#34;: \u0026#34;b09d03bf-9500-460c-a3cc-028aa4812b46\u0026#34;}, value={\u0026#39;order_id\u0026#39;: \u0026#39;b09d03bf-9500-460c-a3cc-028aa4812b46\u0026#39;, \u0026#39;ordered_at\u0026#39;: datetime.datetime(2023, 5, 22, 12, 30, 52, 463000, tzinfo=datetime.timezone.utc), \u0026#39;user_id\u0026#39;: \u0026#39;071\u0026#39;, \u0026#39;order_items\u0026#39;: [{\u0026#39;product_id\u0026#39;: 369, \u0026#39;quantity\u0026#39;: 6}, {\u0026#39;product_id\u0026#39;: 602, \u0026#39;quantity\u0026#39;: 6}, {\u0026#39;product_id\u0026#39;: 252, \u0026#39;quantity\u0026#39;: 10}, {\u0026#39;product_id\u0026#39;: 910, \u0026#39;quantity\u0026#39;: 7}]}, topic=orders, partition=0, offset=4894, ts=1684794652629 12INFO:root:key={\u0026#34;order_id\u0026#34;: \u0026#34;265c64a0-a520-494f-84d5-ebaf4496fe1c\u0026#34;}, value={\u0026#39;order_id\u0026#39;: \u0026#39;265c64a0-a520-494f-84d5-ebaf4496fe1c\u0026#39;, \u0026#39;ordered_at\u0026#39;: datetime.datetime(2023, 5, 22, 12, 30, 52, 463000, tzinfo=datetime.timezone.utc), \u0026#39;user_id\u0026#39;: \u0026#39;009\u0026#39;, \u0026#39;order_items\u0026#39;: [{\u0026#39;product_id\u0026#39;: 663, \u0026#39;quantity\u0026#39;: 5}, {\u0026#39;product_id\u0026#39;: 351, \u0026#39;quantity\u0026#39;: 4}, {\u0026#39;product_id\u0026#39;: 373, \u0026#39;quantity\u0026#39;: 5}]}, topic=orders, partition=0, offset=4895, ts=1684794652630 We can also check the consumers with the management apps. For example, the 3 running consumers can be seen in the Consumers menu of kafka-ui. As expected, each consumer subscribes to its own topic partition. We can run the management apps by docker-compose -f compose-ui.yml up -d.\nSchemas On AWS Console, we can check the schema of the value is created.\nAlso, we are able to see it on kpow. The community edition only supports a single schema registry and its name is marked as glue1.\nKafka Topics The orders topic can be found in the Topics menu of kafka-ui.\nWe can browse individual messages in the Messages tab. Note that we should select the Glue serializer plugin name (online-order) on the Value Serde drop down list. Otherwise, records won\u0026rsquo;t be deserialized correctly.\nWe can check the topic messages on kpow as well. If we select AVRO on the Value Deserializer drop down list, it requires to select the associating schema registry. We can select the pre-set schema registry name of glue1. Upon hitting the Search button, messages show up after being deserialized properly.\nSummary In Part 4, we developed Kafka producer and consumer applications using the kafka-python package without integrating schema registry. Later we discussed the benefits of schema registry when developing Kafka applications in Part 5. In this post, I demonstrated how to enhance the existing applications by integrating AWS Glue Schema Registry.\n","date":"June 22, 2023","img":"/blog/2023-06-22-kafka-development-with-docker-part-7/featured.png","lang":"en","langName":"English","largeImg":"/blog/2023-06-22-kafka-development-with-docker-part-7/featured_hu13c1b1b7b1d0fd49e8378a753b0e3b10_57175_500x0_resize_box_3.png","permalink":"/blog/2023-06-22-kafka-development-with-docker-part-7/","series":[{"title":"Kafka Development With Docker","url":"/series/kafka-development-with-docker/"}],"smallImg":"/blog/2023-06-22-kafka-development-with-docker-part-7/featured_hu13c1b1b7b1d0fd49e8378a753b0e3b10_57175_180x0_resize_box_3.png","tags":[{"title":"Apache Kafka","url":"/tags/apache-kafka/"},{"title":"Glue Schema Registry","url":"/tags/glue-schema-registry/"},{"title":"Python","url":"/tags/python/"},{"title":"Docker","url":"/tags/docker/"},{"title":"Docker Compose","url":"/tags/docker-compose/"}],"timestamp":1687392000,"title":"Kafka Development With Docker - Part 7 Producer and Consumer With Glue Schema Registry"},{"categories":[{"title":"Apache Kafka","url":"/categories/apache-kafka/"}],"content":"In Part 3, we developed a data ingestion pipeline with fake online order data using Kafka Connect source and sink connectors. Schemas are not enabled on both of them as there was not an integrated schema registry. Later we discussed how producers and consumers to Kafka topics can use schemas to ensure data consistency and compatibility as schemas evolve in Part 5. In this post, I\u0026rsquo;ll demonstrate how to enhance the existing data ingestion pipeline by integrating AWS Glue Schema Registry.\nPart 1 Cluster Setup Part 2 Management App Part 3 Kafka Connect Part 4 Producer and Consumer Part 5 Glue Schema Registry Part 6 Kafka Connect with Glue Schema Registry (this post) Part 7 Producer and Consumer with Glue Schema Registry Part 8 SSL Encryption Part 9 SSL Authentication Part 10 SASL Authentication Part 11 Kafka Authorization Kafka Connect Setup We can use the same Docker image because Kafka Connect is included in the Kafka distribution. The Kafka Connect server runs as a separate docker compose service, and its key configurations are listed below.\nWe\u0026rsquo;ll run it as the distributed mode, and it can be started by executing connect-distributed.sh on the Docker command. The startup script requires the properties file (connect-distributed.properties). It includes configurations such as Kafka broker server addresses - see below for details. The Connect server is accessible on port 8083, and we can manage connectors via a REST API as demonstrated below. The properties file, connector sources, and binary of Kafka Connect Avro converter are volume-mapped. AWS credentials are added to environment variables as the sink connector requires permission to write data into S3. The source can be found in the GitHub repository of this post.\n1# /kafka-dev-with-docker/part-06/compose-connect.yml 2version: \u0026#34;3.5\u0026#34; 3 4services: 5 kafka-connect: 6 image: bitnami/kafka:2.8.1 7 container_name: connect 8 command: \u0026gt; 9 /opt/bitnami/kafka/bin/connect-distributed.sh 10 /opt/bitnami/kafka/config/connect-distributed.properties 11 ports: 12 - \u0026#34;8083:8083\u0026#34; 13 networks: 14 - kafkanet 15 environment: 16 AWS_ACCESS_KEY_ID: $AWS_ACCESS_KEY_ID 17 AWS_SECRET_ACCESS_KEY: $AWS_SECRET_ACCESS_KEY 18 AWS_SESSION_TOKEN: $AWS_SESSION_TOKEN 19 volumes: 20 - \u0026#34;./configs/connect-distributed.properties:/opt/bitnami/kafka/config/connect-distributed.properties\u0026#34; 21 - \u0026#34;./connectors/confluent-s3/lib:/opt/connectors/confluent-s3\u0026#34; 22 - \u0026#34;./connectors/msk-datagen:/opt/connectors/msk-datagen\u0026#34; 23 - \u0026#34;./plugins/aws-glue-schema-registry-v.1.1.15/avro-kafkaconnect-converter/target:/opt/glue-schema-registry/avro\u0026#34; 24 25networks: 26 kafkanet: 27 external: true 28 name: kafka-network Connect Properties File The properties file includes configurations of the Connect server. Below shows key config values.\nBootstrap Server I changed the Kafka bootstrap server addresses. As it shares the same Docker network, we can take the service names (e.g. kafka-0) on port 9092. Cluster group id In distributed mode, multiple worker processes use the same group.id, and they automatically coordinate to schedule execution of connectors and tasks across all available workers. Converter-related properties Converters are necessary to have a Kafka Connect deployment support a particular data format when writing to or reading from Kafka. By default, org.apache.kafka.connect.json.JsonConverter is set for both the key and value converters and schemas are enabled for both of them. As shown later, these properties can be overridden when creating a connector. Topics for offsets, configs, status Several topics are created to manage connectors by multiple worker processes. Plugin path Paths that contains plugins (connectors, converters, transformations) can be set to a list of filesystem paths separated by commas (,) /opt/connectors is added and connector sources will be volume-mapped to it. /opt/glue-schema-registry is for the binary file of Kafka Connect Avro converter. 1# kafka-dev-with-docker/part-06/configs/connect-distributed.properties 2 3# A list of host/port pairs to use for establishing the initial connection to the Kafka cluster. 4bootstrap.servers=kafka-0:9092,kafka-1:9092,kafka-2:9092 5 6# unique name for the cluster, used in forming the Connect cluster group. Note that this must not conflict with consumer group IDs 7group.id=connect-cluster 8 9# The converters specify the format of data in Kafka and how to translate it into Connect data. Every Connect user will 10# need to configure these based on the format they want their data in when loaded from or stored into Kafka 11key.converter=org.apache.kafka.connect.json.JsonConverter 12value.converter=org.apache.kafka.connect.json.JsonConverter 13# Converter-specific settings can be passed in by prefixing the Converter\u0026#39;s setting with the converter we want to apply 14# it to 15key.converter.schemas.enable=true 16value.converter.schemas.enable=true 17 18# Topic to use for storing offsets. 19offset.storage.topic=connect-offsets 20offset.storage.replication.factor=1 21#offset.storage.partitions=25 22 23# Topic to use for storing connector and task configurations. 24config.storage.topic=connect-configs 25config.storage.replication.factor=1 26 27# Topic to use for storing statuses. 28status.storage.topic=connect-status 29status.storage.replication.factor=1 30#status.storage.partitions=5 31 32... 33 34# Set to a list of filesystem paths separated by commas (,) to enable class loading isolation for plugins 35# (connectors, converters, transformations). 36plugin.path=/opt/connectors,/opt/glue-schema-registry Download Connectors The connector sources need to be downloaded into the respective host paths (./connectors/confluent-s3 and ./connectors/msk-datagen) so that they are volume-mapped to the container\u0026rsquo;s plugin path (/opt/connectors). The following script downloads them into the host paths. Not that it also downloads the serde binary of kafka-ui, and it\u0026rsquo;ll be used separately for the kafka-ui service.\n1# /kafka-dev-with-docker/part-06/download.sh 2#!/usr/bin/env bash 3SCRIPT_DIR=\u0026#34;$(cd $(dirname \u0026#34;$0\u0026#34;); pwd)\u0026#34; 4 5SRC_PATH=${SCRIPT_DIR}/connectors 6rm -rf ${SRC_PATH} \u0026amp;\u0026amp; mkdir -p ${SRC_PATH}/msk-datagen 7 8## Confluent S3 Sink Connector 9echo \u0026#34;downloading confluent s3 connector...\u0026#34; 10DOWNLOAD_URL=https://d1i4a15mxbxib1.cloudfront.net/api/plugins/confluentinc/kafka-connect-s3/versions/10.4.3/confluentinc-kafka-connect-s3-10.4.3.zip 11 12curl -o ${SRC_PATH}/confluent.zip ${DOWNLOAD_URL} \\ 13 \u0026amp;\u0026amp; unzip -qq ${SRC_PATH}/confluent.zip -d ${SRC_PATH} \\ 14 \u0026amp;\u0026amp; rm ${SRC_PATH}/confluent.zip \\ 15 \u0026amp;\u0026amp; mv ${SRC_PATH}/$(ls ${SRC_PATH} | grep confluentinc-kafka-connect-s3) ${SRC_PATH}/confluent-s3 16 17## MSK Data Generator Souce Connector 18echo \u0026#34;downloading msk data generator...\u0026#34; 19DOWNLOAD_URL=https://github.com/awslabs/amazon-msk-data-generator/releases/download/v0.4.0/msk-data-generator-0.4-jar-with-dependencies.jar 20 21curl -L -o ${SRC_PATH}/msk-datagen/msk-data-generator.jar ${DOWNLOAD_URL} 22 23## Kafka UI Glue SERDE 24echo \u0026#34;downloading kafka ui glue serde...\u0026#34; 25DOWNLOAD_URL=https://github.com/provectus/kafkaui-glue-sr-serde/releases/download/v1.0.3/kafkaui-glue-serde-v1.0.3-jar-with-dependencies.jar 26 27curl -L -o ${SCRIPT_DIR}/kafkaui-glue-serde-v1.0.3-jar-with-dependencies.jar ${DOWNLOAD_URL} Below shows the folder structure after the connectors are downloaded successfully.\n1$ tree connectors/ -d 2connectors/ 3├── confluent-s3 4│ ├── assets 5│ ├── doc 6│ │ ├── licenses 7│ │ └── notices 8│ ├── etc 9│ └── lib 10└── msk-datagen Build Glue Schema Registry Client As demonstrated in Part 5, we need to build the Glue Schema Registry Client library as it provides serializers/deserializers and related functionalities. It can be built with the following script - see the previous post for details.\n1# /kafka-dev-with-docker/part-06/build.sh 2#!/usr/bin/env bash 3SCRIPT_DIR=\u0026#34;$(cd $(dirname \u0026#34;$0\u0026#34;); pwd)\u0026#34; 4 5SRC_PATH=${SCRIPT_DIR}/plugins 6rm -rf ${SRC_PATH} \u0026amp;\u0026amp; mkdir ${SRC_PATH} 7 8## Dwonload and build glue schema registry 9echo \u0026#34;downloading glue schema registry...\u0026#34; 10VERSION=v.1.1.15 11DOWNLOAD_URL=https://github.com/awslabs/aws-glue-schema-registry/archive/refs/tags/$VERSION.zip 12SOURCE_NAME=aws-glue-schema-registry-$VERSION 13 14curl -L -o ${SRC_PATH}/$SOURCE_NAME.zip ${DOWNLOAD_URL} \\ 15 \u0026amp;\u0026amp; unzip -qq ${SRC_PATH}/$SOURCE_NAME.zip -d ${SRC_PATH} \\ 16 \u0026amp;\u0026amp; rm ${SRC_PATH}/$SOURCE_NAME.zip 17 18echo \u0026#34;building glue schema registry...\u0026#34; 19cd plugins/$SOURCE_NAME/build-tools \\ 20 \u0026amp;\u0026amp; mvn clean install -DskipTests -Dcheckstyle.skip \\ 21 \u0026amp;\u0026amp; cd .. \\ 22 \u0026amp;\u0026amp; mvn clean install -DskipTests \\ 23 \u0026amp;\u0026amp; mvn dependency:copy-dependencies Once it is build successfully, we should be able to use the following binary files. We\u0026rsquo;ll only use the Avro converter in this post.\n1## kafka connect 2plugins/aws-glue-schema-registry-v.1.1.15/avro-kafkaconnect-converter/target/ 3├... 4├── schema-registry-kafkaconnect-converter-1.1.15.jar 5plugins/aws-glue-schema-registry-v.1.1.15/jsonschema-kafkaconnect-converter/target/ 6├... 7├── jsonschema-kafkaconnect-converter-1.1.15.jar 8plugins/aws-glue-schema-registry-v.1.1.15/protobuf-kafkaconnect-converter/target/ 9├... 10├── protobuf-kafkaconnect-converter-1.1.15.jar 11... Kafka Management App We should configure additional details in environment variables in order to integrate Glue Schema Registry. While both apps provide serializers/deserializers, kpow supports to manage schemas as well.\nFor kafka-ui, we can add one or more serialization plugins. I added the Glue registry serializer as a plugin and named it online-order*. It requires the plugin binary file path, class name, registry name and AWS region name. Another key configuration values are the key and value schema templates values, which are used for finding schema names. They are left unchanged because I will not enable schema for the key and the default template rule (%s) for the value matches the default naming convention of the client library. Note that those template properties are only applicable for message production on the UI, and we can leave them commented out if we don\u0026rsquo;t attempt that.\nThe configuration of kpow is simpler as it only requires the registry ARN and AWS region. Note that the app fails to start if the registry doesn\u0026rsquo;t exit. I created the registry named online-order before starting it.\n1# /kafka-dev-with-docker/part-06/compose-ui.yml 2version: \u0026#34;3.5\u0026#34; 3 4services: 5 kafka-ui: 6 image: provectuslabs/kafka-ui:master 7 container_name: kafka-ui 8 ports: 9 - \u0026#34;8080:8080\u0026#34; 10 networks: 11 - kafkanet 12 environment: 13 AWS_ACCESS_KEY_ID: $AWS_ACCESS_KEY_ID 14 AWS_SECRET_ACCESS_KEY: $AWS_SECRET_ACCESS_KEY 15 AWS_SESSION_TOKEN: $AWS_SESSION_TOKEN 16 # kafka cluster 17 KAFKA_CLUSTERS_0_NAME: local 18 KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka-0:9092,kafka-1:9092,kafka-2:9092 19 KAFKA_CLUSTERS_0_ZOOKEEPER: zookeeper:2181 20 # kafka connect 21 KAFKA_CLUSTERS_0_KAFKACONNECT_0_NAME: local 22 KAFKA_CLUSTERS_0_KAFKACONNECT_0_ADDRESS: http://kafka-connect:8083 23 # glue schema registry serde 24 KAFKA_CLUSTERS_0_SERDE_0_NAME: online-order 25 KAFKA_CLUSTERS_0_SERDE_0_FILEPATH: /glue-serde/kafkaui-glue-serde-v1.0.3-jar-with-dependencies.jar 26 KAFKA_CLUSTERS_0_SERDE_0_CLASSNAME: com.provectus.kafka.ui.serdes.glue.GlueSerde 27 KAFKA_CLUSTERS_0_SERDE_0_PROPERTIES_REGION: $AWS_DEFAULT_REGION #required 28 KAFKA_CLUSTERS_0_SERDE_0_PROPERTIES_REGISTRY: online-order #required, name of Glue Schema Registry 29 # template that will be used to find schema name for topic key. Optional, default is null (not set). 30 # KAFKA_CLUSTERS_0_SERDE_0_PROPERTIES_KEYSCHEMANAMETEMPLATE: \u0026#34;%s-key\u0026#34; 31 # template that will be used to find schema name for topic value. Optional, default is \u0026#39;%s\u0026#39; 32 # KAFKA_CLUSTERS_0_SERDE_0_PROPERTIES_VALUESCHEMANAMETEMPLATE: \u0026#34;%s-value\u0026#34; 33 volumes: 34 - ./kafkaui-glue-serde-v1.0.3-jar-with-dependencies.jar:/glue-serde/kafkaui-glue-serde-v1.0.3-jar-with-dependencies.jar 35 kpow: 36 image: factorhouse/kpow-ce:91.2.1 37 container_name: kpow 38 ports: 39 - \u0026#34;3000:3000\u0026#34; 40 networks: 41 - kafkanet 42 environment: 43 AWS_ACCESS_KEY_ID: $AWS_ACCESS_KEY_ID 44 AWS_SECRET_ACCESS_KEY: $AWS_SECRET_ACCESS_KEY 45 AWS_SESSION_TOKEN: $AWS_SESSION_TOKEN 46 # kafka cluster 47 BOOTSTRAP: kafka-0:9092,kafka-1:9092,kafka-2:9092 48 # glue schema registry 49 SCHEMA_REGISTRY_ARN: $SCHEMA_REGISTRY_ARN 50 SCHEMA_REGISTRY_REGION: $AWS_DEFAULT_REGION 51 # kafka connect 52 CONNECT_REST_URL: http://kafka-connect:8083 53 54networks: 55 kafkanet: 56 external: true 57 name: kafka-network Start Docker Compose Services There are 3 docker compose files for the Kafka cluster, Kafka Connect and management applications. We can run the whole services by starting them in order. The order matters as the Connect server relies on the Kafka cluster and kpow in compose-ui.yml fails if the Connect server is not up and running.\n1$ cd kafka-dev-with-docker/part-06 2# download connectors 3$ ./download.sh 4# build glue schema registry client library 5$ ./build.sh 6# starts 3 node kafka cluster 7$ docker-compose -f compose-kafka.yml up -d 8# starts kafka connect server in distributed mode 9$ docker-compose -f compose-connect.yml up -d 10# starts kafka-ui and kpow 11$ docker-compose -f compose-ui.yml up -d Source Connector Creation As mentioned earlier, Kafka Connect provides a REST API that manages connectors, and we can create a connector programmatically using it. The REST endpoint requires a JSON payload that includes connector configurations.\n1$ cd kafka-dev-with-docker/part-06 2$ curl -i -X POST -H \u0026#34;Accept:application/json\u0026#34; -H \u0026#34;Content-Type:application/json\u0026#34; \\ 3 http://localhost:8083/connectors/ -d @configs/source.json The connector class (connector.class) is required for any connector and I set it for the MSK Data Generator. Also, as many as two workers are allocated to the connector (tasks.max). As mentioned earlier, the converter-related properties are overridden. Specifically, the key converter is set to the string converter as the keys of both topics are set to be primitive values (genkp). Also, schema is not enabled for the key. On the other hand, schema is enabled for the value and the value converter is configured to the Avro converter of the Glue Schema Registry client library. The converter requires additional properties that cover AWS region, registry name, record type and flag to indicate whether to auto-generate schemas. Note that the generated schema name is the same to the topic name by default. Or we can configure a custom schema name by the schemaName property.\nThe remaining properties are specific to the source connectors. Basically it sends messages to two topics (customer and order). They are linked by the customer_id attribute of the order topic where the value is from the key of the customer topic. This is useful for practicing stream processing e.g. for joining two streams.\n1// kafka-dev-with-docker/part-06/configs/source.json 2{ 3 \u0026#34;name\u0026#34;: \u0026#34;order-source\u0026#34;, 4 \u0026#34;config\u0026#34;: { 5 \u0026#34;connector.class\u0026#34;: \u0026#34;com.amazonaws.mskdatagen.GeneratorSourceConnector\u0026#34;, 6 \u0026#34;tasks.max\u0026#34;: \u0026#34;2\u0026#34;, 7 \u0026#34;key.converter\u0026#34;: \u0026#34;org.apache.kafka.connect.storage.StringConverter\u0026#34;, 8 \u0026#34;key.converter.schemas.enable\u0026#34;: false, 9 \u0026#34;value.converter\u0026#34;: \u0026#34;com.amazonaws.services.schemaregistry.kafkaconnect.AWSKafkaAvroConverter\u0026#34;, 10 \u0026#34;value.converter.schemas.enable\u0026#34;: true, 11 \u0026#34;value.converter.region\u0026#34;: \u0026#34;ap-southeast-2\u0026#34;, 12 \u0026#34;value.converter.schemaAutoRegistrationEnabled\u0026#34;: true, 13 \u0026#34;value.converter.avroRecordType\u0026#34;: \u0026#34;GENERIC_RECORD\u0026#34;, 14 \u0026#34;value.converter.registry.name\u0026#34;: \u0026#34;online-order\u0026#34;, 15 16 \u0026#34;genkp.customer.with\u0026#34;: \u0026#34;#{Code.isbn10}\u0026#34;, 17 \u0026#34;genv.customer.name.with\u0026#34;: \u0026#34;#{Name.full_name}\u0026#34;, 18 19 \u0026#34;genkp.order.with\u0026#34;: \u0026#34;#{Internet.uuid}\u0026#34;, 20 \u0026#34;genv.order.product_id.with\u0026#34;: \u0026#34;#{number.number_between \u0026#39;101\u0026#39;,\u0026#39;109\u0026#39;}\u0026#34;, 21 \u0026#34;genv.order.quantity.with\u0026#34;: \u0026#34;#{number.number_between \u0026#39;1\u0026#39;,\u0026#39;5\u0026#39;}\u0026#34;, 22 \u0026#34;genv.order.customer_id.matching\u0026#34;: \u0026#34;customer.key\u0026#34;, 23 24 \u0026#34;global.throttle.ms\u0026#34;: \u0026#34;500\u0026#34;, 25 \u0026#34;global.history.records.max\u0026#34;: \u0026#34;1000\u0026#34; 26 } 27} Once created successfully, we can check the connector status as shown below.\n1$ curl http://localhost:8083/connectors/order-source/status 1{ 2\t\u0026#34;name\u0026#34;: \u0026#34;order-source\u0026#34;, 3\t\u0026#34;connector\u0026#34;: { 4\t\u0026#34;state\u0026#34;: \u0026#34;RUNNING\u0026#34;, 5\t\u0026#34;worker_id\u0026#34;: \u0026#34;172.19.0.6:8083\u0026#34; 6\t}, 7\t\u0026#34;tasks\u0026#34;: [ 8\t{ 9\t\u0026#34;id\u0026#34;: 0, 10\t\u0026#34;state\u0026#34;: \u0026#34;RUNNING\u0026#34;, 11\t\u0026#34;worker_id\u0026#34;: \u0026#34;172.19.0.6:8083\u0026#34; 12\t}, 13\t{ 14\t\u0026#34;id\u0026#34;: 1, 15\t\u0026#34;state\u0026#34;: \u0026#34;RUNNING\u0026#34;, 16\t\u0026#34;worker_id\u0026#34;: \u0026#34;172.19.0.6:8083\u0026#34; 17\t} 18\t], 19\t\u0026#34;type\u0026#34;: \u0026#34;source\u0026#34; 20} As we\u0026rsquo;ve added the connector URL, the Kafka Connect menu gets appeared on kafka-ui. We can check the details of the connector on the app as well.\nSchemas As we enabled auto-registration of schemas, the source connector generates two schemas. Below shows the schema for the order topic.\n1// kafka-dev-with-docker/part-06/configs/order.avsc 2{ 3 \u0026#34;type\u0026#34;: \u0026#34;record\u0026#34;, 4 \u0026#34;name\u0026#34;: \u0026#34;Gen0\u0026#34;, 5 \u0026#34;namespace\u0026#34;: \u0026#34;com.amazonaws.mskdatagen\u0026#34;, 6 \u0026#34;fields\u0026#34;: [ 7 { 8 \u0026#34;name\u0026#34;: \u0026#34;quantity\u0026#34;, 9 \u0026#34;type\u0026#34;: [\u0026#34;null\u0026#34;, \u0026#34;string\u0026#34;], 10 \u0026#34;default\u0026#34;: null 11 }, 12 { 13 \u0026#34;name\u0026#34;: \u0026#34;product_id\u0026#34;, 14 \u0026#34;type\u0026#34;: [\u0026#34;null\u0026#34;, \u0026#34;string\u0026#34;], 15 \u0026#34;default\u0026#34;: null 16 }, 17 { 18 \u0026#34;name\u0026#34;: \u0026#34;customer_id\u0026#34;, 19 \u0026#34;type\u0026#34;: [\u0026#34;null\u0026#34;, \u0026#34;string\u0026#34;], 20 \u0026#34;default\u0026#34;: null 21 } 22 ], 23 \u0026#34;connect.name\u0026#34;: \u0026#34;com.amazonaws.mskdatagen.Gen0\u0026#34; 24} On AWS Console, we can check the schemas of the two topics are created.\nAlso, we are able to see the schemas on kpow. The community edition only supports a single schema registry and its name is marked as glue1.\nKafka Topics As configured, the source connector ingests messages to the customer and order topics.\nWe can browse individual messages in the Messages tab. Note that we should select the Glue serializer plugin name (online-order) on the Value Serde drop down list. Otherwise, records won\u0026rsquo;t be deserialized correctly.\nWe can check the topic messages on kpow as well. If we select AVRO on the Value Deserializer drop down list, it requires to select the associating schema registry. We can select the pre-set schema registry name of glue1. Upon hitting the Search button, messages show up after being deserialized properly.\nSink Connector Creation Similar to the source connector, we can create the sink connector using the REST API.\n1$ cd kafka-dev-with-docker/part-06 2$ curl -i -X POST -H \u0026#34;Accept:application/json\u0026#34; -H \u0026#34;Content-Type:application/json\u0026#34; \\ 3 http://localhost:8083/connectors/ -d @configs/sink.json The connector is configured to write messages from both the topics (topics) into a S3 bucket (s3.bucket.name) where files are prefixed by the partition number (DefaultPartitioner). Also, it invokes file commits every 60 seconds (rotate.schedule.interval.ms) or the number of messages reach 100 (flush.size). Like the source connector, it overrides the converter-related properties.\n1// kafka-dev-with-docker/part-06/configs/sink.json 2{ 3 \u0026#34;name\u0026#34;: \u0026#34;order-sink\u0026#34;, 4 \u0026#34;config\u0026#34;: { 5 \u0026#34;connector.class\u0026#34;: \u0026#34;io.confluent.connect.s3.S3SinkConnector\u0026#34;, 6 \u0026#34;storage.class\u0026#34;: \u0026#34;io.confluent.connect.s3.storage.S3Storage\u0026#34;, 7 \u0026#34;format.class\u0026#34;: \u0026#34;io.confluent.connect.s3.format.json.JsonFormat\u0026#34;, 8 \u0026#34;tasks.max\u0026#34;: \u0026#34;2\u0026#34;, 9 \u0026#34;topics\u0026#34;: \u0026#34;order,customer\u0026#34;, 10 \u0026#34;s3.bucket.name\u0026#34;: \u0026#34;kafka-dev-ap-southeast-2\u0026#34;, 11 \u0026#34;s3.region\u0026#34;: \u0026#34;ap-southeast-2\u0026#34;, 12 \u0026#34;flush.size\u0026#34;: \u0026#34;100\u0026#34;, 13 \u0026#34;rotate.schedule.interval.ms\u0026#34;: \u0026#34;60000\u0026#34;, 14 \u0026#34;timezone\u0026#34;: \u0026#34;Australia/Sydney\u0026#34;, 15 \u0026#34;partitioner.class\u0026#34;: \u0026#34;io.confluent.connect.storage.partitioner.DefaultPartitioner\u0026#34;, 16 \u0026#34;key.converter\u0026#34;: \u0026#34;org.apache.kafka.connect.storage.StringConverter\u0026#34;, 17 \u0026#34;key.converter.schemas.enable\u0026#34;: false, 18 \u0026#34;value.converter\u0026#34;: \u0026#34;com.amazonaws.services.schemaregistry.kafkaconnect.AWSKafkaAvroConverter\u0026#34;, 19 \u0026#34;value.converter.schemas.enable\u0026#34;: true, 20 \u0026#34;value.converter.region\u0026#34;: \u0026#34;ap-southeast-2\u0026#34;, 21 \u0026#34;value.converter.avroRecordType\u0026#34;: \u0026#34;GENERIC_RECORD\u0026#34;, 22 \u0026#34;value.converter.registry.name\u0026#34;: \u0026#34;online-order\u0026#34;, 23 \u0026#34;errors.log.enable\u0026#34;: \u0026#34;true\u0026#34; 24 } 25} Below shows the sink connector details on kafka-ui.\nKafka Consumers The sink connector creates a Kafka consumer, and it is named as connect-order-sink. We see that it subscribes the two topics and is in the stable state. It has two members because it is configured to have as many as 2 tasks.\nS3 Destination The sink connector writes messages of the two topics (customer and order), and topic names are used as prefixes.\nAs mentioned, the default partitioner prefixes files further by the partition number, and it can be checked below.\nThe files are generated by \u0026lt;topic\u0026gt;+\u0026lt;partiton\u0026gt;+\u0026lt;start-offset\u0026gt;.json. The sink connector\u0026rsquo;s format class is set to io.confluent.connect.s3.format.json.JsonFormat so that it writes to Json files.\nSummary In Part 3, we developed a data ingestion pipeline using Kafka Connect source and sink connectors without enabling schemas. Later we discussed the benefits of schema registry when developing Kafka applications in Part 5. In this post, I demonstrated how to enhance the existing data ingestion pipeline by integrating AWS Glue Schema Registry.\n","date":"June 15, 2023","img":"/blog/2023-06-15-kafka-development-with-docker-part-6/featured.png","lang":"en","langName":"English","largeImg":"/blog/2023-06-15-kafka-development-with-docker-part-6/featured_hua377c0a3d87d017a3ba7a3b4569fee0e_60354_500x0_resize_box_3.png","permalink":"/blog/2023-06-15-kafka-development-with-docker-part-6/","series":[{"title":"Kafka Development With Docker","url":"/series/kafka-development-with-docker/"}],"smallImg":"/blog/2023-06-15-kafka-development-with-docker-part-6/featured_hua377c0a3d87d017a3ba7a3b4569fee0e_60354_180x0_resize_box_3.png","tags":[{"title":"Apache Kafka","url":"/tags/apache-kafka/"},{"title":"Kafka Connect","url":"/tags/kafka-connect/"},{"title":"Glue Schema Registry","url":"/tags/glue-schema-registry/"},{"title":"Docker","url":"/tags/docker/"},{"title":"Docker Compose","url":"/tags/docker-compose/"}],"timestamp":1686787200,"title":"Kafka Development With Docker - Part 6 Kafka Connect With Glue Schema Registry"},{"categories":[{"title":"Apache Kafka","url":"/categories/apache-kafka/"}],"content":"As described in the Confluent document, Schema Registry provides a centralized repository for managing and validating schemas for topic message data, and for serialization and deserialization of the data over the network. Producers and consumers to Kafka topics can use schemas to ensure data consistency and compatibility as schemas evolve. In AWS, the Glue Schema Registry supports features to manage and enforce schemas on data streaming applications using convenient integrations with Apache Kafka, Amazon Managed Streaming for Apache Kafka, Amazon Kinesis Data Streams, Amazon Kinesis Data Analytics for Apache Flink, and AWS Lambda.\nIn order to integrate the Glue Schema Registry with an application, we need to use the AWS Glue Schema Registry Client library, which primarily provides serializers and deserializers for Avro, Json and Portobuf formats. It also supports other necessary features such as registering schemas and performing compatibility check. As the project doesn\u0026rsquo;t provide pre-built binaries, we have to build them on our own. In this post, I\u0026rsquo;ll illustrate how to build the client library after introducing how it works to integrate the Glue Schema Registry with Kafka producer and consumer apps. Once built successfully, we can obtain multiple binaries not only for Kafka Connect but also other applications such as Flink for Kinesis Data Analytics. Therefore, this post can be considered as a stepping stone for later posts.\nPart 1 Cluster Setup Part 2 Management App Part 3 Kafka Connect Part 4 Producer and Consumer Part 5 Glue Schema Registry (this post) Part 6 Kafka Connect with Glue Schema Registry Part 7 Producer and Consumer with Glue Schema Registry Part 8 SSL Encryption Part 9 SSL Authentication Part 10 SASL Authentication Part 11 Kafka Authorization How It Works with Apache Kafka The below diagram shows how Kafka producer and consumer apps are integrated with the Glue Schema Registry. As Kafka producer and consumer apps are decoupled, they operate on Kafka topics rather than communicating with each other directly. Therefore, it is important to have a schema registry that manages/stores schemas and validates them.\nThe producer checks whether the schema that is used for serializing records is valid. Also, a new schema version is registered if it is yet to be done so. Note the schema registry preforms compatibility checks while registering a new schema version. If it turns out to be incompatible, registration fails and the producer fails to send messages. The producer serializes and compresses messages and sends them to the Kafka cluster. The consumer reads the serialized and compressed messages. The consumer retrieves the schema from the schema registry (if it is yet to be cached) and uses it to decompress and deserialize messages. Glue Schema Registry Client Library As mentioned earlier, the Glue Schema Registry Client library primarily provides serializers and deserializers for Avro, Json and Portobuf formats. It also supports other necessary features such as registering schemas and performing compatibility check. Below lists the main features of the library.\nMessages/records are serialized on producer front and deserialized on the consumer front by using schema-registry-serde. Support for three data formats: AVRO, JSON (with JSON Schema Draft04, Draft06, Draft07), and Protocol Buffers (Protobuf syntax versions 2 and 3). Kafka Streams support for AWS Glue Schema Registry. Records can be compressed to reduce message size. An inbuilt local in-memory cache to save calls to AWS Glue Schema Registry. The schema version id for a schema definition is cached on Producer side and schema for a schema version id is cached on the Consumer side. Auto registration of schema can be enabled for any new schema to be auto-registered. For Schemas, Evolution check is performed while registering. Migration from a third party Schema Registry. Flink support for AWS Glue Schema Registry. Kafka Connect support for AWS Glue Schema Registry. It can work with Apache Kafka as well as other AWS services. See this AWS documentation for its integration use cases listed below.\nConnecting Schema Registry to Amazon MSK or Apache Kafka Integrating Amazon Kinesis Data Streams with the AWS Glue Schema Registry Amazon Kinesis Data Analytics for Apache Flink Integration with AWS Lambda AWS Glue Data Catalog AWS Glue streaming Apache Kafka Streams Apache Kafka Connect Build Glue Schema Registry Client Library In order to build the client library, we need to have both the JDK and Maven installed. I use Ubuntu 18.04 on WSL2 and both the apps are downloaded from the Ubuntu package manager.\n1$ sudo apt update \u0026amp;\u0026amp; sudo apt install -y openjdk-11-jdk maven 2... 3$ mvn --version 4Apache Maven 3.6.3 5Maven home: /usr/share/maven 6Java version: 11.0.19, vendor: Ubuntu, runtime: /usr/lib/jvm/java-11-openjdk-amd64 7Default locale: en, platform encoding: UTF-8 8OS name: \u0026#34;linux\u0026#34;, version: \u0026#34;5.4.72-microsoft-standard-wsl2\u0026#34;, arch: \u0026#34;amd64\u0026#34;, family: \u0026#34;unix\u0026#34; We first need to download the source archive from the project repository. The latest version is v.1.1.15 at the time of writing this post, and it can be downloaded using curl with -L flag in order to follow the redirected download URL. Once downloaded, we can build the binaries as indicated in the project repository. The script shown below downloads and builds the client library. It can also be found in the GitHub repository of this post.\n1# kafka-dev-with-docker/part-05/build.sh 2#!/usr/bin/env bash 3SCRIPT_DIR=\u0026#34;$(cd $(dirname \u0026#34;$0\u0026#34;); pwd)\u0026#34; 4 5SRC_PATH=${SCRIPT_DIR}/plugins 6rm -rf ${SRC_PATH} \u0026amp;\u0026amp; mkdir ${SRC_PATH} 7 8## Dwonload and build glue schema registry 9echo \u0026#34;downloading glue schema registry...\u0026#34; 10VERSION=v.1.1.15 11DOWNLOAD_URL=https://github.com/awslabs/aws-glue-schema-registry/archive/refs/tags/$VERSION.zip 12SOURCE_NAME=aws-glue-schema-registry-$VERSION 13 14curl -L -o ${SRC_PATH}/$SOURCE_NAME.zip ${DOWNLOAD_URL} \\ 15 \u0026amp;\u0026amp; unzip -qq ${SRC_PATH}/$SOURCE_NAME.zip -d ${SRC_PATH} \\ 16 \u0026amp;\u0026amp; rm ${SRC_PATH}/$SOURCE_NAME.zip 17 18echo \u0026#34;building glue schema registry...\u0026#34; 19cd plugins/$SOURCE_NAME/build-tools \\ 20 \u0026amp;\u0026amp; mvn clean install -DskipTests -Dcheckstyle.skip \\ 21 \u0026amp;\u0026amp; cd .. \\ 22 \u0026amp;\u0026amp; mvn clean install -DskipTests \\ 23 \u0026amp;\u0026amp; mvn dependency:copy-dependencies Note that I skipped tests with the -DskipTests option in order to save build time. Note further that I also skipped checkstyle execution with the -Dcheckstyle.skip option as I encountered the following error.\n1[ERROR] Failed to execute goal org.apache.maven.plugins:maven-checkstyle-plugin:3.1.2:check (default) on project schema-registry-build-tools: Failed during checkstyle execution: Unable to find suppressions file at location: /tmp/kafka-pocs/kafka-dev-with-docker/part-05/plugins/aws-glue-schema-registry-v.1.1.15/build-tools/build-tools/src/main/resources/suppressions.xml: Could not find resource \u0026#39;/tmp/kafka-pocs/kafka-dev-with-docker/part-05/plugins/aws-glue-schema-registry-v.1.1.15/build-tools/build-tools/src/main/resources/suppressions.xml\u0026#39;. -\u0026gt; [Help 1] Once it was built successfully, I was able to see the following messages.\n1[INFO] ------------------------------------------------------------------------ 2[INFO] Reactor Summary for AWS Glue Schema Registry Library 1.1.15: 3[INFO] 4[INFO] AWS Glue Schema Registry Library ................... SUCCESS [ 0.644 s] 5[INFO] AWS Glue Schema Registry Build Tools ............... SUCCESS [ 0.038 s] 6[INFO] AWS Glue Schema Registry common .................... SUCCESS [ 0.432 s] 7[INFO] AWS Glue Schema Registry Serializer Deserializer ... SUCCESS [ 0.689 s] 8[INFO] AWS Glue Schema Registry Serializer Deserializer with MSK IAM Authentication client SUCCESS [ 0.216 s] 9[INFO] AWS Glue Schema Registry Kafka Streams SerDe ....... SUCCESS [ 0.173 s] 10[INFO] AWS Glue Schema Registry Kafka Connect AVRO Converter SUCCESS [ 0.190 s] 11[INFO] AWS Glue Schema Registry Flink Avro Serialization Deserialization Schema SUCCESS [ 0.541 s] 12[INFO] AWS Glue Schema Registry examples .................. SUCCESS [ 0.211 s] 13[INFO] AWS Glue Schema Registry Integration Tests ......... SUCCESS [ 0.648 s] 14[INFO] AWS Glue Schema Registry Kafka Connect JSONSchema Converter SUCCESS [ 0.239 s] 15[INFO] AWS Glue Schema Registry Kafka Connect Converter for Protobuf SUCCESS [ 0.296 s] 16[INFO] ------------------------------------------------------------------------ 17[INFO] BUILD SUCCESS 18[INFO] ------------------------------------------------------------------------ 19[INFO] Total time: 5.287 s 20[INFO] Finished at: 2023-05-19T08:08:51+10:00 21[INFO] ------------------------------------------------------------------------ As can be checked in the build messages, we can obtain binaries not only for Kafka Connect but also other applications such as Flink for Kinesis Data Analytics. Below shows all the available binaries.\n1## kafka connect 2plugins/aws-glue-schema-registry-v.1.1.15/avro-kafkaconnect-converter/target/ 3├... 4├── schema-registry-kafkaconnect-converter-1.1.15.jar 5plugins/aws-glue-schema-registry-v.1.1.15/jsonschema-kafkaconnect-converter/target/ 6├... 7├── jsonschema-kafkaconnect-converter-1.1.15.jar 8plugins/aws-glue-schema-registry-v.1.1.15/protobuf-kafkaconnect-converter/target/ 9├... 10├── protobuf-kafkaconnect-converter-1.1.15.jar 11## flink 12plugins/aws-glue-schema-registry-v.1.1.15/avro-flink-serde/target/ 13├... 14├── schema-registry-flink-serde-1.1.15.jar 15## kafka streams 16plugins/aws-glue-schema-registry-v.1.1.15/kafkastreams-serde/target/ 17├... 18├── schema-registry-kafkastreams-serde-1.1.15.jar 19## serializer/descrializer 20plugins/aws-glue-schema-registry-v.1.1.15/serializer-deserializer/target/ 21├... 22├── schema-registry-serde-1.1.15.jar 23plugins/aws-glue-schema-registry-v.1.1.15/serializer-deserializer-msk-iam/target/ 24├... 25├── schema-registry-serde-msk-iam-1.1.15.jar Summary The Glue Schema Registry supports features to manage and enforce schemas on data streaming applications using convenient integrations with Apache Kafka and other AWS managed services. In order to utilise those features, we need to use the client library. In this post, I illustrated how to build the client library after introducing how it works to integrate the Glue Schema Registry with Kafka producer and consumer apps.\n","date":"June 8, 2023","img":"/blog/2023-06-08-kafka-development-with-docker-part-5/featured.png","lang":"en","langName":"English","largeImg":"/blog/2023-06-08-kafka-development-with-docker-part-5/featured_hu599f963e3d9c99161bd2c8ee86db35a4_51170_500x0_resize_box_3.png","permalink":"/blog/2023-06-08-kafka-development-with-docker-part-5/","series":[{"title":"Kafka Development With Docker","url":"/series/kafka-development-with-docker/"}],"smallImg":"/blog/2023-06-08-kafka-development-with-docker-part-5/featured_hu599f963e3d9c99161bd2c8ee86db35a4_51170_180x0_resize_box_3.png","tags":[{"title":"Apache Kafka","url":"/tags/apache-kafka/"},{"title":"Glue Schema Registry","url":"/tags/glue-schema-registry/"}],"timestamp":1686182400,"title":"Kafka Development With Docker - Part 5 Glue Schema Registry"},{"categories":[{"title":"Data Streaming","url":"/categories/data-streaming/"}],"content":"In Part 1, we reviewed Kafka connectors focusing on AWS services integration. Among the available connectors, the suite of Apache Camel Kafka connectors and the Kinesis Kafka connector from the AWS Labs can be effective for building data ingestion pipelines on AWS. In this post, I will illustrate how to develop the Camel DynamoDB sink connector using Docker. Fake order data will be generated using the MSK Data Generator source connector, and the sink connector will be configured to consume the topic messages to ingest them into a DynamoDB table.\nPart 1 Introduction Part 2 Develop Camel DynamoDB Sink Connector (this post) Part 3 Deploy Camel DynamoDB Sink Connector Part 4 Develop Aiven OpenSearch Sink Connector Part 5 Deploy Aiven OpenSearch Sink Connector Kafka Cluster We will create a Kafka cluster with 3 brokers and 1 Zookeeper node using the bitnami/kafka image. Kafka 2.8.1 is used as it is the recommended Kafka version by Amazon MSK. The following Docker Compose file is used to create the Kafka cluster, and the source can also be found in the GitHub repository of this post. The resources created by the compose file are illustrated below.\nservices zookeeper A Zookeeper node is created with minimal configuration. It allows anonymous login. kafka-[id] Each broker has a unique ID (KAFKA_CFG_BROKER_ID) and shares the same Zookeeper connect parameter (KAFKA_CFG_ZOOKEEPER_CONNECT). These are required to connect to the Zookeeper node. Each has two listeners - INTERNAL and EXTERNAL. The former is accessed on port 9092, and it is used within the same Docker network. The latter is mapped from port 29092 to 29094, and it can be used to connect from outside the network. Each can be accessed without authentication (ALLOW_PLAINTEXT_LISTENER). The number of partitions (KAFKA_CFG_NUM_PARTITIONS) and default replica factor (KAFKA_CFG_DEFAULT_REPLICATION_FACTOR) are set to 3 respectively. networks A network named kafka-network is created and used by all services. Having a custom network can be beneficial when services are launched by multiple Docker Compose files. This custom network can be referred to by services in other compose files. volumes Each service has its own volume that will be mapped to the container\u0026rsquo;s data folder. We can check the contents of the folder in the Docker volume path. More importantly data is preserved in the Docker volume unless it is deleted so that we don\u0026rsquo;t have to recreate data every time the Kafka cluster gets started. 1# kafka-connect-for-aws/part-02/compose-kafka.yml 2version: \u0026#34;3.5\u0026#34; 3 4services: 5 zookeeper: 6 image: bitnami/zookeeper:3.5 7 container_name: zookeeper 8 ports: 9 - \u0026#34;2181\u0026#34; 10 networks: 11 - kafkanet 12 environment: 13 - ALLOW_ANONYMOUS_LOGIN=yes 14 volumes: 15 - zookeeper_data:/bitnami/zookeeper 16 kafka-0: 17 image: bitnami/kafka:2.8.1 18 container_name: kafka-0 19 expose: 20 - 9092 21 ports: 22 - \u0026#34;29092:29092\u0026#34; 23 networks: 24 - kafkanet 25 environment: 26 - ALLOW_PLAINTEXT_LISTENER=yes 27 - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181 28 - KAFKA_CFG_BROKER_ID=0 29 - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT 30 - KAFKA_CFG_LISTENERS=INTERNAL://:9092,EXTERNAL://:29092 31 - KAFKA_CFG_ADVERTISED_LISTENERS=INTERNAL://kafka-0:9092,EXTERNAL://localhost:29092 32 - KAFKA_CFG_INTER_BROKER_LISTENER_NAME=INTERNAL 33 - KAFKA_CFG_NUM_PARTITIONS=3 34 - KAFKA_CFG_DEFAULT_REPLICATION_FACTOR=3 35 volumes: 36 - kafka_0_data:/bitnami/kafka 37 depends_on: 38 - zookeeper 39 kafka-1: 40 image: bitnami/kafka:2.8.1 41 container_name: kafka-1 42 expose: 43 - 9092 44 ports: 45 - \u0026#34;29093:29093\u0026#34; 46 networks: 47 - kafkanet 48 environment: 49 - ALLOW_PLAINTEXT_LISTENER=yes 50 - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181 51 - KAFKA_CFG_BROKER_ID=1 52 - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT 53 - KAFKA_CFG_LISTENERS=INTERNAL://:9092,EXTERNAL://:29093 54 - KAFKA_CFG_ADVERTISED_LISTENERS=INTERNAL://kafka-1:9092,EXTERNAL://localhost:29093 55 - KAFKA_CFG_INTER_BROKER_LISTENER_NAME=INTERNAL 56 - KAFKA_CFG_NUM_PARTITIONS=3 57 - KAFKA_CFG_DEFAULT_REPLICATION_FACTOR=3 58 volumes: 59 - kafka_1_data:/bitnami/kafka 60 depends_on: 61 - zookeeper 62 kafka-2: 63 image: bitnami/kafka:2.8.1 64 container_name: kafka-2 65 expose: 66 - 9092 67 ports: 68 - \u0026#34;29094:29094\u0026#34; 69 networks: 70 - kafkanet 71 environment: 72 - ALLOW_PLAINTEXT_LISTENER=yes 73 - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181 74 - KAFKA_CFG_BROKER_ID=2 75 - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT 76 - KAFKA_CFG_LISTENERS=INTERNAL://:9092,EXTERNAL://:29094 77 - KAFKA_CFG_ADVERTISED_LISTENERS=INTERNAL://kafka-2:9092,EXTERNAL://localhost:29094 78 - KAFKA_CFG_INTER_BROKER_LISTENER_NAME=INTERNAL 79 - KAFKA_CFG_NUM_PARTITIONS=3 80 - KAFKA_CFG_DEFAULT_REPLICATION_FACTOR=3 81 volumes: 82 - kafka_2_data:/bitnami/kafka 83 depends_on: 84 - zookeeper 85 86networks: 87 kafkanet: 88 name: kafka-network 89 90volumes: 91 zookeeper_data: 92 driver: local 93 name: zookeeper_data 94 kafka_0_data: 95 driver: local 96 name: kafka_0_data 97 kafka_1_data: 98 driver: local 99 name: kafka_1_data 100 kafka_2_data: 101 driver: local 102 name: kafka_2_data Kafka Connect We can use the same Docker image because Kafka Connect is included in the Kafka distribution. The Kafka Connect server runs as a separate docker compose service, and its key configurations are listed below.\nWe run it as the distributed mode, and it can be started by executing connect-distributed.sh on the Docker command. The startup script requires the properties file (connect-distributed.properties). It includes configurations such as Kafka broker server addresses - see below for details. The Connect server is accessible on port 8083, and we can manage connectors via a REST API as demonstrated below. The properties file and connector sources are volume-mapped. AWS credentials are added to environment variables as the sink connector requires permission to write data into DynamoDB. 1# kafka-connect-for-aws/part-02/compose-connect.yml 2version: \u0026#34;3.5\u0026#34; 3 4services: 5 kafka-connect: 6 image: bitnami/kafka:2.8.1 7 container_name: connect 8 command: \u0026gt; 9 /opt/bitnami/kafka/bin/connect-distributed.sh 10 /opt/bitnami/kafka/config/connect-distributed.properties 11 ports: 12 - \u0026#34;8083:8083\u0026#34; 13 networks: 14 - kafkanet 15 environment: 16 AWS_ACCESS_KEY_ID: $AWS_ACCESS_KEY_ID 17 AWS_SECRET_ACCESS_KEY: $AWS_SECRET_ACCESS_KEY 18 AWS_SESSION_TOKEN: $AWS_SESSION_TOKEN 19 volumes: 20 - \u0026#34;./configs/connect-distributed.properties:/opt/bitnami/kafka/config/connect-distributed.properties\u0026#34; 21 - \u0026#34;./connectors/msk-data-generator.jar:/opt/connectors/datagen/msk-data-generator.jar\u0026#34; 22 - \u0026#34;./connectors/camel-aws-ddb-sink-kafka-connector:/opt/connectors/camel-aws-ddb-sink-kafka-connector\u0026#34; 23 24networks: 25 kafkanet: 26 external: true 27 name: kafka-network Connect Properties File The properties file includes configurations of the Connect server. Below shows key config values.\nBootstrap Server I changed the Kafka bootstrap server addresses. As it shares the same Docker network, we can take the service names (e.g. kafka-0) on port 9092. Cluster group id In distributed mode, multiple worker processes use the same group.id, and they automatically coordinate to schedule execution of connectors and tasks across all available workers. Converter-related properties Converters are necessary to have a Kafka Connect deployment support a particular data format when writing to or reading from Kafka. By default, org.apache.kafka.connect.json.JsonConverter is set for both the key and value converters and schemas are enabled for both of them. As shown later, these properties can be overridden when creating a connector. Topics for offsets, configs, status Several topics are created to manage connectors by multiple worker processes. Plugin path Paths that contains plugins (connectors, converters, transformations) can be set to a list of filesystem paths separated by commas (,) /opt/connectors is added and connector sources will be volume-mapped to it. 1## kafka-connect-for-aws/part-02/connect-distributed.properties 2# A list of host/port pairs to use for establishing the initial connection to the Kafka cluster. 3bootstrap.servers=kafka-0:9092,kafka-1:9092,kafka-2:9092 4 5# unique name for the cluster, used in forming the Connect cluster group. Note that this must not conflict with consumer group IDs 6group.id=connect-cluster 7 8# The converters specify the format of data in Kafka and how to translate it into Connect data. Every Connect user will 9# need to configure these based on the format they want their data in when loaded from or stored into Kafka 10key.converter=org.apache.kafka.connect.json.JsonConverter 11value.converter=org.apache.kafka.connect.json.JsonConverter 12# Converter-specific settings can be passed in by prefixing the Converter\u0026#39;s setting with the converter we want to apply 13# it to 14key.converter.schemas.enable=true 15value.converter.schemas.enable=true 16 17# Topic to use for storing offsets. 18offset.storage.topic=connect-offsets 19offset.storage.replication.factor=1 20#offset.storage.partitions=25 21 22# Topic to use for storing connector and task configurations. 23config.storage.topic=connect-configs 24config.storage.replication.factor=1 25 26# Topic to use for storing statuses. 27status.storage.topic=connect-status 28status.storage.replication.factor=1 29#status.storage.partitions=5 30 31... 32 33# Set to a list of filesystem paths separated by commas (,) to enable class loading isolation for plugins 34# (connectors, converters, transformations). 35plugin.path=/opt/connectors Download Connectors The connector sources need to be downloaded into the ./connectors path so that they can be volume-mapped to the container\u0026rsquo;s plugin path (/opt/connectors). The MSK Data Generator is a single Jar file, and it can be kept as it is. On the other hand, the Camel DynamoDB sink connector is an archive file, and it should be decompressed. Note a separate zip file is made as well, and it will be used to create a custom plugin of MSK Connect in a later post. The following script downloads them into the host path.\n1# kafka-connect-for-aws/part-02/download.sh 2#!/usr/bin/env bash 3SCRIPT_DIR=\u0026#34;$(cd $(dirname \u0026#34;$0\u0026#34;); pwd)\u0026#34; 4 5SRC_PATH=${SCRIPT_DIR}/connectors 6rm -rf ${SRC_PATH} \u0026amp;\u0026amp; mkdir ${SRC_PATH} 7 8## MSK Data Generator Souce Connector 9echo \u0026#34;downloading msk data generator...\u0026#34; 10DOWNLOAD_URL=https://github.com/awslabs/amazon-msk-data-generator/releases/download/v0.4.0/msk-data-generator-0.4-jar-with-dependencies.jar 11 12curl -L -o ${SRC_PATH}/msk-data-generator.jar ${DOWNLOAD_URL} 13 14## Download camel dynamodb sink connector 15echo \u0026#34;download camel dynamodb sink connector...\u0026#34; 16DOWNLOAD_URL=https://repo.maven.apache.org/maven2/org/apache/camel/kafkaconnector/camel-aws-ddb-sink-kafka-connector/3.20.3/camel-aws-ddb-sink-kafka-connector-3.20.3-package.tar.gz 17 18# decompress and zip contents to create custom plugin of msk connect later 19curl -o ${SRC_PATH}/camel-aws-ddb-sink-kafka-connector.tar.gz ${DOWNLOAD_URL} \\ 20 \u0026amp;\u0026amp; tar -xvzf ${SRC_PATH}/camel-aws-ddb-sink-kafka-connector.tar.gz -C ${SRC_PATH} \\ 21 \u0026amp;\u0026amp; cd ${SRC_PATH}/camel-aws-ddb-sink-kafka-connector \\ 22 \u0026amp;\u0026amp; zip -r camel-aws-ddb-sink-kafka-connector.zip . \\ 23 \u0026amp;\u0026amp; mv camel-aws-ddb-sink-kafka-connector.zip ${SRC_PATH} \\ 24 \u0026amp;\u0026amp; rm ${SRC_PATH}/camel-aws-ddb-sink-kafka-connector.tar.gz Below shows the folder structure after the connectors are downloaded successfully.\n1connectors/ 2├── camel-aws-ddb-sink-kafka-connector 3... 4│ ├── camel-api-3.20.3.jar 5│ ├── camel-aws-ddb-sink-kafka-connector-3.20.3.jar ** 6│ ├── camel-aws2-ddb-3.20.3.jar 7... 8├── camel-aws-ddb-sink-kafka-connector.zip ** 9... 10└── msk-data-generator.jar ** 11 123 directories, 128 files Kafka Management App A Kafka management app can be a good companion for development as it helps monitor and manage resources on an easy-to-use user interface. We\u0026rsquo;ll use kafka-ui in this post. It provides a docker image, and we can link one or more Kafka clusters and related resources to it. In the following compose file, we added connection details of the Kafka cluster and Kafka Connect server.\n1# kafka-connect-for-aws/part-02/compose-ui.yml 2version: \u0026#34;3.5\u0026#34; 3 4services: 5 kafka-ui: 6 image: provectuslabs/kafka-ui:master 7 container_name: kafka-ui 8 ports: 9 - \u0026#34;8080:8080\u0026#34; 10 networks: 11 - kafkanet 12 environment: 13 KAFKA_CLUSTERS_0_NAME: local 14 KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka-0:9092,kafka-1:9092,kafka-2:9092 15 KAFKA_CLUSTERS_0_ZOOKEEPER: zookeeper:2181 16 KAFKA_CLUSTERS_0_KAFKACONNECT_0_NAME: local 17 KAFKA_CLUSTERS_0_KAFKACONNECT_0_ADDRESS: http://kafka-connect:8083 18 19networks: 20 kafkanet: 21 external: true 22 name: kafka-network Start Services There are 3 docker compose files for the Kafka cluster, Kafka Connect and management application. We can run the whole services by starting them in order as illustrated below.\n1$ cd kafka-connect-for-aws/part-02 2# download connectors 3$ ./download.sh 4# starts 3 node kafka cluster 5$ docker-compose -f compose-kafka.yml up -d 6# starts kafka connect server in distributed mode 7$ docker-compose -f compose-connect.yml up -d 8# starts kafka-ui 9$ docker-compose -f compose-ui.yml up -d Data Ingestion to Kafka Topic Source Connector Creation As mentioned earlier, Kafka Connect provides a REST API that manages connectors, and we can create a connector programmatically using it. The REST endpoint requires a JSON payload that includes connector configurations.\n1$ cd kafka-connect-for-aws/part-02 2$ curl -i -X POST -H \u0026#34;Accept:application/json\u0026#34; -H \u0026#34;Content-Type:application/json\u0026#34; \\ 3 http://localhost:8083/connectors/ -d @configs/source.json The connector class (connector.class) is required for any connector and I set it for the MSK Data Generator. Also, a single worker is allocated to the connector (tasks.max). As mentioned earlier, the converter-related properties are overridden. Specifically, the key converter is set to the string converter as the key of the topic is set to be primitive values (genkp). Also, schemas are not enabled for both the key and value.\nThose properties in the middle are specific to the source connector. Basically it sends messages to a topic named order. The key is marked as to-replace as it will be replaced with the order_id attribute of the value - see below. The value has order_id, product_id, quantity, customer_id and customer_name attributes, and they are generated by the Java faker library.\nIt can be easier to manage messages if the same order ID is shared with the key and value. We can achieve it using single message transforms (SMTs). Specifically I used two transforms - ValueToKey and ExtractField to achieve it. As the name suggests, the former copies the order_id value into the key. The latter is used additionally because the key is set to have primitive string values. Finally, the last transform (Cast) is to change the quantity value into integer.\n1// kafka-connect-for-aws/part-02/configs/source.json 2{ 3 \u0026#34;name\u0026#34;: \u0026#34;order-source\u0026#34;, 4 \u0026#34;config\u0026#34;: { 5 \u0026#34;connector.class\u0026#34;: \u0026#34;com.amazonaws.mskdatagen.GeneratorSourceConnector\u0026#34;, 6 \u0026#34;tasks.max\u0026#34;: \u0026#34;1\u0026#34;, 7 \u0026#34;key.converter\u0026#34;: \u0026#34;org.apache.kafka.connect.storage.StringConverter\u0026#34;, 8 \u0026#34;key.converter.schemas.enable\u0026#34;: false, 9 \u0026#34;value.converter\u0026#34;: \u0026#34;org.apache.kafka.connect.json.JsonConverter\u0026#34;, 10 \u0026#34;value.converter.schemas.enable\u0026#34;: false, 11 12 \u0026#34;genkp.order.with\u0026#34;: \u0026#34;to-replace\u0026#34;, 13 \u0026#34;genv.order.order_id.with\u0026#34;: \u0026#34;#{Internet.uuid}\u0026#34;, 14 \u0026#34;genv.order.product_id.with\u0026#34;: \u0026#34;#{Code.isbn10}\u0026#34;, 15 \u0026#34;genv.order.quantity.with\u0026#34;: \u0026#34;#{number.number_between \u0026#39;1\u0026#39;,\u0026#39;5\u0026#39;}\u0026#34;, 16 \u0026#34;genv.order.customer_id.with\u0026#34;: \u0026#34;#{number.number_between \u0026#39;100\u0026#39;,\u0026#39;199\u0026#39;}\u0026#34;, 17 \u0026#34;genv.order.customer_name.with\u0026#34;: \u0026#34;#{Name.full_name}\u0026#34;, 18 \u0026#34;global.throttle.ms\u0026#34;: \u0026#34;500\u0026#34;, 19 \u0026#34;global.history.records.max\u0026#34;: \u0026#34;1000\u0026#34;, 20 21 \u0026#34;transforms\u0026#34;: \u0026#34;copyIdToKey,extractKeyFromStruct,cast\u0026#34;, 22 \u0026#34;transforms.copyIdToKey.type\u0026#34;: \u0026#34;org.apache.kafka.connect.transforms.ValueToKey\u0026#34;, 23 \u0026#34;transforms.copyIdToKey.fields\u0026#34;: \u0026#34;order_id\u0026#34;, 24 \u0026#34;transforms.extractKeyFromStruct.type\u0026#34;: \u0026#34;org.apache.kafka.connect.transforms.ExtractField$Key\u0026#34;, 25 \u0026#34;transforms.extractKeyFromStruct.field\u0026#34;: \u0026#34;order_id\u0026#34;, 26 \u0026#34;transforms.cast.type\u0026#34;: \u0026#34;org.apache.kafka.connect.transforms.Cast$Value\u0026#34;, 27 \u0026#34;transforms.cast.spec\u0026#34;: \u0026#34;quantity:int8\u0026#34; 28 } 29} Once created successfully, we can check the connector status as shown below.\n1$ curl http://localhost:8083/connectors/order-source/status 1{ 2\t\u0026#34;name\u0026#34;: \u0026#34;order-source\u0026#34;, 3\t\u0026#34;connector\u0026#34;: { 4\t\u0026#34;state\u0026#34;: \u0026#34;RUNNING\u0026#34;, 5\t\u0026#34;worker_id\u0026#34;: \u0026#34;172.19.0.6:8083\u0026#34; 6\t}, 7\t\u0026#34;tasks\u0026#34;: [ 8\t{ 9\t\u0026#34;id\u0026#34;: 0, 10\t\u0026#34;state\u0026#34;: \u0026#34;RUNNING\u0026#34;, 11\t\u0026#34;worker_id\u0026#34;: \u0026#34;172.19.0.6:8083\u0026#34; 12\t} 13\t], 14\t\u0026#34;type\u0026#34;: \u0026#34;source\u0026#34; 15} As we\u0026rsquo;ve added the connector URL, the Kafka Connect menu appears on kafka-ui. We can check the details of the connector on the app as well.\nKafka Topics As configured, the source connector ingests messages to the order topic.\nWe can browse individual messages in the Messages tab of the topic.\nData Ingestion to DynamoDB Table Creation The destination table is named orders, and it has the primary key where order_id and ordered_at are the hash and range key respectively. It also has a global secondary index where customer_id and ordered_at constitute the primary key. Note that ordered_at is not generated by the source connector as the Java faker library doesn\u0026rsquo;t have a method to generate a current timestamp. As illustrated below it\u0026rsquo;ll be created by the sink connector using SMTs. The table can be created using the AWS CLI as shown below.\n1aws dynamodb create-table \\ 2 --cli-input-json file://configs/ddb.json 1// kafka-connect-for-aws/part-02/configs/ddb.json 2{ 3 \u0026#34;TableName\u0026#34;: \u0026#34;orders\u0026#34;, 4 \u0026#34;KeySchema\u0026#34;: [ 5 { \u0026#34;AttributeName\u0026#34;: \u0026#34;order_id\u0026#34;, \u0026#34;KeyType\u0026#34;: \u0026#34;HASH\u0026#34; }, 6 { \u0026#34;AttributeName\u0026#34;: \u0026#34;ordered_at\u0026#34;, \u0026#34;KeyType\u0026#34;: \u0026#34;RANGE\u0026#34; } 7 ], 8 \u0026#34;AttributeDefinitions\u0026#34;: [ 9 { \u0026#34;AttributeName\u0026#34;: \u0026#34;order_id\u0026#34;, \u0026#34;AttributeType\u0026#34;: \u0026#34;S\u0026#34; }, 10 { \u0026#34;AttributeName\u0026#34;: \u0026#34;customer_id\u0026#34;, \u0026#34;AttributeType\u0026#34;: \u0026#34;S\u0026#34; }, 11 { \u0026#34;AttributeName\u0026#34;: \u0026#34;ordered_at\u0026#34;, \u0026#34;AttributeType\u0026#34;: \u0026#34;S\u0026#34; } 12 ], 13 \u0026#34;ProvisionedThroughput\u0026#34;: { 14 \u0026#34;ReadCapacityUnits\u0026#34;: 1, 15 \u0026#34;WriteCapacityUnits\u0026#34;: 1 16 }, 17 \u0026#34;GlobalSecondaryIndexes\u0026#34;: [ 18 { 19 \u0026#34;IndexName\u0026#34;: \u0026#34;customer\u0026#34;, 20 \u0026#34;KeySchema\u0026#34;: [ 21 { \u0026#34;AttributeName\u0026#34;: \u0026#34;customer_id\u0026#34;, \u0026#34;KeyType\u0026#34;: \u0026#34;HASH\u0026#34; }, 22 { \u0026#34;AttributeName\u0026#34;: \u0026#34;ordered_at\u0026#34;, \u0026#34;KeyType\u0026#34;: \u0026#34;RANGE\u0026#34; } 23 ], 24 \u0026#34;Projection\u0026#34;: { \u0026#34;ProjectionType\u0026#34;: \u0026#34;ALL\u0026#34; }, 25 \u0026#34;ProvisionedThroughput\u0026#34;: { 26 \u0026#34;ReadCapacityUnits\u0026#34;: 1, 27 \u0026#34;WriteCapacityUnits\u0026#34;: 1 28 } 29 } 30 ] 31} Sink Connector Creation Similar to the source connector, we can create the sink connector using the REST API.\n1$ cd kafka-connect-for-aws/part-02 2$ curl -i -X POST -H \u0026#34;Accept:application/json\u0026#34; -H \u0026#34;Content-Type:application/json\u0026#34; \\ 3 http://localhost:8083/connectors/ -d @configs/sink.json The connector is configured to write messages from the order topic into the DynamoDB table created earlier. It requires to specify the table name, AWS region, operation, write capacity and whether to use the default credential provider - see the documentation for details. Note that, if you don\u0026rsquo;t use the default credential provider, you have to specify the access key id and secret access key. Note further that, although the current LTS version is v3.18.2, the default credential provider option didn\u0026rsquo;t work for me, and I was recommended to use v3.20.3 instead. Finally, the camel.sink.unmarshal option is to convert data from the internal java.util.HashMap type into the required java.io.InputStream type. Without this configuration, the connector fails with org.apache.camel.NoTypeConversionAvailableException error.\nAlthough the destination table has ordered_at as the range key, it is not created by the source connector because the Java faker library doesn\u0026rsquo;t have a method to generate a current timestamp. Therefore, it is created by the sink connector using two SMTs - InsertField and TimestampConverter. Specifically they add a timestamp value to the order_at attribute, format the value as yyyy-MM-dd HH:mm:ss:SSS, and convert its type into string.\n1// kafka-connect-for-aws/part-02/configs/sink.json 2{ 3 \u0026#34;name\u0026#34;: \u0026#34;order-sink\u0026#34;, 4 \u0026#34;config\u0026#34;: { 5 \u0026#34;connector.class\u0026#34;: \u0026#34;org.apache.camel.kafkaconnector.awsddbsink.CamelAwsddbsinkSinkConnector\u0026#34;, 6 \u0026#34;tasks.max\u0026#34;: \u0026#34;1\u0026#34;, 7 \u0026#34;key.converter\u0026#34;: \u0026#34;org.apache.kafka.connect.storage.StringConverter\u0026#34;, 8 \u0026#34;key.converter.schemas.enable\u0026#34;: false, 9 \u0026#34;value.converter\u0026#34;: \u0026#34;org.apache.kafka.connect.json.JsonConverter\u0026#34;, 10 \u0026#34;value.converter.schemas.enable\u0026#34;: false, 11 \u0026#34;topics\u0026#34;: \u0026#34;order\u0026#34;, 12 13 \u0026#34;camel.kamelet.aws-ddb-sink.table\u0026#34;: \u0026#34;orders\u0026#34;, 14 \u0026#34;camel.kamelet.aws-ddb-sink.region\u0026#34;: \u0026#34;ap-southeast-2\u0026#34;, 15 \u0026#34;camel.kamelet.aws-ddb-sink.operation\u0026#34;: \u0026#34;PutItem\u0026#34;, 16 \u0026#34;camel.kamelet.aws-ddb-sink.writeCapacity\u0026#34;: 1, 17 \u0026#34;camel.kamelet.aws-ddb-sink.useDefaultCredentialsProvider\u0026#34;: true, 18 \u0026#34;camel.sink.unmarshal\u0026#34;: \u0026#34;jackson\u0026#34;, 19 20 \u0026#34;transforms\u0026#34;: \u0026#34;insertTS,formatTS\u0026#34;, 21 \u0026#34;transforms.insertTS.type\u0026#34;: \u0026#34;org.apache.kafka.connect.transforms.InsertField$Value\u0026#34;, 22 \u0026#34;transforms.insertTS.timestamp.field\u0026#34;: \u0026#34;ordered_at\u0026#34;, 23 \u0026#34;transforms.formatTS.type\u0026#34;: \u0026#34;org.apache.kafka.connect.transforms.TimestampConverter$Value\u0026#34;, 24 \u0026#34;transforms.formatTS.format\u0026#34;: \u0026#34;yyyy-MM-dd HH:mm:ss:SSS\u0026#34;, 25 \u0026#34;transforms.formatTS.field\u0026#34;: \u0026#34;ordered_at\u0026#34;, 26 \u0026#34;transforms.formatTS.target.type\u0026#34;: \u0026#34;string\u0026#34; 27 } 28} Below shows the sink connector details on kafka-ui.\nDynamoDB Destination We can check the ingested records on the DynamoDB table items view. Below shows a list of scanned records. As expected, it has the order_id, ordered_at and other attributes.\nWe can also obtain an individual Json record by clicking an order_id value as shown below.\nSummary The suite of Apache Camel Kafka connectors and the Kinesis Kafka connector from the AWS Labs can be effective for building data ingestion pipelines that integrate AWS services. In this post, I illustrated how to develop the Camel DynamoDB sink connector using Docker. Fake order data was generated using the MSK Data Generator source connector, and the sink connector was configured to consume the topic messages to ingest them into a DynamoDB table.\n","date":"June 4, 2023","img":"/blog/2023-06-04-kafka-connect-for-aws-part-2/featured.png","lang":"en","langName":"English","largeImg":"/blog/2023-06-04-kafka-connect-for-aws-part-2/featured_hu2dd3564be2cf5a85a1bc0ecc02fccc55_87044_500x0_resize_box_3.png","permalink":"/blog/2023-06-04-kafka-connect-for-aws-part-2/","series":[{"title":"Kafka Connect for AWS Services Integration","url":"/series/kafka-connect-for-aws-services-integration/"}],"smallImg":"/blog/2023-06-04-kafka-connect-for-aws-part-2/featured_hu2dd3564be2cf5a85a1bc0ecc02fccc55_87044_180x0_resize_box_3.png","tags":[{"title":"AWS","url":"/tags/aws/"},{"title":"Apache Kafka","url":"/tags/apache-kafka/"},{"title":"Kafka Connect","url":"/tags/kafka-connect/"},{"title":"Apache Camel","url":"/tags/apache-camel/"},{"title":"Amazon DynamoDB","url":"/tags/amazon-dynamodb/"},{"title":"Docker","url":"/tags/docker/"},{"title":"Docker Compose","url":"/tags/docker-compose/"}],"timestamp":1685836800,"title":"Kafka Connect for AWS Services Integration - Part 2 Develop Camel DynamoDB Sink Connector"},{"categories":[{"title":"Apache Kafka","url":"/categories/apache-kafka/"}],"content":"In the previous post, we discussed Kafka Connect to stream data to/from a Kafka cluster. Kafka also includes the Producer/Consumer APIs that allow client applications to send/read streams of data to/from topics in a Kafka cluster. While the main Kafka project maintains only the Java clients, there are several open source projects that provide the Kafka client APIs in Python. In this post, I\u0026rsquo;ll demonstrate how to develop producer/consumer applications using the kafka-python package.\nPart 1 Cluster Setup Part 2 Management App Part 3 Kafka Connect Part 4 Producer and Consumer (this post) Part 5 Glue Schema Registry Part 6 Kafka Connect with Glue Schema Registry Part 7 Producer and Consumer with Glue Schema Registry Part 8 SSL Encryption Part 9 SSL Authentication Part 10 SASL Authentication Part 11 Kafka Authorization Producer The same Kafka producer app that is introduced in Part 2 is used again. It sends fake order data that is generated by the Faker package. The Order class generates one or more fake order records by the create method, and a record includes order ID, order timestamp, user ID and order items. Both the key and value are serialized as JSON. Note, as the producer runs outside the Docker network, the host name of the external listener (localhost:29092) is used as the bootstrap server address. It can run simply by python producer.py and the source can be found in the GitHub repository of this post.\n1# kafka-pocs/kafka-dev-with-docker/part-04/producer.py 2import os 3import datetime 4import time 5import json 6import typing 7import logging 8import dataclasses 9 10from faker import Faker 11from kafka import KafkaProducer 12 13logging.basicConfig(level=logging.INFO) 14 15 16@dataclasses.dataclass 17class OrderItem: 18 product_id: int 19 quantity: int 20 21 22@dataclasses.dataclass 23class Order: 24 order_id: str 25 ordered_at: datetime.datetime 26 user_id: str 27 order_items: typing.List[OrderItem] 28 29 def asdict(self): 30 return dataclasses.asdict(self) 31 32 @classmethod 33 def auto(cls, fake: Faker = Faker()): 34 user_id = str(fake.random_int(1, 100)).zfill(3) 35 order_items = [ 36 OrderItem(fake.random_int(1, 1000), fake.random_int(1, 10)) 37 for _ in range(fake.random_int(1, 4)) 38 ] 39 return cls(fake.uuid4(), datetime.datetime.utcnow(), user_id, order_items) 40 41 def create(self, num: int): 42 return [self.auto() for _ in range(num)] 43 44 45class Producer: 46 def __init__(self, bootstrap_servers: list, topic: str): 47 self.bootstrap_servers = bootstrap_servers 48 self.topic = topic 49 self.producer = self.create() 50 51 def create(self): 52 return KafkaProducer( 53 bootstrap_servers=self.bootstrap_servers, 54 value_serializer=lambda v: json.dumps(v, default=self.serialize).encode(\u0026#34;utf-8\u0026#34;), 55 key_serializer=lambda v: json.dumps(v, default=self.serialize).encode(\u0026#34;utf-8\u0026#34;), 56 ) 57 58 def send(self, orders: typing.List[Order]): 59 for order in orders: 60 try: 61 self.producer.send( 62 self.topic, key={\u0026#34;order_id\u0026#34;: order.order_id}, value=order.asdict() 63 ) 64 except Exception as e: 65 raise RuntimeError(\u0026#34;fails to send a message\u0026#34;) from e 66 self.producer.flush() 67 68 def serialize(self, obj): 69 if isinstance(obj, datetime.datetime): 70 return obj.isoformat() 71 if isinstance(obj, datetime.date): 72 return str(obj) 73 return obj 74 75 76if __name__ == \u0026#34;__main__\u0026#34;: 77 producer = Producer( 78 bootstrap_servers=os.getenv(\u0026#34;BOOTSTRAP_SERVERS\u0026#34;, \u0026#34;localhost:29092\u0026#34;).split(\u0026#34;,\u0026#34;), 79 topic=os.getenv(\u0026#34;TOPIC_NAME\u0026#34;, \u0026#34;orders\u0026#34;), 80 ) 81 max_run = int(os.getenv(\u0026#34;MAX_RUN\u0026#34;, \u0026#34;-1\u0026#34;)) 82 logging.info(f\u0026#34;max run - {max_run}\u0026#34;) 83 current_run = 0 84 while True: 85 current_run += 1 86 logging.info(f\u0026#34;current run - {current_run}\u0026#34;) 87 if current_run \u0026gt; max_run and max_run \u0026gt;= 0: 88 logging.info(f\u0026#34;exceeds max run, finish\u0026#34;) 89 producer.producer.close() 90 break 91 producer.send(Order.auto().create(100)) 92 time.sleep(1) A sample order record is shown below.\n1{ 2\t\u0026#34;order_id\u0026#34;: \u0026#34;79c0c393-9eca-4a44-8efd-3965752f3e16\u0026#34;, 3\t\u0026#34;ordered_at\u0026#34;: \u0026#34;2023-05-13T18:02:54.510497\u0026#34;, 4\t\u0026#34;user_id\u0026#34;: \u0026#34;050\u0026#34;, 5\t\u0026#34;order_items\u0026#34;: [ 6\t{ 7\t\u0026#34;product_id\u0026#34;: 113, 8\t\u0026#34;quantity\u0026#34;: 9 9\t}, 10\t{ 11\t\u0026#34;product_id\u0026#34;: 58, 12\t\u0026#34;quantity\u0026#34;: 5 13\t} 14\t] 15} Consumer The Consumer class instantiates the KafkaConsumer in the create method. The main consumer configuration values are provided by the constructor arguments: Kafka bootstrap server addresses (bootstrap_servers), topic names (topics) and consumer group ID (group_id). The process() method of the class polls messages and logs details of consumer records.\n1# kafka-pocs/kafka-dev-with-docker/part-04/consumer.py 2import os 3import time 4import logging 5 6from kafka import KafkaConsumer 7from kafka.errors import KafkaError 8 9logging.basicConfig(level=logging.INFO) 10 11 12class Consumer: 13 def __init__(self, bootstrap_servers: list, topics: list, group_id: str) -\u0026gt; None: 14 self.bootstrap_servers = bootstrap_servers 15 self.topics = topics 16 self.group_id = group_id 17 self.consumer = self.create() 18 19 def create(self): 20 return KafkaConsumer( 21 *self.topics, 22 bootstrap_servers=self.bootstrap_servers, 23 auto_offset_reset=\u0026#34;earliest\u0026#34;, 24 enable_auto_commit=True, 25 group_id=self.group_id, 26 key_deserializer=lambda v: v.decode(\u0026#34;utf-8\u0026#34;), 27 value_deserializer=lambda v: v.decode(\u0026#34;utf-8\u0026#34;), 28 ) 29 30 def process(self): 31 try: 32 while True: 33 msg = self.consumer.poll(timeout_ms=1000) 34 if msg is None: 35 continue 36 self.print_info(msg) 37 time.sleep(1) 38 except KafkaError as error: 39 logging.error(error) 40 41 def print_info(self, msg: dict): 42 for t, v in msg.items(): 43 for r in v: 44 logging.info( 45 f\u0026#34;key={r.key}, value={r.value}, topic={t.topic}, partition={t.partition}, offset={r.offset}, ts={r.timestamp}\u0026#34; 46 ) 47 48 49if __name__ == \u0026#34;__main__\u0026#34;: 50 consumer = Consumer( 51 bootstrap_servers=os.getenv(\u0026#34;BOOTSTRAP_SERVERS\u0026#34;, \u0026#34;localhost:29092\u0026#34;).split(\u0026#34;,\u0026#34;), 52 topics=os.getenv(\u0026#34;TOPIC_NAME\u0026#34;, \u0026#34;orders\u0026#34;).split(\u0026#34;,\u0026#34;), 53 group_id=os.getenv(\u0026#34;GROUP_ID\u0026#34;, \u0026#34;orders-group\u0026#34;), 54 ) 55 consumer.process() Consumer Services As the default number of partitions is set to be 3 in the compose-kafka.yml, a docker compose file is created for the consumer in order to deploy 3 instances of the app using the scale option. As the service uses the same docker network (kafkanet), we can take the service name of the brokers (e.g. kafka-0) on port 9092. Once started, it installs required packages and starts the consumer.\n1# kafka-pocs/kafka-dev-with-docker/part-04/compose-consumer.yml 2version: \u0026#34;3.5\u0026#34; 3 4services: 5 app: 6 image: bitnami/python:3.9 7 command: \u0026#34;sh -c \u0026#39;pip install -r requirements.txt \u0026amp;\u0026amp; python consumer.py\u0026#39;\u0026#34; 8 networks: 9 - kafkanet 10 environment: 11 BOOTSTRAP_SERVERS: kafka-0:9092,kafka-1:9092,kafka-2:9092 12 TOPIC_NAME: orders 13 GROUP_ID: orders-group 14 TZ: Australia/Sydney 15 volumes: 16 - .:/app 17 18networks: 19 kafkanet: 20 external: true 21 name: kafka-network Start Applications We can run the Kafka cluster by docker-compose -f compose-kafka.yml up -d and the producer by python producer.py. As mentioned earlier, we\u0026rsquo;ll deploy 3 instances of the consumer, and they can be deployed with the scale option as shown below. We can check the running consumer instances using the ps command of Docker Compose.\n1$ docker-compose -f compose-consumer.yml up -d --scale app=3 2$ docker-compose -f compose-consumer.yml ps 3 Name Command State Ports 4----------------------------------------------------------------- 5part-04_app_1 sh -c pip install -r requi ... Up 8000/tcp 6part-04_app_2 sh -c pip install -r requi ... Up 8000/tcp 7part-04_app_3 sh -c pip install -r requi ... Up 8000/tcp Each instance of the consumer subscribes to its own topic partition, and we can check that by container logs. Below shows the last 10 log entries of one of the instances. It shows that it polls messages from partition 0 only.\n1$ docker logs -f --tail 10 part-04_app_1 2 3INFO:root:key={\u0026#34;order_id\u0026#34;: \u0026#34;79c0c393-9eca-4a44-8efd-3965752f3e16\u0026#34;}, value={\u0026#34;order_id\u0026#34;: \u0026#34;79c0c393-9eca-4a44-8efd-3965752f3e16\u0026#34;, \u0026#34;ordered_at\u0026#34;: \u0026#34;2023-05-13T18:02:54.510497\u0026#34;, \u0026#34;user_id\u0026#34;: \u0026#34;050\u0026#34;, \u0026#34;order_items\u0026#34;: [{\u0026#34;product_id\u0026#34;: 113, \u0026#34;quantity\u0026#34;: 9}, {\u0026#34;product_id\u0026#34;: 58, \u0026#34;quantity\u0026#34;: 5}]}, topic=orders, partition=0, offset=11407, ts=1684000974514 4INFO:root:key={\u0026#34;order_id\u0026#34;: \u0026#34;d57427fc-5325-49eb-9fb7-e4fac1eca9b4\u0026#34;}, value={\u0026#34;order_id\u0026#34;: \u0026#34;d57427fc-5325-49eb-9fb7-e4fac1eca9b4\u0026#34;, \u0026#34;ordered_at\u0026#34;: \u0026#34;2023-05-13T18:02:54.510548\u0026#34;, \u0026#34;user_id\u0026#34;: \u0026#34;078\u0026#34;, \u0026#34;order_items\u0026#34;: [{\u0026#34;product_id\u0026#34;: 111, \u0026#34;quantity\u0026#34;: 4}]}, topic=orders, partition=0, offset=11408, ts=1684000974514 5INFO:root:key={\u0026#34;order_id\u0026#34;: \u0026#34;66c4ca6f-30e2-4f94-a971-ec23c9952430\u0026#34;}, value={\u0026#34;order_id\u0026#34;: \u0026#34;66c4ca6f-30e2-4f94-a971-ec23c9952430\u0026#34;, \u0026#34;ordered_at\u0026#34;: \u0026#34;2023-05-13T18:02:54.510565\u0026#34;, \u0026#34;user_id\u0026#34;: \u0026#34;004\u0026#34;, \u0026#34;order_items\u0026#34;: [{\u0026#34;product_id\u0026#34;: 647, \u0026#34;quantity\u0026#34;: 2}, {\u0026#34;product_id\u0026#34;: 894, \u0026#34;quantity\u0026#34;: 1}]}, topic=orders, partition=0, offset=11409, ts=1684000974514 6INFO:root:key={\u0026#34;order_id\u0026#34;: \u0026#34;518a6812-4357-4ec1-9e5c-aad7853646ee\u0026#34;}, value={\u0026#34;order_id\u0026#34;: \u0026#34;518a6812-4357-4ec1-9e5c-aad7853646ee\u0026#34;, \u0026#34;ordered_at\u0026#34;: \u0026#34;2023-05-13T18:02:54.510609\u0026#34;, \u0026#34;user_id\u0026#34;: \u0026#34;043\u0026#34;, \u0026#34;order_items\u0026#34;: [{\u0026#34;product_id\u0026#34;: 882, \u0026#34;quantity\u0026#34;: 5}]}, topic=orders, partition=0, offset=11410, ts=1684000974514 7INFO:root:key={\u0026#34;order_id\u0026#34;: \u0026#34;b22922e8-8ad0-48c3-b970-a486d4576d5c\u0026#34;}, value={\u0026#34;order_id\u0026#34;: \u0026#34;b22922e8-8ad0-48c3-b970-a486d4576d5c\u0026#34;, \u0026#34;ordered_at\u0026#34;: \u0026#34;2023-05-13T18:02:54.510625\u0026#34;, \u0026#34;user_id\u0026#34;: \u0026#34;002\u0026#34;, \u0026#34;order_items\u0026#34;: [{\u0026#34;product_id\u0026#34;: 206, \u0026#34;quantity\u0026#34;: 6}, {\u0026#34;product_id\u0026#34;: 810, \u0026#34;quantity\u0026#34;: 9}]}, topic=orders, partition=0, offset=11411, ts=1684000974514 8INFO:root:key={\u0026#34;order_id\u0026#34;: \u0026#34;1ef36da0-6a0b-4ec2-9ecd-10a020acfbfd\u0026#34;}, value={\u0026#34;order_id\u0026#34;: \u0026#34;1ef36da0-6a0b-4ec2-9ecd-10a020acfbfd\u0026#34;, \u0026#34;ordered_at\u0026#34;: \u0026#34;2023-05-13T18:02:54.510660\u0026#34;, \u0026#34;user_id\u0026#34;: \u0026#34;085\u0026#34;, \u0026#34;order_items\u0026#34;: [{\u0026#34;product_id\u0026#34;: 18, \u0026#34;quantity\u0026#34;: 3}]}, topic=orders, partition=0, offset=11412, ts=1684000974515 9INFO:root:key={\u0026#34;order_id\u0026#34;: \u0026#34;e9efdfd8-dc55-47b9-9cb0-c2e87e864435\u0026#34;}, value={\u0026#34;order_id\u0026#34;: \u0026#34;e9efdfd8-dc55-47b9-9cb0-c2e87e864435\u0026#34;, \u0026#34;ordered_at\u0026#34;: \u0026#34;2023-05-13T18:02:54.510692\u0026#34;, \u0026#34;user_id\u0026#34;: \u0026#34;051\u0026#34;, \u0026#34;order_items\u0026#34;: [{\u0026#34;product_id\u0026#34;: 951, \u0026#34;quantity\u0026#34;: 6}]}, topic=orders, partition=0, offset=11413, ts=1684000974515 10INFO:root:key={\u0026#34;order_id\u0026#34;: \u0026#34;b24ed1c0-150a-41b3-b1cb-a27fb5581b2b\u0026#34;}, value={\u0026#34;order_id\u0026#34;: \u0026#34;b24ed1c0-150a-41b3-b1cb-a27fb5581b2b\u0026#34;, \u0026#34;ordered_at\u0026#34;: \u0026#34;2023-05-13T18:02:54.510737\u0026#34;, \u0026#34;user_id\u0026#34;: \u0026#34;096\u0026#34;, \u0026#34;order_items\u0026#34;: [{\u0026#34;product_id\u0026#34;: 734, \u0026#34;quantity\u0026#34;: 3}]}, topic=orders, partition=0, offset=11414, ts=1684000974515 11INFO:root:key={\u0026#34;order_id\u0026#34;: \u0026#34;74b06957-2c6c-4e46-be49-d2915cc80b74\u0026#34;}, value={\u0026#34;order_id\u0026#34;: \u0026#34;74b06957-2c6c-4e46-be49-d2915cc80b74\u0026#34;, \u0026#34;ordered_at\u0026#34;: \u0026#34;2023-05-13T18:02:54.510774\u0026#34;, \u0026#34;user_id\u0026#34;: \u0026#34;072\u0026#34;, \u0026#34;order_items\u0026#34;: [{\u0026#34;product_id\u0026#34;: 968, \u0026#34;quantity\u0026#34;: 2}, {\u0026#34;product_id\u0026#34;: 602, \u0026#34;quantity\u0026#34;: 3}, {\u0026#34;product_id\u0026#34;: 316, \u0026#34;quantity\u0026#34;: 9}, {\u0026#34;product_id\u0026#34;: 971, \u0026#34;quantity\u0026#34;: 8}]}, topic=orders, partition=0, offset=11415, ts=1684000974515 12INFO:root:key={\u0026#34;order_id\u0026#34;: \u0026#34;fce38c6b-4806-4579-b11e-8eac24b5166b\u0026#34;}, value={\u0026#34;order_id\u0026#34;: \u0026#34;fce38c6b-4806-4579-b11e-8eac24b5166b\u0026#34;, \u0026#34;ordered_at\u0026#34;: \u0026#34;2023-05-13T18:02:54.510863\u0026#34;, \u0026#34;user_id\u0026#34;: \u0026#34;071\u0026#34;, \u0026#34;order_items\u0026#34;: [{\u0026#34;product_id\u0026#34;: 751, \u0026#34;quantity\u0026#34;: 8}]}, topic=orders, partition=0, offset=11416, ts=1684000974515 We can also check the consumers with management apps. For example, the 3 running consumers can be seen in the Consumers menu of kafka-ui. As expected, each consumer subscribes to its own topic partition. We can run the management apps by docker-compose -f compose-ui.yml up -d.\nSummary Kafka includes the Producer/Consumer APIs that allow client applications to send/read streams of data to/from topics in a Kafka cluster. While the main Kafka project maintains only the Java clients, there are several open source projects that provide the Kafka client APIs in Python. In this post, I demonstrated how to develop producer/consumer applications using the kafka-python package.\n","date":"June 1, 2023","img":"/blog/2023-06-01-kafka-development-with-docker-part-4/featured.png","lang":"en","langName":"English","largeImg":"/blog/2023-06-01-kafka-development-with-docker-part-4/featured_hu577191e1103a5dba84b139cbfa921627_75255_500x0_resize_box_3.png","permalink":"/blog/2023-06-01-kafka-development-with-docker-part-4/","series":[{"title":"Kafka Development With Docker","url":"/series/kafka-development-with-docker/"}],"smallImg":"/blog/2023-06-01-kafka-development-with-docker-part-4/featured_hu577191e1103a5dba84b139cbfa921627_75255_180x0_resize_box_3.png","tags":[{"title":"Apache Kafka","url":"/tags/apache-kafka/"},{"title":"Python","url":"/tags/python/"},{"title":"Docker","url":"/tags/docker/"},{"title":"Docker Compose","url":"/tags/docker-compose/"}],"timestamp":1685577600,"title":"Kafka Development With Docker - Part 4 Producer and Consumer"},{"categories":[{"title":"Apache Kafka","url":"/categories/apache-kafka/"}],"content":"According to the documentation of Apache Kafka, Kafka Connect is a tool for scalably and reliably streaming data between Apache Kafka and other systems. It makes it simple to quickly define connectors that move large collections of data into and out of Kafka. Kafka Connect supports two types of connectors - source and sink. Source connectors are used to ingest messages from external systems into Kafka topics while messages are ingested into external systems form Kafka topics with sink connectors. In this post, I will illustrate how to set up a data ingestion pipeline using Kafka connectors. Fake customer and order data will be ingested into the corresponding topics using the MSK Data Generator source connector. The topic messages will then be saved into a S3 bucket using the Confluent S3 sink connector.\nPart 1 Cluster Setup Part 2 Management App Part 3 Kafka Connect (this post) Part 4 Producer and Consumer Part 5 Glue Schema Registry Part 6 Kafka Connect with Glue Schema Registry Part 7 Producer and Consumer with Glue Schema Registry Part 8 SSL Encryption Part 9 SSL Authentication Part 10 SASL Authentication Part 11 Kafka Authorization Kafka Connect Setup We can use the same Docker image because Kafka Connect is included in the Kafka distribution. The Kafka Connect server runs as a separate docker compose service, and its key configurations are listed below.\nWe\u0026rsquo;ll run it as the distributed mode, and it can be started by executing connect-distributed.sh on the Docker command. The startup script requires the properties file (connect-distributed.properties). It includes configurations such as Kafka broker server addresses - see below for details. The Connect server is accessible on port 8083, and we can manage connectors via a REST API as demonstrated below. The properties file and connector sources are volume-mapped. AWS credentials are added to environment variables as the sink connector requires permission to write data into S3. The source can be found in the GitHub repository of this post.\n1# /kafka-dev-with-docker/part-03/compose-connect.yml 2version: \u0026#34;3.5\u0026#34; 3 4services: 5 kafka-connect: 6 image: bitnami/kafka:2.8.1 7 container_name: connect 8 command: \u0026gt; 9 /opt/bitnami/kafka/bin/connect-distributed.sh 10 /opt/bitnami/kafka/config/connect-distributed.properties 11 ports: 12 - \u0026#34;8083:8083\u0026#34; 13 networks: 14 - kafkanet 15 environment: 16 AWS_ACCESS_KEY_ID: $AWS_ACCESS_KEY_ID 17 AWS_SECRET_ACCESS_KEY: $AWS_SECRET_ACCESS_KEY 18 AWS_SESSION_TOKEN: $AWS_SESSION_TOKEN 19 volumes: 20 - \u0026#34;./configs/connect-distributed.properties:/opt/bitnami/kafka/config/connect-distributed.properties\u0026#34; 21 - \u0026#34;./connectors/confluent-s3/lib:/opt/connectors/confluent-s3\u0026#34; 22 - \u0026#34;./connectors/msk-datagen:/opt/connectors/msk-datagen\u0026#34; 23 24networks: 25 kafkanet: 26 external: true 27 name: kafka-network Connect Properties File The properties file includes configurations of the Connect server. Below shows key config values.\nBootstrap Server I changed the Kafka bootstrap server addresses. As it shares the same Docker network, we can take the service names (e.g. kafka-0) on port 9092. Cluster group id In distributed mode, multiple worker processes use the same group.id, and they automatically coordinate to schedule execution of connectors and tasks across all available workers. Converter-related properties Converters are necessary to have a Kafka Connect deployment support a particular data format when writing to or reading from Kafka. By default, org.apache.kafka.connect.json.JsonConverter is set for both the key and value converters and schemas are enabled for both of them. As shown later, these properties can be overridden when creating a connector. Topics for offsets, configs, status Several topics are created to manage connectors by multiple worker processes. Plugin path Paths that contains plugins (connectors, converters, transformations) can be set to a list of filesystem paths separated by commas (,) /opt/connectors is added and connector sources will be volume-mapped to it. 1# kafka-dev-with-docker/part-03/configs/connect-distributed.properties 2 3# A list of host/port pairs to use for establishing the initial connection to the Kafka cluster. 4bootstrap.servers=kafka-0:9092,kafka-1:9092,kafka-2:9092 5 6# unique name for the cluster, used in forming the Connect cluster group. Note that this must not conflict with consumer group IDs 7group.id=connect-cluster 8 9# The converters specify the format of data in Kafka and how to translate it into Connect data. Every Connect user will 10# need to configure these based on the format they want their data in when loaded from or stored into Kafka 11key.converter=org.apache.kafka.connect.json.JsonConverter 12value.converter=org.apache.kafka.connect.json.JsonConverter 13# Converter-specific settings can be passed in by prefixing the Converter\u0026#39;s setting with the converter we want to apply 14# it to 15key.converter.schemas.enable=true 16value.converter.schemas.enable=true 17 18# Topic to use for storing offsets. 19offset.storage.topic=connect-offsets 20offset.storage.replication.factor=1 21#offset.storage.partitions=25 22 23# Topic to use for storing connector and task configurations. 24config.storage.topic=connect-configs 25config.storage.replication.factor=1 26 27# Topic to use for storing statuses. 28status.storage.topic=connect-status 29status.storage.replication.factor=1 30#status.storage.partitions=5 31 32... 33 34# Set to a list of filesystem paths separated by commas (,) to enable class loading isolation for plugins 35# (connectors, converters, transformations). 36plugin.path=/opt/connectors Download Connectors The connector sources need to be downloaded into the respective host paths (./connectors/confluent-s3 and ./connectors/msk-datagen) so that they are volume-mapped to the container\u0026rsquo;s plugin path (/opt/connectors). The following script downloads them into the host paths.\n1# /kafka-dev-with-docker/part-03/download.sh 2#!/usr/bin/env bash 3SCRIPT_DIR=\u0026#34;$(cd $(dirname \u0026#34;$0\u0026#34;); pwd)\u0026#34; 4 5SRC_PATH=${SCRIPT_DIR}/connectors 6rm -rf ${SRC_PATH} \u0026amp;\u0026amp; mkdir -p ${SRC_PATH}/msk-datagen 7 8## Confluent S3 Sink Connector 9echo \u0026#34;downloading confluent s3 connector...\u0026#34; 10DOWNLOAD_URL=https://d1i4a15mxbxib1.cloudfront.net/api/plugins/confluentinc/kafka-connect-s3/versions/10.4.3/confluentinc-kafka-connect-s3-10.4.3.zip 11 12curl -o ${SRC_PATH}/confluent.zip ${DOWNLOAD_URL} \\ 13 \u0026amp;\u0026amp; unzip -qq ${SRC_PATH}/confluent.zip -d ${SRC_PATH} \\ 14 \u0026amp;\u0026amp; rm ${SRC_PATH}/confluent.zip \\ 15 \u0026amp;\u0026amp; mv ${SRC_PATH}/$(ls ${SRC_PATH} | grep confluentinc-kafka-connect-s3) ${SRC_PATH}/confluent-s3 16 17## MSK Data Generator Souce Connector 18echo \u0026#34;downloading msk data generator...\u0026#34; 19DOWNLOAD_URL=https://github.com/awslabs/amazon-msk-data-generator/releases/download/v0.4.0/msk-data-generator-0.4-jar-with-dependencies.jar 20 21curl -L -o ${SRC_PATH}/msk-datagen/msk-data-generator.jar ${DOWNLOAD_URL} Below shows the folder structure after the connectors are downloaded successfully.\n1$ tree connectors/ -d 2connectors/ 3├── confluent-s3 4│ ├── assets 5│ ├── doc 6│ │ ├── licenses 7│ │ └── notices 8│ ├── etc 9│ └── lib 10└── msk-datagen Start Docker Compose Services There are 3 docker compose files for the Kafka cluster, Kafka Connect and management applications. We can run the whole services by starting them in order. The order matters as the Connect server relies on the Kafka cluster and kpow in compose-ui.yml fails if the Connect server is not up and running. Note the Connect server address is added to both the Kafka management apps in compose-ui.yml, and we are able to monitor and manage connectors on them.\n1$ cd kafka-dev-with-docker/part-03 2# download connectors 3$ ./download.sh 4# starts 3 node kafka cluster 5$ docker-compose -f compose-kafka.yml up -d 6# starts kafka connect server in distributed mode 7$ docker-compose -f compose-connect.yml up -d 8# starts kafka-ui and kpow 9# connect server address (http://kafka-connect:8083) is added 10# check updated environment variables of each service 11$ docker-compose -f compose-ui.yml up -d Source Connector Creation As mentioned earlier, Kafka Connect provides a REST API that manages connectors. We can create a connector programmatically. The REST endpoint requires a JSON payload that includes connector configurations.\n1$ cd kafka-dev-with-docker/part-03 2$ curl -i -X POST -H \u0026#34;Accept:application/json\u0026#34; -H \u0026#34;Content-Type:application/json\u0026#34; \\ 3 http://localhost:8083/connectors/ -d @configs/source.json The connector class (connector.class) is required for any connector and I set it for the MSK Data Generator. Also, as many as two workers are allocated to the connector (tasks.max). As mentioned earlier, the converter-related properties are overridden. Specifically, the key converter is set to the string converter as the keys of both topics are set to be primitive values (genkp). Also, schemas are not enabled for both the key and value.\nThe remaining properties are specific to the source connector. Basically it sends messages to two topics (customer and order). They are linked by the customer_id attribute of the order topic where the value is from the key of the customer topic. This is useful for practicing stream processing e.g. for joining two streams.\n1// kafka-dev-with-docker/part-03/configs/source.json 2{ 3 \u0026#34;name\u0026#34;: \u0026#34;order-source\u0026#34;, 4 \u0026#34;config\u0026#34;: { 5 \u0026#34;connector.class\u0026#34;: \u0026#34;com.amazonaws.mskdatagen.GeneratorSourceConnector\u0026#34;, 6 \u0026#34;tasks.max\u0026#34;: \u0026#34;2\u0026#34;, 7 \u0026#34;key.converter\u0026#34;: \u0026#34;org.apache.kafka.connect.storage.StringConverter\u0026#34;, 8 \u0026#34;key.converter.schemas.enable\u0026#34;: false, 9 \u0026#34;value.converter\u0026#34;: \u0026#34;org.apache.kafka.connect.json.JsonConverter\u0026#34;, 10 \u0026#34;value.converter.schemas.enable\u0026#34;: false, 11 12 \u0026#34;genkp.customer.with\u0026#34;: \u0026#34;#{Code.isbn10}\u0026#34;, 13 \u0026#34;genv.customer.name.with\u0026#34;: \u0026#34;#{Name.full_name}\u0026#34;, 14 15 \u0026#34;genkp.order.with\u0026#34;: \u0026#34;#{Internet.uuid}\u0026#34;, 16 \u0026#34;genv.order.product_id.with\u0026#34;: \u0026#34;#{number.number_between \u0026#39;101\u0026#39;,\u0026#39;109\u0026#39;}\u0026#34;, 17 \u0026#34;genv.order.quantity.with\u0026#34;: \u0026#34;#{number.number_between \u0026#39;1\u0026#39;,\u0026#39;5\u0026#39;}\u0026#34;, 18 \u0026#34;genv.order.customer_id.matching\u0026#34;: \u0026#34;customer.key\u0026#34;, 19 20 \u0026#34;global.throttle.ms\u0026#34;: \u0026#34;500\u0026#34;, 21 \u0026#34;global.history.records.max\u0026#34;: \u0026#34;1000\u0026#34; 22 } 23} Once created successfully, we can check the connector status as shown below.\n1$ curl http://localhost:8083/connectors/order-source/status 1{ 2\t\u0026#34;name\u0026#34;: \u0026#34;order-source\u0026#34;, 3\t\u0026#34;connector\u0026#34;: { 4\t\u0026#34;state\u0026#34;: \u0026#34;RUNNING\u0026#34;, 5\t\u0026#34;worker_id\u0026#34;: \u0026#34;172.19.0.6:8083\u0026#34; 6\t}, 7\t\u0026#34;tasks\u0026#34;: [ 8\t{ 9\t\u0026#34;id\u0026#34;: 0, 10\t\u0026#34;state\u0026#34;: \u0026#34;RUNNING\u0026#34;, 11\t\u0026#34;worker_id\u0026#34;: \u0026#34;172.19.0.6:8083\u0026#34; 12\t}, 13\t{ 14\t\u0026#34;id\u0026#34;: 1, 15\t\u0026#34;state\u0026#34;: \u0026#34;RUNNING\u0026#34;, 16\t\u0026#34;worker_id\u0026#34;: \u0026#34;172.19.0.6:8083\u0026#34; 17\t} 18\t], 19\t\u0026#34;type\u0026#34;: \u0026#34;source\u0026#34; 20} As we\u0026rsquo;ve added the connector URL, the Kafka Connect menu gets appeared on kafka-ui. We can check the details of the connector on the app as well.\nKafka Topics As configured, the source connector ingests messages to the customer and order topics.\nWe can browse individual messages in the Messages tab of a topic.\nSink Connector Creation Similar to the source connector, we can create the sink connector using the REST API.\n1$ cd kafka-dev-with-docker/part-03 2$ curl -i -X POST -H \u0026#34;Accept:application/json\u0026#34; -H \u0026#34;Content-Type:application/json\u0026#34; \\ 3 http://localhost:8083/connectors/ -d @configs/sink.json The connector is configured to write messages from both the topics (topics) into a S3 bucket (s3.bucket.name) where files are prefixed by the partition number (DefaultPartitioner). Also, it invokes file commits every 60 seconds (rotate.schedule.interval.ms) or the number of messages reach 100 (flush.size). Like the source connector, it overrides the converter-related properties.\n1// kafka-dev-with-docker/part-03/configs/sink.json 2{ 3 \u0026#34;name\u0026#34;: \u0026#34;order-sink\u0026#34;, 4 \u0026#34;config\u0026#34;: { 5 \u0026#34;connector.class\u0026#34;: \u0026#34;io.confluent.connect.s3.S3SinkConnector\u0026#34;, 6 \u0026#34;storage.class\u0026#34;: \u0026#34;io.confluent.connect.s3.storage.S3Storage\u0026#34;, 7 \u0026#34;format.class\u0026#34;: \u0026#34;io.confluent.connect.s3.format.json.JsonFormat\u0026#34;, 8 \u0026#34;tasks.max\u0026#34;: \u0026#34;2\u0026#34;, 9 \u0026#34;topics\u0026#34;: \u0026#34;order,customer\u0026#34;, 10 \u0026#34;s3.bucket.name\u0026#34;: \u0026#34;kafka-dev-ap-southeast-2\u0026#34;, 11 \u0026#34;s3.region\u0026#34;: \u0026#34;ap-southeast-2\u0026#34;, 12 \u0026#34;flush.size\u0026#34;: \u0026#34;100\u0026#34;, 13 \u0026#34;rotate.schedule.interval.ms\u0026#34;: \u0026#34;60000\u0026#34;, 14 \u0026#34;timezone\u0026#34;: \u0026#34;Australia/Sydney\u0026#34;, 15 \u0026#34;partitioner.class\u0026#34;: \u0026#34;io.confluent.connect.storage.partitioner.DefaultPartitioner\u0026#34;, 16 \u0026#34;key.converter\u0026#34;: \u0026#34;org.apache.kafka.connect.storage.StringConverter\u0026#34;, 17 \u0026#34;key.converter.schemas.enable\u0026#34;: false, 18 \u0026#34;value.converter\u0026#34;: \u0026#34;org.apache.kafka.connect.json.JsonConverter\u0026#34;, 19 \u0026#34;value.converter.schemas.enable\u0026#34;: false, 20 \u0026#34;errors.log.enable\u0026#34;: \u0026#34;true\u0026#34; 21 } 22} Below shows the sink connector details on kafka-ui.\nKafka Consumers The sink connector creates a Kafka consumer, and it is named as connect-order-sink. We see that it subscribes the two topics and is in the stable state. It has two members because it is configured to have as many as 2 tasks.\nS3 Destination The sink connector writes messages of the two topics (customer and order), and topic names are used as prefixes.\nAs mentioned, the default partitioner prefixes files further by the partition number, and it can be checked below.\nThe files are generated by \u0026lt;topic\u0026gt;+\u0026lt;partiton\u0026gt;+\u0026lt;start-offset\u0026gt;.json. The sink connector\u0026rsquo;s format class is set to io.confluent.connect.s3.format.json.JsonFormat so that it writes to Json files.\nSummary Kafka Connect is a tool for scalably and reliably streaming data between Apache Kafka and other systems. In this post, I illustrated how to set up a data ingestion pipeline using Kafka connectors. Fake customer and order data was ingested into the corresponding topics using the MSK Data Generator source connector. Also, the topic messages were saved into a S3 bucket using the Confluent S3 sink connector.\n","date":"May 25, 2023","img":"/blog/2023-05-25-kafka-development-with-docker-part-3/featured.png","lang":"en","langName":"English","largeImg":"/blog/2023-05-25-kafka-development-with-docker-part-3/featured_hu7953614f0ee63f59b300b3ecccd65289_69998_500x0_resize_box_3.png","permalink":"/blog/2023-05-25-kafka-development-with-docker-part-3/","series":[{"title":"Kafka Development With Docker","url":"/series/kafka-development-with-docker/"}],"smallImg":"/blog/2023-05-25-kafka-development-with-docker-part-3/featured_hu7953614f0ee63f59b300b3ecccd65289_69998_180x0_resize_box_3.png","tags":[{"title":"Apache Kafka","url":"/tags/apache-kafka/"},{"title":"Kafka Connect","url":"/tags/kafka-connect/"},{"title":"Docker","url":"/tags/docker/"},{"title":"Docker Compose","url":"/tags/docker-compose/"}],"timestamp":1684972800,"title":"Kafka Development With Docker - Part 3 Kafka Connect"},{"categories":[{"title":"Apache Kafka","url":"/categories/apache-kafka/"}],"content":"In the previous post, I illustrated how to create a topic and to produce/consume messages using the command utilities provided by Apache Kafka. It is not convenient, however, for example, when you consume serialised messages where their schemas are stored in a schema registry. Also, the utilities don\u0026rsquo;t support to browse or manage related resources such as connectors and schemas. Therefore, a Kafka management app can be a good companion for development, which helps monitor and manage resources on an easy-to-use user interface. An app can be more useful if it supports features that are desirable for Kafka development on AWS. Those features cover IAM access control of Amazon MSK and integration with Amazon MSK Connect and AWS Glue Schema Registry. In this post, I\u0026rsquo;ll introduce several management apps that meet those requirements.\nPart 1 Cluster Setup Part 2 Management App (this post) Part 3 Kafka Connect Part 4 Producer and Consumer Part 5 Glue Schema Registry Part 6 Kafka Connect with Glue Schema Registry Part 7 Producer and Consumer with Glue Schema Registry Part 8 SSL Encryption Part 9 SSL Authentication Part 10 SASL Authentication Part 11 Kafka Authorization Overview of Kafka Management App Generally good Kafka management apps support to monitor and manage one or more Kafka clusters. They also allow you to view and/or manage Kafka-related resources such as brokers, topics, consumer groups, connectors, schemas etc. Furthermore, they help produce, browse, and filter messages with default or custom serialisation/deserialisation methods.\nWhile the majority of Kafka management apps share the features mentioned above, we need additional features for developing Kafka on AWS. They cover IAM access control of Amazon MSK and integration with Amazon MSK Connect and AWS Glue Schema Registry. As far as I\u0026rsquo;ve searched, there are 3 management apps that support these features.\nUI for Apache Kafka (kafka-ui) is free and open-source, and multiple clusters can be registered to it. It supports IAM access control by default and Glue Schema Registry integration is partially implementation, which means it doesn\u0026rsquo;t seem to allow you to view/manage schemas while message deserialisation is implemented by custom serde registration. Besides, MSK Connect integration is yet to be in their roadmap. I believe these limitations are not critical as we can manage schemas/connectors on the associating AWS Console anyway.\nBoth Kpow and Conduktor Desktop support all the features out-of-box. However, their free editions are limited to a single cluster. Moreover, the latter has a more strict restriction, which is limited to a cluster having a single broker. Even we are not able to link our local Kafka cluster as it has 3 brokers. However, I find its paid edition is the most intuitive and feature-rich, and it should be taken seriously when deciding an app for your team.\nBelow shows a comparison of the 3 apps in terms of the features for Kafka development on AWS.\nApplication IAM Access Control MSK Connect Glue Schema Registry Note UI for Apache Kafka (kafka-ui) ✔ ❌ ⚠ UI for Apache Kafka is a free, open-source web UI to monitor and manage Apache Kafka clusters. It will remain free and open-source, without any paid features or subscription plans to be added in the future. Kpow ✔ ✔ ✔ Kpow CE allows you to manage one Kafka Cluster, one Schema Registry, and one Connect Cluster, with the UI supporting a single user session at a time. Conduktor Desktop ✔ ✔ ✔ The Free plan is limited to integrating 1 unsecure cluster (of a single broker) and restricted to browse 10 viewable topics. There are other popular Kafka management apps, and they can be useful if your development is not on AWS.\nAKHQ Redpanda Console formerly Kowl Kafdrop In the subsequent sections, I will introduce UI for Apache Kafka (kafka-ui) and Kpow. The source can be found in the GitHub repository of this post.\nStart Management Apps I assume the local Kafka cluster demonstrated in Part 1 is up and running, which can be run by docker-compose -f compose-kafka.yml up -d. I created a separate compose file for the management apps. The cluster details are configured by environment variables, and only the Kafka cluster details are added in this post - more complete examples will be covered in later posts. As kafka-ui supports multiple clusters, cluster config variables are indexed while only a single cluster config is allowed for Kpow CE. Note that, as the services share the same network to the Kafka cluster, they can use the inter broker listener, which means the bootstrap servers can be indicated as kafka-0:9092,kafka-1:9092,kafka-2:9092. The services can be started by docker-compose -f compose-ui.yml up -d, and kafka-ui and Kpow CE are accessible on port 8080 and 3000 respectively.\n1# kafka-dev-with-docker/part-02/kafka-ui.yml 2version: \u0026#34;3.5\u0026#34; 3 4services: 5 kafka-ui: 6 image: provectuslabs/kafka-ui:master 7 container_name: kafka-ui 8 ports: 9 - \u0026#34;8080:8080\u0026#34; 10 networks: 11 - kafkanet 12 environment: 13 KAFKA_CLUSTERS_0_NAME: local 14 KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka-0:9092,kafka-1:9092,kafka-2:9092 15 KAFKA_CLUSTERS_0_ZOOKEEPER: zookeeper:2181 16 kpow: 17 image: factorhouse/kpow-ce:91.2.1 18 container_name: kpow 19 ports: 20 - \u0026#34;3000:3000\u0026#34; 21 networks: 22 - kafkanet 23 environment: 24 BOOTSTRAP: kafka-0:9092,kafka-1:9092,kafka-2:9092 25 26networks: 27 kafkanet: 28 external: true 29 name: kafka-network Below shows the landing page of kafka-ui. It shows details of the single cluster (local) and it allows you to check brokers, topics and consumers.\nThe overview section of Kpow CE shows more details by default, although we haven\u0026rsquo;t specified many of them (stream/connect cluster, schema registry \u0026hellip;).\nCreate Topic UI for Apache Kafka (kafka-ui) In the Topics menu, we can click the Add a Topic button to begin creating a topic.\nWe can create a topic by clicking the Create topic button after entering the topic name, number of partitions, and additional configuration values. I created the topic named orders here, and it\u0026rsquo;ll be used later.\nKpow Similarly, we can click the Create Topic button to begin creating a topic in the Topics menu.\nWe can create a topic by clicking the Create Topic button after entering the topic name and additional configuration values.\nProduce Messages A Kafka producer is created to demonstrate how to browse and filter topic messages. It sends fake order data that is generated by the Faker package. The Order class generates one or more fake order records by the create method, and a record includes order ID, order timestamp, user ID and order items. Both the key and value are serialised as JSON. Note, as the producer runs outside the Docker network, the host name of the external listener (localhost:29092) is used as the bootstrap server address. It can run simply by python producer.py.\n1# kafka-dev-with-docker/part-02/producer.py 2import os 3import datetime 4import time 5import json 6import typing 7import dataclasses 8 9from faker import Faker 10from kafka import KafkaProducer 11 12@dataclasses.dataclass 13class OrderItem: 14 product_id: int 15 quantity: int 16 17@dataclasses.dataclass 18class Order: 19 order_id: str 20 ordered_at: datetime.datetime 21 user_id: str 22 order_items: typing.List[OrderItem] 23 24 def asdict(self): 25 return dataclasses.asdict(self) 26 27 @classmethod 28 def auto(cls, fake: Faker = Faker()): 29 user_id = str(fake.random_int(1, 100)).zfill(3) 30 order_items = [ 31 OrderItem(fake.random_int(1, 1000), fake.random_int(1, 10)) 32 for _ in range(fake.random_int(1, 4)) 33 ] 34 return cls(fake.uuid4(), datetime.datetime.utcnow(), user_id, order_items) 35 36 def create(self, num: int): 37 return [self.auto() for _ in range(num)] 38 39class Producer: 40 def __init__(self, bootstrap_servers: list, topic: str): 41 self.bootstrap_servers = bootstrap_servers 42 self.topic = topic 43 self.producer = self.create() 44 45 def create(self): 46 return KafkaProducer( 47 bootstrap_servers=self.bootstrap_servers, 48 value_serializer=lambda v: json.dumps(v, default=self.serialize).encode(\u0026#34;utf-8\u0026#34;), 49 key_serializer=lambda v: json.dumps(v, default=self.serialize).encode(\u0026#34;utf-8\u0026#34;), 50 ) 51 52 def send(self, orders: typing.List[Order]): 53 for order in orders: 54 try: 55 self.producer.send( 56 self.topic, key={\u0026#34;order_id\u0026#34;: order.order_id}, value=order.asdict() 57 ) 58 except Exception as e: 59 raise RuntimeError(\u0026#34;fails to send a message\u0026#34;) from e 60 self.producer.flush() 61 62 def serialize(self, obj): 63 if isinstance(obj, datetime.datetime): 64 return obj.isoformat() 65 if isinstance(obj, datetime.date): 66 return str(obj) 67 return obj 68 69if __name__ == \u0026#34;__main__\u0026#34;: 70 producer = Producer( 71 bootstrap_servers=os.getenv(\u0026#34;BOOTSTRAP_SERVERS\u0026#34;, \u0026#34;localhost:29092\u0026#34;).split(\u0026#34;,\u0026#34;), 72 topic=os.getenv(\u0026#34;TOPIC_NAME\u0026#34;, \u0026#34;orders\u0026#34;), 73 ) 74 max_run = int(os.getenv(\u0026#34;MAX_RUN\u0026#34;, \u0026#34;20\u0026#34;)) 75 print(f\u0026#34;max run - {max_run}\u0026#34;) 76 current_run = 0 77 while True: 78 current_run += 1 79 print(f\u0026#34;current run - {current_run}\u0026#34;) 80 if current_run \u0026gt; max_run: 81 print(f\u0026#34;exceeds max run, finish\u0026#34;) 82 producer.producer.close() 83 break 84 producer.send(Order.auto().create(100)) 85 time.sleep(0.5) Browse Messages UI for Apache Kafka (kafka-ui) In the Messages tab of the orders topic, we can browse the order messages. Be default, it lists messages from the oldest one. It has options to filter messages by Seek Type (offset or timestamp) and Partitions. Also, it allows you to sort messages by timestamp - Oldest First or Newest First.\nIt also supports to filter messages by key or value. Below shows an example where messages are filtered by a specific user ID (072).\nKpow In the Data menu, we can select one or more topics in order to browse messages. In Mode, we can select one of Sample, Partition and Key options - specific values should be entered if other than Sample is selected. In Window, it allows you to select the following conditions - Recent, Last minute, Last 15 minutes, Last hour, Last 24 hours, Earliest, Custom timestamp, and Custom datetime. Unlike kafka-ui, it requires to select the appropriate key/value serialisers and JSON is selected for both key and value.\nSimilar to kafka-ui, it supports to filter messages by key or value. Below shows an example where messages are filtered by a specific user ID (072).\nSummary Several Kafka management apps are introduced in this post. On top of typical features of monitoring and managing Kafka-related resources, they support features that are desirable for Kafka development on AWS - IAM access control and integration with MSK Connect and Glue Schema Registry. More complete examples will be covered in later posts.\n","date":"May 18, 2023","img":"/blog/2023-05-18-kafka-development-with-docker-part-2/featured.png","lang":"en","langName":"English","largeImg":"/blog/2023-05-18-kafka-development-with-docker-part-2/featured_hu7e86619ffdcd5d184364188304c39e01_59675_500x0_resize_box_3.png","permalink":"/blog/2023-05-18-kafka-development-with-docker-part-2/","series":[{"title":"Kafka Development With Docker","url":"/series/kafka-development-with-docker/"}],"smallImg":"/blog/2023-05-18-kafka-development-with-docker-part-2/featured_hu7e86619ffdcd5d184364188304c39e01_59675_180x0_resize_box_3.png","tags":[{"title":"Apache Kafka","url":"/tags/apache-kafka/"},{"title":"Docker","url":"/tags/docker/"},{"title":"Docker Compose","url":"/tags/docker-compose/"}],"timestamp":1684368000,"title":"Kafka Development With Docker - Part 2 Management App"},{"categories":[{"title":"General","url":"/categories/general/"}],"content":"I recently obtained the Confluent Certified Developer for Apache Kafka (CCDAK) certification. It focuses on knowledge of developing applications that work with Kafka, and is targeted to developers and solutions architects. As it assumes Java APIs for development and testing, I am contacted to share how I prepared for it as a non-Java developer from time to time. I thought it would be better to write a post to summarise how I did it rather than answering to them individually.\nBefore I decided to take the exam, I had some exposure to Kafka. One of them is building a data pipeline using Change Data Capture (CDC), and it aimed to ingest data from a PostgreSQL DB into S3 using MSK Connect. Another project is developing a Kafka consumer that upserts into/deletes from multiple database tables. Although I was not sure if I would work on the Confluent platform, I decided to take the exam as I thought it would be a good chance to learn more on Apache Kafka and related technologies.\nI searched online courses and found Confluent Certified Developer for Apache Kafka (CCDAK) from A Cloud Grue. I chose it as it (1) seems to cover all exam topics, (2) gives chances to apply course contents to real world scenarios on lab sessions, and (3) provides a practice test. It covers the following topics - Kafka fundamentals, application design concepts, Kafka producer/consumer, Kafka Streams, Kafka Connect, Confluent Schema Registry, REST Proxy, ksqlDB, testing, security and monitoring. The lectures are well-organised and mostly easy to follow. It is based on a Kafka cluster on 3 VMs, but I used a cluster on Docker instead. Using Docker made me confused on Kafka security (SSL authentication and ACL). I couldn\u0026rsquo;t complete that chapter fully, but moved on as security is not covered in depth.\nAlso, as I don\u0026rsquo;t develop in Java, I couldn\u0026rsquo;t follow some chapters on its own, and I had to convert them in Python. For Kafka producer and consumer, it was not difficult to do so using the kafka-python package as I used it on earlier projects. Kafka Streams is a bit tricky as there doesn\u0026rsquo;t seem to be a Python package that matches it tightly. Although hands-on implementation is not required for the exam, I\u0026rsquo;d like to try on my own. After some investigation, I found several candidates for streams processing in Python - Faust, Bytewax, and PyFlink. Among those, I chose PyFlink and implemented all examples and labs with it. If you don\u0026rsquo;t use the Kafka Streams API in Java, I recommend you to try Flink. Recently Confluent announced to acquire Immerok, a startup offering a fully managed service for Apache Flink. Maybe Flink questions will start to come up in the future.\nAfter finishing the lectures, I did the practice test. The questions much simpler than the sample questions from Confluent. Then I bought a practice test course from Udemy. It provides 3 practice tests, and I was able to gain additional knowledge while going through those. I think you would be okay if you pass the practice exams.\nI hope you find this post useful and good luck with your exam!\n","date":"May 11, 2023","img":"/blog/2023-05-11-how-i-prepared-for-ccdak/featured.png","lang":"en","langName":"English","largeImg":"/blog/2023-05-11-how-i-prepared-for-ccdak/featured_hu7986024347fc7f8f9182c862049fa008_215227_500x0_resize_box_3.png","permalink":"/blog/2023-05-11-how-i-prepared-for-ccdak/","series":[],"smallImg":"/blog/2023-05-11-how-i-prepared-for-ccdak/featured_hu7986024347fc7f8f9182c862049fa008_215227_180x0_resize_box_3.png","tags":[{"title":"Apache Kafka","url":"/tags/apache-kafka/"},{"title":"Confluent","url":"/tags/confluent/"},{"title":"Certification","url":"/tags/certification/"}],"timestamp":1683763200,"title":"How I Prepared for Confluent Certified Developer for Apache Kafka as a Non-Java Developer"},{"categories":[{"title":"Apache Kafka","url":"/categories/apache-kafka/"}],"content":"I\u0026rsquo;m teaching myself modern data streaming architectures on AWS, and Apache Kafka is one of the key technologies, which can be used for messaging, activity tracking, stream processing and so on. While applications tend to be deployed to cloud, it can be much easier if we develop and test those with Docker and Docker Compose locally. As the series title indicates, I plan to publish articles that demonstrate Kafka and related tools in Dockerized environments. Although I covered some of them in previous posts, they are implemented differently in terms of the Kafka Docker image, the number of brokers, Docker volume mapping etc. It can be confusing, and one of the purposes of this series is to illustrate reference implementations that can be applied to future development projects. Also, I can extend my knowledge while preparing for this series. In fact Kafka security is one of the areas that I expect to learn further. Below shows a list of posts that I plan for now.\nPart 1 Cluster Setup (this post) Part 2 Management App Part 3 Kafka Connect Part 4 Producer and Consumer Part 5 Glue Schema Registry Part 6 Kafka Connect with Glue Schema Registry Part 7 Producer and Consumer with Glue Schema Registry Part 8 SSL Encryption Part 9 SSL Authentication Part 10 SASL Authentication Part 11 Kafka Authorization Setup Kafka Cluster We are going to create a Kafka cluster with 3 brokers and 1 Zookeeper node. Having multiple brokers are advantageous to test Kafka features. For example, the number of replication factor of a topic partition is limited to the number of brokers. Therefore, if we have multiple brokers, we can check what happens when the minimum in-sync replica configuration doesn\u0026rsquo;t meet due to broker failure. We also need Zookeeper for metadata management - see this article for details about the role of Zookeeper.\nDocker Compose File There are popular docker images for Kafka development. Some of them are confluentinc/cp-kafka from Confluent, wurstmeister/kafka from wurstmeister, and bitnami/kafka from Bitnami. I initially used the image from Confluent, but I was not sure how to select a specific version of Kafka - note the recommended Kafka version of Amazon MSK is 2.8.1. Also, the second image doesn\u0026rsquo;t cover Kafka 3+ and it may be limited to use a newer version of Kafka in the future. In this regard, I chose the image from Bitnami.\nThe following Docker Compose file is used to create the Kafka cluster indicated earlier - it can also be found in the GitHub repository of this post. The resources created by the compose file is illustrated below.\nservices zookeeper A Zookeeper node is created with minimal configuration. It allows anonymous login. kafka-[id] Each broker has a unique ID (KAFKA_CFG_BROKER_ID) and shares the same Zookeeper connect parameter (KAFKA_CFG_ZOOKEEPER_CONNECT). These are required to connect to the Zookeeper node. Each has two listeners - INTERNAL and EXTERNAL. The former is accessed on port 9092, and it is used within the same Docker network. The latter is mapped from port 9093 to 9095, and it can be used to connect from outside the network. [UPDATE 2023-05-09] The external ports are updated from 29092 to 29094, which is because it is planned to use 9093 for SSL encryption. [UPDATE 2023-05-15] The inter broker listener name is changed from CLIENT to INTERNAL. Each can be accessed without authentication (ALLOW_PLAINTEXT_LISTENER). networks A network named kafka-network is created and used by all services. Having a custom network can be beneficial when services are launched by multiple Docker Compose files. This custom network can be referred by services in other compose files. volumes Each service has its own volume that will be mapped to the container\u0026rsquo;s data folder. We can check contents of the folder in the Docker volume path. More importantly data is preserved in the Docker volume unless it is deleted so that we don\u0026rsquo;t have to recreate data every time the Kafka cluster gets started. Docker volume mapping doesn\u0026rsquo;t work as expected for me with WSL 2 and Docker Desktop. Therefore, I installed Docker and Docker Compose as Linux apps on WSL 2 and start the Docker daemon as sudo service docker start. Note I only need to run the command when the system (WSL 2) boots, and I haven\u0026rsquo;t found a way to start it automatically. [UPDATE 2023-08-17] A newer version of WSL2 (0.67.6+) supports Systemd on Windows 11. I updated my WSL version (wsl --update) and was able to start Docker automatically by enabling Systemd in /etc/wsl.conf. #/etc/wsl.conf [boot] systemd=true 1# /kafka-dev-with-docker/part-01/compose-kafka.yml 2version: \u0026#34;3.5\u0026#34; 3 4services: 5 zookeeper: 6 image: bitnami/zookeeper:3.5 7 container_name: zookeeper 8 ports: 9 - \u0026#34;2181\u0026#34; 10 networks: 11 - kafkanet 12 environment: 13 - ALLOW_ANONYMOUS_LOGIN=yes 14 volumes: 15 - zookeeper_data:/bitnami/zookeeper 16 kafka-0: 17 image: bitnami/kafka:2.8.1 18 container_name: kafka-0 19 expose: 20 - 9092 21 ports: 22 - \u0026#34;29092:29092\u0026#34; 23 networks: 24 - kafkanet 25 environment: 26 - ALLOW_PLAINTEXT_LISTENER=yes 27 - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181 28 - KAFKA_CFG_BROKER_ID=0 29 - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT 30 - KAFKA_CFG_LISTENERS=INTERNAL://:9092,EXTERNAL://:29092 31 - KAFKA_CFG_ADVERTISED_LISTENERS=INTERNAL://kafka-0:9092,EXTERNAL://localhost:29092 32 - KAFKA_CFG_INTER_BROKER_LISTENER_NAME=INTERNAL 33 volumes: 34 - kafka_0_data:/bitnami/kafka 35 depends_on: 36 - zookeeper 37 kafka-1: 38 image: bitnami/kafka:2.8.1 39 container_name: kafka-1 40 expose: 41 - 9092 42 ports: 43 - \u0026#34;29093:29093\u0026#34; 44 networks: 45 - kafkanet 46 environment: 47 - ALLOW_PLAINTEXT_LISTENER=yes 48 - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181 49 - KAFKA_CFG_BROKER_ID=1 50 - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT 51 - KAFKA_CFG_LISTENERS=INTERNAL://:9092,EXTERNAL://:29093 52 - KAFKA_CFG_ADVERTISED_LISTENERS=INTERNAL://kafka-1:9092,EXTERNAL://localhost:29093 53 - KAFKA_CFG_INTER_BROKER_LISTENER_NAME=INTERNAL 54 volumes: 55 - kafka_1_data:/bitnami/kafka 56 depends_on: 57 - zookeeper 58 kafka-2: 59 image: bitnami/kafka:2.8.1 60 container_name: kafka-2 61 expose: 62 - 9092 63 ports: 64 - \u0026#34;29094:29094\u0026#34; 65 networks: 66 - kafkanet 67 environment: 68 - ALLOW_PLAINTEXT_LISTENER=yes 69 - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181 70 - KAFKA_CFG_BROKER_ID=2 71 - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT 72 - KAFKA_CFG_LISTENERS=INTERNAL://:9092,EXTERNAL://:29094 73 - KAFKA_CFG_ADVERTISED_LISTENERS=INTERNAL://kafka-2:9092,EXTERNAL://localhost:29094 74 - KAFKA_CFG_INTER_BROKER_LISTENER_NAME=INTERNAL 75 volumes: 76 - kafka_2_data:/bitnami/kafka 77 depends_on: 78 - zookeeper 79 80networks: 81 kafkanet: 82 name: kafka-network 83 84volumes: 85 zookeeper_data: 86 driver: local 87 name: zookeeper_data 88 kafka_0_data: 89 driver: local 90 name: kafka_0_data 91 kafka_1_data: 92 driver: local 93 name: kafka_1_data 94 kafka_2_data: 95 driver: local 96 name: kafka_2_data Start Containers The Kafka cluster can be started by the docker-compose up command. As the compose file has a custom name (compose-kafka.yml), we need to specify the file name with the -f flag, and the -d flag makes the containers to run in the background. We can see that it creates the network, volumes and services in order.\n1$ cd kafka-dev-with-docker/part-01 2$ docker-compose -f compose-kafka.yml up -d 3# Creating network \u0026#34;kafka-network\u0026#34; with the default driver 4# Creating volume \u0026#34;zookeeper_data\u0026#34; with local driver 5# Creating volume \u0026#34;kafka_0_data\u0026#34; with local driver 6# Creating volume \u0026#34;kafka_1_data\u0026#34; with local driver 7# Creating volume \u0026#34;kafka_2_data\u0026#34; with local driver 8# Creating zookeeper ... done 9# Creating kafka-0 ... done 10# Creating kafka-2 ... done 11# Creating kafka-1 ... done Once created, we can check the state of the containers with the docker-compose ps command.\n1$ docker-compose -f compose-kafka.yml ps 2# Name Command State Ports 3# ----------------------------------------------------------------------------------------------------------------------------- 4# kafka-0 /opt/bitnami/scripts/kafka ... Up 9092/tcp, 0.0.0.0:9093-\u0026gt;9093/tcp,:::9093-\u0026gt;9093/tcp 5# kafka-1 /opt/bitnami/scripts/kafka ... Up 9092/tcp, 0.0.0.0:9094-\u0026gt;9094/tcp,:::9094-\u0026gt;9094/tcp 6# kafka-2 /opt/bitnami/scripts/kafka ... Up 9092/tcp, 0.0.0.0:9095-\u0026gt;9095/tcp,:::9095-\u0026gt;9095/tcp 7# zookeeper /opt/bitnami/scripts/zooke ... Up 0.0.0.0:49153-\u0026gt;2181/tcp,:::49153-\u0026gt;2181/tcp, 2888/tcp, 3888/tcp, 8080/tcp Produce and Consume Messages I will demonstrate how to produce and consume messages with Kafka command utilities after entering into one of the broker containers.\nProduce Messages The command utilities locate in the /opt/bitnami/kafka/bin/ directory. After moving to that directory, we can first create a topic with kafka-topics.sh by specifying the bootstrap server, topic name, number of partitions and replication factors - the last two are optional. Once the topic is created, we can produce messages with kafka-console-producer.sh, and it can be finished by pressing Ctrl + C.\n1$ docker exec -it kafka-0 bash 2I have no name!@b04233b0bbba:/$ cd /opt/bitnami/kafka/bin/ 3## create topic 4I have no name!@b04233b0bbba:/opt/bitnami/kafka/bin$ ./kafka-topics.sh \\ 5 --bootstrap-server localhost:9092 --create \\ 6 --topic orders --partitions 3 --replication-factor 3 7# Created topic orders. 8 9## produce messages 10I have no name!@b04233b0bbba:/opt/bitnami/kafka/bin$ ./kafka-console-producer.sh \\ 11 --bootstrap-server localhost:9092 --topic orders 12\u0026gt;product: apples, quantity: 5 13\u0026gt;product: lemons, quantity: 7 14# press Ctrl + C to finish Consume Messages We can use kafka-console-consumer.sh to consume messages. In this example, it polls messages from the beginning. Again we can finish it by pressing Ctrl + C.\n1$ docker exec -it kafka-0 bash 2I have no name!@b04233b0bbba:/$ cd /opt/bitnami/kafka/bin/ 3## consume messages 4I have no name!@b04233b0bbba:/opt/bitnami/kafka/bin$ ./kafka-console-consumer.sh \\ 5 --bootstrap-server localhost:9092 --topic orders --from-beginning 6product: apples, quantity: 5 7product: lemons, quantity: 7 8# press Ctrl + C to finish Note on Data Persistence Sometimes we need to remove and recreate the Kafka containers, and it can be convenient if we can preserve data of the previous run. It is possible with the Docker volumes as data gets persisted in a later run as long as we keep using the same volumes. Note, by default, Docker Compose doesn\u0026rsquo;t remove volumes, and they remain even if we run docker-compose down. Therefore, if we recreate the containers later, data is persisted in the volumes.\nTo give additional details, below shows the volumes created by the Docker Compose file and data of one of the brokers.\n1$ docker volume ls | grep data$ 2# local kafka_0_data 3# local kafka_1_data 4# local kafka_2_data 5# local zookeeper_data 6$ sudo ls -l /var/lib/docker/volumes/kafka_0_data/_data/data 7# total 96 8# drwxr-xr-x 2 hadoop root 4096 May 3 07:59 __consumer_offsets-0 9# drwxr-xr-x 2 hadoop root 4096 May 3 07:59 __consumer_offsets-12 10 11... 12 13# drwxr-xr-x 2 hadoop root 4096 May 3 07:59 __consumer_offsets-6 14# drwxr-xr-x 2 hadoop root 4096 May 3 07:58 __consumer_offsets-9 15# -rw-r--r-- 1 hadoop root 0 May 3 07:52 cleaner-offset-checkpoint 16# -rw-r--r-- 1 hadoop root 4 May 3 07:59 log-start-offset-checkpoint 17# -rw-r--r-- 1 hadoop root 88 May 3 07:52 meta.properties 18# drwxr-xr-x 2 hadoop root 4096 May 3 07:59 orders-0 19# drwxr-xr-x 2 hadoop root 4096 May 3 07:57 orders-1 20# drwxr-xr-x 2 hadoop root 4096 May 3 07:59 orders-2 21# -rw-r--r-- 1 hadoop root 442 May 3 07:59 recovery-point-offset-checkpoint 22# -rw-r--r-- 1 hadoop root 442 May 3 07:59 replication-offset-checkpoint If you want to remove everything including the volumes, add -v flag as shown below.\n1$ docker-compose -f compose-kafka.yml down -v 2# Stopping kafka-1 ... done 3# Stopping kafka-0 ... done 4# Stopping kafka-2 ... done 5# Stopping zookeeper ... done 6# Removing kafka-1 ... done 7# Removing kafka-0 ... done 8# Removing kafka-2 ... done 9# Removing zookeeper ... done 10# Removing network kafka-network 11# Removing volume zookeeper_data 12# Removing volume kafka_0_data 13# Removing volume kafka_1_data 14# Removing volume kafka_2_data Summary In this post, we discussed how to set up a Kafka cluster with 3 brokers and a single Zookeeper node. A simple example of producing and consuming messages are illustrated. More reference implementations in relation to Kafka and related tools will be discussed in subsequent posts.\n","date":"May 4, 2023","img":"/blog/2023-05-04-kafka-development-with-docker-part-1/featured.png","lang":"en","langName":"English","largeImg":"/blog/2023-05-04-kafka-development-with-docker-part-1/featured_hu351ee075bb995e8f69034dc54fe8364b_98355_500x0_resize_box_3.png","permalink":"/blog/2023-05-04-kafka-development-with-docker-part-1/","series":[{"title":"Kafka Development With Docker","url":"/series/kafka-development-with-docker/"}],"smallImg":"/blog/2023-05-04-kafka-development-with-docker-part-1/featured_hu351ee075bb995e8f69034dc54fe8364b_98355_180x0_resize_box_3.png","tags":[{"title":"Apache Kafka","url":"/tags/apache-kafka/"},{"title":"Docker","url":"/tags/docker/"},{"title":"Docker Compose","url":"/tags/docker-compose/"}],"timestamp":1683158400,"title":"Kafka Development With Docker - Part 1 Cluster Setup"},{"categories":[{"title":"Data Streaming","url":"/categories/data-streaming/"}],"content":"Amazon Kinesis Data Streams and Amazon Managed Streaming for Apache Kafka (MSK) are two managed streaming services offered by AWS. Many resources on the web indicate Kinesis Data Streams is better when it comes to integrating with AWS services. However, it is not necessarily the case with the help of Kafka Connect. According to the documentation of Apache Kafka, Kafka Connect is a tool for scalably and reliably streaming data between Apache Kafka and other systems. It makes it simple to quickly define connectors that move large collections of data into and out of Kafka. Kafka Connect supports two types of connectors - source and sink. Source connectors are used to ingest messages from external systems into Kafka topics while messages are ingested into external systems form Kafka topics with sink connectors. In this post, I will introduce available Kafka connectors mainly for AWS services integration. Also, developing and deploying some of them will be covered in later posts.\nPart 1 Introduction (this post) Part 2 Develop Camel DynamoDB Sink Connector Part 3 Deploy Camel DynamoDB Sink Connector Part 4 Develop Aiven OpenSearch Sink Connector Part 5 Deploy Aiven OpenSearch Sink Connector Amazon As far as I\u0026rsquo;ve searched, there are two GitHub repositories by AWS. The Kinesis Kafka Connector includes sink connectors for Kinesis Data Streams and Kinesis Data Firehose. Also, recently AWS released a Kafka connector for Amazon Personalize and the project repository can be found here. The available connectors are summarised below.\nService Source Sink Kinesis ✔ Kinesis - Firehose ✔ Personalize ✔ EventBridge ✔ Note that, if we use the sink connector for Kinesis Data Firehose, we can build data pipelines to the AWS services that are supported by it, which covers S3, Redshift and OpenSearch mainly.\nThere is one more source connector by AWS Labs to be noted, although it doesn\u0026rsquo;t support integration with a specific AWS service. The Amazon MSK Data Generator is a translation of the Voluble connector, and it can be used to generate test or fake data and ingest into a Kafka topic.\nConfluent Hub Confluent is a leading provider of Apache Kafka and related services. It manages the Confluent Hub where we can discover (and/or submit) Kafka connectors for various integration use cases. Although it keeps a wide range of connectors, only less than 10 connectors can be used to integrate with AWS services reliably at the time of writing this post. Moreover, all of them except for the S3 sink connector are licensed under Commercial (Standard), which requires purchase of Confluent Platform subscription. Or it can only be used on a single broker cluster or evaluated for 30 days otherwise. Therefore, practically most of them cannot be used unless you have a subscription for the Confluent Platform.\nService Source Sink Licence S3 ✔ Free S3 ✔ Commercial (Standard) Redshift ✔ Commercial (Standard) SQS ✔ Commercial (Standard) Kinesis ✔ Commercial (Standard) DynamoDB ✔ Commercial (Standard) CloudWatch Metrics ✔ Commercial (Standard) Lambda ✔ Commercial (Standard) CloudWatch Logs ✔ Commercial (Standard) Camel Kafka Connector Apache Camel is a versatile open-source integration framework based on known Enterprise Integration Patterns. It supports Camel Kafka connectors, which allows you to use all Camel components as Kafka Connect connectors. The latest LTS version is 3.18.x (LTS), and it is supported until July 2023. Note that it works with Apache Kafka at version 2.8.0 as a dependency. In spite of the compatibility requirement, the connectors are using the Kafka Client which often is compatible with different broker versions, especially when the two versions are closer. Therefore, we can use them with a different Kafka version e.g. 2.8.1, which is recommended by Amazon MSK.\nAt the time of writing this post, there are 23 source and sink connectors that target specific AWS services - see the summary table below. Moreover, there are connectors targeting popular RDBMS, which cover MariaDB, MySQL, Oracle, PostgreSQL and MS SQL Server. Together with the Debezium connectors, we can build effective data pipelines on Amazon RDS as well. Overall Camel connectors can be quite beneficial when building real-time data pipelines on AWS.\nService Source Sink S3 ✔ S3 ✔ S3 - Streaming Upload ✔ Redshift ✔ Redshift ✔ SQS ✔ SQS ✔ SQS - FIFO ✔ SQS - Batch ✔ Kinesis ✔ Kinesis ✔ Kinesis - Firehose ✔ DynamoDB ✔ DynamoDB - Streams ✔ CloudWatch Metrics ✔ Lambda ✔ EC2 ✔ EventBridge ✔ Secrets Manager ✔ SES ✔ SNS ✔ SNS - FIFO ✔ IAM ✔ KMS ✔ Other Providers Two other vendors (Aiven and Lenses) provide Kafka connectors that target AWS services, and those are related to S3 and OpenSearch - see below. I find the OpenSearch sink connector from Aiven would be worth a close look.\nService Source Sink Provider S3 ✔ Aiven OpenSearch ✔ Aiven S3 ✔ Lenses S3 ✔ Lenses Summary Kafka Connect is a tool for scalably and reliably streaming data between Apache Kafka and other systems. It can be used to build real-time data pipelines on AWS effectively. We have discussed a range of Kafka connectors both from Amazon and 3rd-party projects. We will showcase some of them in later posts.\n","date":"May 3, 2023","img":"/blog/2023-05-03-kafka-connect-for-aws-part-1/featured.png","lang":"en","langName":"English","largeImg":"/blog/2023-05-03-kafka-connect-for-aws-part-1/featured_hu86011ae8fa75383c328c2b5f29f8b87d_22272_500x0_resize_box_3.png","permalink":"/blog/2023-05-03-kafka-connect-for-aws-part-1/","series":[{"title":"Kafka Connect for AWS Services Integration","url":"/series/kafka-connect-for-aws-services-integration/"}],"smallImg":"/blog/2023-05-03-kafka-connect-for-aws-part-1/featured_hu86011ae8fa75383c328c2b5f29f8b87d_22272_180x0_resize_box_3.png","tags":[{"title":"AWS","url":"/tags/aws/"},{"title":"Apache Kafka","url":"/tags/apache-kafka/"},{"title":"Kafka Connect","url":"/tags/kafka-connect/"}],"timestamp":1683072000,"title":"Kafka Connect for AWS Services Integration - Part 1 Introduction"},{"categories":[{"title":"General","url":"/categories/general/"}],"content":"I started blogging in 2014. At first, it was based on a simple Jekyll theme that supports posting Markdown files, which are converted from R Markdown files. Most of my work was in R at that time and the simple theme was good enough, and it was hosted via GitHub Pages. It was around 2018 when I changed my blog with a single page application, built by Vue.js and hosted on AWS. It was fun as I was teaching myself web development while building an analytics portal at work. In 2020, I paused blogging for some time while expecting a baby and restarted publishing posts to my company\u0026rsquo;s blog page from mid-2021. It is good as I can have peer-reviews and the company provides an incentive for each post published. However, it is not the right place to publish all the posts that I plan. For example, I am recommended to keep in mind e.g. how an article translates into better customer outcomes. That\u0026rsquo;s understandable but not all posts can fit into it. Currently, I am teaching myself modern data streaming architectures, and some articles could be inadequate for customers until I gain competency. Therefore, I thought I need another place that I can publish posts without worrying about undermining my company\u0026rsquo;s reputation. I\u0026rsquo;d keep publishing to the company site, and I probably repost some of them to this new blog with delay.\nI wouldn\u0026rsquo;t like to use the existing site as it misses some features, and it\u0026rsquo;d take time to add those to it. As I prefer a self-managed blog over a blog platform, I thought a static site generator and GitHub Pages would be one of the quickest options. After searching mainly Hugo, Jekyll and Pelican, Hugo caught up my eyes as it has more themes with good features. Among those I chose the Hugo Bootstrap Theme. In this post, I\u0026rsquo;ll demonstrate how I set up this blog site.\nQuick Start I used the starter template of the Hugo Bootstrap Theme. After installing Node.js (16+), I cloned the template and installed NPN packages by executing npm install. Then I changed the module name in the go.mod file into mine (i.e. module github.com/jaehyeon-kim/jaehyeon-kim.github.io). Finally, I started the site using docker-compose with the compose file shown below. Without docker, I have to install Hugo (0.97.0+) but the apt repo has an older version and I wouldn\u0026rsquo;t like to be bothered to install a supported version from source. Using docker-compose is a much easier way for me. Note it reloads the site when there is a change in the current directory so that updated contents can be checked on a browser seamlessly.\n1version: \u0026#34;3.5\u0026#34; 2 3services: 4 hugo: 5 image: klakegg/hugo:0.107.0-ext-ubuntu 6 command: server -D -F -E --poll 700ms 7 container_name: hugo 8 volumes: 9 - $PWD:/src 10 - /etc/ssl/certs:/etc/ssl/certs 11 ports: 12 - \u0026#34;1313:1313\u0026#34; Below shows the site generated by the template. As it has way more sections and menus, I reduced those by updating configurations.\nUpdate Configuration The config/_default folder contains default configurations and I mainly updated the site configuration (config.yaml), site parameters (params.yaml) and menu configuration (menu.yaml). Note they can be overridden by those in the config/production folder.\n1$ tree config 2config 3├── _default 4│ ├── author.yaml 5│ ├── config.yaml 6│ ├── languages.yaml 7│ ├── menu.yaml 8│ ├── params.yaml 9│ ├── server.yaml 10│ └── social.yaml 11└── production 12 ├── config.yaml 13 └── params.yaml Site Configuration The site configuration section includes Hugo-defined variables. I mainly updated title, copyright and taxonomies. Also, the markup config is added to override the default config of code highlight and table of contents.\n1# config/_default/config.yaml 2baseURL: / 3title: Jaehyeon 4theme: github.com/razonyang/hugo-theme-bootstrap 5copyright: \u0026#39;Copyright © 2023-{year} Jaehyeon Kim. All Rights Reserved.\u0026#39; 6defaultContentLanguage: en 7... 8taxonomies: 9 category: categories 10 series: series 11 tag: tags 12... 13markup: 14 highlight: 15 lineNos: true 16 lineNumbersInTable: false 17 noClasses: false 18 tableOfContents: 19 endLevel: 6 20 ordered: false 21 startLevel: 2 Site Parameters The site parameters section includes specific config variables for the Hugo theme. They are self-explanatory and have comments that explain further. Configuration of site parameters would require trials and errors, and we can quickly check the updated content thanks to Hugo\u0026rsquo;s hot reloading feature.\n1# config/_default/params.yaml 2mainSections: 3 - blog 4 - posts 5description: \u0026lt;description\u0026gt; 6keywords: \u0026lt;keyword-1\u0026gt;, \u0026lt;keyword-2\u0026gt; 7images: 8 - site-feature-image.png 9... 10logo: false # Disable Logo 11brand: Jaehyeon Kim 12palette: blue-gray 13color: light # light, dark or auto. Default to auto. 14... 15googleAdsense: \u0026lt;AdSense Publisher ID\u0026gt; 16math: true # Enable math globally. 17toc: true # Disable TOC globally. 18tocPosition: content # sidebar or content 19... 20searchBar: true # disable search-bar 21poweredBy: false # Whether to show powered by. 22readingTime: true # Whether to display the reading time. 23postDate: true # Whether to display the post date in the post meta section. 24... 25sidebarTaxonomies: [categories, tags, series] # The order of taxonomies on the sidebar. 26... 27codeBlock: 28 maxLines: 20 29 lineNos: true # true/false represents that show/hide the line numbers by default. 30... 31post: 32 excerpt: description 33 excerptMaxLength: 200 34 copyright: false # Whether to display copyright section on each post. 35 featuredImage: true # Show the featured image above the content. 36 numberifyHeadings: false # Count headings automatically. 37 numberifyHeadingsSeparator: . # The separator between of number and headings. 38 imageTitleAsCaption: true 39... 40search: 41 paginate: 5 # Pagination. Default to 10. 42 fuse: 43 threshold: 0.1 44... 45topAppBar: 46 social: 47 github: \u0026lt;GitHub User ID\u0026gt; 48 linkedin: \u0026lt;LinkedIn ID\u0026gt; 49 paypal: \u0026lt;Paypal.Me Link ID\u0026gt; Menu Configuration For now, I only need the blog menu and all other items are removed from the menu configuration.\n1# config/_default/menu.yaml 2main: 3 - name: Blog 4 identifier: blog 5 params: 6 icon: \u0026#39;\u0026lt;i class=\u0026#34;fas fa-fw fa-blog text-warning\u0026#34;\u0026gt;\u0026lt;/i\u0026gt;\u0026#39; 7 description: Production Configuration As mentioned, the production configuration can be overridden. I\u0026rsquo;ll host it using a custom domain, and the base URL is updated accordingly. I added the Google Analytics measurement ID as well, and it\u0026rsquo;ll ensure the site activities are tracked only when it is deployed.\n1# config/production/config.yaml 2baseURL: https://jaehyeon.me/ 3googleAnalytics: \u0026lt;Measurement ID\u0026gt; Content Configuration Contents of a post are kept in a subfolder of the content/blog folder - e.g. content/blog/2023-04-27-self-hosted-blog. The article is written in index.md, and the post folder includes figures that can be referenced within the article. Note featured.png will be showing at the top of the post page as well as in the post list - see the featured image configuration for details.\n1$ tree content/blog/2023-04-24-self-hosted-blog/ 2content/blog/2023-04-24-self-hosted-blog/ 3├── after.png 4├── before.png 5├── comment-1.png 6├── comment-2.png 7├── comment-3.png 8├── comment-4.png 9├── custom-domain-1.png 10├── custom-domain-2.png 11├── custom-domain-3.png 12├── discussion-1.png 13├── discussion-2.png 14├── featured.png 15├── giscus-1.png 16├── giscus-2.png 17└── index.md The page parameters can be added at the top. It allows you to configure post components (e.g. comment, toc, carousel, \u0026hellip;), visibility (e.g. draft) etc.\n1--- 2title: Self-managed Blog with Hugo and GitHub Pages 3date: 2023-04-24 4draft: true 5featured: false 6comment: true 7toc: true 8reward: false 9pinned: false 10carousel: false 11featuredImage: false 12# series: 13# - 14categories: 15 - Blog 16tags: 17 - Hugo 18 - Bootstrap 19 - GitHub Pages 20authors: 21 - JaehyeonKim 22images: [] 23description: \u0026lt;description\u0026gt; 24--- 25 26\u0026lt;article contents\u0026gt; Once created, we can check it appears in the post list as shown below. We can visit the post by clicking the title or the Read More button.\nConfigure Custom Domain The GitHub Pages has a good documentation about custom domain configuration. I\u0026rsquo;ll sketch how I set it up so that my custom domain (jaehyeon.me) is used to serve the site.\nCreate ACM Certificate I bought the domain from Route53, and a public hosted zone is created automatically. In order to serve the site via HTTPS, an ACM certificate is create for the domain and all of its subdomains. We can create a certificate by CloudFormation, and domain validation can be automated via DNS validation.\n1AWSTemplateFormatVersion: \u0026#39;2010-09-09\u0026#39; 2Description: Create an ACM certificate for a domain and all of its subdomains. 3Parameters: 4 DomainName: 5 Description: Fully qualified domain name (eg example.com) 6 Type: String 7 HostedZoneId: 8 Description: Route53 hosted zone id 9 Type: String 10Resources: 11 ACMCertificate: 12 Type: AWS::CertificateManager::Certificate 13 Properties: 14 DomainName: Ref: DomainName 15 DomainValidationOptions: 16 - DomainName: Ref: DomainName 17 HostedZoneId: Ref: HostedZoneId 18 SubjectAlternativeNames: 19 - !Sub \u0026#34;*.${DomainName}\u0026#34; 20 ValidationMethod: DNS 21Outputs: 22 ACMCertificateArn: 23 Value: 24 Ref: ACMCertificate Manage Custom Domain I plan to configure the apex domain (jaehyeon.me) primarily and added the A and AAAA records to the hosted zone as indicated in the documentation. Also, the WWW subdomain (www.jaehyeon.me) is configured by adding a CNAME record to it. In this way, if someone visits the site by www.jaehyeon.me, it\u0026rsquo;ll be redirected to jaehyeon.me.\nI saved the custom domain in the repository setting, and it is checked successfully. Note I do not enforce HTTPS as Google AdSense fails to verify the ads.txt file when it was enforced.\nVerify Custom Domain The GitHub Pages recommends verifying the custom domain in order to increase security by avoiding takeover attacks. The domain is verified as shown below.\nDeployment The starter template has a GitHub workflow (.github/workflows/gh-pages.yml). It builds the site and deploys the ./public folder to the remote gh-pages branch. Note GitHub Pages should be linked to the gh-pages branch.\nI added three additional files to the ./public folder so that they are found in the deployed site. CNAME includes the custom domain name (jaehyeon.me) so that it keeps being linked to the site. ads.txt is required by Google AdSense, and site-feature-image.png is as per the site parameter configuration.\n1$ tree public 2public 3├── CNAME 4├── ads.txt 5└── site-feature-image.png Once it is deployed, we can visit the site using the custom domain.\n(Optional) Comment Widget While it supports multiple comment widgets, I decided to use Giscus, which is a lightweight comment widget built on GitHub discussions.\nI first enabled Discussions in the repository Settings as shown below.\nAfter that the Discussions tab is showing at the top, and we can see multiple discussion categories in that menu. As shown below, the widget requires a category where comments are saved, and I chose the General category.\nThen we need to install the widget as a GitHub app, and it can be installed by visiting its GitHub App page.\nI only selected the repository for the blog site and clicked the Install button.\nThe Giscus configuration can be found in the site parameter configuration. It requires the repository name/ID and category ID.\n1# config/_default/params.yaml 2... 3giscus: 4 repo: \u0026lt;repo-owner\u0026gt;/\u0026lt;repository-name\u0026gt; 5 repoId: \u0026#34;\u0026lt;repository-id\u0026gt;\u0026#34; 6 # category: \u0026#34;\u0026#34; 7 categoryId: \u0026#34;\u0026lt;catetory-id\u0026gt;\u0026#34; 8 # theme: \u0026#34;dark\u0026#34; # Default to auto. 9 # mapping: \u0026#34;title\u0026#34; # Default to pathname. 10 inputPosition: \u0026#34;bottom\u0026#34; # Default to top. 11 reactions: true # Disable reactions. 12 metadata: true # Emit discussion metadata. 13 # lang: \u0026#34;en\u0026#34; # Specify language, default to site language. 14 # lazyLoading: false # Default to true. 15... The repository and category IDs can be obtained from the GitHub GraphQL API Explorer. In the query, it is limited to show only the first one category, and you may increase the number if the category you want to use is not queried.\n1query { 2 repository(owner: \u0026#34;\u0026lt;repo-owner\u0026gt;\u0026#34;, name: \u0026#34;\u0026lt;repository-name\u0026gt;\u0026#34;) { 3 id # RepositoryID 4 name 5 discussionCategories(first: 1) { 6 nodes { 7 id # CategoryID 8 name 9 } 10 } 11 } 12} 1{ 2 \u0026#34;data\u0026#34;: { 3 \u0026#34;repository\u0026#34;: { 4 \u0026#34;id\u0026#34;: \u0026#34;\u0026lt;repository-id\u0026gt;\u0026#34;, 5 \u0026#34;name\u0026#34;: \u0026#34;\u0026lt;repository-name\u0026gt;\u0026#34;, 6 \u0026#34;discussionCategories\u0026#34;: { 7 \u0026#34;nodes\u0026#34;: [ 8 { 9 \u0026#34;id\u0026#34;: \u0026#34;\u0026lt;catetory-id\u0026gt;\u0026#34;, 10 \u0026#34;name\u0026#34;: \u0026#34;General\u0026#34; 11 } 12 ] 13 } 14 } 15 } 16} Once the configuration is updated, we can see the Comments section appears correctly. We first need to sign in with GitHub to leave a comment.\nOnce signed in, the text area is enabled, and we can leave a comment.\nThe comment can also be checked on the Discussion \u0026gt; General of the repository and a reply can be made from it.\nOn the post page, we can see the reply\nUpdates 2023-05-28 The Hugo version is updated to 0.112.3, and the following changes are made.\nA new docker images is used as the existing one doesn\u0026rsquo;t support the updated Hugo version. As a result, the docker compose file is updated as following. 1version: \u0026#34;3.5\u0026#34; 2 3services: 4 hugo: 5 image: peaceiris/hugo:v0.112.3-full 6 command: server -D -F -E --poll 700ms --bind=0.0.0.0 7 container_name: hugo 8 volumes: 9 - $PWD:/src 10 - /etc/ssl/certs:/etc/ssl/certs 11 ports: 12 - \u0026#34;1313:1313\u0026#34; The template module is updated to v1.0.2. As I don\u0026rsquo;t have go installed, it is updated inside the new docker image. 1$ docker run --rm -it -v $PWD:/src --entrypoint bash peaceiris/hugo:v0.112.3-full 2## inside docker 3root@\u0026lt;container-id\u0026gt;:/src# hugo mod get github.com/razonyang/hugo-theme-bootstrap@v1.0.2 4root@\u0026lt;container-id\u0026gt;:/src# hugo mod npm pack 5root@\u0026lt;container-id\u0026gt;:/src# npm update 6## the changes will be committed on the host outside docker 7root@\u0026lt;container-id\u0026gt;:/src# exit 8$ git add go.mod go.sum package.json package-lock.json 9$ git commit -m \u0026#39;update them to v1.0.2\u0026#39; ","date":"April 24, 2023","img":"/blog/2023-04-24-self-hosted-blog/featured.png","lang":"en","langName":"English","largeImg":"/blog/2023-04-24-self-hosted-blog/featured_hu1fddab158d55343a3c1c024afda22657_41850_500x0_resize_box_3.png","permalink":"/blog/2023-04-24-self-hosted-blog/","series":[],"smallImg":"/blog/2023-04-24-self-hosted-blog/featured_hu1fddab158d55343a3c1c024afda22657_41850_180x0_resize_box_3.png","tags":[{"title":"Hugo","url":"/tags/hugo/"},{"title":"Bootstrap","url":"/tags/bootstrap/"},{"title":"GitHub Pages","url":"/tags/github-pages/"}],"timestamp":1682294400,"title":"Self-Managed Blog With Hugo and GitHub Pages"},{"categories":[{"title":"Data Streaming","url":"/categories/data-streaming/"}],"content":"As Kafka producer and consumer apps are decoupled, they operate on Kafka topics rather than communicating with each other directly. As described in the Confluent document, Schema Registry provides a centralized repository for managing and validating schemas for topic message data, and for serialization and deserialization of the data over the network. Producers and consumers to Kafka topics can use schemas to ensure data consistency and compatibility as schemas evolve. In AWS, the Glue Schema Registry supports features to manage and enforce schemas on data streaming applications using convenient integrations with Apache Kafka, Amazon Managed Streaming for Apache Kafka, Amazon Kinesis Data Streams, Amazon Kinesis Data Analytics for Apache Flink, and AWS Lambda. In this post, we will discuss how to integrate Python Kafka producer and consumer apps In AWS Lambda with the Glue Schema Registry.\nArchitecture Fake online order data is generated by multiple Lambda functions that are invoked by an EventBridge schedule rule. The schedule is set to run every minute and the associating rule has a configurable number (e.g. 5) of targets. Each target points to the same Kafka producer Lambda function. In this way we are able to generate test data using multiple Lambda functions according to the desired volume of messages. Note that, before sending a message, the producer validates the schema and registers a new one if it is not registered yet. Then it serializes the message and sends it to the cluster.\nOnce messages are sent to a Kafka topic, they can be consumed by Lambda where Amazon MSK is configured as an event source. The serialized record (message key or value) includes the schema ID so that the consumer can request the schema from the schema registry (if not cached) in order to deserialize it.\nThe infrastructure is built by Terraform and the AWS SAM CLI is used to develop the producer Lambda function locally before deploying to AWS.\nInfrastructure A VPC with 3 public and private subnets is created using the AWS VPC Terraform module (vpc.tf). Also, a SoftEther VPN server is deployed in order to access the resources in the private subnets from the developer machine (vpn.tf). It is particularly useful to monitor and manage the MSK cluster and Kafka topic as well as developing the Kafka producer Lambda function locally. The details about how to configure the VPN server can be found in an earlier post. The source can be found in the GitHub repository of this post.\nMSK An MSK cluster with 3 brokers is created. The broker nodes are deployed with the kafka.m5.large instance type in private subnets and IAM authentication is used for the client authentication method. Finally, additional server configurations are added such as enabling auto creation of topics and topic deletion.\n1# glue-schema-registry/infra/variable.tf 2locals { 3 ... 4 msk = { 5 version = \u0026#34;3.3.1\u0026#34; 6 instance_size = \u0026#34;kafka.m5.large\u0026#34; 7 ebs_volume_size = 20 8 log_retention_ms = 604800000 # 7 days 9 } 10 ... 11} 12# glue-schema-registry/infra/msk.tf 13resource \u0026#34;aws_msk_cluster\u0026#34; \u0026#34;msk_data_cluster\u0026#34; { 14 cluster_name = \u0026#34;${local.name}-msk-cluster\u0026#34; 15 kafka_version = local.msk.version 16 number_of_broker_nodes = length(module.vpc.private_subnets) 17 configuration_info { 18 arn = aws_msk_configuration.msk_config.arn 19 revision = aws_msk_configuration.msk_config.latest_revision 20 } 21 22 broker_node_group_info { 23 instance_type = local.msk.instance_size 24 client_subnets = module.vpc.private_subnets 25 security_groups = [aws_security_group.msk.id] 26 storage_info { 27 ebs_storage_info { 28 volume_size = local.msk.ebs_volume_size 29 } 30 } 31 } 32 33 client_authentication { 34 sasl { 35 iam = true 36 } 37 } 38 39 logging_info { 40 broker_logs { 41 cloudwatch_logs { 42 enabled = true 43 log_group = aws_cloudwatch_log_group.msk_cluster_lg.name 44 } 45 s3 { 46 enabled = true 47 bucket = aws_s3_bucket.default_bucket.id 48 prefix = \u0026#34;logs/msk/cluster-\u0026#34; 49 } 50 } 51 } 52 53 tags = local.tags 54 55 depends_on = [aws_msk_configuration.msk_config] 56} 57 58resource \u0026#34;aws_msk_configuration\u0026#34; \u0026#34;msk_config\u0026#34; { 59 name = \u0026#34;${local.name}-msk-configuration\u0026#34; 60 61 kafka_versions = [local.msk.version] 62 63 server_properties = \u0026lt;\u0026lt;PROPERTIES 64 auto.create.topics.enable = true 65 delete.topic.enable = true 66 log.retention.ms = ${local.msk.log_retention_ms} 67 PROPERTIES 68} Security Groups Two security groups are created - one for the MSK cluster and the other for the Lambda apps.\nThe inbound/outbound rules of the former are created for accessing the cluster by\nEvent Source Mapping (ESM) for Lambda This is for the Lambda consumer that subscribes the MSK cluster. As described in the AWS re:Post doc, when a Lambda function is configured with an Amazon MSK trigger or a self-managed Kafka trigger, an ESM resource is automatically created. An ESM is separate from the Lambda function, and it continuously polls records from the topic in the Kafka cluster. The ESM bundles those records into a payload. Then, it calls the Lambda Invoke API to deliver the payload to your Lambda function for processing. Note it doesn\u0026rsquo;t inherit the VPC network settings of the Lambda function but uses the subnet and security group settings that are configured on the target MSK cluster. Therefore, the MSK cluster\u0026rsquo;s security group must include a rule that grants ingress traffic from itself and egress traffic to itself. For us, the rules on port 9098 as the cluster only supports IAM authentication. Also, an additional egress rule is created to access the Glue Schema Registry. Other Resources Two ingress rules are created for the VPN server and Lambda. The latter is only for the Lambda producer because the consumer doesn\u0026rsquo;t rely on the Lambda network setting. The second security group is created here, while the Lambda function is created in a different Terraform stack. This is for ease of adding it to the inbound rule of the MSK\u0026rsquo;s security group. Later we will discuss how to make use of it with the Lambda function. The outbound rule allows all outbound traffic although only port 9098 for the MSK cluster and 443 for the Glue Schema Registry would be sufficient.\n1# glue-schema-registry/infra/msk.tf 2resource \u0026#34;aws_security_group\u0026#34; \u0026#34;msk\u0026#34; { 3 name = \u0026#34;${local.name}-msk-sg\u0026#34; 4 vpc_id = module.vpc.vpc_id 5 6 lifecycle { 7 create_before_destroy = true 8 } 9 10 tags = local.tags 11} 12 13# for lambda event source mapping 14resource \u0026#34;aws_security_group_rule\u0026#34; \u0026#34;msk_ingress_self_broker\u0026#34; { 15 type = \u0026#34;ingress\u0026#34; 16 description = \u0026#34;msk ingress self\u0026#34; 17 security_group_id = aws_security_group.msk.id 18 protocol = \u0026#34;tcp\u0026#34; 19 from_port = 9098 20 to_port = 9098 21 source_security_group_id = aws_security_group.msk.id 22} 23 24resource \u0026#34;aws_security_group_rule\u0026#34; \u0026#34;msk_egress_self_broker\u0026#34; { 25 type = \u0026#34;egress\u0026#34; 26 description = \u0026#34;msk egress self\u0026#34; 27 security_group_id = aws_security_group.msk.id 28 protocol = \u0026#34;tcp\u0026#34; 29 from_port = 9098 30 to_port = 9098 31 source_security_group_id = aws_security_group.msk.id 32} 33 34resource \u0026#34;aws_security_group_rule\u0026#34; \u0026#34;msk_all_outbound\u0026#34; { 35 type = \u0026#34;egress\u0026#34; 36 description = \u0026#34;allow outbound all\u0026#34; 37 security_group_id = aws_security_group.msk.id 38 protocol = \u0026#34;-1\u0026#34; 39 from_port = \u0026#34;0\u0026#34; 40 to_port = \u0026#34;0\u0026#34; 41 cidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;] 42} 43 44# for other resources 45resource \u0026#34;aws_security_group_rule\u0026#34; \u0026#34;msk_vpn_inbound\u0026#34; { 46 count = local.vpn.to_create ? 1 : 0 47 type = \u0026#34;ingress\u0026#34; 48 description = \u0026#34;VPN access\u0026#34; 49 security_group_id = aws_security_group.msk.id 50 protocol = \u0026#34;tcp\u0026#34; 51 from_port = 9098 52 to_port = 9098 53 source_security_group_id = aws_security_group.vpn[0].id 54} 55 56resource \u0026#34;aws_security_group_rule\u0026#34; \u0026#34;msk_lambda_inbound\u0026#34; { 57 type = \u0026#34;ingress\u0026#34; 58 description = \u0026#34;lambda access\u0026#34; 59 security_group_id = aws_security_group.msk.id 60 protocol = \u0026#34;tcp\u0026#34; 61 from_port = 9098 62 to_port = 9098 63 source_security_group_id = aws_security_group.kafka_app_lambda.id 64} 65 66... 67 68# lambda security group 69resource \u0026#34;aws_security_group\u0026#34; \u0026#34;kafka_app_lambda\u0026#34; { 70 name = \u0026#34;${local.name}-lambda-msk-access\u0026#34; 71 vpc_id = module.vpc.vpc_id 72 73 lifecycle { 74 create_before_destroy = true 75 } 76 77 tags = local.tags 78} 79 80resource \u0026#34;aws_security_group_rule\u0026#34; \u0026#34;kafka_app_lambda_msk_egress\u0026#34; { 81 type = \u0026#34;egress\u0026#34; 82 description = \u0026#34;allow outbound all\u0026#34; 83 security_group_id = aws_security_group.kafka_app_lambda.id 84 protocol = \u0026#34;-1\u0026#34; 85 from_port = 0 86 to_port = 0 87 cidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;] 88} Kafka Apps The resources related to the Kafka producer and consumer Lambda functions are managed in a separate Terraform stack. This is because it is easier to build the relevant resources iteratively. Note the SAM CLI builds the whole Terraform stack even for a small change of code, and it wouldn’t be convenient if the entire resources are managed in the same stack.\nProducer App Order Data Fake order data is generated using the Faker package and the dataclasses_avroschema package is used to automatically generate the Avro schema according to its attributes. A mixin class called InjectCompatMixin is injected into the Order class, which specifies a schema compatibility mode into the generated schema. The auto() class method is used to instantiate the class automatically. Finally, the OrderMore class is created for the schema evolution demo, which will be discussed later.\n1# glue-schema-registry/app/producer/src/order.py 2import datetime 3import string 4import json 5import typing 6import dataclasses 7import enum 8 9from faker import Faker 10from dataclasses_avroschema import AvroModel 11 12 13class Compatibility(enum.Enum): 14 NONE = \u0026#34;NONE\u0026#34; 15 DISABLED = \u0026#34;DISABLED\u0026#34; 16 BACKWARD = \u0026#34;BACKWARD\u0026#34; 17 BACKWARD_ALL = \u0026#34;BACKWARD_ALL\u0026#34; 18 FORWARD = \u0026#34;FORWARD\u0026#34; 19 FORWARD_ALL = \u0026#34;FORWARD_ALL\u0026#34; 20 FULL = \u0026#34;FULL\u0026#34; 21 FULL_ALL = \u0026#34;FULL_ALL\u0026#34; 22 23 24class InjectCompatMixin: 25 @classmethod 26 def updated_avro_schema_to_python(cls, compat: Compatibility = Compatibility.BACKWARD): 27 schema = cls.avro_schema_to_python() 28 schema[\u0026#34;compatibility\u0026#34;] = compat.value 29 return schema 30 31 @classmethod 32 def updated_avro_schema(cls, compat: Compatibility = Compatibility.BACKWARD): 33 schema = cls.updated_avro_schema_to_python(compat) 34 return json.dumps(schema) 35 36 37@dataclasses.dataclass 38class OrderItem(AvroModel): 39 product_id: int 40 quantity: int 41 42 43@dataclasses.dataclass 44class Order(AvroModel, InjectCompatMixin): 45 \u0026#34;Online fake order item\u0026#34; 46 order_id: str 47 ordered_at: datetime.datetime 48 user_id: str 49 order_items: typing.List[OrderItem] 50 51 class Meta: 52 namespace = \u0026#34;Order V1\u0026#34; 53 54 def asdict(self): 55 return dataclasses.asdict(self) 56 57 @classmethod 58 def auto(cls, fake: Faker = Faker()): 59 rand_int = fake.random_int(1, 1000) 60 user_id = \u0026#34;\u0026#34;.join( 61 [string.ascii_lowercase[int(s)] if s.isdigit() else s for s in hex(rand_int)] 62 )[::-1] 63 order_items = [ 64 OrderItem(fake.random_int(1, 9999), fake.random_int(1, 10)) 65 for _ in range(fake.random_int(1, 4)) 66 ] 67 return cls(fake.uuid4(), datetime.datetime.utcnow(), user_id, order_items) 68 69 def create(self, num: int): 70 return [self.auto() for _ in range(num)] 71 72 73@dataclasses.dataclass 74class OrderMore(Order): 75 is_prime: bool 76 77 @classmethod 78 def auto(cls, fake: Faker = Faker()): 79 o = Order.auto() 80 return cls(o.order_id, o.ordered_at, o.user_id, o.order_items, fake.pybool()) The generated schema of the Order class can be found below.\n1{ 2\t\u0026#34;doc\u0026#34;: \u0026#34;Online fake order item\u0026#34;, 3\t\u0026#34;namespace\u0026#34;: \u0026#34;Order V1\u0026#34;, 4\t\u0026#34;name\u0026#34;: \u0026#34;Order\u0026#34;, 5\t\u0026#34;compatibility\u0026#34;: \u0026#34;BACKWARD\u0026#34;, 6\t\u0026#34;type\u0026#34;: \u0026#34;record\u0026#34;, 7\t\u0026#34;fields\u0026#34;: [ 8\t{ 9\t\u0026#34;name\u0026#34;: \u0026#34;order_id\u0026#34;, 10\t\u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; 11\t}, 12\t{ 13\t\u0026#34;name\u0026#34;: \u0026#34;ordered_at\u0026#34;, 14\t\u0026#34;type\u0026#34;: { 15\t\u0026#34;type\u0026#34;: \u0026#34;long\u0026#34;, 16\t\u0026#34;logicalType\u0026#34;: \u0026#34;timestamp-millis\u0026#34; 17\t} 18\t}, 19\t{ 20\t\u0026#34;name\u0026#34;: \u0026#34;user_id\u0026#34;, 21\t\u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; 22\t}, 23\t{ 24\t\u0026#34;name\u0026#34;: \u0026#34;order_items\u0026#34;, 25\t\u0026#34;type\u0026#34;: { 26\t\u0026#34;type\u0026#34;: \u0026#34;array\u0026#34;, 27\t\u0026#34;items\u0026#34;: { 28\t\u0026#34;type\u0026#34;: \u0026#34;record\u0026#34;, 29\t\u0026#34;name\u0026#34;: \u0026#34;OrderItem\u0026#34;, 30\t\u0026#34;fields\u0026#34;: [ 31\t{ 32\t\u0026#34;name\u0026#34;: \u0026#34;product_id\u0026#34;, 33\t\u0026#34;type\u0026#34;: \u0026#34;long\u0026#34; 34\t}, 35\t{ 36\t\u0026#34;name\u0026#34;: \u0026#34;quantity\u0026#34;, 37\t\u0026#34;type\u0026#34;: \u0026#34;long\u0026#34; 38\t} 39\t] 40\t}, 41\t\u0026#34;name\u0026#34;: \u0026#34;order_item\u0026#34; 42\t} 43\t} 44\t] 45} Below shows an example order record.\n1{ 2 \u0026#34;order_id\u0026#34;: \u0026#34;53263c42-81b3-4a53-8067-fcdb44fa5479\u0026#34;, 3 \u0026#34;ordered_at\u0026#34;: 1680745813045, 4 \u0026#34;user_id\u0026#34;: \u0026#34;dicxa\u0026#34;, 5 \u0026#34;order_items\u0026#34;: [ 6 { 7 \u0026#34;product_id\u0026#34;: 5947, 8 \u0026#34;quantity\u0026#34;: 8 9 } 10 ] 11} Producer The aws-glue-schema-registry package is used serialize the value of order messages. It provides the KafkaSerializer class that validates, registers and serializes the relevant records. It supports Json and Avro schemas, and we can add it to the value_serializer argument of the KafkaProducer class. By default, the schemas are named as \u0026lt;topic\u0026gt;-key and \u0026lt;topic\u0026gt;-value and it can be changed by updating the schema_naming_strategy argument. Note that, when sending a message, the value should be a tuple of data and schema. Note also that the stable version of the kafka-python package does not support the IAM authentication method. Therefore, we need to install the package from a forked repository as discussed in this GitHub issue.\n1# glue-schema-registry/app/producer/src/producer.py 2import os 3import datetime 4import json 5import typing 6 7import boto3 8import botocore.exceptions 9from kafka import KafkaProducer 10from aws_schema_registry import SchemaRegistryClient 11from aws_schema_registry.avro import AvroSchema 12from aws_schema_registry.adapter.kafka import KafkaSerializer 13from aws_schema_registry.exception import SchemaRegistryException 14 15from .order import Order 16 17 18class Producer: 19 def __init__(self, bootstrap_servers: list, topic: str, registry: str, is_local: bool = False): 20 self.bootstrap_servers = bootstrap_servers 21 self.topic = topic 22 self.registry = registry 23 self.glue_client = boto3.client( 24 \u0026#34;glue\u0026#34;, region_name=os.getenv(\u0026#34;AWS_DEFAULT_REGION\u0026#34;, \u0026#34;ap-southeast-2\u0026#34;) 25 ) 26 self.is_local = is_local 27 self.producer = self.create() 28 29 @property 30 def serializer(self): 31 client = SchemaRegistryClient(self.glue_client, registry_name=self.registry) 32 return KafkaSerializer(client) 33 34 def create(self): 35 params = { 36 \u0026#34;bootstrap_servers\u0026#34;: self.bootstrap_servers, 37 \u0026#34;key_serializer\u0026#34;: lambda v: json.dumps(v, default=self.serialize).encode(\u0026#34;utf-8\u0026#34;), 38 \u0026#34;value_serializer\u0026#34;: self.serializer, 39 } 40 if not self.is_local: 41 params = { 42 **params, 43 **{\u0026#34;security_protocol\u0026#34;: \u0026#34;SASL_SSL\u0026#34;, \u0026#34;sasl_mechanism\u0026#34;: \u0026#34;AWS_MSK_IAM\u0026#34;}, 44 } 45 return KafkaProducer(**params) 46 47 def send(self, orders: typing.List[Order], schema: AvroSchema): 48 if not self.check_registry(): 49 print(f\u0026#34;registry not found, create {self.registry}\u0026#34;) 50 self.create_registry() 51 52 for order in orders: 53 data = order.asdict() 54 try: 55 self.producer.send( 56 self.topic, key={\u0026#34;order_id\u0026#34;: data[\u0026#34;order_id\u0026#34;]}, value=(data, schema) 57 ) 58 except SchemaRegistryException as e: 59 raise RuntimeError(\u0026#34;fails to send a message\u0026#34;) from e 60 self.producer.flush() 61 62 def serialize(self, obj): 63 if isinstance(obj, datetime.datetime): 64 return obj.isoformat() 65 if isinstance(obj, datetime.date): 66 return str(obj) 67 return obj 68 69 def check_registry(self): 70 try: 71 self.glue_client.get_registry(RegistryId={\u0026#34;RegistryName\u0026#34;: self.registry}) 72 return True 73 except botocore.exceptions.ClientError as e: 74 if e.response[\u0026#34;Error\u0026#34;][\u0026#34;Code\u0026#34;] == \u0026#34;EntityNotFoundException\u0026#34;: 75 return False 76 else: 77 raise e 78 79 def create_registry(self): 80 try: 81 self.glue_client.create_registry(RegistryName=self.registry) 82 return True 83 except botocore.exceptions.ClientError as e: 84 if e.response[\u0026#34;Error\u0026#34;][\u0026#34;Code\u0026#34;] == \u0026#34;AlreadyExistsException\u0026#34;: 85 return True 86 else: 87 raise e Lambda Handler The Lambda function sends 100 records at a time followed by sleeping for 1 second. It repeats until it reaches MAX_RUN_SEC (e.g. 60) environment variable value. The last conditional block is for demonstrating a schema evolution example, which will be discussed later.\n1# glue-schema-registry/app/producer/lambda_handler.py 2import os 3import datetime 4import time 5 6from aws_schema_registry.avro import AvroSchema 7 8from src.order import Order, OrderMore, Compatibility 9from src.producer import Producer 10 11 12def lambda_function(event, context): 13 producer = Producer( 14 bootstrap_servers=os.environ[\u0026#34;BOOTSTRAP_SERVERS\u0026#34;].split(\u0026#34;,\u0026#34;), 15 topic=os.environ[\u0026#34;TOPIC_NAME\u0026#34;], 16 registry=os.environ[\u0026#34;REGISTRY_NAME\u0026#34;], 17 ) 18 s = datetime.datetime.now() 19 ttl_rec = 0 20 while True: 21 orders = Order.auto().create(100) 22 schema = AvroSchema(Order.updated_avro_schema(Compatibility.BACKWARD)) 23 producer.send(orders, schema) 24 ttl_rec += len(orders) 25 print(f\u0026#34;sent {len(orders)} messages\u0026#34;) 26 elapsed_sec = (datetime.datetime.now() - s).seconds 27 if elapsed_sec \u0026gt; int(os.getenv(\u0026#34;MAX_RUN_SEC\u0026#34;, \u0026#34;60\u0026#34;)): 28 print(f\u0026#34;{ttl_rec} records are sent in {elapsed_sec} seconds ...\u0026#34;) 29 break 30 time.sleep(1) 31 32 33if __name__ == \u0026#34;__main__\u0026#34;: 34 producer = Producer( 35 bootstrap_servers=os.environ[\u0026#34;BOOTSTRAP_SERVERS\u0026#34;].split(\u0026#34;,\u0026#34;), 36 topic=os.environ[\u0026#34;TOPIC_NAME\u0026#34;], 37 registry=os.environ[\u0026#34;REGISTRY_NAME\u0026#34;], 38 is_local=True, 39 ) 40 use_more = os.getenv(\u0026#34;USE_MORE\u0026#34;) is not None 41 if not use_more: 42 orders = Order.auto().create(1) 43 schema = AvroSchema(Order.updated_avro_schema(Compatibility.BACKWARD)) 44 else: 45 orders = OrderMore.auto().create(1) 46 schema = AvroSchema(OrderMore.updated_avro_schema(Compatibility.BACKWARD)) 47 print(orders) 48 producer.send(orders, schema) Lambda Resource The VPC, subnets, Lambda security group and MSK cluster are created in the infra Terraform stack, and they need to be obtained from the Kafka app stack. It can be achieved using the Terraform data sources as shown below. Note that the private subnets can be filtered by a specific tag (Tier: Private), which is added when creating them.\n1# glue-schema-registry/infra/vpc.tf 2module \u0026#34;vpc\u0026#34; { 3 source = \u0026#34;terraform-aws-modules/vpc/aws\u0026#34; 4 version = \u0026#34;~\u0026gt; 3.14\u0026#34; 5 6 name = \u0026#34;${local.name}-vpc\u0026#34; 7 cidr = local.vpc.cidr 8 9 azs = local.vpc.azs 10 public_subnets = [for k, v in local.vpc.azs : cidrsubnet(local.vpc.cidr, 3, k)] 11 private_subnets = [for k, v in local.vpc.azs : cidrsubnet(local.vpc.cidr, 3, k + 3)] 12 13 ... 14 15 private_subnet_tags = { 16 \u0026#34;Tier\u0026#34; = \u0026#34;Private\u0026#34; 17 } 18 19 tags = local.tags 20} 21 22# glue-schema-registry/app/variables.tf 23data \u0026#34;aws_caller_identity\u0026#34; \u0026#34;current\u0026#34; {} 24 25data \u0026#34;aws_region\u0026#34; \u0026#34;current\u0026#34; {} 26 27data \u0026#34;aws_vpc\u0026#34; \u0026#34;selected\u0026#34; { 28 filter { 29 name = \u0026#34;tag:Name\u0026#34; 30 values = [\u0026#34;${local.infra_prefix}\u0026#34;] 31 } 32} 33 34data \u0026#34;aws_subnets\u0026#34; \u0026#34;private\u0026#34; { 35 filter { 36 name = \u0026#34;vpc-id\u0026#34; 37 values = [data.aws_vpc.selected.id] 38 } 39 40 tags = { 41 Tier = \u0026#34;Private\u0026#34; 42 } 43} 44 45data \u0026#34;aws_msk_cluster\u0026#34; \u0026#34;msk_data_cluster\u0026#34; { 46 cluster_name = \u0026#34;${local.infra_prefix}-msk-cluster\u0026#34; 47} 48 49data \u0026#34;aws_security_group\u0026#34; \u0026#34;kafka_producer_lambda\u0026#34; { 50 name = \u0026#34;${local.infra_prefix}-lambda-msk-access\u0026#34; 51} 52 53 54locals { 55 ... 56 infra_prefix = \u0026#34;glue-schema-registry\u0026#34; 57 ... 58} The AWS Lambda Terraform module is used to create the producer Lambda function. Note that, in order to develop a Lambda function using AWS SAM, we need to create SAM metadata resource, which provides the AWS SAM CLI with the information it needs to locate Lambda functions and layers, along with their source code, build dependencies, and build logic from within your Terraform project. It is created by default by the Terraform module, which is convenient. Also, we need to give permission to the EventBridge rule to invoke the Lambda function, and it is given by the aws_lambda_permission resource.\n1# glue-schema-registry/app/variables.tf 2locals { 3 name = local.infra_prefix 4 region = data.aws_region.current.name 5 environment = \u0026#34;dev\u0026#34; 6 7 infra_prefix = \u0026#34;glue-schema-registry\u0026#34; 8 9 producer = { 10 src_path = \u0026#34;producer\u0026#34; 11 function_name = \u0026#34;kafka_producer\u0026#34; 12 handler = \u0026#34;lambda_handler.lambda_function\u0026#34; 13 concurrency = 5 14 timeout = 90 15 memory_size = 128 16 runtime = \u0026#34;python3.8\u0026#34; 17 schedule_rate = \u0026#34;rate(1 minute)\u0026#34; 18 to_enable_trigger = true 19 environment = { 20 topic_name = \u0026#34;orders\u0026#34; 21 registry_name = \u0026#34;customer\u0026#34; 22 max_run_sec = 60 23 } 24 } 25 26 ... 27} 28 29# glue-schema-registry/app/main.tf 30module \u0026#34;kafka_producer_lambda\u0026#34; { 31 source = \u0026#34;terraform-aws-modules/lambda/aws\u0026#34; 32 33 function_name = local.producer.function_name 34 handler = local.producer.handler 35 runtime = local.producer.runtime 36 timeout = local.producer.timeout 37 memory_size = local.producer.memory_size 38 source_path = local.producer.src_path 39 vpc_subnet_ids = data.aws_subnets.private.ids 40 vpc_security_group_ids = [data.aws_security_group.kafka_app_lambda.id] 41 attach_network_policy = true 42 attach_policies = true 43 policies = [aws_iam_policy.msk_lambda_producer_permission.arn] 44 number_of_policies = 1 45 environment_variables = { 46 BOOTSTRAP_SERVERS = data.aws_msk_cluster.msk_data_cluster.bootstrap_brokers_sasl_iam 47 TOPIC_NAME = local.producer.environment.topic_name 48 REGISTRY_NAME = local.producer.environment.registry_name 49 MAX_RUN_SEC = local.producer.environment.max_run_sec 50 } 51 52 tags = local.tags 53} 54 55resource \u0026#34;aws_lambda_function_event_invoke_config\u0026#34; \u0026#34;kafka_producer_lambda\u0026#34; { 56 function_name = module.kafka_producer_lambda.lambda_function_name 57 maximum_retry_attempts = 0 58} 59 60resource \u0026#34;aws_lambda_permission\u0026#34; \u0026#34;allow_eventbridge\u0026#34; { 61 count = local.producer.to_enable_trigger ? 1 : 0 62 statement_id = \u0026#34;InvokeLambdaFunction\u0026#34; 63 action = \u0026#34;lambda:InvokeFunction\u0026#34; 64 function_name = local.producer.function_name 65 principal = \u0026#34;events.amazonaws.com\u0026#34; 66 source_arn = module.eventbridge.eventbridge_rule_arns[\u0026#34;crons\u0026#34;] 67 68 depends_on = [ 69 module.eventbridge 70 ] 71} IAM Permission The producer Lambda function needs permission to send messages to the orders topic of the MSK cluster. Also, it needs permission on the Glue schema registry and schema. The following IAM policy is added to the Lambda function.\n1# glue-schema-registry/app/main.tf 2resource \u0026#34;aws_iam_policy\u0026#34; \u0026#34;msk_lambda_producer_permission\u0026#34; { 3 name = \u0026#34;${local.producer.function_name}-msk-lambda-producer-permission\u0026#34; 4 5 policy = jsonencode({ 6 Version = \u0026#34;2012-10-17\u0026#34; 7 Statement = [ 8 { 9 Sid = \u0026#34;PermissionOnCluster\u0026#34; 10 Action = [ 11 \u0026#34;kafka-cluster:Connect\u0026#34;, 12 \u0026#34;kafka-cluster:AlterCluster\u0026#34;, 13 \u0026#34;kafka-cluster:DescribeCluster\u0026#34; 14 ] 15 Effect = \u0026#34;Allow\u0026#34; 16 Resource = \u0026#34;arn:aws:kafka:${local.region}:${data.aws_caller_identity.current.account_id}:cluster/${local.infra_prefix}-msk-cluster/*\u0026#34; 17 }, 18 { 19 Sid = \u0026#34;PermissionOnTopics\u0026#34; 20 Action = [ 21 \u0026#34;kafka-cluster:*Topic*\u0026#34;, 22 \u0026#34;kafka-cluster:WriteData\u0026#34;, 23 \u0026#34;kafka-cluster:ReadData\u0026#34; 24 ] 25 Effect = \u0026#34;Allow\u0026#34; 26 Resource = \u0026#34;arn:aws:kafka:${local.region}:${data.aws_caller_identity.current.account_id}:topic/${local.infra_prefix}-msk-cluster/*\u0026#34; 27 }, 28 { 29 Sid = \u0026#34;PermissionOnGroups\u0026#34; 30 Action = [ 31 \u0026#34;kafka-cluster:AlterGroup\u0026#34;, 32 \u0026#34;kafka-cluster:DescribeGroup\u0026#34; 33 ] 34 Effect = \u0026#34;Allow\u0026#34; 35 Resource = \u0026#34;arn:aws:kafka:${local.region}:${data.aws_caller_identity.current.account_id}:group/${local.infra_prefix}-msk-cluster/*\u0026#34; 36 }, 37 { 38 Sid = \u0026#34;PermissionOnGlueSchema\u0026#34; 39 Action = [ 40 \u0026#34;glue:*Schema*\u0026#34;, 41 \u0026#34;glue:GetRegistry\u0026#34;, 42 \u0026#34;glue:CreateRegistry\u0026#34;, 43 \u0026#34;glue:ListRegistries\u0026#34;, 44 ] 45 Effect = \u0026#34;Allow\u0026#34; 46 Resource = \u0026#34;*\u0026#34; 47 } 48 ] 49 }) 50} EventBridge Rule The AWS EventBridge Terraform module is used to create the EventBridge schedule rule and targets. Note that 5 targets that point to the Kafka producer Lambda function are created so that it is invoked concurrently every minute.\n1module \u0026#34;eventbridge\u0026#34; { 2 source = \u0026#34;terraform-aws-modules/eventbridge/aws\u0026#34; 3 4 create_bus = false 5 6 rules = { 7 crons = { 8 description = \u0026#34;Kafka producer lambda schedule\u0026#34; 9 schedule_expression = local.producer.schedule_rate 10 } 11 } 12 13 targets = { 14 crons = [for i in range(local.producer.concurrency) : { 15 name = \u0026#34;lambda-target-${i}\u0026#34; 16 arn = module.kafka_producer_lambda.lambda_function_arn 17 }] 18 } 19 20 depends_on = [ 21 module.kafka_producer_lambda 22 ] 23 24 tags = local.tags 25} Consumer App Lambda Handler The Lambda event includes records, which is a dictionary where the key is a topic partition (topic_name-partiton_number) and the value is a list of consumer records. The consumer records include both the message metadata (topic, partition, offset, timestamp\u0026hellip;), key and value. An example payload is shown below.\n1{ 2 \u0026#34;eventSource\u0026#34;: \u0026#34;aws:kafka\u0026#34;, 3 \u0026#34;eventSourceArn\u0026#34;: \u0026#34;\u0026lt;msk-cluster-arn\u0026gt;\u0026#34;, 4 \u0026#34;bootstrapServers\u0026#34;: \u0026#34;\u0026lt;bootstrap-server-addresses\u0026gt;\u0026#34;, 5 \u0026#34;records\u0026#34;: { 6 \u0026#34;orders-2\u0026#34;: [ 7 { 8 \u0026#34;topic\u0026#34;: \u0026#34;orders\u0026#34;, 9 \u0026#34;partition\u0026#34;: 2, 10 \u0026#34;offset\u0026#34;: 10293, 11 \u0026#34;timestamp\u0026#34;: 1680631941838, 12 \u0026#34;timestampType\u0026#34;: \u0026#34;CREATE_TIME\u0026#34;, 13 \u0026#34;key\u0026#34;: \u0026#34;eyJvcmRlcl9pZCI6ICJkNmQ4ZDJjNi1hODYwLTQyNTYtYWY1Yi04ZjU3NDkxZmM4YWYifQ==\u0026#34;, 14 \u0026#34;value\u0026#34;: \u0026#34;AwDeD/rgjCxCeawN/ZaIO6VuSGQ2ZDhkMmM2LWE4NjAtNDI1Ni1hZjViLThmNTc0OTFmYzhhZu6pwtfpYQppYWJ4YQa8UBSEHgbkVAYA\u0026#34;, 15 \u0026#34;headers\u0026#34;: [], 16 } 17 ] 18 } 19} The ConsumerRecord class parses/formats a consumer record. As the key and value are returned as base64 encoded string, it is decoded into bytes, followed by decoding or deserializing appropriately. The LambdaDeserializer class is created to deserialize the value. Also, the message timestamp is converted into the datetime object. The parse_record() method returns the consumer record with parsed/formatted values.\n1# glue-schema-registry/app/consumer/lambda_handler.py 2import os 3import json 4import base64 5import datetime 6 7import boto3 8from aws_schema_registry import SchemaRegistryClient 9from aws_schema_registry.adapter.kafka import Deserializer, KafkaDeserializer 10 11 12class LambdaDeserializer(Deserializer): 13 def __init__(self, registry: str): 14 self.registry = registry 15 16 @property 17 def deserializer(self): 18 glue_client = boto3.client( 19 \u0026#34;glue\u0026#34;, region_name=os.getenv(\u0026#34;AWS_DEFAULT_REGION\u0026#34;, \u0026#34;ap-southeast-2\u0026#34;) 20 ) 21 client = SchemaRegistryClient(glue_client, registry_name=self.registry) 22 return KafkaDeserializer(client) 23 24 def deserialize(self, topic: str, bytes_: bytes): 25 return self.deserializer.deserialize(topic, bytes_) 26 27 28class ConsumerRecord: 29 def __init__(self, record: dict): 30 self.topic = record[\u0026#34;topic\u0026#34;] 31 self.partition = record[\u0026#34;partition\u0026#34;] 32 self.offset = record[\u0026#34;offset\u0026#34;] 33 self.timestamp = record[\u0026#34;timestamp\u0026#34;] 34 self.timestamp_type = record[\u0026#34;timestampType\u0026#34;] 35 self.key = record[\u0026#34;key\u0026#34;] 36 self.value = record[\u0026#34;value\u0026#34;] 37 self.headers = record[\u0026#34;headers\u0026#34;] 38 39 def parse_key(self): 40 return base64.b64decode(self.key).decode() 41 42 def parse_value(self, deserializer: LambdaDeserializer): 43 parsed = deserializer.deserialize(self.topic, base64.b64decode(self.value)) 44 return parsed.data 45 46 def format_timestamp(self, to_str: bool = True): 47 ts = datetime.datetime.fromtimestamp(self.timestamp / 1000) 48 if to_str: 49 return ts.isoformat() 50 return ts 51 52 def parse_record( 53 self, deserializer: LambdaDeserializer, to_str: bool = True, to_json: bool = True 54 ): 55 rec = { 56 **self.__dict__, 57 **{ 58 \u0026#34;key\u0026#34;: self.parse_key(), 59 \u0026#34;value\u0026#34;: self.parse_value(deserializer), 60 \u0026#34;timestamp\u0026#34;: self.format_timestamp(to_str), 61 }, 62 } 63 if to_json: 64 return json.dumps(rec, default=self.serialize) 65 return rec 66 67 def serialize(self, obj): 68 if isinstance(obj, datetime.datetime): 69 return obj.isoformat() 70 if isinstance(obj, datetime.date): 71 return str(obj) 72 return obj 73 74 75def lambda_function(event, context): 76 deserializer = LambdaDeserializer(os.getenv(\u0026#34;REGISTRY_NAME\u0026#34;, \u0026#34;customer\u0026#34;)) 77 for _, records in event[\u0026#34;records\u0026#34;].items(): 78 for record in records: 79 cr = ConsumerRecord(record) 80 print(cr.parse_record(deserializer)) Lambda Resource The AWS Lambda Terraform module is used to create the consumer Lambda function as well. Lambda event source mapping is created so that it polls messages from the orders topic and invoke the consumer function. Also, we need to give permission to the MSK cluster to invoke the Lambda function, and it is given by the aws_lambda_permission resource.\n1# glue-schema-registry/app/variables.tf 2locals { 3 name = local.infra_prefix 4 region = data.aws_region.current.name 5 environment = \u0026#34;dev\u0026#34; 6 7 infra_prefix = \u0026#34;glue-schema-registry\u0026#34; 8 9 ... 10 11 consumer = { 12 src_path = \u0026#34;consumer\u0026#34; 13 function_name = \u0026#34;kafka_consumer\u0026#34; 14 handler = \u0026#34;lambda_handler.lambda_function\u0026#34; 15 timeout = 90 16 memory_size = 128 17 runtime = \u0026#34;python3.8\u0026#34; 18 topic_name = \u0026#34;orders\u0026#34; 19 starting_position = \u0026#34;TRIM_HORIZON\u0026#34; 20 environment = { 21 registry_name = \u0026#34;customer\u0026#34; 22 } 23 } 24} 25 26# glue-schema-registry/app/main.tf 27module \u0026#34;kafka_consumer_lambda\u0026#34; { 28 source = \u0026#34;terraform-aws-modules/lambda/aws\u0026#34; 29 30 function_name = local.consumer.function_name 31 handler = local.consumer.handler 32 runtime = local.consumer.runtime 33 timeout = local.consumer.timeout 34 memory_size = local.consumer.memory_size 35 source_path = local.consumer.src_path 36 vpc_subnet_ids = data.aws_subnets.private.ids 37 vpc_security_group_ids = [data.aws_security_group.kafka_app_lambda.id] 38 attach_network_policy = true 39 attach_policies = true 40 policies = [aws_iam_policy.msk_lambda_consumer_permission.arn] 41 number_of_policies = 1 42 environment_variables = { 43 REGISTRY_NAME = local.producer.environment.registry_name 44 } 45 46 tags = local.tags 47} 48 49resource \u0026#34;aws_lambda_event_source_mapping\u0026#34; \u0026#34;kafka_consumer_lambda\u0026#34; { 50 event_source_arn = data.aws_msk_cluster.msk_data_cluster.arn 51 function_name = module.kafka_consumer_lambda.lambda_function_name 52 topics = [local.consumer.topic_name] 53 starting_position = local.consumer.starting_position 54 amazon_managed_kafka_event_source_config { 55 consumer_group_id = \u0026#34;${local.consumer.topic_name}-group-01\u0026#34; 56 } 57} 58 59resource \u0026#34;aws_lambda_permission\u0026#34; \u0026#34;allow_msk\u0026#34; { 60 statement_id = \u0026#34;InvokeLambdaFunction\u0026#34; 61 action = \u0026#34;lambda:InvokeFunction\u0026#34; 62 function_name = local.consumer.function_name 63 principal = \u0026#34;kafka.amazonaws.com\u0026#34; 64 source_arn = data.aws_msk_cluster.msk_data_cluster.arn 65} IAM Permission As the Lambda event source mapping uses the permission of the Lambda function, we need to add permission related to Kafka cluster, Kafka and networking - see the AWS documentation for details. Finally, permission on the Glue schema registry and schema is added as the consumer should be able to request relevant schemas.\n1# glue-schema-registry/app/main.tf 2resource \u0026#34;aws_iam_policy\u0026#34; \u0026#34;msk_lambda_consumer_permission\u0026#34; { 3 name = \u0026#34;${local.consumer.function_name}-msk-lambda-consumer-permission\u0026#34; 4 5 policy = jsonencode({ 6 Version = \u0026#34;2012-10-17\u0026#34; 7 Statement = [ 8 { 9 Sid = \u0026#34;PermissionOnKafkaCluster\u0026#34; 10 Action = [ 11 \u0026#34;kafka-cluster:Connect\u0026#34;, 12 \u0026#34;kafka-cluster:DescribeGroup\u0026#34;, 13 \u0026#34;kafka-cluster:AlterGroup\u0026#34;, 14 \u0026#34;kafka-cluster:DescribeTopic\u0026#34;, 15 \u0026#34;kafka-cluster:ReadData\u0026#34;, 16 \u0026#34;kafka-cluster:DescribeClusterDynamicConfiguration\u0026#34; 17 ] 18 Effect = \u0026#34;Allow\u0026#34; 19 Resource = [ 20 \u0026#34;arn:aws:kafka:${local.region}:${data.aws_caller_identity.current.account_id}:cluster/${local.infra_prefix}-msk-cluster/*\u0026#34;, 21 \u0026#34;arn:aws:kafka:${local.region}:${data.aws_caller_identity.current.account_id}:topic/${local.infra_prefix}-msk-cluster/*\u0026#34;, 22 \u0026#34;arn:aws:kafka:${local.region}:${data.aws_caller_identity.current.account_id}:group/${local.infra_prefix}-msk-cluster/*\u0026#34; 23 ] 24 }, 25 { 26 Sid = \u0026#34;PermissionOnKafka\u0026#34; 27 Action = [ 28 \u0026#34;kafka:DescribeCluster\u0026#34;, 29 \u0026#34;kafka:GetBootstrapBrokers\u0026#34; 30 ] 31 Effect = \u0026#34;Allow\u0026#34; 32 Resource = \u0026#34;*\u0026#34; 33 }, 34 { 35 Sid = \u0026#34;PermissionOnNetwork\u0026#34; 36 Action = [ 37 # The first three actions also exist in netwrok policy attachment in lambda module 38 # \u0026#34;ec2:CreateNetworkInterface\u0026#34;, 39 # \u0026#34;ec2:DescribeNetworkInterfaces\u0026#34;, 40 # \u0026#34;ec2:DeleteNetworkInterface\u0026#34;, 41 \u0026#34;ec2:DescribeVpcs\u0026#34;, 42 \u0026#34;ec2:DescribeSubnets\u0026#34;, 43 \u0026#34;ec2:DescribeSecurityGroups\u0026#34; 44 ] 45 Effect = \u0026#34;Allow\u0026#34; 46 Resource = \u0026#34;*\u0026#34; 47 }, 48 { 49 Sid = \u0026#34;PermissionOnGlueSchema\u0026#34; 50 Action = [ 51 \u0026#34;glue:*Schema*\u0026#34;, 52 \u0026#34;glue:ListRegistries\u0026#34; 53 ] 54 Effect = \u0026#34;Allow\u0026#34; 55 Resource = \u0026#34;*\u0026#34; 56 } 57 ] 58 }) 59} Schema Evolution Demo Before testing the Kafka applications, I\u0026rsquo;ll quickly demonstrate how the schema registry can be used for managing and validating schemas for topic message data. Each schema can have a compatibility mode (or disabled) and the scope of changes is restricted by it. For example, the default BACKWARD mode only allows to delete fields or add optional fields. (See the Confluent document for a quick summary.) Therefore, if we add a mandatory field to an existing schema, it will be not validated, and it fails to send a message to the topic. In order to illustrate it, I created a single node Kafka cluster using docker-compose as shown below.\n1# glue-schema-registry/compose-demo.yml 2version: \u0026#34;3.5\u0026#34; 3 4services: 5 zookeeper: 6 image: docker.io/bitnami/zookeeper:3.8 7 container_name: zookeeper 8 ports: 9 - \u0026#34;2181\u0026#34; 10 networks: 11 - kafkanet 12 environment: 13 - ALLOW_ANONYMOUS_LOGIN=yes 14 volumes: 15 - zookeeper_data:/bitnami/zookeeper 16 kafka-0: 17 image: docker.io/bitnami/kafka:3.3 18 container_name: kafka-0 19 expose: 20 - 9092 21 ports: 22 - \u0026#34;9093:9093\u0026#34; 23 networks: 24 - kafkanet 25 environment: 26 - ALLOW_PLAINTEXT_LISTENER=yes 27 - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181 28 - KAFKA_CFG_BROKER_ID=0 29 - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=CLIENT:PLAINTEXT,EXTERNAL:PLAINTEXT 30 - KAFKA_CFG_LISTENERS=CLIENT://:9092,EXTERNAL://:9093 31 - KAFKA_CFG_ADVERTISED_LISTENERS=CLIENT://kafka-0:9092,EXTERNAL://localhost:9093 32 - KAFKA_CFG_INTER_BROKER_LISTENER_NAME=CLIENT 33 volumes: 34 - kafka_0_data:/bitnami/kafka 35 depends_on: 36 - zookeeper 37 38networks: 39 kafkanet: 40 name: kafka-network 41 42volumes: 43 zookeeper_data: 44 driver: local 45 kafka_0_data: 46 driver: local Recall that the producer lambda handler has a conditional block, and it is executed when it is called as a script. If we don\u0026rsquo;t specify the environment variable of USE_MORE, it sends a messages based on the Order class. Otherwise, a message is created from the OrderMore class, which has an additional boolean attribute called is_prime. As the compatibility mode is set to be BACKWARD, we can expect the second round of execution will not be successful. As shown below, I executed the lambda handler twice and the second round failed with the following error, which indicates schema validation failure.\naws_schema_registry.exception.SchemaRegistryException: Schema Found but status is FAILURE\n1export BOOTSTRAP_SERVERS=localhost:9093 2export TOPIC_NAME=demo 3export REGISTRY_NAME=customer 4 5cd glue-schema-registry/app/producer 6 7## Round 1 - send message from the Order class 8python lambda_handler.py 9 10## Round 2 - send message from the OrderMore class 11export USE_MORE=1 12 13python lambda_handler.py We can see the details from the schema version and the second version is marked as failed.\nNote that schema versioning and validation would be more relevant to the clients that tightly link the schema and message records. However, it would still be important for a Python client in order to work together with those clients or Kafka connect.\nDeployment Topic Creation We plan to create the orders topic with multiple partitions. Although we can use the Kafka CLI tool, it can be performed easily using Kpow. It is a Kafka monitoring and management tool, which provides a web UI. Also, it supports the Glue Schema Registry and MSK Connect out-of-box, which is quite convenient. In the docker-compose file, we added environment variables for the MSK cluster, MSK Connect and Glue Schema Registry details. Note it fails to start if the schema registry does not exist. I created the registry while I demonstrated schema evolution, or it can be created simply as shown below.\n1$ aws glue create-registry --registry-name customer 1# glue-schema-registry/docker-compose.yml 2version: \u0026#34;3.5\u0026#34; 3 4services: 5 kpow: 6 image: factorhouse/kpow-ce:91.2.1 7 container_name: kpow 8 ports: 9 - \u0026#34;3000:3000\u0026#34; 10 networks: 11 - kafkanet 12 environment: 13 AWS_ACCESS_KEY_ID: $AWS_ACCESS_KEY_ID 14 AWS_SECRET_ACCESS_KEY: $AWS_SECRET_ACCESS_KEY 15 AWS_SESSION_TOKEN: $AWS_SESSION_TOKEN 16 # kafka cluster 17 BOOTSTRAP: $BOOTSTRAP_SERVERS 18 SECURITY_PROTOCOL: SASL_SSL 19 SASL_MECHANISM: AWS_MSK_IAM 20 SASL_CLIENT_CALLBACK_HANDLER_CLASS: software.amazon.msk.auth.iam.IAMClientCallbackHandler 21 SASL_JAAS_CONFIG: software.amazon.msk.auth.iam.IAMLoginModule required; 22 # msk connect 23 CONNECT_AWS_REGION: $AWS_DEFAULT_REGION 24 # glue schema registry 25 SCHEMA_REGISTRY_ARN: $SCHEMA_REGISTRY_ARN 26 SCHEMA_REGISTRY_REGION: $AWS_DEFAULT_REGION 27 28networks: 29 kafkanet: 30 name: kafka-network Once started, we can visit the UI on port 3000. The topic is created in the Topic menu by specifying the topic name and the number of partitions.\nOnce created, we can check details of the topic by selecting the topic from the drop-down menu.\nLocal Testing with SAM To simplify development, the EventBridge permission is disabled by setting to_enable_trigger to false. Also, it is shortened to loop before it gets stopped by reducing max_run_sec to 10.\n1# glue-schema-registry/app/variables.tf 2locals { 3 ... 4 5 producer = { 6 ... 7 to_enable_trigger = false 8 environment = { 9 topic_name = \u0026#34;orders\u0026#34; 10 registry_name = \u0026#34;customer\u0026#34; 11 max_run_sec = 10 12 } 13 } 14 ... 15} The Lambda function can be built with the SAM build command while specifying the hook name as terraform and enabling beta features. Once completed, it stores the build artifacts and template in the .aws-sam folder.\n1$ sam build --hook-name terraform --beta-features 2 3# Apply complete! Resources: 3 added, 0 changed, 0 destroyed. 4 5# Build Succeeded 6 7# Built Artifacts : .aws-sam/build 8# Built Template : .aws-sam/build/template.yaml 9 10# Commands you can use next 11# ========================= 12# [*] Invoke Function: sam local invoke --hook-name terraform 13# [*] Emulate local Lambda functions: sam local start-lambda --hook-name terraform 14 15# SAM CLI update available (1.78.0); (1.70.0 installed) 16# To download: https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/serverless-sam-cli-install.html We can invoke the Lambda function locally using the SAM local invoke command. The Lambda function is invoked in a Docker container and the invocation logs are printed in the terminal as shown below. Note that we should be connected to the VPN server in order to send messages into the MSK cluster, which is deployed in private subnets.\n1$ sam local invoke --hook-name terraform module.kafka_producer_lambda.aws_lambda_function.this[0] --beta-features 2 3# Experimental features are enabled for this session. 4# Visit the docs page to learn more about the AWS Beta terms https://aws.amazon.com/service-terms/. 5 6# Skipped prepare hook. Current application is already prepared. 7# Invoking lambda_handler.lambda_function (python3.8) 8# Skip pulling image and use local one: public.ecr.aws/sam/emulation-python3.8:rapid-1.70.0-x86_64. 9 10# Mounting /home/jaehyeon/personal/kafka-pocs/glue-schema-registry/app/.aws-sam/build/ModuleKafkaProducerLambdaAwsLambdaFunctionThis069E06354 as /var/task:ro,delegated inside runtime container 11# START RequestId: fdbba255-e5b0-4e21-90d3-fe0b2ebbf629 Version: $LATEST 12# sent 100 messages 13# sent 100 messages 14# sent 100 messages 15# sent 100 messages 16# sent 100 messages 17# sent 100 messages 18# sent 100 messages 19# sent 100 messages 20# sent 100 messages 21# sent 100 messages 22# 1000 records are sent in 11 seconds ... 23# END RequestId: fdbba255-e5b0-4e21-90d3-fe0b2ebbf629 24# REPORT RequestId: fdbba255-e5b0-4e21-90d3-fe0b2ebbf629 Init Duration: 0.22 ms Duration: 12146.61 ms Billed Duration: 12147 ms Memory Size: 128 MB Max Memory Used: 128 MB 25# null Once completed, we can check the value schema (orders-value) is created in the Kpow UI as shown below.\nWe can check the messages. In order to check them correctly, we need to select AVRO as the value deserializer and glue1 as the schema registry.\nKafka App Deployment Now we can deploy the Kafka applications using Terraform as usual after resetting the configuration variables. Once deployed, we can see that the scheduler rule has 5 targets of the same Lambda function.\nWe can see the Lambda consumer parses the consumer records correctly in CloudWatch logs.\nSummary Schema registry provides a centralized repository for managing and validating schemas for topic message data. In AWS, the Glue Schema Registry supports features to manage and enforce schemas on data streaming applications using convenient integrations with a range of AWS services. In this post, we discussed how to integrate Python Kafka producer and consumer apps in AWS Lambda with the Glue Schema Registry.\n","date":"April 12, 2023","img":"/blog/2023-04-12-integrate-glue-schema-registry/featured.png","lang":"en","langName":"English","largeImg":"/blog/2023-04-12-integrate-glue-schema-registry/featured_hu57e44328735e0754f0de2b0f6335f8bb_46040_500x0_resize_box_3.png","permalink":"/blog/2023-04-12-integrate-glue-schema-registry/","series":[],"smallImg":"/blog/2023-04-12-integrate-glue-schema-registry/featured_hu57e44328735e0754f0de2b0f6335f8bb_46040_180x0_resize_box_3.png","tags":[{"title":"AWS","url":"/tags/aws/"},{"title":"Amazon MSK","url":"/tags/amazon-msk/"},{"title":"AWS Glue Schema Registry","url":"/tags/aws-glue-schema-registry/"},{"title":"AWS Lambda","url":"/tags/aws-lambda/"},{"title":"AWS Serverless Application Model","url":"/tags/aws-serverless-application-model/"},{"title":"Apache Kafka","url":"/tags/apache-kafka/"},{"title":"Python","url":"/tags/python/"},{"title":"Docker","url":"/tags/docker/"},{"title":"Docker Compose","url":"/tags/docker-compose/"},{"title":"Terraform","url":"/tags/terraform/"}],"timestamp":1681257600,"title":"Integrate Glue Schema Registry With Your Python Kafka App"},{"categories":[{"title":"Data Streaming","url":"/categories/data-streaming/"}],"content":"In Part 1, we discussed a streaming ingestion solution using EventBridge, Lambda, MSK and Redshift Serverless. Athena provides the MSK connector to enable SQL queries on Apache Kafka topics directly, and it can also facilitate the extraction of insights without setting up an additional pipeline to store data into S3. In this post, we discuss how to update the streaming ingestion solution so that data in the Kafka topic can be queried by Athena instead of Redshift.\nPart 1 MSK and Redshift Part 2 MSK and Athena (this post) Architecture As Part 1, fake online order data is generated by multiple Lambda functions that are invoked by an EventBridge schedule rule. The schedule is set to run every minute and the associating rule has a configurable number (e.g. 5) of targets. Each target points to the same Kafka producer Lambda function. In this way we are able to generate test data using multiple Lambda functions according to the desired volume of messages. Once messages are sent to a Kafka topic, they can be consumed by the Athena MSK Connector, which is a Lambda function that can be installed from the AWS Serverless Application Repository. A new Athena data source needs to be created in order to deploy the connector and the schema of the topic should be registered with AWS Glue Schema Registry. The infrastructure is built by Terraform and the AWS SAM CLI is used to develop the producer Lambda function locally before deploying to AWS.\nInfrastructure The ingestion solution shares a large portion of infrastructure and only new resources are covered in this post. The source can be found in the GitHub repository of this post.\nGlue Schema The order data is JSON format, and it has 4 attributes - order_id, ordered_at, _user_id _and items. Although the items attribute keeps an array of objects that includes _product_id _and quantity, it is specified as VARCHAR because the MSK connector doesn\u0026rsquo;t support complex types.\n1{ 2 \u0026#34;topicName\u0026#34;: \u0026#34;orders\u0026#34;, 3 \u0026#34;message\u0026#34;: { 4 \u0026#34;dataFormat\u0026#34;: \u0026#34;json\u0026#34;, 5 \u0026#34;fields\u0026#34;: [ 6 { 7 \u0026#34;name\u0026#34;: \u0026#34;order_id\u0026#34;, 8 \u0026#34;mapping\u0026#34;: \u0026#34;order_id\u0026#34;, 9 \u0026#34;type\u0026#34;: \u0026#34;VARCHAR\u0026#34; 10 }, 11 { 12 \u0026#34;name\u0026#34;: \u0026#34;ordered_at\u0026#34;, 13 \u0026#34;mapping\u0026#34;: \u0026#34;ordered_at\u0026#34;, 14 \u0026#34;type\u0026#34;: \u0026#34;TIMESTAMP\u0026#34;, 15 \u0026#34;formatHint\u0026#34;: \u0026#34;yyyy-MM-dd HH:mm:ss.SSS\u0026#34; 16 }, 17 { 18 \u0026#34;name\u0026#34;: \u0026#34;user_id\u0026#34;, 19 \u0026#34;mapping\u0026#34;: \u0026#34;user_id\u0026#34;, 20 \u0026#34;type\u0026#34;: \u0026#34;VARCHAR\u0026#34; 21 }, 22 { 23 \u0026#34;name\u0026#34;: \u0026#34;items\u0026#34;, 24 \u0026#34;mapping\u0026#34;: \u0026#34;items\u0026#34;, 25 \u0026#34;type\u0026#34;: \u0026#34;VARCHAR\u0026#34; 26 } 27 ] 28 } 29} The registry and schema can be created as shown below. Note the description should include the string {AthenaFederationMSK} as the marker string is required for AWS Glue Registries that you use with the Amazon Athena MSK connector.\n1# integration-athena/infra/athena.tf 2resource \u0026#34;aws_glue_registry\u0026#34; \u0026#34;msk_registry\u0026#34; { 3 registry_name = \u0026#34;customer\u0026#34; 4 description = \u0026#34;{AthenaFederationMSK}\u0026#34; 5 6 tags = local.tags 7} 8 9resource \u0026#34;aws_glue_schema\u0026#34; \u0026#34;msk_schema\u0026#34; { 10 schema_name = \u0026#34;orders\u0026#34; 11 registry_arn = aws_glue_registry.msk_registry.arn 12 data_format = \u0026#34;JSON\u0026#34; 13 compatibility = \u0026#34;NONE\u0026#34; 14 schema_definition = jsonencode({ \u0026#34;topicName\u0026#34; : \u0026#34;orders\u0026#34;, \u0026#34;message\u0026#34; : { \u0026#34;dataFormat\u0026#34; : \u0026#34;json\u0026#34;, \u0026#34;fields\u0026#34; : [{ \u0026#34;name\u0026#34; : \u0026#34;order_id\u0026#34;, \u0026#34;mapping\u0026#34; : \u0026#34;order_id\u0026#34;, \u0026#34;type\u0026#34; : \u0026#34;VARCHAR\u0026#34; }, { \u0026#34;name\u0026#34; : \u0026#34;ordered_at\u0026#34;, \u0026#34;mapping\u0026#34; : \u0026#34;ordered_at\u0026#34;, \u0026#34;type\u0026#34; : \u0026#34;TIMESTAMP\u0026#34;, \u0026#34;formatHint\u0026#34; : \u0026#34;yyyy-MM-dd HH:mm:ss.SSS\u0026#34; }, { \u0026#34;name\u0026#34; : \u0026#34;user_id\u0026#34;, \u0026#34;mapping\u0026#34; : \u0026#34;user_id\u0026#34;, \u0026#34;type\u0026#34; : \u0026#34;VARCHAR\u0026#34; }, { \u0026#34;name\u0026#34; : \u0026#34;items\u0026#34;, \u0026#34;mapping\u0026#34; : \u0026#34;items\u0026#34;, \u0026#34;type\u0026#34; : \u0026#34;VARCHAR\u0026#34; }] } }) 15 16 tags = local.tags 17} Athena MSK Connector In Terraform, the MSK Connector Lambda function can be created by deploying the associated CloudFormation stack from the AWS Serverless Application Repository. The stack parameters are passed into environment variables of the function, and they are mostly used to establish connection to Kafka topics.\n1# integration-athena/infra/athena.tf 2resource \u0026#34;aws_serverlessapplicationrepository_cloudformation_stack\u0026#34; \u0026#34;athena_msk_connector\u0026#34; { 3 name = \u0026#34;${local.name}-athena-msk-connector\u0026#34; 4 application_id = \u0026#34;arn:aws:serverlessrepo:us-east-1:292517598671:applications/AthenaMSKConnector\u0026#34; 5 semantic_version = \u0026#34;2023.8.3\u0026#34; 6 capabilities = [ 7 \u0026#34;CAPABILITY_IAM\u0026#34;, 8 \u0026#34;CAPABILITY_RESOURCE_POLICY\u0026#34;, 9 ] 10 parameters = { 11 AuthType = \u0026#34;SASL_SSL_AWS_MSK_IAM\u0026#34; 12 KafkaEndpoint = aws_msk_cluster.msk_data_cluster.bootstrap_brokers_sasl_iam 13 LambdaFunctionName = \u0026#34;${local.name}-ingest-orders\u0026#34; 14 SpillBucket = aws_s3_bucket.default_bucket.id 15 SpillPrefix = \u0026#34;athena-spill\u0026#34; 16 SecurityGroupIds = aws_security_group.athena_connector.id 17 SubnetIds = join(\u0026#34;,\u0026#34;, module.vpc.private_subnets) 18 LambdaRoleARN = aws_iam_role.athena_connector_role.arn 19 } 20} Lambda Execution Role The AWS document doesn\u0026rsquo;t include the specific IAM permissions that are necessary for the connector function, and they are updated by making trials and errors. Therefore, some of them are too generous, and it should be refined later.\nFirst it needs permission to access an MSK cluster and topics, and they are copied from Part 1. Next access to the Glue registry and schema is required. I consider the required permission would have been more specific if a specific registry or schema could be specified to the connector Lambda function. Rather it searches applicable registries using a string marker and that requires an additional set of permissions. Then permission to the spill S3 bucket is added. I initially included a typical read/write permission on a specific bucket and objects, but the Lambda function complained by throwing 403 authorized errors. Therefore, I escalated the level of permissions, which is by no means acceptable in a strict environment. Further investigation is necessary for it. Finally, permission to get Athena query executions is added. 1# integration-athena/infra/athena.tf 2resource \u0026#34;aws_iam_role\u0026#34; \u0026#34;athena_connector_role\u0026#34; { 3 name = \u0026#34;${local.name}-athena-connector-role\u0026#34; 4 5 assume_role_policy = data.aws_iam_policy_document.athena_connector_assume_role_policy.json 6 managed_policy_arns = [ 7 \u0026#34;arn:aws:iam::aws:policy/service-role/AWSLambdaVPCAccessExecutionRole\u0026#34;, 8 aws_iam_policy.athena_connector_permission.arn 9 ] 10} 11 12data \u0026#34;aws_iam_policy_document\u0026#34; \u0026#34;athena_connector_assume_role_policy\u0026#34; { 13 statement { 14 actions = [\u0026#34;sts:AssumeRole\u0026#34;] 15 16 principals { 17 type = \u0026#34;Service\u0026#34; 18 identifiers = [ 19 \u0026#34;lambda.amazonaws.com\u0026#34; 20 ] 21 } 22 } 23} 24 25resource \u0026#34;aws_iam_policy\u0026#34; \u0026#34;athena_connector_permission\u0026#34; { 26 name = \u0026#34;${local.name}-athena-connector-permission\u0026#34; 27 28 policy = jsonencode({ 29 Version = \u0026#34;2012-10-17\u0026#34; 30 Statement = [ 31 { 32 Sid = \u0026#34;PermissionOnCluster\u0026#34; 33 Action = [ 34 \u0026#34;kafka-cluster:ReadData\u0026#34;, 35 \u0026#34;kafka-cluster:DescribeTopic\u0026#34;, 36 \u0026#34;kafka-cluster:Connect\u0026#34;, 37 ] 38 Effect = \u0026#34;Allow\u0026#34; 39 Resource = [ 40 \u0026#34;arn:aws:kafka:*:${data.aws_caller_identity.current.account_id}:cluster/*/*\u0026#34;, 41 \u0026#34;arn:aws:kafka:*:${data.aws_caller_identity.current.account_id}:topic/*/*\u0026#34; 42 ] 43 }, 44 { 45 Sid = \u0026#34;PermissionOnGroups\u0026#34; 46 Action = [ 47 \u0026#34;kafka:GetBootstrapBrokers\u0026#34; 48 ] 49 Effect = \u0026#34;Allow\u0026#34; 50 Resource = \u0026#34;*\u0026#34; 51 }, 52 { 53 Sid = \u0026#34;PermissionOnGlueSchema\u0026#34; 54 Action = [ 55 \u0026#34;glue:*Schema*\u0026#34;, 56 \u0026#34;glue:ListRegistries\u0026#34; 57 ] 58 Effect = \u0026#34;Allow\u0026#34; 59 Resource = \u0026#34;*\u0026#34; 60 }, 61 { 62 Sid = \u0026#34;PermissionOnS3\u0026#34; 63 Action = [\u0026#34;s3:*\u0026#34;] 64 Effect = \u0026#34;Allow\u0026#34; 65 Resource = \u0026#34;arn:aws:s3:::*\u0026#34; 66 }, 67 { 68 Sid = \u0026#34;PermissionOnAthenaQuery\u0026#34; 69 Action = [ 70 \u0026#34;athena:GetQueryExecution\u0026#34; 71 ] 72 Effect = \u0026#34;Allow\u0026#34; 73 Resource = \u0026#34;*\u0026#34; 74 } 75 ] 76 }) 77} Security Group The security group and rules are shown below. Although the outbound rule is set to allow all protocol and port ranges, only port 443 and 9098 with the TCP protocol would be sufficient. The former is to access the Glue schema registry while the latter is for an MSK cluster with IAM authentication.\n1# integration-athena/infra/athena.tf 2resource \u0026#34;aws_security_group\u0026#34; \u0026#34;athena_connector\u0026#34; { 3 name = \u0026#34;${local.name}-athena-connector\u0026#34; 4 vpc_id = module.vpc.vpc_id 5 6 lifecycle { 7 create_before_destroy = true 8 } 9 10 tags = local.tags 11} 12 13resource \u0026#34;aws_security_group_rule\u0026#34; \u0026#34;athena_connector_msk_egress\u0026#34; { 14 type = \u0026#34;egress\u0026#34; 15 description = \u0026#34;allow outbound all\u0026#34; 16 security_group_id = aws_security_group.athena_connector.id 17 protocol = \u0026#34;-1\u0026#34; 18 from_port = 0 19 to_port = 0 20 cidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;] 21} Athena Data Source Unfortunately connecting to MSK from Athena is yet to be supported by CloudFormation or Terraform, and it is performed on AWS console as shown below. First we begin by clicking on the Create data source button.\nThen we can search the Amazon MSK data source and proceed by clicking on the _Next _button.\nWe can update data source details followed by selecting the connector Lambda function ARN in connection details.\nOnce the data source connection is established, we are able to see the customer database we created earlier - the Glue registry name becomes the database name.\nAlso, we can check the table details from the Athena editor as shown below.\nKafka Producer As in Part 1, the resources related to the Kafka producer Lambda function are managed in a separate Terraform stack. This is because it is easier to build the relevant resources iteratively. Note the SAM CLI builds the whole Terraform stack even for a small change of code, and it wouldn\u0026rsquo;t be convenient if the entire resources are managed in the same stack. The terraform stack of the producer is the same as Part 1, and it won\u0026rsquo;t be covered here. Only the producer Lambda function source is covered here as it is modified in order to comply with the MSK connector.\nProducer Source The Kafka producer is created to send messages to a topic named orders where fake order data is generated using the Faker package. The Order class generates one or more fake order records by the _create _method and an order record includes order ID, order timestamp, user ID and order items. Note order items are converted into string. It is because the MSK connector fails to parse them correctly. Actually the AWS document indicates the MSK connector interprets complex types as strings and I thought it would be converted into strings internally. However, it turned out the list items (or array of objects) cannot be queried by Athena. Therefore, it is converted into string in the first place. The Lambda function sends 100 records at a time followed by sleeping for 1 second. It repeats until it reaches MAX_RUN_SEC (e.g. 60) environment variable value. A Kafka message is made up of an order ID as the key and an order record as the value. Both the key and value are serialised as JSON. Note that the stable version of the kafka-python package does not support the IAM authentication method. Therefore, we need to install the package from a forked repository as discussed in this GitHub issue.\n1# integration-athena/kafka_producer/src/app.py 2import os 3import re 4import datetime 5import string 6import json 7import time 8from kafka import KafkaProducer 9from faker import Faker 10 11 12class Order: 13 def __init__(self, fake: Faker = None): 14 self.fake = fake or Faker() 15 16 def order(self): 17 rand_int = self.fake.random_int(1, 1000) 18 user_id = \u0026#34;\u0026#34;.join( 19 [string.ascii_lowercase[int(s)] if s.isdigit() else s for s in hex(rand_int)] 20 )[::-1] 21 return { 22 \u0026#34;order_id\u0026#34;: self.fake.uuid4(), 23 \u0026#34;ordered_at\u0026#34;: datetime.datetime.utcnow(), 24 \u0026#34;user_id\u0026#34;: user_id, 25 } 26 27 def items(self): 28 return [ 29 { 30 \u0026#34;product_id\u0026#34;: self.fake.random_int(1, 9999), 31 \u0026#34;quantity\u0026#34;: self.fake.random_int(1, 10), 32 } 33 for _ in range(self.fake.random_int(1, 4)) 34 ] 35 36 def create(self, num: int): 37 return [{**self.order(), **{\u0026#34;items\u0026#34;: json.dumps(self.items())}} for _ in range(num)] 38 39 40class Producer: 41 def __init__(self, bootstrap_servers: list, topic: str): 42 self.bootstrap_servers = bootstrap_servers 43 self.topic = topic 44 self.producer = self.create() 45 46 def create(self): 47 return KafkaProducer( 48 security_protocol=\u0026#34;SASL_SSL\u0026#34;, 49 sasl_mechanism=\u0026#34;AWS_MSK_IAM\u0026#34;, 50 bootstrap_servers=self.bootstrap_servers, 51 value_serializer=lambda v: json.dumps(v, default=self.serialize).encode(\u0026#34;utf-8\u0026#34;), 52 key_serializer=lambda v: json.dumps(v, default=self.serialize).encode(\u0026#34;utf-8\u0026#34;), 53 ) 54 55 def send(self, orders: list): 56 for order in orders: 57 self.producer.send(self.topic, key={\u0026#34;order_id\u0026#34;: order[\u0026#34;order_id\u0026#34;]}, value=order) 58 self.producer.flush() 59 60 def serialize(self, obj): 61 if isinstance(obj, datetime.datetime): 62 return re.sub(\u0026#34;T\u0026#34;, \u0026#34; \u0026#34;, obj.isoformat(timespec=\u0026#34;milliseconds\u0026#34;)) 63 if isinstance(obj, datetime.date): 64 return str(obj) 65 return obj 66 67 68def lambda_function(event, context): 69 if os.getenv(\u0026#34;BOOTSTRAP_SERVERS\u0026#34;, \u0026#34;\u0026#34;) == \u0026#34;\u0026#34;: 70 return 71 fake = Faker() 72 producer = Producer( 73 bootstrap_servers=os.getenv(\u0026#34;BOOTSTRAP_SERVERS\u0026#34;).split(\u0026#34;,\u0026#34;), topic=os.getenv(\u0026#34;TOPIC_NAME\u0026#34;) 74 ) 75 s = datetime.datetime.now() 76 ttl_rec = 0 77 while True: 78 orders = Order(fake).create(100) 79 producer.send(orders) 80 ttl_rec += len(orders) 81 print(f\u0026#34;sent {len(orders)} messages\u0026#34;) 82 elapsed_sec = (datetime.datetime.now() - s).seconds 83 if elapsed_sec \u0026gt; int(os.getenv(\u0026#34;MAX_RUN_SEC\u0026#34;, \u0026#34;60\u0026#34;)): 84 print(f\u0026#34;{ttl_rec} records are sent in {elapsed_sec} seconds ...\u0026#34;) 85 break 86 time.sleep(1) A sample order record is shown below.\n1{ 2 \u0026#34;order_id\u0026#34;: \u0026#34;6049dc71-063b-49bd-8b68-f2326d1c8544\u0026#34;, 3 \u0026#34;ordered_at\u0026#34;: \u0026#34;2023-03-09 21:05:00.073\u0026#34;, 4 \u0026#34;user_id\u0026#34;: \u0026#34;febxa\u0026#34;, 5 \u0026#34;items\u0026#34;: \u0026#34;[{\\\u0026#34;product_id\\\u0026#34;: 4793, \\\u0026#34;quantity\\\u0026#34;: 8}]\u0026#34; 6} Deployment In this section, we skip shared steps except for local development with SAM and analytics query building. See Part 1 for other steps.\nLocal Testing with SAM To simplify development, the Eventbridge permission is disabled by setting to_enable_trigger to false. Also, it is shortened to loop before it gets stopped by reducing msx_run_sec to 10.\n1# integration-athena/kafka_producer/variables.tf 2locals { 3 producer = { 4 ... 5 to_enable_trigger = false 6 environment = { 7 topic_name = \u0026#34;orders\u0026#34; 8 max_run_sec = 10 9 } 10 } 11 ... 12} The Lambda function can be built with the SAM build command while specifying the hook name as terraform and enabling beta features. Once completed, it stores the build artifacts and template in the .aws-sam folder.\n1$ sam build --hook-name terraform --beta-features 2 3# Apply complete! Resources: 3 added, 0 changed, 0 destroyed. 4 5 6# Build Succeeded 7 8# Built Artifacts : .aws-sam/build 9# Built Template : .aws-sam/build/template.yaml 10 11# Commands you can use next 12# ========================= 13# [*] Invoke Function: sam local invoke --hook-name terraform 14# [*] Emulate local Lambda functions: sam local start-lambda --hook-name terraform We can invoke the Lambda function locally using the SAM local invoke command. The Lambda function is invoked in a Docker container and the invocation logs are printed in the terminal as shown below.\n1$ sam local invoke --hook-name terraform module.kafka_producer_lambda.aws_lambda_function.this[0] --beta-features 2 3# Experimental features are enabled for this session. 4# Visit the docs page to learn more about the AWS Beta terms https://aws.amazon.com/service-terms/. 5 6# Skipped prepare hook. Current application is already prepared. 7# Invoking app.lambda_function (python3.8) 8# Skip pulling image and use local one: public.ecr.aws/sam/emulation-python3.8:rapid-1.70.0-x86_64. 9 10# Mounting .../kafka-pocs/integration-athena/kafka_producer/.aws-sam/build/ModuleKafkaProducerLambdaAwsLambdaFunctionThis069E06354 as /var/task:ro,delegated inside runtime container 11# START RequestId: d800173a-ceb5-4002-be0e-6f0d9628b639 Version: $LATEST 12# sent 100 messages 13# sent 100 messages 14# sent 100 messages 15# sent 100 messages 16# sent 100 messages 17# sent 100 messages 18# sent 100 messages 19# sent 100 messages 20# sent 100 messages 21# sent 100 messages 22# sent 100 messages 23# sent 100 messages 24# 1200 records are sent in 11 seconds ... 25# END RequestId: d800173a-ceb5-4002-be0e-6f0d9628b639 26# REPORT RequestId: d800173a-ceb5-4002-be0e-6f0d9628b639 Init Duration: 0.16 ms Duration: 12117.64 ms Billed Duration: 12118 ms Memory Size: 128 MB Max Memory Used: 128 MB 27# null We can also check the messages using kafka-ui.\nOrder Items Query Below shows the query result of the orders table. The _items _column is a JSON array but it is stored as string. In order to build analytics queries, we need to flatten the array elements into rows and it is discussed below.\nWe can flatten the order items using the _UNNEST _function and CROSS JOIN. We first need to convert it into an array type, and it is implemented by parsing the column into JSON followed by type-casting it into an array in a CTE.\n1WITH parsed AS ( 2 SELECT 3 order_id, 4 ordered_at, 5 user_id, 6 CAST(json_parse(items) as ARRAY(ROW(product_id INT, quantity INT))) AS items 7 FROM msk.customer.orders 8) 9SELECT 10 order_id, 11 ordered_at, 12 user_id, 13 items_unnested.product_id, 14 items_unnested.quantity 15FROM parsed 16CROSS JOIN unnest(parsed.items) AS t(items_unnested) We can see the flattened order items as shown below.\nThe remaining sections cover deploying the Kafka producer Lambda, producing messages and executing an analytics query. They are skipped in this post as they are exactly and/or almost the same. See Part 1 if you would like to check it.\nSummary Streaming ingestion to Redshift and Athena becomes much simpler thanks to new features. In this series of posts, we discussed those features by building a solution using EventBridge, Lambda, MSK, Redshift and Athena. We also covered AWS SAM integrated with Terraform for developing a Lambda function locally.\n","date":"March 14, 2023","img":"/blog/2023-03-14-simplify-streaming-ingestion-athena/featured.png","lang":"en","langName":"English","largeImg":"/blog/2023-03-14-simplify-streaming-ingestion-athena/featured_huef6334952f8505bdd5ee5a48de258194_43403_500x0_resize_box_3.png","permalink":"/blog/2023-03-14-simplify-streaming-ingestion-athena/","series":[{"title":"Simplify Streaming Ingestion on AWS","url":"/series/simplify-streaming-ingestion-on-aws/"}],"smallImg":"/blog/2023-03-14-simplify-streaming-ingestion-athena/featured_huef6334952f8505bdd5ee5a48de258194_43403_180x0_resize_box_3.png","tags":[{"title":"AWS","url":"/tags/aws/"},{"title":"Amazon MSK","url":"/tags/amazon-msk/"},{"title":"Amazon Athena","url":"/tags/amazon-athena/"},{"title":"Amazon EventBridge","url":"/tags/amazon-eventbridge/"},{"title":"AWS Lambda","url":"/tags/aws-lambda/"},{"title":"AWS SAM","url":"/tags/aws-sam/"},{"title":"Apache Kafka","url":"/tags/apache-kafka/"},{"title":"Python","url":"/tags/python/"},{"title":"Terraform","url":"/tags/terraform/"}],"timestamp":1678752000,"title":"Simplify Streaming Ingestion on AWS – Part 2 MSK and Athena"},{"categories":[{"title":"Data Streaming","url":"/categories/data-streaming/"}],"content":"Apache Kafka is a popular distributed event store and stream processing platform. Previously loading data from Kafka into Redshift and Athena usually required Kafka connectors (e.g. Amazon Redshift Sink Connector and Amazon S3 Sink Connector). Recently these AWS services provide features to ingest data from Kafka directly, which facilitates a simpler architecture that achieves low-latency and high-speed ingestion of streaming data. In part 1 of the simplify streaming ingestion on AWS series, we discuss how to develop an end-to-end streaming ingestion solution using EventBridge, Lambda, MSK and Redshift Serverless on AWS.\nPart 1 MSK and Redshift (this post) Part 2 MSK and Athena Architecture Fake online order data is generated by multiple Lambda functions that are invoked by an EventBridge schedule rule. The schedule is set to run every minute and the associating rule has a configurable number (e.g. 5) of targets. Each target points to the same Kafka producer Lambda function. In this way we are able to generate test data using multiple Lambda functions according to the desired volume of messages. Once the messages are sent to a Kafka topic, they can be consumed by a materialized view in an external schema that sources data from the MKS cluster. The infrastructure is built by Terraform and the AWS SAM CLI is used to develop the producer Lambda function locally before deploying to AWS.\nInfrastructure A VPC with 3 public and private subnets is created using the AWS VPC Terraform module (vpc.tf). Also, a SoftEther VPN server is deployed in order to access the resources in the private subnets from the developer machine (vpn.tf). It is particularly useful to monitor and manage the MSK cluster and Kafka topic as well as developing the Kafka producer Lambda function locally. The details about how to configure the VPN server can be found in an earlier post. The source can be found in the GitHub repository of this post.\nMSK An MSK cluster with 3 brokers is created. The broker nodes are deployed with the kafka.m5.large instance type in the private subnets. IAM authentication is used for the client authentication method. Note this method is the only secured authentication method supported by Redshift because the external schema supports either the no authentication or IAM authentication method only.\n1# integration-redshift/infra/variable.tf 2locals { 3 ... 4 msk = { 5 version = \u0026#34;3.3.1\u0026#34; 6 instance_size = \u0026#34;kafka.m5.large\u0026#34; 7 ebs_volume_size = 20 8 log_retention_ms = 604800000 # 7 days 9 } 10 ... 11} 12 13# integration-redshift/infra/msk.tf 14resource \u0026#34;aws_msk_cluster\u0026#34; \u0026#34;msk_data_cluster\u0026#34; { 15 cluster_name = \u0026#34;${local.name}-msk-cluster\u0026#34; 16 kafka_version = local.msk.version 17 number_of_broker_nodes = length(module.vpc.private_subnets) 18 configuration_info { 19 arn = aws_msk_configuration.msk_config.arn 20 revision = aws_msk_configuration.msk_config.latest_revision 21 } 22 23 broker_node_group_info { 24 instance_type = local.msk.instance_size 25 client_subnets = module.vpc.private_subnets 26 security_groups = [aws_security_group.msk.id] 27 storage_info { 28 ebs_storage_info { 29 volume_size = local.msk.ebs_volume_size 30 } 31 } 32 } 33 34 client_authentication { 35 sasl { 36 iam = true 37 } 38 } 39 40 logging_info { 41 broker_logs { 42 cloudwatch_logs { 43 enabled = true 44 log_group = aws_cloudwatch_log_group.msk_cluster_lg.name 45 } 46 s3 { 47 enabled = true 48 bucket = aws_s3_bucket.default_bucket.id 49 prefix = \u0026#34;logs/msk/cluster-\u0026#34; 50 } 51 } 52 } 53 54 tags = local.tags 55 56 depends_on = [aws_msk_configuration.msk_config] 57} 58 59resource \u0026#34;aws_msk_configuration\u0026#34; \u0026#34;msk_config\u0026#34; { 60 name = \u0026#34;${local.name}-msk-configuration\u0026#34; 61 62 kafka_versions = [local.msk.version] 63 64 server_properties = \u0026lt;\u0026lt;PROPERTIES 65 auto.create.topics.enable = true 66 delete.topic.enable = true 67 log.retention.ms = ${local.msk.log_retention_ms} 68 PROPERTIES 69} Inbound Rules for MSK Cluster We need to allow access to the MSK cluster from multiple AWS resources. Specifically the VPN server needs access for monitoring/managing the cluster and topic as well as developing the producer Lambda function locally. Also, the Lambda function and Redshift cluster need access for producing and consuming messages respectively. Only the port 9098 is added to the inbound/outbound rules because client access is enabled by the IAM authentication method exclusively. Note that the security group and outbound rule of the Lambda function are created here while the Lambda function is created in a different Terraform stack. This is for ease of adding it to the inbound rule of the MSK’s security group, and later we will discuss how to make use of it with the Lambda function.\n1# integration-redshift/infra/msk.tf 2resource \u0026#34;aws_security_group\u0026#34; \u0026#34;msk\u0026#34; { 3 name = \u0026#34;${local.name}-msk-sg\u0026#34; 4 vpc_id = module.vpc.vpc_id 5 6 lifecycle { 7 create_before_destroy = true 8 } 9 10 tags = local.tags 11} 12 13resource \u0026#34;aws_security_group_rule\u0026#34; \u0026#34;msk_vpn_inbound\u0026#34; { 14 count = local.vpn.to_create ? 1 : 0 15 type = \u0026#34;ingress\u0026#34; 16 description = \u0026#34;VPN access\u0026#34; 17 security_group_id = aws_security_group.msk.id 18 protocol = \u0026#34;tcp\u0026#34; 19 from_port = 9098 20 to_port = 9098 21 source_security_group_id = aws_security_group.vpn[0].id 22} 23 24resource \u0026#34;aws_security_group_rule\u0026#34; \u0026#34;msk_lambda_inbound\u0026#34; { 25 type = \u0026#34;ingress\u0026#34; 26 description = \u0026#34;lambda access\u0026#34; 27 security_group_id = aws_security_group.msk.id 28 protocol = \u0026#34;tcp\u0026#34; 29 from_port = 9098 30 to_port = 9098 31 source_security_group_id = aws_security_group.kafka_producer_lambda.id 32} 33 34resource \u0026#34;aws_security_group_rule\u0026#34; \u0026#34;msk_redshift_inbound\u0026#34; { 35 type = \u0026#34;ingress\u0026#34; 36 description = \u0026#34;redshift access\u0026#34; 37 security_group_id = aws_security_group.msk.id 38 protocol = \u0026#34;tcp\u0026#34; 39 from_port = 9098 40 to_port = 9098 41 source_security_group_id = aws_security_group.redshift_serverless.id 42} 43 44... 45 46resource \u0026#34;aws_security_group\u0026#34; \u0026#34;kafka_producer_lambda\u0026#34; { 47 name = \u0026#34;${local.name}-lambda-msk-access\u0026#34; 48 vpc_id = module.vpc.vpc_id 49 50 lifecycle { 51 create_before_destroy = true 52 } 53 54 tags = local.tags 55} 56 57resource \u0026#34;aws_security_group_rule\u0026#34; \u0026#34;kafka_producer_lambda_msk_egress\u0026#34; { 58 type = \u0026#34;egress\u0026#34; 59 description = \u0026#34;lambda msk access\u0026#34; 60 security_group_id = aws_security_group.kafka_producer_lambda.id 61 protocol = \u0026#34;tcp\u0026#34; 62 from_port = 9098 63 to_port = 9098 64 cidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;] 65} 66 67# integration-redshift/infra/redshift.tf 68resource \u0026#34;aws_security_group\u0026#34; \u0026#34;redshift_serverless\u0026#34; { 69 name = \u0026#34;${local.name}-redshift-serverless\u0026#34; 70 vpc_id = module.vpc.vpc_id 71 72 lifecycle { 73 create_before_destroy = true 74 } 75 76 tags = local.tags 77} 78 79... 80 81resource \u0026#34;aws_security_group_rule\u0026#34; \u0026#34;redshift_msk_egress\u0026#34; { 82 type = \u0026#34;egress\u0026#34; 83 description = \u0026#34;lambda msk access\u0026#34; 84 security_group_id = aws_security_group.redshift_serverless.id 85 protocol = \u0026#34;tcp\u0026#34; 86 from_port = 9098 87 to_port = 9098 88 cidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;] 89} Redshift Serverless A namespace and workgroup are created to deploy a Redshift serverless cluster. As explained in the Redshift user guide, a namespace is a collection of database objects and users and a workgroup is a collection of compute resources.\n1# integration-redshift/infra/redshift.tf 2resource \u0026#34;aws_redshiftserverless_namespace\u0026#34; \u0026#34;namespace\u0026#34; { 3 namespace_name = \u0026#34;${local.name}-namespace\u0026#34; 4 5 admin_username = local.redshift.admin_username 6 admin_user_password = random_password.redshift_admin_pw.result 7 db_name = local.redshift.db_name 8 default_iam_role_arn = aws_iam_role.redshift_serverless_role.arn 9 iam_roles = [aws_iam_role.redshift_serverless_role.arn] 10 11 tags = local.tags 12} 13 14resource \u0026#34;aws_redshiftserverless_workgroup\u0026#34; \u0026#34;workgroup\u0026#34; { 15 namespace_name = aws_redshiftserverless_namespace.namespace.id 16 workgroup_name = \u0026#34;${local.name}-workgroup\u0026#34; 17 18 base_capacity = local.redshift.base_capacity 19 subnet_ids = module.vpc.private_subnets 20 security_group_ids = [aws_security_group.redshift_serverless.id] 21 22 tags = local.tags 23} IAM Permission for MSK Access As illustrated in the AWS documentation, we need an IAM policy that provides permission for communication with the Amazon MSK cluster. The applicable policy is added to the default IAM role of the cluster.\n1# integration-redshift/infra/redshift.tf 2resource \u0026#34;aws_iam_role\u0026#34; \u0026#34;redshift_serverless_role\u0026#34; { 3 name = \u0026#34;${local.name}-redshift-serverless-role\u0026#34; 4 5 assume_role_policy = data.aws_iam_policy_document.redshift_serverless_assume_role_policy.json 6 managed_policy_arns = [ 7 \u0026#34;arn:aws:iam::aws:policy/AmazonS3FullAccess\u0026#34;, 8 \u0026#34;arn:aws:iam::aws:policy/AWSGlueConsoleFullAccess\u0026#34;, 9 \u0026#34;arn:aws:iam::aws:policy/AmazonRedshiftFullAccess\u0026#34;, 10 \u0026#34;arn:aws:iam::aws:policy/AmazonSageMakerFullAccess\u0026#34;, 11 aws_iam_policy.msk_redshift_permission.arn 12 ] 13} 14 15data \u0026#34;aws_iam_policy_document\u0026#34; \u0026#34;redshift_serverless_assume_role_policy\u0026#34; { 16 statement { 17 actions = [\u0026#34;sts:AssumeRole\u0026#34;] 18 19 principals { 20 type = \u0026#34;Service\u0026#34; 21 identifiers = [ 22 \u0026#34;redshift.amazonaws.com\u0026#34;, 23 \u0026#34;sagemaker.amazonaws.com\u0026#34;, 24 \u0026#34;events.amazonaws.com\u0026#34;, 25 \u0026#34;scheduler.redshift.amazonaws.com\u0026#34; 26 ] 27 } 28 29 principals { 30 type = \u0026#34;AWS\u0026#34; 31 identifiers = [\u0026#34;arn:aws:iam::${data.aws_caller_identity.current.account_id}:root\u0026#34;] 32 } 33 } 34} 35 36resource \u0026#34;aws_iam_policy\u0026#34; \u0026#34;msk_redshift_permission\u0026#34; { 37 name = \u0026#34;${local.name}-msk-redshift-permission\u0026#34; 38 39 policy = jsonencode({ 40 Version = \u0026#34;2012-10-17\u0026#34; 41 Statement = [ 42 { 43 Sid = \u0026#34;PermissionOnCluster\u0026#34; 44 Action = [ 45 \u0026#34;kafka-cluster:ReadData\u0026#34;, 46 \u0026#34;kafka-cluster:DescribeTopic\u0026#34;, 47 \u0026#34;kafka-cluster:Connect\u0026#34;, 48 ] 49 Effect = \u0026#34;Allow\u0026#34; 50 Resource = [ 51 \u0026#34;arn:aws:kafka:*:${data.aws_caller_identity.current.account_id}:cluster/*/*\u0026#34;, 52 \u0026#34;arn:aws:kafka:*:${data.aws_caller_identity.current.account_id}:topic/*/*\u0026#34; 53 ] 54 }, 55 { 56 Sid = \u0026#34;PermissionOnGroups\u0026#34; 57 Action = [ 58 \u0026#34;kafka:GetBootstrapBrokers\u0026#34; 59 ] 60 Effect = \u0026#34;Allow\u0026#34; 61 Resource = \u0026#34;*\u0026#34; 62 } 63 ] 64 }) 65} Kafka Producer The resources related to the Kafka producer Lambda function are managed in a separate Terraform stack. This is because it is easier to build the relevant resources iteratively. Note the SAM CLI builds the whole Terraform stack even for a small change of code, and it wouldn’t be convenient if the entire resources are managed in the same stack.\nProducer Source The Kafka producer is created to send messages to a topic named orders where fake order data is generated using the Faker package. The Order class generates one or more fake order records by the _create _method and an order record includes order ID, order timestamp, user ID and order items. The Lambda function sends 100 records at a time followed by sleeping for 1 second. It repeats until it reaches MAX_RUN_SEC (e.g. 60) environment variable value. A Kafka message is made up of an order ID as the key and an order record as the value. Both the key and value are serialised as JSON. Note that the stable version of the kafka-python package does not support the IAM authentication method. Therefore we need to install the package from a forked repository as discussed in this GitHub issue.\n1# integration-redshift/kafka_producer/src/app.py 2import os 3import datetime 4import string 5import json 6import time 7from kafka import KafkaProducer 8from faker import Faker 9 10 11class Order: 12 def __init__(self, fake: Faker = None): 13 self.fake = fake or Faker() 14 15 def order(self): 16 rand_int = self.fake.random_int(1, 1000) 17 user_id = \u0026#34;\u0026#34;.join( 18 [string.ascii_lowercase[int(s)] if s.isdigit() else s for s in hex(rand_int)] 19 )[::-1] 20 return { 21 \u0026#34;order_id\u0026#34;: self.fake.uuid4(), 22 \u0026#34;ordered_at\u0026#34;: datetime.datetime.utcnow(), 23 \u0026#34;user_id\u0026#34;: user_id, 24 } 25 26 def items(self): 27 return [ 28 { 29 \u0026#34;product_id\u0026#34;: self.fake.random_int(1, 9999), 30 \u0026#34;quantity\u0026#34;: self.fake.random_int(1, 10), 31 } 32 for _ in range(self.fake.random_int(1, 4)) 33 ] 34 35 def create(self, num: int): 36 return [{**self.order(), **{\u0026#34;items\u0026#34;: self.items()}} for _ in range(num)] 37 38 39class Producer: 40 def __init__(self, bootstrap_servers: list, topic: str): 41 self.bootstrap_servers = bootstrap_servers 42 self.topic = topic 43 self.producer = self.create() 44 45 def create(self): 46 return KafkaProducer( 47 security_protocol=\u0026#34;SASL_SSL\u0026#34;, 48 sasl_mechanism=\u0026#34;AWS_MSK_IAM\u0026#34;, 49 bootstrap_servers=self.bootstrap_servers, 50 value_serializer=lambda v: json.dumps(v, default=self.serialize).encode(\u0026#34;utf-8\u0026#34;), 51 key_serializer=lambda v: json.dumps(v, default=self.serialize).encode(\u0026#34;utf-8\u0026#34;), 52 ) 53 54 def send(self, orders: list): 55 for order in orders: 56 self.producer.send(self.topic, key={\u0026#34;order_id\u0026#34;: order[\u0026#34;order_id\u0026#34;]}, value=order) 57 self.producer.flush() 58 59 def serialize(self, obj): 60 if isinstance(obj, datetime.datetime): 61 return obj.isoformat() 62 if isinstance(obj, datetime.date): 63 return str(obj) 64 return obj 65 66 67def lambda_function(event, context): 68 if os.getenv(\u0026#34;BOOTSTRAP_SERVERS\u0026#34;, \u0026#34;\u0026#34;) == \u0026#34;\u0026#34;: 69 return 70 fake = Faker() 71 producer = Producer( 72 bootstrap_servers=os.getenv(\u0026#34;BOOTSTRAP_SERVERS\u0026#34;).split(\u0026#34;,\u0026#34;), topic=os.getenv(\u0026#34;TOPIC_NAME\u0026#34;) 73 ) 74 s = datetime.datetime.now() 75 ttl_rec = 0 76 while True: 77 orders = Order(fake).create(100) 78 producer.send(orders) 79 ttl_rec += len(orders) 80 print(f\u0026#34;sent {len(orders)} messages\u0026#34;) 81 elapsed_sec = (datetime.datetime.now() - s).seconds 82 if elapsed_sec \u0026gt; int(os.getenv(\u0026#34;MAX_RUN_SEC\u0026#34;, \u0026#34;60\u0026#34;)): 83 print(f\u0026#34;{ttl_rec} records are sent in {elapsed_sec} seconds ...\u0026#34;) 84 break 85 time.sleep(1) A sample order record is shown below.\n1{ 2 \u0026#34;order_id\u0026#34;: \u0026#34;fc72ccf4-8e98-42b1-9a48-4a6222996be4\u0026#34;, 3 \u0026#34;ordered_at\u0026#34;: \u0026#34;2023-02-05T04:30:58.722158\u0026#34;, 4 \u0026#34;user_id\u0026#34;: \u0026#34;hfbxa\u0026#34;, 5 \u0026#34;items\u0026#34;: [ 6 { 7 \u0026#34;product_id\u0026#34;: 8576, 8 \u0026#34;quantity\u0026#34;: 5 9 }, 10 { 11 \u0026#34;product_id\u0026#34;: 3101, 12 \u0026#34;quantity\u0026#34;: 8 13 } 14 ] 15} Lambda Function As the VPC, subnets, Lambda security group and MSK cluster are created in the infra Terraform stack, they need to be obtained from the producer Lambda stack. It can be achieved using the Terraform data sources as shown below. Note that the private subnets can be filtered by a specific tag (Tier: Private), which is added while creating them.\n1# integration-redshift/infra/vpc.tf 2module \u0026#34;vpc\u0026#34; { 3 source = \u0026#34;terraform-aws-modules/vpc/aws\u0026#34; 4 version = \u0026#34;~\u0026gt; 3.14\u0026#34; 5 6 name = \u0026#34;${local.name}-vpc\u0026#34; 7 cidr = local.vpc.cidr 8 9 azs = local.vpc.azs 10 public_subnets = [for k, v in local.vpc.azs : cidrsubnet(local.vpc.cidr, 3, k)] 11 private_subnets = [for k, v in local.vpc.azs : cidrsubnet(local.vpc.cidr, 3, k + 3)] 12 ... 13 14 private_subnet_tags = { 15 \u0026#34;Tier\u0026#34; = \u0026#34;Private\u0026#34; 16 } 17 18 tags = local.tags 19} 20 21# integration-redshift/kafka_producer/variables.tf 22data \u0026#34;aws_caller_identity\u0026#34; \u0026#34;current\u0026#34; {} 23 24data \u0026#34;aws_region\u0026#34; \u0026#34;current\u0026#34; {} 25 26data \u0026#34;aws_vpc\u0026#34; \u0026#34;selected\u0026#34; { 27 filter { 28 name = \u0026#34;tag:Name\u0026#34; 29 values = [\u0026#34;${local.infra_prefix}\u0026#34;] 30 } 31} 32 33data \u0026#34;aws_subnets\u0026#34; \u0026#34;private\u0026#34; { 34 filter { 35 name = \u0026#34;vpc-id\u0026#34; 36 values = [data.aws_vpc.selected.id] 37 } 38 39 tags = { 40 Tier = \u0026#34;Private\u0026#34; 41 } 42} 43 44data \u0026#34;aws_msk_cluster\u0026#34; \u0026#34;msk_data_cluster\u0026#34; { 45 cluster_name = \u0026#34;${local.infra_prefix}-msk-cluster\u0026#34; 46} 47 48data \u0026#34;aws_security_group\u0026#34; \u0026#34;kafka_producer_lambda\u0026#34; { 49 name = \u0026#34;${local.infra_prefix}-lambda-msk-access\u0026#34; 50} 51locals { 52 ... 53 infra_prefix = \u0026#34;integration-redshift\u0026#34; 54 ... 55} The AWS Lambda Terraform module is used to create the producer Lambda function. Note that, in order to develop a Lambda function using AWS SAM, we need to create sam metadata resource, which provides the AWS SAM CLI with the information it needs to locate Lambda functions and layers, along with their source code, build dependencies, and build logic from within your Terraform project. It is created by default by the Terraform module, which is convenient. Also, we need to give permission to the EventBridge rule to invoke the Lambda function, and it is given by the aws_lambda_permission resource.\n1# integration-redshift/kafka_producer/variables.tf 2locals { 3 name = local.infra_prefix 4 region = data.aws_region.current.name 5 environment = \u0026#34;dev\u0026#34; 6 7 infra_prefix = \u0026#34;integration-redshift\u0026#34; 8 9 producer = { 10 src_path = \u0026#34;src\u0026#34; 11 function_name = \u0026#34;kafka_producer\u0026#34; 12 handler = \u0026#34;app.lambda_function\u0026#34; 13 concurrency = 5 14 timeout = 90 15 memory_size = 128 16 runtime = \u0026#34;python3.8\u0026#34; 17 schedule_rate = \u0026#34;rate(1 minute)\u0026#34; 18 to_enable_trigger = false 19 environment = { 20 topic_name = \u0026#34;orders\u0026#34; 21 max_run_sec = 60 22 } 23 } 24 ... 25} 26 27# integration-redshift/kafka_producer/main.tf 28module \u0026#34;kafka_producer_lambda\u0026#34; { 29 source = \u0026#34;terraform-aws-modules/lambda/aws\u0026#34; 30 31 function_name = local.producer.function_name 32 handler = local.producer.handler 33 runtime = local.producer.runtime 34 timeout = local.producer.timeout 35 memory_size = local.producer.memory_size 36 source_path = local.producer.src_path 37 vpc_subnet_ids = data.aws_subnets.private.ids 38 vpc_security_group_ids = [data.aws_security_group.kafka_producer_lambda.id] 39 attach_network_policy = true 40 attach_policies = true 41 policies = [aws_iam_policy.msk_lambda_permission.arn] 42 number_of_policies = 1 43 environment_variables = { 44 BOOTSTRAP_SERVERS = data.aws_msk_cluster.msk_data_cluster.bootstrap_brokers_sasl_iam 45 TOPIC_NAME = local.producer.environment.topic_name 46 MAX_RUN_SEC = local.producer.environment.max_run_sec 47 } 48 49 tags = local.tags 50} 51 52resource \u0026#34;aws_lambda_function_event_invoke_config\u0026#34; \u0026#34;kafka_producer_lambda\u0026#34; { 53 function_name = module.kafka_producer_lambda.lambda_function_name 54 maximum_retry_attempts = 0 55} 56 57resource \u0026#34;aws_lambda_permission\u0026#34; \u0026#34;allow_eventbridge\u0026#34; { 58 count = local.producer.to_enable_trigger ? 1 : 0 59 statement_id = \u0026#34;InvokeLambdaFunction\u0026#34; 60 action = \u0026#34;lambda:InvokeFunction\u0026#34; 61 function_name = local.producer.function_name 62 principal = \u0026#34;events.amazonaws.com\u0026#34; 63 source_arn = module.eventbridge.eventbridge_rule_arns[\u0026#34;crons\u0026#34;] 64 65 depends_on = [ 66 module.eventbridge 67 ] 68} IAM Permission for MSK The producer Lambda function needs permission to send messages to the orders topic of the MSK cluster. The following IAM policy is added to the Lambda function according to the AWS documentation.\n1# integration-redshift/kafka_producer/main.tf 2resource \u0026#34;aws_iam_policy\u0026#34; \u0026#34;msk_lambda_permission\u0026#34; { 3 name = \u0026#34;${local.name}-msk-lambda-permission\u0026#34; 4 5 policy = jsonencode({ 6 Version = \u0026#34;2012-10-17\u0026#34; 7 Statement = [ 8 { 9 Sid = \u0026#34;PermissionOnCluster\u0026#34; 10 Action = [ 11 \u0026#34;kafka-cluster:Connect\u0026#34;, 12 \u0026#34;kafka-cluster:AlterCluster\u0026#34;, 13 \u0026#34;kafka-cluster:DescribeCluster\u0026#34; 14 ] 15 Effect = \u0026#34;Allow\u0026#34; 16 Resource = \u0026#34;arn:aws:kafka:${local.region}:${data.aws_caller_identity.current.account_id}:cluster/${local.infra_prefix}-msk-cluster/*\u0026#34; 17 }, 18 { 19 Sid = \u0026#34;PermissionOnTopics\u0026#34; 20 Action = [ 21 \u0026#34;kafka-cluster:*Topic*\u0026#34;, 22 \u0026#34;kafka-cluster:WriteData\u0026#34;, 23 \u0026#34;kafka-cluster:ReadData\u0026#34; 24 ] 25 Effect = \u0026#34;Allow\u0026#34; 26 Resource = \u0026#34;arn:aws:kafka:${local.region}:${data.aws_caller_identity.current.account_id}:topic/${local.infra_prefix}-msk-cluster/*\u0026#34; 27 }, 28 { 29 Sid = \u0026#34;PermissionOnGroups\u0026#34; 30 Action = [ 31 \u0026#34;kafka-cluster:AlterGroup\u0026#34;, 32 \u0026#34;kafka-cluster:DescribeGroup\u0026#34; 33 ] 34 Effect = \u0026#34;Allow\u0026#34; 35 Resource = \u0026#34;arn:aws:kafka:${local.region}:${data.aws_caller_identity.current.account_id}:group/${local.infra_prefix}-msk-cluster/*\u0026#34; 36 } 37 ] 38 }) 39} EventBridge Rule The AWS EventBridge Terraform module is used to create the EventBridge schedule rule and targets. Note that 5 targets that point to the Kafka producer Lambda function are created so that it is invoked concurrently every minute.\n1# integration-redshift/kafka_producer/main.tf 2module \u0026#34;eventbridge\u0026#34; { 3 source = \u0026#34;terraform-aws-modules/eventbridge/aws\u0026#34; 4 5 create_bus = false 6 7 rules = { 8 crons = { 9 description = \u0026#34;Kafka producer lambda schedule\u0026#34; 10 schedule_expression = local.producer.schedule_rate 11 } 12 } 13 14 targets = { 15 crons = [for i in range(local.producer.concurrency) : { 16 name = \u0026#34;lambda-target-${i}\u0026#34; 17 arn = module.kafka_producer_lambda.lambda_function_arn 18 }] 19 } 20 21 depends_on = [ 22 module.kafka_producer_lambda 23 ] 24 25 tags = local.tags 26} Deployment Topic Creation We first need to create the Kafka topic and it is done using kafka-ui. The UI can be started using docker-compose with the following compose file. Note the VPN connection has to be established in order to access the cluster from the developer machine.\n1# integration-redshift/docker-compose.yml 2version: \u0026#34;3\u0026#34; 3 4services: 5 kafka-ui: 6 image: provectuslabs/kafka-ui:master 7 container_name: kafka-ui 8 ports: 9 - \u0026#34;8080:8080\u0026#34; 10 networks: 11 - kafkanet 12 environment: 13 AWS_ACCESS_KEY_ID: $AWS_ACCESS_KEY_ID 14 AWS_SECRET_ACCESS_KEY: $AWS_SECRET_ACCESS_KEY 15 AWS_SESSION_TOKEN: $AWS_SESSION_TOKEN 16 KAFKA_CLUSTERS_0_NAME: msk 17 KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: $BOOTSTRAP_SERVERS 18 KAFKA_CLUSTERS_0_PROPERTIES_SECURITY_PROTOCOL: SASL_SSL 19 KAFKA_CLUSTERS_0_PROPERTIES_SASL_MECHANISM: AWS_MSK_IAM 20 KAFKA_CLUSTERS_0_PROPERTIES_SASL_CLIENT_CALLBACK_HANDLER_CLASS: software.amazon.msk.auth.iam.IAMClientCallbackHandler 21 KAFKA_CLUSTERS_0_PROPERTIES_SASL_JAAS_CONFIG: software.amazon.msk.auth.iam.IAMLoginModule required; 22 23networks: 24 kafkanet: 25 name: kafka-network A topic named orders is created that has 3 partitions and replication factors. Also, it is set to retain data for 4 weeks.\nOnce created, it redirects to the overview section of the topic.\nLocal Testing with SAM To simplify development, the Eventbridge permission is disabled by setting to_enable_trigger to false. Also, it is shortened to loop before it gets stopped by reducing msx_run_sec to 10.\n1# integration-redshift/kafka_producer/variables.tf 2locals { 3 producer = { 4 ... 5 to_enable_trigger = false 6 environment = { 7 topic_name = \u0026#34;orders\u0026#34; 8 max_run_sec = 10 9 } 10 } 11 ... 12} The Lambda function can be built with the SAM build command while specifying the hook name as terraform and enabling beta features. Once completed, it stores the build artifacts and template in the .aws-sam folder.\n1$ sam build --hook-name terraform --beta-features 2# ... 3# 4# Apply complete! ... 5 6# Build Succeeded 7 8# Built Artifacts : .aws-sam/build 9# Built Template : .aws-sam/build/template.yaml 10 11# Commands you can use next 12# ========================= 13# [*] Invoke Function: sam local invoke --hook-name terraform 14# [*] Emulate local Lambda functions: sam local start-lambda --hook-name terraform We can invoke the Lambda function locally using the SAM local invoke command. The Lambda function is invoked in a Docker container and the invocation logs are printed in the terminal as shown below.\n1$ sam local invoke --hook-name terraform module.kafka_producer_lambda.aws_lambda_function.this[0] --beta-features 2# Experimental features are enabled for this session. 3# Visit the docs page to learn more about the AWS Beta terms https://aws.amazon.com/service-terms/. 4 5# Skipped prepare hook. Current application is already prepared. 6# Invoking app.lambda_function (python3.8) 7# Skip pulling image and use local one: public.ecr.aws/sam/emulation-python3.8:rapid-1.70.0-x86_64. 8 9# Mounting .../kafka-pocs/integration-redshift/kafka_producer/.aws-sam/build/ModuleKafkaProducerLambdaAwsLambdaFunctionThis069E06354 as /var/task:ro,delegated inside runtime container 10# START RequestId: fbfc11be-362a-48df-b894-232cc88234ee Version: $LATEST 11# sent 100 messages 12# sent 100 messages 13# sent 100 messages 14# sent 100 messages 15# sent 100 messages 16# sent 100 messages 17# sent 100 messages 18# sent 100 messages 19# sent 100 messages 20# sent 100 messages 21# sent 100 messages 22# 1100 records are sent in 11 seconds ... 23# END RequestId: fbfc11be-362a-48df-b894-232cc88234ee 24# REPORT RequestId: fbfc11be-362a-48df-b894-232cc88234ee Init Duration: 0.18 ms Duration: 12494.86 ms Billed Duration: 12495 ms Memory Size: 128 MB Max Memory Used: 128 MB 25# null We can also check the messages using kafka-ui.\nExternal Schema and Materialized View Creation As we have messages in the orders topic, we can create a materialized view to consume data from it. First we need to create an external schema that sources data from the MSK cluster. We can use the default IAM role as we have already added the necessary IAM permission to it. Also, we should specify the IAM authentication method as it is the only allowed method for the MSK cluster. The materialized view selects key Kafka configuration variables and parses the Kafka value as data. The _JSON_PARSE _function converts the JSON string into the SUPER type, which makes it easy to select individual attributes. Also, it is configured to refresh automatically so that it ingests up-to-date data without manual refresh.\n1CREATE EXTERNAL SCHEMA msk_orders 2FROM MSK 3IAM_ROLE default 4AUTHENTICATION iam 5CLUSTER_ARN \u0026#39;\u0026lt;MSK Cluster ARN\u0026gt;\u0026#39;; 6 7CREATE MATERIALIZED VIEW orders AUTO REFRESH YES AS 8SELECT 9 \u0026#34;kafka_partition\u0026#34;, 10 \u0026#34;kafka_offset\u0026#34;, 11 \u0026#34;kafka_timestamp_type\u0026#34;, 12 \u0026#34;kafka_timestamp\u0026#34;, 13 \u0026#34;kafka_key\u0026#34;, 14 JSON_PARSE(\u0026#34;kafka_value\u0026#34;) as data, 15 \u0026#34;kafka_headers\u0026#34; 16FROM msk_orders.orders; We can see the ingested Kafka messages as shown below.\nOrder Items View Creation The materialized view keeps the entire order data in a single column, and it is not easy to build queries for analytics. As mentioned earlier, we can easily select individual attributes from the data column, but the issue is each record has an array of order items that has a variable length. Redshift doesn’t have a function to explode an array into rows, but we can achieve it using a recursive CTE. Below shows a view that converts order items array into rows recursively.\n1CREATE OR REPLACE VIEW order_items AS 2 WITH RECURSIVE exploded_items (order_id, ordered_at, user_id, idx, product_id, quantity) AS ( 3 WITH cte AS ( 4 SELECT 5 data.order_id::character(36) AS order_id, 6 data.ordered_at::timestamp AS ordered_at, 7 data.user_id::varchar(10) AS user_id, 8 data.items AS items, 9 get_array_length(data.items) AS num_items 10 FROM orders 11 ) 12 SELECT 13 order_id, 14 ordered_at, 15 user_id, 16 0 AS idx, 17 items[0].product_id::int AS product_id, 18 items[0].quantity::int AS quantity 19 FROM cte 20 UNION ALL 21 SELECT 22 cte.order_id, 23 cte.ordered_at, 24 cte.user_id, 25 idx + 1, 26 cte.items[idx + 1].product_id::int, 27 cte.items[idx + 1].quantity::int 28 FROM cte 29 JOIN exploded_items ON cte.order_id = exploded_items.order_id 30 WHERE idx \u0026lt; cte.num_items - 1 31 ) 32 SELECT * 33 FROM exploded_items 34 ORDER BY order_id; We can see the exploded order items as shown below.\nKafka Producer Deployment Now we can deploy the Kafka producer Lambda function and EventBridge scheduler using Terraform as usual after resetting the configuration variables. Once deployed, we can see that the scheduler rule has 5 targets of the same Lambda function.\nWe can check if the Kafka producer sends messages correctly using kafka-ui. After about 30 minutes, we see about 840,000 messages are created in the orders topic.\nQuery Order Items As the materialized view is set to refresh automatically, we don’t have to refresh it manually. Using the order items view, we can query the top 10 popular products as shown below.\nSummary Streaming ingestion from Kafka (MSK) into Redshift and Athena can be much simpler as they now support direct integration. In part 1 of this series, we discussed an end-to-end streaming ingestion solution using EventBridge, Lambda, MSK and Redshift. We also used AWS SAM integrated with Terraform for developing a Lambda function locally.\n","date":"February 8, 2023","img":"/blog/2023-02-08-simplify-streaming-ingestion-redshift/featured.png","lang":"en","langName":"English","largeImg":"/blog/2023-02-08-simplify-streaming-ingestion-redshift/featured_hu77abd5c8ed84cebc51bde5ecaefc7320_32864_500x0_resize_box_3.png","permalink":"/blog/2023-02-08-simplify-streaming-ingestion-redshift/","series":[{"title":"Simplify Streaming Ingestion on AWS","url":"/series/simplify-streaming-ingestion-on-aws/"}],"smallImg":"/blog/2023-02-08-simplify-streaming-ingestion-redshift/featured_hu77abd5c8ed84cebc51bde5ecaefc7320_32864_180x0_resize_box_3.png","tags":[{"title":"AWS","url":"/tags/aws/"},{"title":"Amazon MSK","url":"/tags/amazon-msk/"},{"title":"Amazon Redshift","url":"/tags/amazon-redshift/"},{"title":"Amazon EventBridge","url":"/tags/amazon-eventbridge/"},{"title":"AWS Lambda","url":"/tags/aws-lambda/"},{"title":"AWS SAM","url":"/tags/aws-sam/"},{"title":"Apache Kafka","url":"/tags/apache-kafka/"},{"title":"Python","url":"/tags/python/"},{"title":"Terraform","url":"/tags/terraform/"}],"timestamp":1675814400,"title":"Simplify Streaming Ingestion on AWS – Part 1 MSK and Redshift"},{"categories":[{"title":"Data Streaming","url":"/categories/data-streaming/"}],"content":"Normally we consume Kafka messages from the beginning/end of a topic or last committed offsets. For backfilling or troubleshooting, however, we need to consume messages from a certain timestamp occasionally. If we know which topic partition to choose e.g. by assigning a topic partition, we can easily override the fetch offset to a specific timestamp. When we deploy multiple consumer instances together, however, we make them subscribe to a topic and topic partitions are dynamically assigned, which means we cannot determine which fetch offset to use for an instance. In this post, we develop Kafka producer and consumer applications using the kafka-python package and discuss how to configure the consumer instances to seek offsets to a specific timestamp where topic partitions are dynamically assigned by subscription.\nKafka Docker Environment A single node Kafka cluster is created as a docker-compose service with Zookeeper, which is used to store the cluster metadata. Note that the Kafka and Zookeeper data directories are mapped to host directories so that Kafka topics and messages are preserved when the services are restarted. As discussed below, fake messages are published into a Kafka topic by a producer application, and it runs outside the docker network (kafkanet). In order for the producer to access the Kafka cluster, we need to add an external listener, and it is configured on port 9093. Finally, the Kafka UI is added for monitoring the Kafka broker and related resources. The source can be found in the GitHub repository for this post.\n1# offset-seeking/compose-kafka.yml 2version: \u0026#34;3\u0026#34; 3 4services: 5 zookeeper: 6 image: bitnami/zookeeper:3.7.0 7 container_name: zookeeper 8 ports: 9 - \u0026#34;2181:2181\u0026#34; 10 networks: 11 - kafkanet 12 environment: 13 - ALLOW_ANONYMOUS_LOGIN=yes 14 volumes: 15 - ./.bitnami/zookeeper/data:/bitnami/zookeeper/data 16 kafka: 17 image: bitnami/kafka:2.8.1 18 container_name: kafka 19 expose: 20 - 9092 21 ports: 22 - \u0026#34;9093:9093\u0026#34; 23 networks: 24 - kafkanet 25 environment: 26 - ALLOW_PLAINTEXT_LISTENER=yes 27 - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181 28 - KAFKA_CFG_BROKER_ID=0 29 - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=CLIENT:PLAINTEXT,EXTERNAL:PLAINTEXT 30 - KAFKA_CFG_LISTENERS=CLIENT://:9092,EXTERNAL://:9093 31 - KAFKA_CFG_ADVERTISED_LISTENERS=CLIENT://kafka:9092,EXTERNAL://localhost:9093 32 - KAFKA_CFG_INTER_BROKER_LISTENER_NAME=CLIENT 33 volumes: 34 - ./.bitnami/kafka/data:/bitnami/kafka/data 35 - ./.bitnami/kafka/logs:/opt/bitnami/kafka/logs 36 depends_on: 37 - zookeeper 38 kafka-ui: 39 image: provectuslabs/kafka-ui:master 40 container_name: kafka-ui 41 ports: 42 - \u0026#34;8080:8080\u0026#34; 43 networks: 44 - kafkanet 45 environment: 46 KAFKA_CLUSTERS_0_NAME: local 47 KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:9092 48 KAFKA_CLUSTERS_0_ZOOKEEPER: zookeeper:2181 49 depends_on: 50 - zookeeper 51 - kafka 52 53networks: 54 kafkanet: 55 name: kafka-network Before we start the services, we need to create the directories that are used for volume-mapping and to update their permission. Then the services can be started as usual. A Kafka topic having two partitions is used in this post, and it is created manually as it is different from the default configuration.\n1# create folders that will be volume-mapped and update permission 2$ mkdir -p .bitnami/zookeeper/data .bitnami/kafka/data .bitnami/kafka/logs \\ 3 \u0026amp;\u0026amp; chmod 777 -R .bitnami 4 5# start docker services - zookeeper, kafka and kafka-ui 6$ docker-compose -f compose-kafka.yml up -d 7 8# create a topic named orders with 2 partitions 9$ docker exec -it kafka \\ 10 bash -c \u0026#34;/opt/bitnami/kafka/bin/kafka-topics.sh \\ 11 --create --topic orders --partitions 2 --bootstrap-server kafka:9092\u0026#34; The topic can be checked in the Kafka UI as shown below.\nKafka Producer Application A Kafka producer is created to send messages to the orders topic and fake messages are generated using the Faker package.\nOrder Data The Order class generates one or more fake order records by the create method. An order record includes order ID, order timestamp, customer and order items.\n1# offset-seeking/producer.py 2class Order: 3 def __init__(self, fake: Faker = None): 4 self.fake = fake or Faker() 5 6 def order(self): 7 return {\u0026#34;order_id\u0026#34;: self.fake.uuid4(), \u0026#34;ordered_at\u0026#34;: self.fake.date_time_this_decade()} 8 9 def items(self): 10 return [ 11 {\u0026#34;product_id\u0026#34;: self.fake.uuid4(), \u0026#34;quantity\u0026#34;: self.fake.random_int(1, 10)} 12 for _ in range(self.fake.random_int(1, 4)) 13 ] 14 15 def customer(self): 16 name = self.fake.name() 17 email = f\u0026#39;{re.sub(\u0026#34; \u0026#34;, \u0026#34;_\u0026#34;, name.lower())}@{re.sub(r\u0026#34;^.*?@\u0026#34;, \u0026#34;\u0026#34;, self.fake.email())}\u0026#39; 18 return { 19 \u0026#34;user_id\u0026#34;: self.fake.uuid4(), 20 \u0026#34;name\u0026#34;: name, 21 \u0026#34;dob\u0026#34;: self.fake.date_of_birth(), 22 \u0026#34;address\u0026#34;: self.fake.address(), 23 \u0026#34;phone\u0026#34;: self.fake.phone_number(), 24 \u0026#34;email\u0026#34;: email, 25 } 26 27 def create(self, num: int): 28 return [ 29 {**self.order(), **{\u0026#34;items\u0026#34;: self.items(), \u0026#34;customer\u0026#34;: self.customer()}} 30 for _ in range(num) 31 ] A sample order record is shown below.\n1{ 2 \u0026#34;order_id\u0026#34;: \u0026#34;567b3036-9ac4-440c-8849-ba4d263796db\u0026#34;, 3 \u0026#34;ordered_at\u0026#34;: \u0026#34;2022-11-09T21:24:55\u0026#34;, 4 \u0026#34;items\u0026#34;: [ 5 { 6 \u0026#34;product_id\u0026#34;: \u0026#34;7289ca92-eabf-4ebc-883c-530e16ecf9a3\u0026#34;, 7 \u0026#34;quantity\u0026#34;: 7 8 }, 9 { 10 \u0026#34;product_id\u0026#34;: \u0026#34;2ab8a155-bb15-4550-9ade-44d0bf2c730a\u0026#34;, 11 \u0026#34;quantity\u0026#34;: 5 12 }, 13 { 14 \u0026#34;product_id\u0026#34;: \u0026#34;81538fa2-6bc0-4903-a40f-a9303e5d3583\u0026#34;, 15 \u0026#34;quantity\u0026#34;: 3 16 } 17 ], 18 \u0026#34;customer\u0026#34;: { 19 \u0026#34;user_id\u0026#34;: \u0026#34;9a18e5f0-62eb-4b50-ae12-9f6f1bd1a80b\u0026#34;, 20 \u0026#34;name\u0026#34;: \u0026#34;David Boyle\u0026#34;, 21 \u0026#34;dob\u0026#34;: \u0026#34;1965-11-25\u0026#34;, 22 \u0026#34;address\u0026#34;: \u0026#34;8128 Whitney Branch\\nNorth Brianmouth, MD 24870\u0026#34;, 23 \u0026#34;phone\u0026#34;: \u0026#34;843-345-1004\u0026#34;, 24 \u0026#34;email\u0026#34;: \u0026#34;david_boyle@example.org\u0026#34; 25 } 26} Kafka Producer The Kafka producer sends one or more order records. A message is made up of an order ID as the key and an order record as the value. Both the key and value are serialised as JSON. Once started, it sends order messages to the topic indefinitely and ten messages are sent in a loop. Note that the external listener (localhost:9093) is specified as the bootstrap server because it runs outside the docker network. We can run the producer app simply by python producer.py.\n1# offset-seeking/producer.py 2class Producer: 3 def __init__(self, bootstrap_servers: list, topic: str): 4 self.bootstrap_servers = bootstrap_servers 5 self.topic = topic 6 self.producer = self.create() 7 8 def create(self): 9 return KafkaProducer( 10 bootstrap_servers=self.bootstrap_servers, 11 value_serializer=lambda v: json.dumps(v, default=self.serialize).encode(\u0026#34;utf-8\u0026#34;), 12 key_serializer=lambda v: json.dumps(v, default=self.serialize).encode(\u0026#34;utf-8\u0026#34;), 13 ) 14 15 def send(self, orders: list): 16 for order in orders: 17 self.producer.send(self.topic, key={\u0026#34;order_id\u0026#34;: order[\u0026#34;order_id\u0026#34;]}, value=order) 18 self.producer.flush() 19 20 def serialize(self, obj): 21 if isinstance(obj, datetime.datetime): 22 return obj.isoformat() 23 if isinstance(obj, datetime.date): 24 return str(obj) 25 return obj 26 27 28if __name__ == \u0026#34;__main__\u0026#34;: 29 fake = Faker() 30 # Faker.seed(1237) 31 producer = Producer(bootstrap_servers=[\u0026#34;localhost:9093\u0026#34;], topic=\u0026#34;orders\u0026#34;) 32 33 while True: 34 orders = Order(fake).create(10) 35 producer.send(orders) 36 print(\u0026#34;messages sent...\u0026#34;) 37 time.sleep(5) After a while, we can see that messages are sent to the orders topic. Out of 2390 messages, 1179 and 1211 messages are sent to the partition 0 and 1 respectively.\nKafka Consumer Application Two consumer instances are deployed in the same consumer group. As the topic has two partitions, it is expected each instance is assigned to a single topic partition. A custom consumer rebalance listener is registered so that the fetch offset is overridden with an offset timestamp environment variable (offset_str) when a topic partition is assigned.\nCustom Consumer Rebalance Listener The consumer rebalancer listener is a callback interface that custom actions can be implemented when topic partitions are assigned or revoked. For each topic partition assigned, it obtains the earliest offset whose timestamp is greater than or equal to the given timestamp in the corresponding partition using the offsets_for_times method. Then it overrides the fetch offset using the seek method. Note that, as consumer instances can be rebalanced multiple times over time, the OFFSET_STR value is better to be stored in an external configuration store. In this way we can control whether to override fetch offsets by changing configuration externally.\n1# offset-seeking/consumer.py 2class RebalanceListener(ConsumerRebalanceListener): 3 def __init__(self, consumer: KafkaConsumer, offset_str: str = None): 4 self.consumer = consumer 5 self.offset_str = offset_str 6 7 def on_partitions_revoked(self, revoked): 8 pass 9 10 def on_partitions_assigned(self, assigned): 11 ts = self.convert_to_ts(self.offset_str) 12 logging.info(f\u0026#34;offset_str - {self.offset_str}, timestamp - {ts}\u0026#34;) 13 if ts is not None: 14 for tp in assigned: 15 logging.info(f\u0026#34;topic partition - {tp}\u0026#34;) 16 self.seek_by_timestamp(tp.topic, tp.partition, ts) 17 18 def convert_to_ts(self, offset_str: str): 19 try: 20 dt = datetime.datetime.fromisoformat(offset_str) 21 return int(dt.timestamp() * 1000) 22 except Exception: 23 return None 24 25 def seek_by_timestamp(self, topic_name: str, partition: int, ts: int): 26 tp = TopicPartition(topic_name, partition) 27 offset_n_ts = self.consumer.offsets_for_times({tp: ts}) 28 logging.info(f\u0026#34;offset and ts - {offset_n_ts}\u0026#34;) 29 if offset_n_ts[tp] is not None: 30 offset = offset_n_ts[tp].offset 31 try: 32 self.consumer.seek(tp, offset) 33 except KafkaError: 34 logging.error(\u0026#34;fails to seek offset\u0026#34;) 35 else: 36 logging.warning(\u0026#34;offset is not looked up\u0026#34;) Kafka Consumer While it is a common practice to specify one or more Kafka topics in the Kafka consumer class when it is instantiated, the consumer omits them in the create method. It is in order to register the custom rebalance listener. In the process method, the consumer subscribes to the orders topic while registering the custom listener. After subscribing to the topic, it polls a single message at a time for ease of tracking.\n1# offset-seeking/consumer.py 2class Consumer: 3 def __init__( 4 self, topics: list, group_id: str, bootstrap_servers: list, offset_str: str = None 5 ): 6 self.topics = topics 7 self.group_id = group_id 8 self.bootstrap_servers = bootstrap_servers 9 self.offset_str = offset_str 10 self.consumer = self.create() 11 12 def create(self): 13 return KafkaConsumer( 14 bootstrap_servers=self.bootstrap_servers, 15 auto_offset_reset=\u0026#34;earliest\u0026#34;, 16 enable_auto_commit=True, 17 group_id=self.group_id, 18 key_deserializer=lambda v: json.loads(v.decode(\u0026#34;utf-8\u0026#34;)), 19 value_deserializer=lambda v: json.loads(v.decode(\u0026#34;utf-8\u0026#34;)), 20 ) 21 22 def process(self): 23 self.consumer.subscribe( 24 self.topics, listener=RebalanceListener(self.consumer, self.offset_str) 25 ) 26 try: 27 while True: 28 msg = self.consumer.poll(timeout_ms=1000, max_records=1) 29 if msg is None: 30 continue 31 self.print_info(msg) 32 time.sleep(5) 33 except KafkaError as error: 34 logging.error(error) 35 finally: 36 self.consumer.close() 37 38 def print_info(self, msg: dict): 39 for _, v in msg.items(): 40 for r in v: 41 ts = r.timestamp 42 dt = datetime.datetime.fromtimestamp(ts / 1000).isoformat() 43 logging.info( 44 f\u0026#34;topic - {r.topic}, partition - {r.partition}, offset - {r.offset}, ts - {ts}, dt - {dt})\u0026#34; 45 ) 46 47 48if __name__ == \u0026#34;__main__\u0026#34;: 49 consumer = Consumer( 50 topics=os.getenv(\u0026#34;TOPICS\u0026#34;, \u0026#34;orders\u0026#34;).split(\u0026#34;,\u0026#34;), 51 group_id=os.getenv(\u0026#34;GROUP_ID\u0026#34;, \u0026#34;orders-group\u0026#34;), 52 bootstrap_servers=os.getenv(\u0026#34;BOOTSTRAP_SERVERS\u0026#34;, \u0026#34;localhost:9093\u0026#34;).split(\u0026#34;,\u0026#34;), 53 offset_str=os.getenv(\u0026#34;OFFSET_STR\u0026#34;, None), 54 ) 55 consumer.process() Docker-compose is used to deploy multiple instances of the consumer. Note that the compose service uses the same docker network (kafkanet) so that it can use kafka:9092 as the bootstrap server address. The OFFSET_STR environment variable is used to override the fetch offset.\n1# offset-seeking/compose-consumer.yml 2version: \u0026#34;3\u0026#34; 3 4services: 5 consumer: 6 image: bitnami/python:3.9 7 command: \u0026#34;sh -c \u0026#39;pip install -r requirements.txt \u0026amp;\u0026amp; python consumer.py\u0026#39;\u0026#34; 8 networks: 9 - kafkanet 10 environment: 11 TOPICS: orders 12 GROUP_ID: orders-group 13 BOOTSTRAP_SERVERS: kafka:9092 14 OFFSET_STR: \u0026#34;2023-01-06T19:00:00\u0026#34; 15 TZ: Australia/Sydney 16 volumes: 17 - .:/app 18networks: 19 kafkanet: 20 external: true 21 name: kafka-network We can start two consumer instances by scaling the consumer service number to 2.\n1# start 2 instances of kafka consumer 2$ docker-compose -f compose-consumer.yml up -d --scale consumer=2 Soon after the instances start to poll messages, we can see that their fetch offsets are updated as the current offset values are much higher than 0.\nWe can check logs of the consumer instances in order to check their behaviour further. Below shows the logs of one of the instances.\n1# check logs of consumer instance 1 2$ docker logs offset-seeking-consumer-1 We see that the partition 1 is assigned to this instance. The offset 901 is taken to override and the message timestamp of that message is 2023-01-06T19:20:16.107000, which is later than the OFFSET_STR environment value.\nWe can also check that the correct offset is obtained as the message timestamp of offset 900 is earlier than the OFFSET_STR value.\nSummary In this post, we discussed how to configure Kafka consumers to seek offsets by timestamp. A single node Kafka cluster was created using docker compose and a Kafka producer was used to send fake order messages. While subscribing to the orders topic, the consumer registered a custom consumer rebalance listener that overrides the fetch offsets by timestamp. Two consumer instances were deployed using docker compose and their behaviour was analysed in detail.\n","date":"January 10, 2023","img":"/blog/2023-01-10-kafka-consumer-seek-offsets/featured.png","lang":"en","langName":"English","largeImg":"/blog/2023-01-10-kafka-consumer-seek-offsets/featured_hua222fe3eeaae92dfa98b24cdf7061694_47217_500x0_resize_box_3.png","permalink":"/blog/2023-01-10-kafka-consumer-seek-offsets/","series":[],"smallImg":"/blog/2023-01-10-kafka-consumer-seek-offsets/featured_hua222fe3eeaae92dfa98b24cdf7061694_47217_180x0_resize_box_3.png","tags":[{"title":"Apache Kafka","url":"/tags/apache-kafka/"},{"title":"Python","url":"/tags/python/"},{"title":"Docker","url":"/tags/docker/"},{"title":"Docker Compose","url":"/tags/docker-compose/"}],"timestamp":1673308800,"title":"How to Configure Kafka Consumers to Seek Offsets by Timestamp"},{"categories":[{"title":"Data Engineering","url":"/categories/data-engineering/"}],"content":"The data build tool (dbt) is an effective data transformation tool and it supports key AWS analytics services - Redshift, Glue, EMR and Athena. In the previous posts, we discussed benefits of a common data transformation tool and the potential of dbt to cover a wide range of data projects from data warehousing to data lake to data lakehouse. Demo data projects that target Redshift Serverless, Glue, EMR on EC2 and EMR on EKS are illustrated as well. In the last part of the dbt on AWS series, we discuss data transformation pipelines using dbt on Amazon Athena. Subsets of IMDb data are used as source and data models are developed in multiple layers according to the dbt best practices. A list of posts of this series can be found below.\nPart 1 Redshift Part 2 Glue Part 3 EMR on EC2 Part 4 EMR on EKS Part 5 Athena (this post) Below shows an overview diagram of the scope of this dbt on AWS series. Athena is highlighted as it is discussed in this post.\nInfrastructure The infrastructure hosting this solution leverages Athena Workgroup, AWS Glue Data Catalog, AWS Glue Crawlers and a S3 bucket. They are deployed using Terraform and the source can be found in the GitHub repository of this post.\nAthena Workgroup The dbt athena adapter requires an Athena workgroup and it only supports the Athena engine version 2. The workgroup used in the dbt project is created as shown below.\n1resource \u0026#34;aws_athena_workgroup\u0026#34; \u0026#34;imdb\u0026#34; { 2 name = \u0026#34;${local.name}-imdb\u0026#34; 3 4 configuration { 5 enforce_workgroup_configuration = true 6 publish_cloudwatch_metrics_enabled = false 7 8 engine_version { 9 selected_engine_version = \u0026#34;Athena engine version 2\u0026#34; 10 } 11 12 result_configuration { 13 output_location = \u0026#34;s3://${local.default_bucket.name}/athena/\u0026#34; 14 15 encryption_configuration { 16 encryption_option = \u0026#34;SSE_S3\u0026#34; 17 } 18 } 19 } 20 21 force_destroy = true 22 23 tags = local.tags 24} Glue Databases We have two Glue databases. The source tables and the tables of the staging and intermediate layers are kept in the imdb database. The tables of the marts layer are stored in the _imdb_analytics _database.\n1# athena/infra/main.tf 2resource \u0026#34;aws_glue_catalog_database\u0026#34; \u0026#34;imdb_db\u0026#34; { 3 name = \u0026#34;imdb\u0026#34; 4 description = \u0026#34;Database that contains IMDb staging/intermediate model datasets\u0026#34; 5} 6 7resource \u0026#34;aws_glue_catalog_database\u0026#34; \u0026#34;imdb_db_marts\u0026#34; { 8 name = \u0026#34;imdb_analytics\u0026#34; 9 description = \u0026#34;Database that contains IMDb marts model datasets\u0026#34; 10} Glue Crawlers We use Glue crawlers to create source tables in the imdb database. We can create a single crawler for the seven source tables but it was not satisfactory, especially header detection. Instead a dedicated crawler is created for each of the tables with its own custom classifier where it includes header columns specifically. The Terraform count meta-argument is used to create the crawlers and classifiers recursively.\n1# athena/infra/main.tf 2resource \u0026#34;aws_glue_crawler\u0026#34; \u0026#34;imdb_crawler\u0026#34; { 3 count = length(local.glue.tables) 4 5 name = local.glue.tables[count.index].name 6 database_name = aws_glue_catalog_database.imdb_db.name 7 role = aws_iam_role.imdb_crawler.arn 8 classifiers = [aws_glue_classifier.imdb_crawler[count.index].id] 9 10 s3_target { 11 path = \u0026#34;s3://${local.default_bucket.name}/${local.glue.tables[count.index].name}\u0026#34; 12 } 13 14 tags = local.tags 15} 16 17resource \u0026#34;aws_glue_classifier\u0026#34; \u0026#34;imdb_crawler\u0026#34; { 18 count = length(local.glue.tables) 19 20 name = local.glue.tables[count.index].name 21 22 csv_classifier { 23 contains_header = \u0026#34;PRESENT\u0026#34; 24 delimiter = \u0026#34;\\t\u0026#34; 25 header = local.glue.tables[count.index].header 26 } 27} 28 29# athena/infra/variables.tf 30locals { 31 name = basename(path.cwd) == \u0026#34;infra\u0026#34; ? basename(dirname(path.cwd)) : basename(path.cwd) 32 region = data.aws_region.current.name 33 environment = \u0026#34;dev\u0026#34; 34 35 default_bucket = { 36 name = \u0026#34;${local.name}-${data.aws_caller_identity.current.account_id}-${local.region}\u0026#34; 37 } 38 39 glue = { 40 tables = [ 41 { name = \u0026#34;name_basics\u0026#34;, header = [\u0026#34;nconst\u0026#34;, \u0026#34;primaryName\u0026#34;, \u0026#34;birthYear\u0026#34;, \u0026#34;deathYear\u0026#34;, \u0026#34;primaryProfession\u0026#34;, \u0026#34;knownForTitles\u0026#34;] }, 42 { name = \u0026#34;title_akas\u0026#34;, header = [\u0026#34;titleId\u0026#34;, \u0026#34;ordering\u0026#34;, \u0026#34;title\u0026#34;, \u0026#34;region\u0026#34;, \u0026#34;language\u0026#34;, \u0026#34;types\u0026#34;, \u0026#34;attributes\u0026#34;, \u0026#34;isOriginalTitle\u0026#34;] }, 43 { name = \u0026#34;title_basics\u0026#34;, header = [\u0026#34;tconst\u0026#34;, \u0026#34;titleType\u0026#34;, \u0026#34;primaryTitle\u0026#34;, \u0026#34;originalTitle\u0026#34;, \u0026#34;isAdult\u0026#34;, \u0026#34;startYear\u0026#34;, \u0026#34;endYear\u0026#34;, \u0026#34;runtimeMinutes\u0026#34;, \u0026#34;genres\u0026#34;] }, 44 { name = \u0026#34;title_crew\u0026#34;, header = [\u0026#34;tconst\u0026#34;, \u0026#34;directors\u0026#34;, \u0026#34;writers\u0026#34;] }, 45 { name = \u0026#34;title_episode\u0026#34;, header = [\u0026#34;tconst\u0026#34;, \u0026#34;parentTconst\u0026#34;, \u0026#34;seasonNumber\u0026#34;, \u0026#34;episodeNumber\u0026#34;] }, 46 { name = \u0026#34;title_principals\u0026#34;, header = [\u0026#34;tconst\u0026#34;, \u0026#34;ordering\u0026#34;, \u0026#34;nconst\u0026#34;, \u0026#34;category\u0026#34;, \u0026#34;job\u0026#34;, \u0026#34;characters\u0026#34;] }, 47 { name = \u0026#34;title_ratings\u0026#34;, header = [\u0026#34;tconst\u0026#34;, \u0026#34;averageRating\u0026#34;, \u0026#34;numVotes\u0026#34;] } 48 ] 49 } 50 51 tags = { 52 Name = local.name 53 Environment = local.environment 54 } 55} Project We build a data transformation pipeline using subsets of IMDb data - seven titles and names related datasets are provided as gzipped, tab-separated-values (TSV) formatted files. This results in three tables that can be used for reporting and analysis.\nSave Data to S3 The Axel download accelerator is used to download the data files locally followed by decompressing with the gzip utility. Note that simple retry logic is added as I see download failure from time to time. Finally, the decompressed files are saved into the project S3 bucket using the S3 sync command.\n1# athena/upload-data.sh 2#!/usr/bin/env bash 3 4s3_bucket=$(terraform -chdir=./infra output --raw default_bucket_name) 5hostname=\u0026#34;datasets.imdbws.com\u0026#34; 6declare -a file_names=( 7 \u0026#34;name.basics.tsv.gz\u0026#34; \\ 8 \u0026#34;title.akas.tsv.gz\u0026#34; \\ 9 \u0026#34;title.basics.tsv.gz\u0026#34; \\ 10 \u0026#34;title.crew.tsv.gz\u0026#34; \\ 11 \u0026#34;title.episode.tsv.gz\u0026#34; \\ 12 \u0026#34;title.principals.tsv.gz\u0026#34; \\ 13 \u0026#34;title.ratings.tsv.gz\u0026#34; 14 ) 15 16rm -rf imdb-data 17 18for fn in \u0026#34;${file_names[@]}\u0026#34; 19do 20 download_url=\u0026#34;https://$hostname/$fn\u0026#34; 21 prefix=$(echo ${fn::-7} | tr \u0026#39;.\u0026#39; \u0026#39;_\u0026#39;) 22 echo \u0026#34;download imdb-data/$prefix/$fn from $download_url\u0026#34; 23 while true; 24 do 25 mkdir -p imdb-data/$prefix 26 axel -n 32 -a -o imdb-data/$prefix/$fn $download_url 27 gzip -d imdb-data/$prefix/$fn 28 num_files=$(ls imdb-data/$prefix | wc -l) 29 if [ $num_files == 1 ]; then 30 break 31 fi 32 rm -rf imdb-data/$prefix 33 done 34done 35 36aws s3 sync ./imdb-data s3://$s3_bucket Run Glue Crawlers The Glue crawlers for the seven source tables are executed as shown below.\n1# athena/start-crawlers.sh 2#!/usr/bin/env bash 3 4declare -a crawler_names=( 5 \u0026#34;name_basics\u0026#34; \\ 6 \u0026#34;title_akas\u0026#34; \\ 7 \u0026#34;title_basics\u0026#34; \\ 8 \u0026#34;title_crew\u0026#34; \\ 9 \u0026#34;title_episode\u0026#34; \\ 10 \u0026#34;title_principals\u0026#34; \\ 11 \u0026#34;title_ratings\u0026#34; 12 ) 13 14for cn in \u0026#34;${crawler_names[@]}\u0026#34; 15do 16 echo \u0026#34;start crawler $cn ...\u0026#34; 17 aws glue start-crawler --name $cn 18done After the crawlers run successfully, we are able to check the seven source tables. Below shows a query example of one of the source tables in Athena.\nSetup dbt Project We need the dbt-core and dbt-athena-adapter packages for the main data transformation project - the former can be installed as dependency of the latter. The dbt project is initialised while skipping the connection profile as it does not allow to select key connection details such as aws_profile_name and threads. Instead the profile is created manually as shown below. Note that, after this post was complete, it announced a new project repository called dbt-athena-community and new features are planned to be supported in the new project. Those new features cover the Athena engine version 3 and Apache Iceberg support and a new dbt targeting Athena is encouraged to use it.\n1$ pip install dbt-athena-adapter 2$ dbt init --skip-profile-setup 3# 09:32:09 Running with dbt=1.3.1 4# Enter a name for your project (letters, digits, underscore): athena_proj The attributes are self-explanatory, and their details can be checked further in the GitHub repository of the dbt-athena adapter.\n1# athena/set-profile.sh 2#!/usr/bin/env bash 3 4aws_region=$(aws ec2 describe-availability-zones --output text --query \u0026#39;AvailabilityZones[0].[RegionName]\u0026#39;) 5dbt_s3_location=$(terraform -chdir=./infra output --raw default_bucket_name) 6dbt_work_group=$(terraform -chdir=./infra output --raw aws_athena_workgroup_name) 7 8cat \u0026lt;\u0026lt; EOF \u0026gt; ~/.dbt/profiles.yml 9athena_proj: 10 outputs: 11 dev: 12 type: athena 13 region_name: ${aws_region} 14 s3_staging_dir: \u0026#34;s3://${dbt_s3_location}/dbt/\u0026#34; 15 schema: imdb 16 database: awsdatacatalog 17 work_group: ${dbt_work_group} 18 threads: 3 19 aws_profile_name: \u0026lt;aws-profile\u0026gt; 20 target: dev 21EOF dbt initialises a project in a folder that matches to the project name and generates project boilerplate as shown below. Some of the main objects are dbt_project.yml, and the model folder. The former is required because dbt doesn\u0026rsquo;t know if a folder is a dbt project without it. Also it contains information that tells dbt how to operate on the project. The latter is for including dbt models, which is basically a set of SQL select statements. See dbt documentation for more details.\n1$ tree athena/athena_proj/ -L 1 2athena/athena_proj/ 3├── README.md 4├── analyses 5├── dbt_packages 6├── dbt_project.yml 7├── logs 8├── macros 9├── models 10├── packages.yml 11├── seeds 12├── snapshots 13├── target 14└── tests We can check Athena connection with the dbt debug command as shown below.\n1$ dbt debug 209:33:53 Running with dbt=1.3.1 3dbt version: 1.3.1 4python version: 3.8.10 5python path: \u0026lt;path-to-python-path\u0026gt; 6os info: Linux-5.4.72-microsoft-standard-WSL2-x86_64-with-glibc2.29 7Using profiles.yml file at /home/\u0026lt;username\u0026gt;/.dbt/profiles.yml 8Using dbt_project.yml file at \u0026lt;path-to-dbt-project\u0026gt;/dbt_project.yml 9 10Configuration: 11 profiles.yml file [OK found and valid] 12 dbt_project.yml file [OK found and valid] 13 14Required dependencies: 15 - git [OK found] 16 17Connection: 18 s3_staging_dir: s3://\u0026lt;s3-bucket-name\u0026gt;/dbt/ 19 work_group: athena-imdb 20 region_name: ap-southeast-2 21 database: imdb 22 schema: imdb 23 poll_interval: 1.0 24 aws_profile_name: \u0026lt;aws-profile\u0026gt; 25 Connection test: [OK connection ok] 26 27All checks passed! After initialisation, the model configuration is updated. The project materialisation is specified as view, although it is the default materialisation. Also tags are added to the entire model folder as well as folders of specific layers - staging, intermediate and marts. As shown below, tags can simplify model execution.\n1# athena/athena_proj/dbt_project.yml 2name: \u0026#34;dbt_glue_proj\u0026#34; 3 4... 5 6models: 7 dbt_glue_proj: 8 +materialized: view 9 +tags: 10 - \u0026#34;imdb\u0026#34; 11 staging: 12 +tags: 13 - \u0026#34;staging\u0026#34; 14 intermediate: 15 +tags: 16 - \u0026#34;intermediate\u0026#34; 17 marts: 18 +tags: 19 - \u0026#34;marts\u0026#34; The dbt_utils package is installed for adding tests to the final marts models. The packages can be installed by the dbt deps command.\n1# athena/athena_proj/packages.yml 2packages: 3 - package: dbt-labs/dbt_utils 4 version: 0.9.5 Create dbt Models The models for this post are organised into three layers according to the dbt best practices - staging, intermediate and marts.\nStaging The seven tables that are loaded from S3 are dbt source tables and their details are declared in a YAML file (_imdb_sources.yml). By doing so, we are able to refer to the source tables with the {{ source() }} function. Also we can add tests to source tables. For example below two tests (unique, not_null) are added to the tconst column of the title_basics table below and these tests can be executed by the dbt test command.\n1# athena/athena_proj/models/staging/imdb/_imdb__sources.yml 2version: 2 3 4sources: 5 - name: imdb 6 description: Subsets of IMDb data, which are available for access to customers for personal and non-commercial use 7 tables: 8 - name: title_basics 9 description: Table that contains basic information of titles 10 columns: 11 - name: tconst 12 description: alphanumeric unique identifier of the title 13 tests: 14 - unique 15 - not_null 16 - name: titletype 17 description: the type/format of the title (e.g. movie, short, tvseries, tvepisode, video, etc) 18 - name: primarytitle 19 description: the more popular title / the title used by the filmmakers on promotional materials at the point of release 20 - name: originaltitle 21 description: original title, in the original language 22 - name: isadult 23 description: flag that indicates whether it is an adult title or not 24 - name: startyear 25 description: represents the release year of a title. In the case of TV Series, it is the series start year 26 - name: endyear 27 description: TV Series end year. NULL for all other title types 28 - name: runtime minutes 29 description: primary runtime of the title, in minutes 30 - name: genres 31 description: includes up to three genres associated with the title 32 ... Based on the source tables, staging models are created. They are created as views, which is the project’s default materialisation. In the SQL statements, column names and data types are modified mainly.\n1# athena/athena_proj/models/staging/imdb/stg_imdb__title_basics.sql 2with source as ( 3 4 select * from {{ source(\u0026#39;imdb\u0026#39;, \u0026#39;title_basics\u0026#39;) }} 5 6), 7 8renamed as ( 9 10 select 11 tconst as title_id, 12 titletype as title_type, 13 primarytitle as primary_title, 14 originaltitle as original_title, 15 cast(isadult as boolean) as is_adult, 16 cast(startyear as int) as start_year, 17 cast(endyear as int) as end_year, 18 cast(runtimeminutes as int) as runtime_minutes, 19 genres 20 from source 21 22) 23 24select * from renamed Below shows the file tree of the staging models. The staging models can be executed using the dbt run command. As we’ve added tags to the staging layer models, we can limit to execute only this layer by dbt run --select staging.\n1$ tree athena/athena_proj/models/staging/ 2athena/athena_proj/models/staging/ 3└── imdb 4 ├── _imdb__models.yml 5 ├── _imdb__sources.yml 6 ├── stg_imdb__name_basics.sql 7 ├── stg_imdb__title_akas.sql 8 ├── stg_imdb__title_basics.sql 9 ├── stg_imdb__title_crews.sql 10 ├── stg_imdb__title_episodes.sql 11 ├── stg_imdb__title_principals.sql 12 └── stg_imdb__title_ratings.sql The views in the staging layer can be queried in Athena as shown below.\nIntermediate We can keep intermediate results in this layer so that the models of the final marts layer can be simplified. The source data includes columns where array values are kept as comma separated strings. For example, the genres column of the stg_imdb__title_basics model includes up to three genre values as shown in the previous screenshot. A total of seven columns in three models are columns of comma-separated strings and it is better to flatten them in the intermediate layer. Also, in order to avoid repetition, a dbt macro (f_latten_fields_) is created to share the column-flattening logic.\n1# athena/athena_proj/macros/flatten_fields.sql 2{% macro flatten_fields(model, field_name, id_field_name) %} 3 select 4 {{ id_field_name }} as id, 5 field 6 from {{ model }} 7 cross join unnest(split({{ field_name }}, \u0026#39;,\u0026#39;)) as x(field) 8{% endmacro %} The macro function can be added inside a common table expression (CTE) by specifying the relevant model, field name to flatten and ID field name.\n1-- athena/athena_proj/models/intermediate/title/int_genres_flattened_from_title_basics.sql 2with flattened as ( 3 {{ flatten_fields(ref(\u0026#39;stg_imdb__title_basics\u0026#39;), \u0026#39;genres\u0026#39;, \u0026#39;title_id\u0026#39;) }} 4) 5 6select 7 id as title_id, 8 field as genre 9from flattened 10order by id The intermediate models are also materialised as views, and we can check the array columns are flattened as expected.\nBelow shows the file tree of the intermediate models. Similar to the staging models, the intermediate models can be executed by dbt run --select intermediate.\n1$ tree athena/athena_proj/models/intermediate/ athena/athena_proj/macros/ 2athena/athena_proj/models/intermediate/ 3├── name 4│ ├── _int_name__models.yml 5│ ├── int_known_for_titles_flattened_from_name_basics.sql 6│ └── int_primary_profession_flattened_from_name_basics.sql 7└── title 8 ├── _int_title__models.yml 9 ├── int_directors_flattened_from_title_crews.sql 10 ├── int_genres_flattened_from_title_basics.sql 11 └── int_writers_flattened_from_title_crews.sql 12 13athena/athena_proj/macros/ 14└── flatten_fields.sql Marts The models in the marts layer are configured to be materialised as tables in a custom schema. Their materialisation is set to table and the custom schema is specified as analytics while taking _parquet _as the file format. Note that the custom schema name becomes imdb_analytics according to the naming convention of dbt custom schemas. Models of both the staging and intermediate layers are used to create final models to be used for reporting and analytics.\n1-- athena/athena_proj/models/marts/analytics/titles.sql 2{{ 3 config( 4 schema=\u0026#39;analytics\u0026#39;, 5 materialized=\u0026#39;table\u0026#39;, 6 file_format=\u0026#39;parquet\u0026#39; 7 ) 8}} 9 10with titles as ( 11 12 select * from {{ ref(\u0026#39;stg_imdb__title_basics\u0026#39;) }} 13 14), 15 16principals as ( 17 18 select 19 title_id, 20 count(name_id) as num_principals 21 from {{ ref(\u0026#39;stg_imdb__title_principals\u0026#39;) }} 22 group by title_id 23 24), 25 26names as ( 27 28 select 29 title_id, 30 count(name_id) as num_names 31 from {{ ref(\u0026#39;int_known_for_titles_flattened_from_name_basics\u0026#39;) }} 32 group by title_id 33 34), 35 36ratings as ( 37 38 select 39 title_id, 40 average_rating, 41 num_votes 42 from {{ ref(\u0026#39;stg_imdb__title_ratings\u0026#39;) }} 43 44), 45 46episodes as ( 47 48 select 49 parent_title_id, 50 count(title_id) as num_episodes 51 from {{ ref(\u0026#39;stg_imdb__title_episodes\u0026#39;) }} 52 group by parent_title_id 53 54), 55 56distributions as ( 57 58 select 59 title_id, 60 count(title) as num_distributions 61 from {{ ref(\u0026#39;stg_imdb__title_akas\u0026#39;) }} 62 group by title_id 63 64), 65 66final as ( 67 68 select 69 t.title_id, 70 t.title_type, 71 t.primary_title, 72 t.original_title, 73 t.is_adult, 74 t.start_year, 75 t.end_year, 76 t.runtime_minutes, 77 t.genres, 78 p.num_principals, 79 n.num_names, 80 r.average_rating, 81 r.num_votes, 82 e.num_episodes, 83 d.num_distributions 84 from titles as t 85 left join principals as p on t.title_id = p.title_id 86 left join names as n on t.title_id = n.title_id 87 left join ratings as r on t.title_id = r.title_id 88 left join episodes as e on t.title_id = e.parent_title_id 89 left join distributions as d on t.title_id = d.title_id 90 91) 92 93select * from final The details of the three models can be found in a YAML file (_analytics__models.yml). We can add tests to models and below we see tests of row count matching to their corresponding staging models.\n1# athena/athena_proj/models/marts/analytics/_analytics__models.yml 2version: 2 3 4models: 5 - name: names 6 description: Table that contains all names with additional details 7 tests: 8 - dbt_utils.equal_rowcount: 9 compare_model: ref(\u0026#39;stg_imdb__name_basics\u0026#39;) 10 - name: titles 11 description: Table that contains all titles with additional details 12 tests: 13 - dbt_utils.equal_rowcount: 14 compare_model: ref(\u0026#39;stg_imdb__title_basics\u0026#39;) 15 - name: genre_titles 16 description: Table that contains basic title details after flattening genres The models of the marts layer can be tested using the dbt test command as shown below.\n1$ dbt test --select marts 207:18:42 Running with dbt=1.3.1 307:18:43 Found 15 models, 17 tests, 0 snapshots, 0 analyses, 473 macros, 0 operations, 0 seed files, 7 sources, 0 exposures, 0 metrics 407:18:43 507:18:48 Concurrency: 3 threads (target=\u0026#39;dev\u0026#39;) 607:18:48 707:18:48 1 of 2 START test dbt_utils_equal_rowcount_names_ref_stg_imdb__name_basics_ .... [RUN] 807:18:48 2 of 2 START test dbt_utils_equal_rowcount_titles_ref_stg_imdb__title_basics_ .. [RUN] 907:18:51 2 of 2 PASS dbt_utils_equal_rowcount_titles_ref_stg_imdb__title_basics_ ........ [PASS in 2.76s] 1007:18:52 1 of 2 PASS dbt_utils_equal_rowcount_names_ref_stg_imdb__name_basics_ .......... [PASS in 3.80s] 1107:18:52 1207:18:52 Finished running 2 tests in 0 hours 0 minutes and 9.17 seconds (9.17s). 1307:18:52 1407:18:52 Completed successfully 1507:18:52 1607:18:52 Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2 Below shows the file tree of the marts models. As with the other layers, the marts models can be executed by dbt run --select marts.\n1$ tree athena/athena_proj/models/marts/ 2athena/athena_proj/models/marts/ 3└── analytics 4 ├── _analytics__models.yml 5 ├── genre_titles.sql 6 ├── names.sql 7 └── titles.sql Build Dashboard The models of the marts layer can be consumed by external tools such as Amazon QuickSight. Below shows an example dashboard. The pie chart on the left shows the proportion of titles by genre while the box plot on the right shows the dispersion of average rating by start year.\nGenerate dbt Documentation A nice feature of dbt is documentation. It provides information about the project and the data warehouse, and it facilitates consumers as well as other developers to discover and understand the datasets better. We can generate the project documents and start a document server as shown below.\n1$ dbt docs generate 2$ dbt docs serve A very useful element of dbt documentation is data lineage, which provides an overall view about how data is transformed and consumed. Below we can see that the final titles model consumes all title-related stating models and an intermediate model from the name basics staging model.\nSummary In this post, we discussed how to build data transformation pipelines using dbt on AWS Athena. Subsets of IMDb data are used as source and data models are developed in multiple layers according to the dbt best practices. dbt can be used as an effective tool for data transformation in a wide range of data projects from data warehousing to data lake to data lakehouse and it supports key AWS analytics services - Redshift, Glue, EMR and Athena. In this series, we mainly focus on how to develop a data project with dbt targeting variable AWS analytics services. It is quite an effective framework for data transformation and advanced features will be covered in a new series of posts.\n","date":"December 6, 2022","img":"/blog/2022-12-06-dbt-on-aws-part-5-athena/featured.png","lang":"en","langName":"English","largeImg":"/blog/2022-12-06-dbt-on-aws-part-5-athena/featured_hu2cb992d854f23cc962e70e6949802760_91796_500x0_resize_box_3.png","permalink":"/blog/2022-12-06-dbt-on-aws-part-5-athena/","series":[{"title":"DBT for Effective Data Transformation on AWS","url":"/series/dbt-for-effective-data-transformation-on-aws/"}],"smallImg":"/blog/2022-12-06-dbt-on-aws-part-5-athena/featured_hu2cb992d854f23cc962e70e6949802760_91796_180x0_resize_box_3.png","tags":[{"title":"AWS","url":"/tags/aws/"},{"title":"Amazon Athena","url":"/tags/amazon-athena/"},{"title":"Amazon QuickSight","url":"/tags/amazon-quicksight/"},{"title":"Data Build Tool (DBT)","url":"/tags/data-build-tool-dbt/"},{"title":"Terraform","url":"/tags/terraform/"}],"timestamp":1670284800,"title":"Data Build Tool (Dbt) for Effective Data Transformation on AWS – Part 5 Athena"},{"categories":[{"title":"Data Engineering","url":"/categories/data-engineering/"}],"content":"The data build tool (dbt) is an effective data transformation tool and it supports key AWS analytics services - Redshift, Glue, EMR and Athena. In the previous posts, we discussed benefits of a common data transformation tool and the potential of dbt to cover a wide range of data projects from data warehousing to data lake to data lakehouse. Demo data projects that target Redshift Serverless, Glue and EMR on EC2 are illustrated as well. In part 4 of the dbt on AWS series, we discuss data transformation pipelines using dbt on Amazon EMR on EKS. As Spark Submit does not allow the spark thrift server to run in cluster mode on Kubernetes, a simple wrapper class is created to overcome the limitation and it makes the thrift server run indefinitely. Subsets of IMDb data are used as source and data models are developed in multiple layers according to the dbt best practices. A list of posts of this series can be found below.\nPart 1 Redshift Part 2 Glue Part 3 EMR on EC2 Part 4 EMR on EKS (this post) Part 5 Athena Below shows an overview diagram of the scope of this dbt on AWS series. EMR is highlighted as it is discussed in this post.\nInfrastructure The main infrastructure hosting this solution leverages an Amazon EKS cluster and EMR virtual cluster. As discussed in one of the earlier posts, EMR job pods (controller, driver and executors) can be configured to be managed by Karpenter, which simplifies autoscaling by provisioning just-in-time capacity as well as reduces scheduling latency. While the infrastructure elements are discussed in depth in the earlier post and part 3, this section focuses on how to set up a long-running Thrift JDBC/ODBC server on EMR on EKS, which is a critical part of using the dbt-spark adapter. The source can be found in the GitHub repository of this post.\nThrift JDBC/ODBC Server The Spark Submit does not allow the spark thrift server to run in cluster mode on Kubernetes. I have found a number of implementations that handle this issue. The first one is executing the thrift server start script in the container command, but it is not allowed in pod templates of EMR on EKS. Besides, it creates the driver and executors in a single pod, which is not scalable. The second example relies on Apache Kyuubi that manages Spark applications while providing JDBC connectivity. However, there is no dbt adapter that supports Kyuubi as well as I am concerned it could make dbt transformations more complicated. The last one is creating a wrapper class that makes the thrift server run indefinitely. It is an interesting approach to deploy the thrift server on EMR on EKS (and the spark kubernetes operator in general) with minimal effort. Following that example, a wrapper class is created in this post.\nSpark Thrift Server Runner The wrapper class (SparkThriftServerRunner) is created as shown below, and it makes the HiveThriftServer2 class run indefinitely. In this way, we are able to use the runner class as the entry point for a spark application.\n1// emr-eks/hive-on-spark-in-kubernetes/examples/spark-thrift-server/src/main/java/io/jaehyeon/hive/SparkThriftServerRunner.java 2package io.jaehyeon.hive; 3 4public class SparkThriftServerRunner { 5 6 public static void main(String[] args) { 7 org.apache.spark.sql.hive.thriftserver.HiveThriftServer2.main(args); 8 9 while (true) { 10 try { 11 Thread.sleep(Long.MAX_VALUE); 12 } catch (Exception e) { 13 e.printStackTrace(); 14 } 15 } 16 } 17} While the reference implementation includes all of its dependent libraries when building the class, only the runner class itself is built for this post. It is because we do not have to include those as they are available in the EMR on EKS container image. To do so, the Project Object Model (POM) file is updated so that all the provided dependency scopes are changed into runtime except for spark-hive-thriftserver_2.12 - see pom.xml for details. The runner class can be built as shown below.\n1cd emr-eks/hive-on-spark-in-kubernetes/examples/spark-thrift-server 2mvn -e -DskipTests=true clean install; Once completed, the JAR file of the runner class can be found in the target folder - spark-thrift-server-1.0.0-SNAPSHOT.jar.\n1$ tree target/ -L 1 2target/ 3├── classes 4├── generated-sources 5├── maven-archiver 6├── maven-status 7├── spark-thrift-server-1.0.0-SNAPSHOT-sources.jar 8├── spark-thrift-server-1.0.0-SNAPSHOT.jar 9└── test-classes 10 115 directories, 2 files Driver Template We can expose the spark driver pod by a service. A label (app) is added to the driver pod template so that it can be selected by the service.\n1# emr-eks/resources/driver-template.yaml 2apiVersion: v1 3kind: Pod 4metadata: 5 labels: 6 app: spark-thrift-server-driver 7spec: 8 nodeSelector: 9 type: karpenter 10 provisioner: spark-driver 11 tolerations: 12 - key: spark-driver 13 operator: Exists 14 effect: NoSchedule 15 containers: 16 - name: spark-kubernetes-driver Spark Job Run The wrapper class and (driver and executor) pod templates are referred from the default S3 bucket of this post. The runner class is specified as the entry point of the spark application and three executor instances with 2G of memory are configured to run. In the application configuration, the dynamic resource allocation is disabled and the AWS Glue Data Catalog is set to be used as the metastore for Spark SQL.\n1# emr-eks/job-run.sh 2export VIRTUAL_CLUSTER_ID=$(terraform -chdir=./infra output --raw emrcontainers_virtual_cluster_id) 3export EMR_ROLE_ARN=$(terraform -chdir=./infra output --json emr_on_eks_role_arn | jq \u0026#39;.[0]\u0026#39; -r) 4export DEFAULT_BUCKET_NAME=$(terraform -chdir=./infra output --raw default_bucket_name) 5export AWS_REGION=$(aws ec2 describe-availability-zones --query \u0026#39;AvailabilityZones[0].[RegionName]\u0026#39; --output text) 6 7aws emr-containers start-job-run \\ 8--virtual-cluster-id $VIRTUAL_CLUSTER_ID \\ 9--name thrift-server \\ 10--execution-role-arn $EMR_ROLE_ARN \\ 11--release-label emr-6.8.0-latest \\ 12--region $AWS_REGION \\ 13--job-driver \u0026#39;{ 14 \u0026#34;sparkSubmitJobDriver\u0026#34;: { 15 \u0026#34;entryPoint\u0026#34;: \u0026#34;s3://\u0026#39;${DEFAULT_BUCKET_NAME}\u0026#39;/resources/jars/spark-thrift-server-1.0.0-SNAPSHOT.jar\u0026#34;, 16 \u0026#34;sparkSubmitParameters\u0026#34;: \u0026#34;--class io.jaehyeon.hive.SparkThriftServerRunner --jars s3://\u0026#39;${DEFAULT_BUCKET_NAME}\u0026#39;/resources/jars/spark-thrift-server-1.0.0-SNAPSHOT.jar --conf spark.executor.instances=3 --conf spark.executor.memory=2G --conf spark.executor.cores=1 --conf spark.driver.cores=1 --conf spark.driver.memory=2G\u0026#34; 17 } 18 }\u0026#39; \\ 19--configuration-overrides \u0026#39;{ 20 \u0026#34;applicationConfiguration\u0026#34;: [ 21 { 22 \u0026#34;classification\u0026#34;: \u0026#34;spark-defaults\u0026#34;, 23 \u0026#34;properties\u0026#34;: { 24 \u0026#34;spark.dynamicAllocation.enabled\u0026#34;:\u0026#34;false\u0026#34;, 25 \u0026#34;spark.kubernetes.executor.deleteOnTermination\u0026#34;: \u0026#34;true\u0026#34;, 26 \u0026#34;spark.kubernetes.driver.podTemplateFile\u0026#34;:\u0026#34;s3://\u0026#39;${DEFAULT_BUCKET_NAME}\u0026#39;/resources/templates/driver-template.yaml\u0026#34;, 27 \u0026#34;spark.kubernetes.executor.podTemplateFile\u0026#34;:\u0026#34;s3://\u0026#39;${DEFAULT_BUCKET_NAME}\u0026#39;/resources/templates/executor-template.yaml\u0026#34;, 28 \u0026#34;spark.hadoop.hive.metastore.client.factory.class\u0026#34;:\u0026#34;com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory\u0026#34; 29 } 30 } 31 ] 32}\u0026#39; Once it is started, we can check the spark job pods as shown below. A single driver and three executor pods are deployed as expected.\n1$ kubectl get pod -n analytics 2NAME READY STATUS RESTARTS AGE 30000000310t0tkbcftg-nmzfh 2/2 Running 0 5m39s 4spark-0000000310t0tkbcftg-6385c0841d09d389-exec-1 1/1 Running 0 3m2s 5spark-0000000310t0tkbcftg-6385c0841d09d389-exec-2 1/1 Running 0 3m1s 6spark-0000000310t0tkbcftg-6385c0841d09d389-exec-3 1/1 Running 0 3m1s 7spark-0000000310t0tkbcftg-driver 2/2 Running 0 5m26s Spark Thrift Server Service As mentioned earlier, the driver pod is exposed by a service. The service manifest file uses a label selector to identify the spark driver pod. As EMR on EC2, the thrift server is mapped to port 10001 by default. Note that the container port is not allowed in the pod templates of EMR on EKS but the service can still access it.\n1# emr-eks/resources/spark-thrift-server-service.yaml 2kind: Service 3apiVersion: v1 4metadata: 5 name: spark-thrift-server-service 6 namespace: analytics 7spec: 8 type: LoadBalancer 9 selector: 10 app: spark-thrift-server-driver 11 ports: 12 - name: jdbc-port 13 protocol: TCP 14 port: 10001 15 targetPort: 10001 The service can be deployed by kubectl apply -f resources/spark-thrift-server-service.yaml, and we can check the service details as shown below - we will use the hostname of the service (EXTERNAL-IP) later.\nSimilar to beeline, we can check the connection using the pyhive package.\n1# emr-eks/resources/test_conn.py 2from pyhive import hive 3import pandas as pd 4 5conn = hive.connect( 6 host=\u0026#34;\u0026lt;hostname-of-spark-thrift-server-service\u0026gt;\u0026#34;, 7 port=10001, 8 username=\u0026#34;hadoop\u0026#34;, 9 auth=None 10 ) 11print(pd.read_sql(con=conn, sql=\u0026#34;show databases\u0026#34;)) 12conn.close() 1$ python resources/test_conn.py 2 print(pd.read_sql(con=conn, sql=\u0026#34;show databases\u0026#34;)) 3 namespace 40 default 51 imdb 62 imdb_analytics Project We build a data transformation pipeline using subsets of IMDb data - seven titles and names related datasets are provided as gzipped, tab-separated-values (TSV) formatted files. The project ends up creating three tables that can be used for reporting and analysis.\nSave Data to S3 The Axel download accelerator is used to download the data files locally followed by decompressing with the gzip utility. Note that simple retry logic is added as I see download failure from time to time. Finally, the decompressed files are saved into the project S3 bucket using the S3 sync command.\n1# dbt-on-aws/emr-eks/upload-data.sh 2#!/usr/bin/env bash 3 4s3_bucket=$(terraform -chdir=./infra output --raw default_bucket_name) 5hostname=\u0026#34;datasets.imdbws.com\u0026#34; 6declare -a file_names=( 7 \u0026#34;name.basics.tsv.gz\u0026#34; \\ 8 \u0026#34;title.akas.tsv.gz\u0026#34; \\ 9 \u0026#34;title.basics.tsv.gz\u0026#34; \\ 10 \u0026#34;title.crew.tsv.gz\u0026#34; \\ 11 \u0026#34;title.episode.tsv.gz\u0026#34; \\ 12 \u0026#34;title.principals.tsv.gz\u0026#34; \\ 13 \u0026#34;title.ratings.tsv.gz\u0026#34; 14 ) 15 16rm -rf imdb-data 17 18for fn in \u0026#34;${file_names[@]}\u0026#34; 19do 20 download_url=\u0026#34;https://$hostname/$fn\u0026#34; 21 prefix=$(echo ${fn::-7} | tr \u0026#39;.\u0026#39; \u0026#39;_\u0026#39;) 22 echo \u0026#34;download imdb-data/$prefix/$fn from $download_url\u0026#34; 23 while true; 24 do 25 mkdir -p imdb-data/$prefix 26 axel -n 32 -a -o imdb-data/$prefix/$fn $download_url 27 gzip -d imdb-data/$prefix/$fn 28 num_files=$(ls imdb-data/$prefix | wc -l) 29 if [ $num_files == 1 ]; then 30 break 31 fi 32 rm -rf imdb-data/$prefix 33 done 34done 35 36aws s3 sync ./imdb-data s3://$s3_bucket Setup dbt Project We use the dbt-spark adapter to work with the EMR cluster. As connection is made by the Thrift JDBC/ODBC server, it is necessary to install the adapter with the PyHive package. I use Ubuntu 20.04 in WSL 2 and it needs to install the libsasl2-dev apt package, which is required for one of the dependent packages of PyHive (pure-sasl). After installing it, we can install the dbt packages as usual.\n1$ sudo apt-get install libsasl2-dev 2$ python3 -m venv venv 3$ source venv/bin/activate 4$ pip install --upgrade pip 5$ pip install dbt-core \u0026#34;dbt-spark[PyHive]\u0026#34; We can initialise a dbt project with the dbt init command. We are required to specify project details - project name, host, connection method, port, schema and the number of threads. Note dbt creates the project profile to .dbt/profile.yml of the user home directory by default.\n1$ dbt init 205:29:39 Running with dbt=1.3.0 3Enter a name for your project (letters, digits, underscore): emr_eks 4Which database would you like to use? 5[1] spark 6 7(Don\u0026#39;t see the one you want? https://docs.getdbt.com/docs/available-adapters) 8 9Enter a number: 1 10host (yourorg.sparkhost.com): \u0026lt;hostname-of-spark-thrift-server-service\u0026gt; 11[1] odbc 12[2] http 13[3] thrift 14Desired authentication method option (enter a number): 3 15port [443]: 10001 16schema (default schema that dbt will build objects in): imdb 17threads (1 or more) [1]: 3 1805:30:13 Profile emr_eks written to /home/\u0026lt;username\u0026gt;/.dbt/profiles.yml using target\u0026#39;s profile_template.yml and your supplied values. Run \u0026#39;dbt debug\u0026#39; to validate the connection. 1905:30:13 20Your new dbt project \u0026#34;emr_eks\u0026#34; was created! 21 22For more information on how to configure the profiles.yml file, 23please consult the dbt documentation here: 24 25 https://docs.getdbt.com/docs/configure-your-profile 26 27One more thing: 28 29Need help? Don\u0026#39;t hesitate to reach out to us via GitHub issues or on Slack: 30 31 https://community.getdbt.com/ 32 33Happy modeling! dbt initialises a project in a folder that matches to the project name and generates project boilerplate as shown below. Some of the main objects are dbt_project.yml, and the model folder. The former is required because dbt doesn\u0026rsquo;t know if a folder is a dbt project without it. Also it contains information that tells dbt how to operate on the project. The latter is for including dbt models, which is basically a set of SQL select statements. See dbt documentation for more details.\n1$ tree emr-eks/emr_eks/ -L 1 2emr-eks/emr_eks/ 3├── README.md 4├── analyses 5├── dbt_packages 6├── dbt_project.yml 7├── logs 8├── macros 9├── models 10├── packages.yml 11├── seeds 12├── snapshots 13├── target 14└── tests We can check connection to the EMR cluster with the dbt debug command as shown below.\n1$ dbt debug 205:31:22 Running with dbt=1.3.0 3dbt version: 1.3.0 4python version: 3.8.10 5python path: \u0026lt;path-to-python-path\u0026gt; 6os info: Linux-5.4.72-microsoft-standard-WSL2-x86_64-with-glibc2.29 7Using profiles.yml file at /home/\u0026lt;username\u0026gt;/.dbt/profiles.yml 8Using dbt_project.yml file at \u0026lt;path-to-dbt-project\u0026gt;/dbt_project.yml 9 10Configuration: 11 profiles.yml file [OK found and valid] 12 dbt_project.yml file [OK found and valid] 13 14Required dependencies: 15 - git [OK found] 16 17Connection: 18 host: \u0026lt;hostname-of-spark-thrift-server-service\u0026gt; 19 port: 10001 20 cluster: None 21 endpoint: None 22 schema: imdb 23 organization: 0 24 Connection test: [OK connection ok] 25 26All checks passed! After initialisation, the model configuration is updated. The project materialisation is specified as view although it is the default materialisation. Also tags are added to the entire model folder as well as folders of specific layers - staging, intermediate and marts. As shown below, tags can simplify model execution.\n1# emr-eks/emr_eks/dbt_project.yml 2name: \u0026#34;emr_eks\u0026#34; 3 4... 5 6models: 7 dbt_glue_proj: 8 +materialized: view 9 +tags: 10 - \u0026#34;imdb\u0026#34; 11 staging: 12 +tags: 13 - \u0026#34;staging\u0026#34; 14 intermediate: 15 +tags: 16 - \u0026#34;intermediate\u0026#34; 17 marts: 18 +tags: 19 - \u0026#34;marts\u0026#34; While we created source tables using Glue crawlers in part 2, they are created directly from S3 by the dbt_external_tables package in this post. Also the dbt_utils package is installed for adding tests to the final marts models. They can be installed by the dbt deps command.\n1# emr-eks/emr_eks/packages.yml 2packages: 3 - package: dbt-labs/dbt_external_tables 4 version: 0.8.2 5 - package: dbt-labs/dbt_utils 6 version: 0.9.2 Create dbt Models The models for this post are organised into three layers according to the dbt best practices - staging, intermediate and marts.\nExternal Source The seven tables that are loaded from S3 are dbt source tables and their details are declared in a YAML file (_imdb_sources.yml). Macros of the dbt_external_tables package parse properties of each table and execute SQL to create each of them. By doing so, we are able to refer to the source tables with the {{ source() }} function. Also, we can add tests to source tables. For example two tests (unique, not_null) are added to the tconst column of the title_basics table below and these tests can be executed by the dbt test command.\n1# emr-eks/emr_eks/models/staging/imdb/_imdb__sources.yml 2version: 2 3 4sources: 5 - name: imdb 6 description: Subsets of IMDb data, which are available for access to customers for personal and non-commercial use 7 tables: 8 - name: title_basics 9 description: Table that contains basic information of titles 10 external: 11 location: \u0026#34;s3://\u0026lt;s3-bucket-name\u0026gt;/title_basics/\u0026#34; 12 row_format: \u0026gt; 13 serde \u0026#39;org.apache.hadoop.hive.serde2.OpenCSVSerde\u0026#39; 14 with serdeproperties ( 15 \u0026#39;separatorChar\u0026#39;=\u0026#39;\\t\u0026#39; 16 ) 17 table_properties: \u0026#34;(\u0026#39;skip.header.line.count\u0026#39;=\u0026#39;1\u0026#39;)\u0026#34; 18 columns: 19 - name: tconst 20 data_type: string 21 description: alphanumeric unique identifier of the title 22 tests: 23 - unique 24 - not_null 25 - name: titletype 26 data_type: string 27 description: the type/format of the title (e.g. movie, short, tvseries, tvepisode, video, etc) 28 - name: primarytitle 29 data_type: string 30 description: the more popular title / the title used by the filmmakers on promotional materials at the point of release 31 - name: originaltitle 32 data_type: string 33 description: original title, in the original language 34 - name: isadult 35 data_type: string 36 description: flag that indicates whether it is an adult title or not 37 - name: startyear 38 data_type: string 39 description: represents the release year of a title. In the case of TV Series, it is the series start year 40 - name: endyear 41 data_type: string 42 description: TV Series end year. NULL for all other title types 43 - name: runtimeminutes 44 data_type: string 45 description: primary runtime of the title, in minutes 46 - name: genres 47 data_type: string 48 description: includes up to three genres associated with the title The source tables can be created by dbt run-operation stage_external_sources. Note that the following SQL is executed for the _title_basics _table under the hood.\n1create table imdb.title_basics ( 2 tconst string, 3 titletype string, 4 primarytitle string, 5 originaltitle string, 6 isadult string, 7 startyear string, 8 endyear string, 9 runtimeminutes string, 10 genres string 11) 12row format serde \u0026#39;org.apache.hadoop.hive.serde2.OpenCSVSerde\u0026#39; with serdeproperties ( 13\u0026#39;separatorChar\u0026#39;=\u0026#39;\\t\u0026#39; 14) 15location \u0026#39;s3://\u0026lt;s3-bucket-name\u0026gt;/title_basics/\u0026#39; 16tblproperties (\u0026#39;skip.header.line.count\u0026#39;=\u0026#39;1\u0026#39;) Interestingly the header rows of the source tables are not skipped when they are queried by spark while they are skipped by Athena. They have to be filtered out in the stage models of the dbt project as spark is the query engine.\nStaging Based on the source tables, staging models are created. They are created as views, which is the project’s default materialisation. In the SQL statements, column names and data types are modified mainly.\n1# emr-eks/emr_eks/models/staging/imdb/stg_imdb__title_basics.sql 2with source as ( 3 4 select * from {{ source(\u0026#39;imdb\u0026#39;, \u0026#39;title_basics\u0026#39;) }} 5 6), 7 8renamed as ( 9 10 select 11 tconst as title_id, 12 titletype as title_type, 13 primarytitle as primary_title, 14 originaltitle as original_title, 15 cast(isadult as boolean) as is_adult, 16 cast(startyear as int) as start_year, 17 cast(endyear as int) as end_year, 18 cast(runtimeminutes as int) as runtime_minutes, 19 case when genres = \u0026#39;N\u0026#39; then null else genres end as genres 20 from source 21 where tconst \u0026lt;\u0026gt; \u0026#39;tconst\u0026#39; 22 23) 24 25select * from renamed Below shows the file tree of the staging models. The staging models can be executed using the dbt run command. As we’ve added tags to the staging layer models, we can limit to execute only this layer by dbt run --select staging.\n1$ tree emr-eks/emr_eks/models/staging/ 2emr-eks/emr_eks/models/staging/ 3└── imdb 4 ├── _imdb__models.yml 5 ├── _imdb__sources.yml 6 ├── stg_imdb__name_basics.sql 7 ├── stg_imdb__title_akas.sql 8 ├── stg_imdb__title_basics.sql 9 ├── stg_imdb__title_crews.sql 10 ├── stg_imdb__title_episodes.sql 11 ├── stg_imdb__title_principals.sql 12 └── stg_imdb__title_ratings.sql Note that the model materialisation of the staging and intermediate models is view and the dbt project creates VIRTUAL_VIEW tables. Although we are able to reference those tables in other models, they cannot be queried by Athena.\n1$ aws glue get-tables --database imdb \\ 2 --query \u0026#34;TableList[?Name==\u0026#39;stg_imdb__title_basics\u0026#39;].[Name, TableType, StorageDescriptor.Columns]\u0026#34; --output yaml 3- - stg_imdb__title_basics 4 - VIRTUAL_VIEW 5 - - Name: title_id 6 Type: string 7 - Name: title_type 8 Type: string 9 - Name: primary_title 10 Type: string 11 - Name: original_title 12 Type: string 13 - Name: is_adult 14 Type: boolean 15 - Name: start_year 16 Type: int 17 - Name: end_year 18 Type: int 19 - Name: runtime_minutes 20 Type: int 21 - Name: genres 22 Type: string Instead we can use spark sql to query the tables. Below shows a query result of the title basics staging table in Glue Studio notebook.\nIntermediate We can keep intermediate results in this layer so that the models of the final marts layer can be simplified. The source data includes columns where array values are kept as comma separated strings. For example, the genres column of the stg_imdb__title_basics model includes up to three genre values as shown in the previous screenshot. A total of seven columns in three models are columns of comma-separated strings, and it is better to flatten them in the intermediate layer. Also, in order to avoid repetition, a dbt macro (f_latten_fields_) is created to share the column-flattening logic.\n1# emr-eks/emr_eks/macros/flatten_fields.sql 2{% macro flatten_fields(model, field_name, id_field_name) %} 3 select 4 {{ id_field_name }} as id, 5 explode(split({{ field_name }}, \u0026#39;,\u0026#39;)) as field 6 from {{ model }} 7{% endmacro %} The macro function can be added inside a common table expression (CTE) by specifying the relevant model, field name to flatten and ID field name.\n1-- emr-eks/emr_eks/models/intermediate/title/int_genres_flattened_from_title_basics.sql 2with flattened as ( 3 {{ flatten_fields(ref(\u0026#39;stg_imdb__title_basics\u0026#39;), \u0026#39;genres\u0026#39;, \u0026#39;title_id\u0026#39;) }} 4) 5 6select 7 id as title_id, 8 field as genre 9from flattened 10order by id The intermediate models are also materialised as views and we can check the array columns are flattened as expected.\nBelow shows the file tree of the intermediate models. Similar to the staging models, the intermediate models can be executed by dbt run --select intermediate.\n1$ tree emr-eks/emr_eks/models/intermediate/ emr-eks/emr_eks/macros/ 2emr-eks/emr_eks/models/intermediate/ 3├── name 4│ ├── _int_name__models.yml 5│ ├── int_known_for_titles_flattened_from_name_basics.sql 6│ └── int_primary_profession_flattened_from_name_basics.sql 7└── title 8 ├── _int_title__models.yml 9 ├── int_directors_flattened_from_title_crews.sql 10 ├── int_genres_flattened_from_title_basics.sql 11 └── int_writers_flattened_from_title_crews.sql 12 13emr-eks/emr_eks/macros/ 14└── flatten_fields.sql Marts The models in the marts layer are configured to be materialised as tables in a custom schema. Their materialisation is set to table and the custom schema is specified as analytics while taking _parquet _as the file format. Note that the custom schema name becomes imdb_analytics according to the naming convention of dbt custom schemas. Models of both the staging and intermediate layers are used to create final models to be used for reporting and analytics.\n1-- emr-eks/emr_eks/models/marts/analytics/titles.sql 2{{ 3 config( 4 schema=\u0026#39;analytics\u0026#39;, 5 materialized=\u0026#39;table\u0026#39;, 6 file_format=\u0026#39;parquet\u0026#39; 7 ) 8}} 9 10with titles as ( 11 12 select * from {{ ref(\u0026#39;stg_imdb__title_basics\u0026#39;) }} 13 14), 15 16principals as ( 17 18 select 19 title_id, 20 count(name_id) as num_principals 21 from {{ ref(\u0026#39;stg_imdb__title_principals\u0026#39;) }} 22 group by title_id 23 24), 25 26names as ( 27 28 select 29 title_id, 30 count(name_id) as num_names 31 from {{ ref(\u0026#39;int_known_for_titles_flattened_from_name_basics\u0026#39;) }} 32 group by title_id 33 34), 35 36ratings as ( 37 38 select 39 title_id, 40 average_rating, 41 num_votes 42 from {{ ref(\u0026#39;stg_imdb__title_ratings\u0026#39;) }} 43 44), 45 46episodes as ( 47 48 select 49 parent_title_id, 50 count(title_id) as num_episodes 51 from {{ ref(\u0026#39;stg_imdb__title_episodes\u0026#39;) }} 52 group by parent_title_id 53 54), 55 56distributions as ( 57 58 select 59 title_id, 60 count(title) as num_distributions 61 from {{ ref(\u0026#39;stg_imdb__title_akas\u0026#39;) }} 62 group by title_id 63 64), 65 66final as ( 67 68 select 69 t.title_id, 70 t.title_type, 71 t.primary_title, 72 t.original_title, 73 t.is_adult, 74 t.start_year, 75 t.end_year, 76 t.runtime_minutes, 77 t.genres, 78 p.num_principals, 79 n.num_names, 80 r.average_rating, 81 r.num_votes, 82 e.num_episodes, 83 d.num_distributions 84 from titles as t 85 left join principals as p on t.title_id = p.title_id 86 left join names as n on t.title_id = n.title_id 87 left join ratings as r on t.title_id = r.title_id 88 left join episodes as e on t.title_id = e.parent_title_id 89 left join distributions as d on t.title_id = d.title_id 90 91) 92 93select * from final The details of the three models can be found in a YAML file (_analytics__models.yml). We can add tests to models and below we see tests of row count matching to their corresponding staging models.\n1# emr-eks/emr_eks/models/marts/analytics/_analytics__models.yml 2version: 2 3 4models: 5 - name: names 6 description: Table that contains all names with additional details 7 tests: 8 - dbt_utils.equal_rowcount: 9 compare_model: ref(\u0026#39;stg_imdb__name_basics\u0026#39;) 10 - name: titles 11 description: Table that contains all titles with additional details 12 tests: 13 - dbt_utils.equal_rowcount: 14 compare_model: ref(\u0026#39;stg_imdb__title_basics\u0026#39;) 15 - name: genre_titles 16 description: Table that contains basic title details after flattening genres The models of the marts layer can be tested using the dbt test command as shown below.\n1$ dbt test --select marts 206:05:30 Running with dbt=1.3.0 306:05:30 Found 15 models, 17 tests, 0 snapshots, 0 analyses, 569 macros, 0 operations, 0 seed files, 7 sources, 0 exposures, 0 metrics 406:05:30 506:06:03 Concurrency: 3 threads (target=\u0026#39;dev\u0026#39;) 606:06:03 706:06:03 1 of 2 START test dbt_utils_equal_rowcount_names_ref_stg_imdb__name_basics_ .... [RUN] 806:06:03 2 of 2 START test dbt_utils_equal_rowcount_titles_ref_stg_imdb__title_basics_ .. [RUN] 906:06:57 1 of 2 PASS dbt_utils_equal_rowcount_names_ref_stg_imdb__name_basics_ .......... [PASS in 53.54s] 1006:06:59 2 of 2 PASS dbt_utils_equal_rowcount_titles_ref_stg_imdb__title_basics_ ........ [PASS in 56.40s] 1106:07:02 1206:07:02 Finished running 2 tests in 0 hours 1 minutes and 31.75 seconds (91.75s). 1306:07:02 1406:07:02 Completed successfully 1506:07:02 1606:07:02 Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2 As with the other layers, the marts models can be executed by dbt run --select marts. While the transformation is performed, we can check the details from the spark history server. The SQL tab shows the three transformations in the marts layer.\nThe file tree of the marts models can be found below.\n1$ tree emr-eks/emr_eks/models/marts/ 2emr-eks/emr_eks/models/marts/ 3└── analytics 4 ├── _analytics__models.yml 5 ├── genre_titles.sql 6 ├── names.sql 7 └── titles.sql Build Dashboard The models of the marts layer can be consumed by external tools such as Amazon QuickSight. Below shows an example dashboard. The pie chart on the left shows the proportion of titles by genre while the box plot on the right shows the dispersion of average rating by start year.\nGenerate dbt Documentation A nice feature of dbt is documentation. It provides information about the project and the data warehouse, and it facilitates consumers as well as other developers to discover and understand the datasets better. We can generate the project documents and start a document server as shown below.\n1$ dbt docs generate 2$ dbt docs serve A very useful element of dbt documentation is data lineage, which provides an overall view about how data is transformed and consumed. Below we can see that the final titles model consumes all title-related stating models and an intermediate model from the name basics staging model.\nSummary In this post, we discussed how to build data transformation pipelines using dbt on Amazon EMR on EKS. As Spark Submit does not allow the spark thrift server to run in cluster mode on Kubernetes, a simple wrapper class was created that makes the thrift server run indefinitely. Subsets of IMDb data are used as source and data models are developed in multiple layers according to the dbt best practices. dbt can be used as an effective tool for data transformation in a wide range of data projects from data warehousing to data lake to data lakehouse, and it supports key AWS analytics services - Redshift, Glue, EMR and Athena. More examples of using dbt will be discussed in subsequent posts.\n","date":"November 1, 2022","img":"/blog/2022-11-01-dbt-on-aws-part-4-emr-eks/featured.png","lang":"en","langName":"English","largeImg":"/blog/2022-11-01-dbt-on-aws-part-4-emr-eks/featured_huad7bc56c3140ba61f8a863cb7496d5ad_91067_500x0_resize_box_3.png","permalink":"/blog/2022-11-01-dbt-on-aws-part-4-emr-eks/","series":[{"title":"DBT for Effective Data Transformation on AWS","url":"/series/dbt-for-effective-data-transformation-on-aws/"}],"smallImg":"/blog/2022-11-01-dbt-on-aws-part-4-emr-eks/featured_huad7bc56c3140ba61f8a863cb7496d5ad_91067_180x0_resize_box_3.png","tags":[{"title":"AWS","url":"/tags/aws/"},{"title":"Amazon EMR","url":"/tags/amazon-emr/"},{"title":"Amazon EKS","url":"/tags/amazon-eks/"},{"title":"Amazon QuickSight","url":"/tags/amazon-quicksight/"},{"title":"Data Build Tool (DBT)","url":"/tags/data-build-tool-dbt/"},{"title":"Apache Spark","url":"/tags/apache-spark/"},{"title":"Terraform","url":"/tags/terraform/"}],"timestamp":1667260800,"title":"Data Build Tool (Dbt) for Effective Data Transformation on AWS – Part 4 EMR on EKS"},{"categories":[{"title":"Data Engineering","url":"/categories/data-engineering/"}],"content":"The data build tool (dbt) is an effective data transformation tool and it supports key AWS analytics services - Redshift, Glue, EMR and Athena. In the previous posts, we discussed benefits of a common data transformation tool and the potential of dbt to cover a wide range of data projects from data warehousing to data lake to data lakehouse. Demo data projects that target Redshift Serverless and Glue are illustrated as well. In part 3 of the dbt on AWS series, we discuss data transformation pipelines using dbt on Amazon EMR. Subsets of IMDb data are used as source and data models are developed in multiple layers according to the dbt best practices. A list of posts of this series can be found below.\nPart 1 Redshift Part 2 Glue Part 3 EMR on EC2 (this post) Part 4 EMR on EKS Part 5 Athena Below shows an overview diagram of the scope of this dbt on AWS series. EMR is highlighted as it is discussed in this post.\nInfrastructure The infrastructure hosting this solution leverages an Amazon EMR cluster and a S3 bucket. We also need a VPN server so that a developer can connect to the EMR cluster in a private subnet. It is extended from a previous post and the resources covered there (VPC, subnets, auto scaling group for VPN etc) are not repeated. All resources are deployed using Terraform and the source can be found in the GitHub repository of this post.\nEMR Cluster The EMR 6.7.0 release is deployed with single master and core node instances. It is configured to use the AWS Glue Data Catalog as the metastore for Hive and Spark SQL and it is done by adding the corresponding configuration classification. Also a managed scaling policy is created so that up to 4 additional task instances are added to the cluster. Note an additional security group is attached to the master and core groups for VPN access - the details of that security group is shown below.\n1# dbt-on-aws/emr-ec2/infra/emr.tf 2resource \u0026#34;aws_emr_cluster\u0026#34; \u0026#34;emr_cluster\u0026#34; { 3 name = \u0026#34;${local.name}-emr-cluster\u0026#34; 4 release_label = local.emr.release_label # emr-6.7.0 5 service_role = aws_iam_role.emr_service_role.arn 6 autoscaling_role = aws_iam_role.emr_autoscaling_role.arn 7 applications = local.emr.applications # [\u0026#34;Spark\u0026#34;, \u0026#34;Livy\u0026#34;, \u0026#34;JupyterEnterpriseGateway\u0026#34;, \u0026#34;Hive\u0026#34;] 8 ebs_root_volume_size = local.emr.ebs_root_volume_size 9 log_uri = \u0026#34;s3n://${aws_s3_bucket.default_bucket[0].id}/elasticmapreduce/\u0026#34; 10 step_concurrency_level = 256 11 keep_job_flow_alive_when_no_steps = true 12 termination_protection = false 13 14 ec2_attributes { 15 key_name = aws_key_pair.emr_key_pair.key_name 16 instance_profile = aws_iam_instance_profile.emr_ec2_instance_profile.arn 17 subnet_id = element(tolist(module.vpc.private_subnets), 0) 18 emr_managed_master_security_group = aws_security_group.emr_master.id 19 emr_managed_slave_security_group = aws_security_group.emr_slave.id 20 service_access_security_group = aws_security_group.emr_service_access.id 21 additional_master_security_groups = aws_security_group.emr_vpn_access.id # grant access to VPN server 22 additional_slave_security_groups = aws_security_group.emr_vpn_access.id # grant access to VPN server 23 } 24 25 master_instance_group { 26 instance_type = local.emr.instance_type # m5.xlarge 27 instance_count = local.emr.instance_count # 1 28 } 29 core_instance_group { 30 instance_type = local.emr.instance_type # m5.xlarge 31 instance_count = local.emr.instance_count # 1 32 } 33 34 configurations_json = \u0026lt;\u0026lt;EOF 35 [ 36 { 37 \u0026#34;Classification\u0026#34;: \u0026#34;hive-site\u0026#34;, 38 \u0026#34;Properties\u0026#34;: { 39 \u0026#34;hive.metastore.client.factory.class\u0026#34;: \u0026#34;com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory\u0026#34; 40 } 41 }, 42 { 43 \u0026#34;Classification\u0026#34;: \u0026#34;spark-hive-site\u0026#34;, 44 \u0026#34;Properties\u0026#34;: { 45 \u0026#34;hive.metastore.client.factory.class\u0026#34;: \u0026#34;com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory\u0026#34; 46 } 47 } 48 ] 49 EOF 50 51 tags = local.tags 52 53 depends_on = [module.vpc] 54} 55 56resource \u0026#34;aws_emr_managed_scaling_policy\u0026#34; \u0026#34;emr_scaling_policy\u0026#34; { 57 cluster_id = aws_emr_cluster.emr_cluster.id 58 59 compute_limits { 60 unit_type = \u0026#34;Instances\u0026#34; 61 minimum_capacity_units = 1 62 maximum_capacity_units = 5 63 } 64} The following security group is created to enable access from the VPN server to the EMR instances. Note that the inbound rule is created only when the local.vpn.to_create variable value is true while the security group is created always - if the value is false, the security group has no inbound rule.\n1# dbt-on-aws/emr-ec2/infra/emr.tf 2resource \u0026#34;aws_security_group\u0026#34; \u0026#34;emr_vpn_access\u0026#34; { 3 name = \u0026#34;${local.name}-emr-vpn-access\u0026#34; 4 vpc_id = module.vpc.vpc_id 5 6 lifecycle { 7 create_before_destroy = true 8 } 9 10 tags = local.tags 11} 12 13resource \u0026#34;aws_security_group_rule\u0026#34; \u0026#34;emr_vpn_inbound\u0026#34; { 14 count = local.vpn.to_create ? 1 : 0 15 type = \u0026#34;ingress\u0026#34; 16 description = \u0026#34;VPN access\u0026#34; 17 security_group_id = aws_security_group.emr_vpn_access.id 18 protocol = \u0026#34;tcp\u0026#34; 19 from_port = 0 20 to_port = 65535 21 source_security_group_id = aws_security_group.vpn[0].id 22} As in the previous post, we connect to the EMR cluster via SoftEther VPN. Instead of providing VPN related secrets as Terraform variables, they are created internally and stored to AWS Secrets Manager. The details can be found in dbt-on-aws/emr-ec2/infra/secrets.tf and the secret string can be retrieved as shown below.\n1$ aws secretsmanager get-secret-value --secret-id emr-ec2-all-secrets --query \u0026#34;SecretString\u0026#34; --output text 2 { 3 \u0026#34;vpn_pre_shared_key\u0026#34;: \u0026#34;\u0026lt;vpn-pre-shared-key\u0026gt;\u0026#34;, 4 \u0026#34;vpn_admin_password\u0026#34;: \u0026#34;\u0026lt;vpn-admin-password\u0026gt;\u0026#34; 5 } The previous post demonstrates how to create a VPN user and to establish connection in detail. An example of a successful connection is shown below.\nGlue Databases We have two Glue databases. The source tables and the tables of the staging and intermediate layers are kept in the imdb database. The tables of the marts layer are stored in the _imdb_analytics _database.\n1# glue databases 2resource \u0026#34;aws_glue_catalog_database\u0026#34; \u0026#34;imdb_db\u0026#34; { 3 name = \u0026#34;imdb\u0026#34; 4 location_uri = \u0026#34;s3://${local.default_bucket.name}/imdb\u0026#34; 5 description = \u0026#34;Database that contains IMDb staging/intermediate model datasets\u0026#34; 6} 7 8resource \u0026#34;aws_glue_catalog_database\u0026#34; \u0026#34;imdb_db_marts\u0026#34; { 9 name = \u0026#34;imdb_analytics\u0026#34; 10 location_uri = \u0026#34;s3://${local.default_bucket.name}/imdb_analytics\u0026#34; 11 description = \u0026#34;Database that contains IMDb marts model datasets\u0026#34; 12} Project We build a data transformation pipeline using subsets of IMDb data - seven titles and names related datasets are provided as gzipped, tab-separated-values (TSV) formatted files. The project ends up creating three tables that can be used for reporting and analysis.\nSave Data to S3 The Axel download accelerator is used to download the data files locally followed by decompressing with the gzip utility. Note that simple retry logic is added as I see download failure from time to time. Finally, the decompressed files are saved into the project S3 bucket using the S3 sync command.\n1# dbt-on-aws/emr-ec2/upload-data.sh 2#!/usr/bin/env bash 3 4s3_bucket=$(terraform -chdir=./infra output --raw default_bucket_name) 5hostname=\u0026#34;datasets.imdbws.com\u0026#34; 6declare -a file_names=( 7 \u0026#34;name.basics.tsv.gz\u0026#34; \\ 8 \u0026#34;title.akas.tsv.gz\u0026#34; \\ 9 \u0026#34;title.basics.tsv.gz\u0026#34; \\ 10 \u0026#34;title.crew.tsv.gz\u0026#34; \\ 11 \u0026#34;title.episode.tsv.gz\u0026#34; \\ 12 \u0026#34;title.principals.tsv.gz\u0026#34; \\ 13 \u0026#34;title.ratings.tsv.gz\u0026#34; 14 ) 15 16rm -rf imdb-data 17 18for fn in \u0026#34;${file_names[@]}\u0026#34; 19do 20 download_url=\u0026#34;https://$hostname/$fn\u0026#34; 21 prefix=$(echo ${fn::-7} | tr \u0026#39;.\u0026#39; \u0026#39;_\u0026#39;) 22 echo \u0026#34;download imdb-data/$prefix/$fn from $download_url\u0026#34; 23 while true; 24 do 25 mkdir -p imdb-data/$prefix 26 axel -n 32 -a -o imdb-data/$prefix/$fn $download_url 27 gzip -d imdb-data/$prefix/$fn 28 num_files=$(ls imdb-data/$prefix | wc -l) 29 if [ $num_files == 1 ]; then 30 break 31 fi 32 rm -rf imdb-data/$prefix 33 done 34done 35 36aws s3 sync ./imdb-data s3://$s3_bucket Start Thrift JDBC/ODBC Server The connection from dbt to the EMR cluster is made by the Thrift JDBC/ODBC server and it can be started by adding an EMR step as shown below.\n1$ cd emr-ec2 2$ CLUSTER_ID=$(terraform -chdir=./infra output --raw emr_cluster_id) 3$ aws emr add-steps \\ 4 --cluster-id $CLUSTER_ID \\ 5 --steps Type=CUSTOM_JAR,Name=\u0026#34;spark thrift server\u0026#34;,ActionOnFailure=CONTINUE,Jar=command-runner.jar,Args=[sudo,/usr/lib/spark/sbin/start-thriftserver.sh] We can quickly check if the thrift server is started using the beeline JDBC client. The port is 10001 and, as connection is made in non-secure mode, we can simply enter the default username and a blank password. When we query databases, we see the Glue databases that are created earlier.\n1$ STACK_NAME=emr-ec2 2$ EMR_CLUSTER_MASTER_DNS=$(terraform -chdir=./infra output --raw emr_cluster_master_dns) 3$ ssh -i infra/key-pair/$STACK_NAME-emr-key.pem hadoop@$EMR_CLUSTER_MASTER_DNS 4Last login: Thu Oct 13 08:59:51 2022 from ip-10-0-32-240.ap-southeast-2.compute.internal 5 6 7... 8 9 10[hadoop@ip-10-0-113-195 ~]$ beeline 11SLF4J: Class path contains multiple SLF4J bindings. 12SLF4J: Found binding in [jar:file:/usr/lib/hive/lib/log4j-slf4j-impl-2.17.1.jar!/org/slf4j/impl/StaticLoggerBinder.class] 13SLF4J: Found binding in [jar:file:/usr/lib/tez/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class] 14SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation. 15SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory] 16SLF4J: Class path contains multiple SLF4J bindings. 17SLF4J: Found binding in [jar:file:/usr/lib/hive/lib/log4j-slf4j-impl-2.17.1.jar!/org/slf4j/impl/StaticLoggerBinder.class] 18SLF4J: Found binding in [jar:file:/usr/lib/tez/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class] 19SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation. 20SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory] 21Beeline version 3.1.3-amzn-0 by Apache Hive 22beeline\u0026gt; !connect jdbc:hive2://localhost:10001 23Connecting to jdbc:hive2://localhost:10001 24Enter username for jdbc:hive2://localhost:10001: hadoop 25Enter password for jdbc:hive2://localhost:10001: 26Connected to: Spark SQL (version 3.2.1-amzn-0) 27Driver: Hive JDBC (version 3.1.3-amzn-0) 28Transaction isolation: TRANSACTION_REPEATABLE_READ 290: jdbc:hive2://localhost:10001\u0026gt; show databases; 30+-------------------------------+ 31| namespace | 32+-------------------------------+ 33| default | 34| imdb | 35| imdb_analytics | 36+-------------------------------+ 373 rows selected (0.311 seconds) Setup dbt Project We use the dbt-spark adapter to work with the EMR cluster. As connection is made by the Thrift JDBC/ODBC server, it is necessary to install the adapter with the PyHive package. I use Ubuntu 20.04 in WSL 2 and it needs to install the libsasl2-dev apt package, which is required for one of the dependent packages of PyHive (pure-sasl). After installing it, we can install the dbt packages as usual.\n1$ sudo apt-get install libsasl2-dev 2$ python3 -m venv venv 3$ source venv/bin/activate 4$ pip install --upgrade pip 5$ pip install dbt-core \u0026#34;dbt-spark[PyHive]\u0026#34; We can initialise a dbt project with the dbt init command. We are required to specify project details - project name, host, connection method, port, schema and the number of threads. Note dbt creates the project profile to .dbt/profile.yml of the user home directory by default.\n1$ dbt init 221:00:16 Running with dbt=1.2.2 3Enter a name for your project (letters, digits, underscore): emr_ec2 4Which database would you like to use? 5[1] spark 6 7(Don\u0026#39;t see the one you want? https://docs.getdbt.com/docs/available-adapters) 8 9Enter a number: 1 10host (yourorg.sparkhost.com): \u0026lt;hostname-or-ip-address-of-master-instance\u0026gt; 11[1] odbc 12[2] http 13[3] thrift 14Desired authentication method option (enter a number): 3 15port [443]: 10001 16schema (default schema that dbt will build objects in): imdb 17threads (1 or more) [1]: 3 1821:50:28 Profile emr_ec2 written to /home/\u0026lt;username\u0026gt;/.dbt/profiles.yml using target\u0026#39;s profile_template.yml and your supplied values. Run \u0026#39;dbt debug\u0026#39; to validate the connection. 1921:50:28 20Your new dbt project \u0026#34;emr_ec2\u0026#34; was created! 21 22For more information on how to configure the profiles.yml file, 23please consult the dbt documentation here: 24 25 https://docs.getdbt.com/docs/configure-your-profile 26 27One more thing: 28 29Need help? Don\u0026#39;t hesitate to reach out to us via GitHub issues or on Slack: 30 31 https://community.getdbt.com/ 32 33Happy modeling! dbt initialises a project in a folder that matches to the project name and generates project boilerplate as shown below. Some of the main objects are dbt_project.yml, and the model folder. The former is required because dbt doesn\u0026rsquo;t know if a folder is a dbt project without it. Also it contains information that tells dbt how to operate on the project. The latter is for including dbt models, which is basically a set of SQL select statements. See dbt documentation for more details.\n1$ tree emr-ec2/emr_ec2/ -L 1 2emr-ec2/emr_ec2/ 3├── README.md 4├── analyses 5├── dbt_packages 6├── dbt_project.yml 7├── logs 8├── macros 9├── models 10├── packages.yml 11├── seeds 12├── snapshots 13├── target 14└── tests We can check connection to the EMR cluster with the dbt debug command as shown below.\n1$ dbt debug 221:51:38 Running with dbt=1.2.2 3dbt version: 1.2.2 4python version: 3.8.10 5python path: \u0026lt;path-to-python-path\u0026gt; 6os info: Linux-5.4.72-microsoft-standard-WSL2-x86_64-with-glibc2.29 7Using profiles.yml file at /home/\u0026lt;username\u0026gt;/.dbt/profiles.yml 8Using dbt_project.yml file at \u0026lt;path-to-dbt-project\u0026gt;/dbt_project.yml 9 10Configuration: 11 profiles.yml file [OK found and valid] 12 dbt_project.yml file [OK found and valid] 13 14Required dependencies: 15 - git [OK found] 16 17Connection: 18 host: 10.0.113.195 19 port: 10001 20 cluster: None 21 endpoint: None 22 schema: imdb 23 organization: 0 24 Connection test: [OK connection ok] 25 26All checks passed! After initialisation, the model configuration is updated. The project materialisation is specified as view, although it is the default materialisation. Also, tags are added to the entire model folder as well as folders of specific layers - staging, intermediate and marts. As shown below, tags can simplify model execution.\n1# emr-ec2/emr_ec2/dbt_project.yml 2name: \u0026#34;emr_ec2\u0026#34; 3 4... 5 6models: 7 dbt_glue_proj: 8 +materialized: view 9 +tags: 10 - \u0026#34;imdb\u0026#34; 11 staging: 12 +tags: 13 - \u0026#34;staging\u0026#34; 14 intermediate: 15 +tags: 16 - \u0026#34;intermediate\u0026#34; 17 marts: 18 +tags: 19 - \u0026#34;marts\u0026#34; While we created source tables using Glue crawlers in part 2, they are created directly from S3 by the dbt_external_tables package in this post. Also, the dbt_utils package is installed for adding tests to the final marts models. They can be installed by the dbt deps command.\n1# emr-ec2/emr_ec2/packages.yml 2packages: 3 - package: dbt-labs/dbt_external_tables 4 version: 0.8.2 5 - package: dbt-labs/dbt_utils 6 version: 0.9.2 Create dbt Models The models for this post are organised into three layers according to the dbt best practices - staging, intermediate and marts.\nExternal Source The seven tables that are loaded from S3 are dbt source tables and their details are declared in a YAML file (_imdb_sources.yml). Macros of the dbt_external_tables package parse properties of each table and execute SQL to create each of them. By doing so, we are able to refer to the source tables with the {{ source() }} function. Also we can add tests to source tables. For example two tests (unique, not_null) are added to the tconst column of the title_basics table below and these tests can be executed by the dbt test command.\n1# emr-ec2/emr_ec2/models/staging/imdb/_imdb__sources.yml 2version: 2 3 4sources: 5 - name: imdb 6 description: Subsets of IMDb data, which are available for access to customers for personal and non-commercial use 7 tables: 8 - name: title_basics 9 description: Table that contains basic information of titles 10 external: 11 location: \u0026#34;s3://\u0026lt;s3-bucket-name\u0026gt;/title_basics/\u0026#34; 12 row_format: \u0026gt; 13 serde \u0026#39;org.apache.hadoop.hive.serde2.OpenCSVSerde\u0026#39; 14 with serdeproperties ( 15 \u0026#39;separatorChar\u0026#39;=\u0026#39;\\t\u0026#39; 16 ) 17 table_properties: \u0026#34;(\u0026#39;skip.header.line.count\u0026#39;=\u0026#39;1\u0026#39;)\u0026#34; 18 columns: 19 - name: tconst 20 data_type: string 21 description: alphanumeric unique identifier of the title 22 tests: 23 - unique 24 - not_null 25 - name: titletype 26 data_type: string 27 description: the type/format of the title (e.g. movie, short, tvseries, tvepisode, video, etc) 28 - name: primarytitle 29 data_type: string 30 description: the more popular title / the title used by the filmmakers on promotional materials at the point of release 31 - name: originaltitle 32 data_type: string 33 description: original title, in the original language 34 - name: isadult 35 data_type: string 36 description: flag that indicates whether it is an adult title or not 37 - name: startyear 38 data_type: string 39 description: represents the release year of a title. In the case of TV Series, it is the series start year 40 - name: endyear 41 data_type: string 42 description: TV Series end year. NULL for all other title types 43 - name: runtimeminutes 44 data_type: string 45 description: primary runtime of the title, in minutes 46 - name: genres 47 data_type: string 48 description: includes up to three genres associated with the title The source tables can be created by dbt run-operation stage_external_sources. Note that the following SQL is executed for the _title_basics _table under the hood.\n1create table imdb.title_basics ( 2 tconst string, 3 titletype string, 4 primarytitle string, 5 originaltitle string, 6 isadult string, 7 startyear string, 8 endyear string, 9 runtimeminutes string, 10 genres string 11) 12row format serde \u0026#39;org.apache.hadoop.hive.serde2.OpenCSVSerde\u0026#39; with serdeproperties ( 13\u0026#39;separatorChar\u0026#39;=\u0026#39;\\t\u0026#39; 14) 15location \u0026#39;s3://\u0026lt;s3-bucket-name\u0026gt;/title_basics/\u0026#39; 16tblproperties (\u0026#39;skip.header.line.count\u0026#39;=\u0026#39;1\u0026#39;) Interestingly the header rows of the source tables are not skipped when they are queried by spark while they are skipped by Athena. They have to be filtered out in the stage models of the dbt project as spark is the query engine.\nStaging Based on the source tables, staging models are created. They are created as views, which is the project’s default materialisation. In the SQL statements, column names and data types are modified mainly.\n1# emr-ec2/emr_ec2/models/staging/imdb/stg_imdb__title_basics.sql 2with source as ( 3 4 select * from {{ source(\u0026#39;imdb\u0026#39;, \u0026#39;title_basics\u0026#39;) }} 5 6), 7 8renamed as ( 9 10 select 11 tconst as title_id, 12 titletype as title_type, 13 primarytitle as primary_title, 14 originaltitle as original_title, 15 cast(isadult as boolean) as is_adult, 16 cast(startyear as int) as start_year, 17 cast(endyear as int) as end_year, 18 cast(runtimeminutes as int) as runtime_minutes, 19 case when genres = \u0026#39;N\u0026#39; then null else genres end as genres 20 from source 21 where tconst \u0026lt;\u0026gt; \u0026#39;tconst\u0026#39; 22 23) 24 25select * from renamed Below shows the file tree of the staging models. The staging models can be executed using the dbt run command. As we’ve added tags to the staging layer models, we can limit to execute only this layer by dbt run \u0026ndash;select staging.\n1$ tree emr-ec2/emr_ec2/models/staging/ 2emr-ec2/emr_ec2/models/staging/ 3└── imdb 4 ├── _imdb__models.yml 5 ├── _imdb__sources.yml 6 ├── stg_imdb__name_basics.sql 7 ├── stg_imdb__title_akas.sql 8 ├── stg_imdb__title_basics.sql 9 ├── stg_imdb__title_crews.sql 10 ├── stg_imdb__title_episodes.sql 11 ├── stg_imdb__title_principals.sql 12 └── stg_imdb__title_ratings.sql Note that the model materialisation of the staging and intermediate models is view and the dbt project creates VIRTUAL_VIEW tables. Although we are able to reference those tables in other models, they cannot be queried by Athena.\n1$ aws glue get-tables --database imdb \\ 2 --query \u0026#34;TableList[?Name==\u0026#39;stg_imdb__title_basics\u0026#39;].[Name, TableType, StorageDescriptor.Columns]\u0026#34; --output yaml 3- - stg_imdb__title_basics 4 - VIRTUAL_VIEW 5 - - Name: title_id 6 Type: string 7 - Name: title_type 8 Type: string 9 - Name: primary_title 10 Type: string 11 - Name: original_title 12 Type: string 13 - Name: is_adult 14 Type: boolean 15 - Name: start_year 16 Type: int 17 - Name: end_year 18 Type: int 19 - Name: runtime_minutes 20 Type: int 21 - Name: genres 22 Type: string Instead we can use spark sql to query the tables as shown below.\nIntermediate We can keep intermediate results in this layer so that the models of the final marts layer can be simplified. The source data includes columns where array values are kept as comma separated strings. For example, the genres column of the stg_imdb__title_basics model includes up to three genre values as shown in the previous screenshot. A total of seven columns in three models are columns of comma-separated strings and it is better to flatten them in the intermediate layer. Also, in order to avoid repetition, a dbt macro (f_latten_fields_) is created to share the column-flattening logic.\n1# emr-ec2/emr_ec2/macros/flatten_fields.sql 2{% macro flatten_fields(model, field_name, id_field_name) %} 3 select 4 {{ id_field_name }} as id, 5 explode(split({{ field_name }}, \u0026#39;,\u0026#39;)) as field 6 from {{ model }} 7{% endmacro %} The macro function can be added inside a common table expression (CTE) by specifying the relevant model, field name to flatten and id field name.\n1-- emr-ec2/emr_ec2/models/intermediate/title/int_genres_flattened_from_title_basics.sql 2with flattened as ( 3 {{ flatten_fields(ref(\u0026#39;stg_imdb__title_basics\u0026#39;), \u0026#39;genres\u0026#39;, \u0026#39;title_id\u0026#39;) }} 4) 5 6select 7 id as title_id, 8 field as genre 9from flattened 10order by id The intermediate models are also materialised as views and we can check the array columns are flattened as expected.\nBelow shows the file tree of the intermediate models. Similar to the staging models, the intermediate models can be executed by dbt run --select intermediate.\n1$ tree emr-ec2/emr_ec2/models/intermediate/ emr-ec2/emr_ec2/macros/ 2emr-ec2/emr_ec2/models/intermediate/ 3├── name 4│ ├── _int_name__models.yml 5│ ├── int_known_for_titles_flattened_from_name_basics.sql 6│ └── int_primary_profession_flattened_from_name_basics.sql 7└── title 8 ├── _int_title__models.yml 9 ├── int_directors_flattened_from_title_crews.sql 10 ├── int_genres_flattened_from_title_basics.sql 11 └── int_writers_flattened_from_title_crews.sql 12 13emr-ec2/emr_ec2/macros/ 14└── flatten_fields.sql Marts The models in the marts layer are configured to be materialised as tables in a custom schema. Their materialisation is set to table and the custom schema is specified as analytics while taking _parquet _as the file format. Note that the custom schema name becomes imdb_analytics according to the naming convention of dbt custom schemas. Models of both the staging and intermediate layers are used to create final models to be used for reporting and analytics.\n1-- emr-ec2/emr_ec2/models/marts/analytics/titles.sql 2{{ 3 config( 4 schema=\u0026#39;analytics\u0026#39;, 5 materialized=\u0026#39;table\u0026#39;, 6 file_format=\u0026#39;parquet\u0026#39; 7 ) 8}} 9 10with titles as ( 11 12 select * from {{ ref(\u0026#39;stg_imdb__title_basics\u0026#39;) }} 13 14), 15 16principals as ( 17 18 select 19 title_id, 20 count(name_id) as num_principals 21 from {{ ref(\u0026#39;stg_imdb__title_principals\u0026#39;) }} 22 group by title_id 23 24), 25 26names as ( 27 28 select 29 title_id, 30 count(name_id) as num_names 31 from {{ ref(\u0026#39;int_known_for_titles_flattened_from_name_basics\u0026#39;) }} 32 group by title_id 33 34), 35 36ratings as ( 37 38 select 39 title_id, 40 average_rating, 41 num_votes 42 from {{ ref(\u0026#39;stg_imdb__title_ratings\u0026#39;) }} 43 44), 45 46episodes as ( 47 48 select 49 parent_title_id, 50 count(title_id) as num_episodes 51 from {{ ref(\u0026#39;stg_imdb__title_episodes\u0026#39;) }} 52 group by parent_title_id 53 54), 55 56distributions as ( 57 58 select 59 title_id, 60 count(title) as num_distributions 61 from {{ ref(\u0026#39;stg_imdb__title_akas\u0026#39;) }} 62 group by title_id 63 64), 65 66final as ( 67 68 select 69 t.title_id, 70 t.title_type, 71 t.primary_title, 72 t.original_title, 73 t.is_adult, 74 t.start_year, 75 t.end_year, 76 t.runtime_minutes, 77 t.genres, 78 p.num_principals, 79 n.num_names, 80 r.average_rating, 81 r.num_votes, 82 e.num_episodes, 83 d.num_distributions 84 from titles as t 85 left join principals as p on t.title_id = p.title_id 86 left join names as n on t.title_id = n.title_id 87 left join ratings as r on t.title_id = r.title_id 88 left join episodes as e on t.title_id = e.parent_title_id 89 left join distributions as d on t.title_id = d.title_id 90 91) 92 93select * from final The details of the three models can be found in a YAML file (_analytics__models.yml). We can add tests to models and below we see tests of row count matching to their corresponding staging models.\n1# emr-ec2/emr_ec2/models/marts/analytics/_analytics__models.yml 2version: 2 3 4models: 5 - name: names 6 description: Table that contains all names with additional details 7 tests: 8 - dbt_utils.equal_rowcount: 9 compare_model: ref(\u0026#39;stg_imdb__name_basics\u0026#39;) 10 - name: titles 11 description: Table that contains all titles with additional details 12 tests: 13 - dbt_utils.equal_rowcount: 14 compare_model: ref(\u0026#39;stg_imdb__title_basics\u0026#39;) 15 - name: genre_titles 16 description: Table that contains basic title details after flattening genres The models of the marts layer can be tested using the dbt test command as shown below.\n1$ dbt test --select marts 219:29:31 Running with dbt=1.2.2 319:29:31 Found 15 models, 17 tests, 0 snapshots, 0 analyses, 533 macros, 0 operations, 0 seed files, 7 sources, 0 exposures, 0 metrics 419:29:31 519:29:41 Concurrency: 3 threads (target=\u0026#39;dev\u0026#39;) 619:29:41 719:29:41 1 of 2 START test dbt_utils_equal_rowcount_names_ref_stg_imdb__name_basics_ .... [RUN] 819:29:41 2 of 2 START test dbt_utils_equal_rowcount_titles_ref_stg_imdb__title_basics_ .. [RUN] 919:29:54 1 of 2 PASS dbt_utils_equal_rowcount_names_ref_stg_imdb__name_basics_ .......... [PASS in 13.11s] 1019:29:56 2 of 2 PASS dbt_utils_equal_rowcount_titles_ref_stg_imdb__title_basics_ ........ [PASS in 15.14s] 1119:29:56 1219:29:56 Finished running 2 tests in 0 hours 0 minutes and 25.54 seconds (25.54s). 1319:29:56 1419:29:56 Completed successfully 1519:29:56 1619:29:56 Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2 Below shows the file tree of the marts models. As with the other layers, the marts models can be executed by dbt run --select marts.\n1$ tree emr-ec2/emr_ec2/models/marts/ 2emr-ec2/emr_ec2/models/marts/ 3└── analytics 4 ├── _analytics__models.yml 5 ├── genre_titles.sql 6 ├── names.sql 7 └── titles.sql Build Dashboard The models of the marts layer can be consumed by external tools such as Amazon QuickSight. Below shows an example dashboard. The pie chart on the left shows the proportion of titles by genre while the box plot on the right shows the dispersion of average rating by title type.\nGenerate dbt Documentation A nice feature of dbt is documentation. It provides information about the project and the data warehouse and it facilitates consumers as well as other developers to discover and understand the datasets better. We can generate the project documents and start a document server as shown below.\n1$ dbt docs generate 2$ dbt docs serve A very useful element of dbt documentation is data lineage, which provides an overall view about how data is transformed and consumed. Below we can see that the final titles model consumes all title-related stating models and an intermediate model from the name basics staging model.\nSummary In this post, we discussed how to build data transformation pipelines using dbt on Amazon EMR. Subsets of IMDb data are used as source and data models are developed in multiple layers according to the dbt best practices. dbt can be used as an effective tool for data transformation in a wide range of data projects from data warehousing to data lake to data lakehouse and it supports key AWS analytics services - Redshift, Glue, EMR and Athena. More examples of using dbt will be discussed in subsequent posts.\n","date":"October 19, 2022","img":"/blog/2022-10-19-dbt-on-aws-part-3-emr-ec2/featured.png","lang":"en","langName":"English","largeImg":"/blog/2022-10-19-dbt-on-aws-part-3-emr-ec2/featured_huad7bc56c3140ba61f8a863cb7496d5ad_91067_500x0_resize_box_3.png","permalink":"/blog/2022-10-19-dbt-on-aws-part-3-emr-ec2/","series":[{"title":"DBT for Effective Data Transformation on AWS","url":"/series/dbt-for-effective-data-transformation-on-aws/"}],"smallImg":"/blog/2022-10-19-dbt-on-aws-part-3-emr-ec2/featured_huad7bc56c3140ba61f8a863cb7496d5ad_91067_180x0_resize_box_3.png","tags":[{"title":"AWS","url":"/tags/aws/"},{"title":"Amazon EMR","url":"/tags/amazon-emr/"},{"title":"Amazon QuickSight","url":"/tags/amazon-quicksight/"},{"title":"Data Build Tool (DBT)","url":"/tags/data-build-tool-dbt/"},{"title":"Apache Spark","url":"/tags/apache-spark/"},{"title":"Terraform","url":"/tags/terraform/"}],"timestamp":1666137600,"title":"Data Build Tool (Dbt) for Effective Data Transformation on AWS – Part 3 EMR on EC2"},{"categories":[{"title":"Data Engineering","url":"/categories/data-engineering/"}],"content":"The data build tool (dbt) is an effective data transformation tool and it supports key AWS analytics services - Redshift, Glue, EMR and Athena. In part 1, we discussed benefits of a common data transformation tool and the potential of dbt to cover a wide range of data projects from data warehousing to data lake to data lakehouse. A demo data project that targets Redshift Serverless is illustrated as well. In part 2 of the dbt on AWS series, we discuss data transformation pipelines using dbt on AWS Glue. Subsets of IMDb data are used as source and data models are developed in multiple layers according to the dbt best practices. A list of posts of this series can be found below.\nPart 1 Redshift Part 2 Glue (this post) Part 3 EMR on EC2 Part 4 EMR on EKS Part 5 Athena Below shows an overview diagram of the scope of this dbt on AWS series. Glue is highlighted as it is discussed in this post.\nInfrastructure The infrastructure hosting this solution leverages AWS Glue Data Catalog, AWS Glue Crawlers and a S3 bucket. We also need a runtime IAM role for AWS Glue interactive sessions for data transformation. They are deployed using Terraform and the source can be found in the GitHub repository of this post.\nGlue Databases We have two Glue databases. The source tables and the tables of the staging and intermediate layers are kept in the imdb database. The tables of the marts layer are stored in the _imdb_analytics _database.\n1# dbt-on-aws/glue/infra/main.tf 2resource \u0026#34;aws_glue_catalog_database\u0026#34; \u0026#34;imdb_db\u0026#34; { 3 name = \u0026#34;imdb\u0026#34; 4 description = \u0026#34;Database that contains IMDb staging/intermediate model datasets\u0026#34; 5} 6 7resource \u0026#34;aws_glue_catalog_database\u0026#34; \u0026#34;imdb_db_marts\u0026#34; { 8 name = \u0026#34;imdb_analytics\u0026#34; 9 description = \u0026#34;Database that contains IMDb marts model datasets\u0026#34; 10} Glue Crawlers We use Glue crawlers to create source tables in the imdb database. We can create a single crawler for the seven source tables but it was not satisfactory, especially header detection. Instead, a dedicated crawler is created for each of the tables with its own custom classifier where it includes header columns specifically. The Terraform count meta-argument is used to create the crawlers and classifiers recursively.\n1# dbt-on-aws/glue/infra/main.tf 2resource \u0026#34;aws_glue_crawler\u0026#34; \u0026#34;imdb_crawler\u0026#34; { 3 count = length(local.glue.tables) 4 5 name = local.glue.tables[count.index].name 6 database_name = aws_glue_catalog_database.imdb_db.name 7 role = aws_iam_role.imdb_crawler.arn 8 classifiers = [aws_glue_classifier.imdb_crawler[count.index].id] 9 10 s3_target { 11 path = \u0026#34;s3://${local.default_bucket.name}/${local.glue.tables[count.index].name}\u0026#34; 12 } 13 14 tags = local.tags 15} 16 17resource \u0026#34;aws_glue_classifier\u0026#34; \u0026#34;imdb_crawler\u0026#34; { 18 count = length(local.glue.tables) 19 20 name = local.glue.tables[count.index].name 21 22 csv_classifier { 23 contains_header = \u0026#34;PRESENT\u0026#34; 24 delimiter = \u0026#34;\\t\u0026#34; 25 header = local.glue.tables[count.index].header 26 } 27} 28 29# dbt-on-aws/glue/infra/variables.tf 30locals { 31 name = basename(path.cwd) == \u0026#34;infra\u0026#34; ? basename(dirname(path.cwd)) : basename(path.cwd) 32 region = data.aws_region.current.name 33 environment = \u0026#34;dev\u0026#34; 34 35 default_bucket = { 36 name = \u0026#34;${local.name}-${data.aws_caller_identity.current.account_id}-${local.region}\u0026#34; 37 } 38 39 glue = { 40 tables = [ 41 { name = \u0026#34;name_basics\u0026#34;, header = [\u0026#34;nconst\u0026#34;, \u0026#34;primaryName\u0026#34;, \u0026#34;birthYear\u0026#34;, \u0026#34;deathYear\u0026#34;, \u0026#34;primaryProfession\u0026#34;, \u0026#34;knownForTitles\u0026#34;] }, 42 { name = \u0026#34;title_akas\u0026#34;, header = [\u0026#34;titleId\u0026#34;, \u0026#34;ordering\u0026#34;, \u0026#34;title\u0026#34;, \u0026#34;region\u0026#34;, \u0026#34;language\u0026#34;, \u0026#34;types\u0026#34;, \u0026#34;attributes\u0026#34;, \u0026#34;isOriginalTitle\u0026#34;] }, 43 { name = \u0026#34;title_basics\u0026#34;, header = [\u0026#34;tconst\u0026#34;, \u0026#34;titleType\u0026#34;, \u0026#34;primaryTitle\u0026#34;, \u0026#34;originalTitle\u0026#34;, \u0026#34;isAdult\u0026#34;, \u0026#34;startYear\u0026#34;, \u0026#34;endYear\u0026#34;, \u0026#34;runtimeMinutes\u0026#34;, \u0026#34;genres\u0026#34;] }, 44 { name = \u0026#34;title_crew\u0026#34;, header = [\u0026#34;tconst\u0026#34;, \u0026#34;directors\u0026#34;, \u0026#34;writers\u0026#34;] }, 45 { name = \u0026#34;title_episode\u0026#34;, header = [\u0026#34;tconst\u0026#34;, \u0026#34;parentTconst\u0026#34;, \u0026#34;seasonNumber\u0026#34;, \u0026#34;episodeNumber\u0026#34;] }, 46 { name = \u0026#34;title_principals\u0026#34;, header = [\u0026#34;tconst\u0026#34;, \u0026#34;ordering\u0026#34;, \u0026#34;nconst\u0026#34;, \u0026#34;category\u0026#34;, \u0026#34;job\u0026#34;, \u0026#34;characters\u0026#34;] }, 47 { name = \u0026#34;title_ratings\u0026#34;, header = [\u0026#34;tconst\u0026#34;, \u0026#34;averageRating\u0026#34;, \u0026#34;numVotes\u0026#34;] } 48 ] 49 } 50 51 tags = { 52 Name = local.name 53 Environment = local.environment 54 } 55} Glue Runtime Role for Interactive Sessions We need a runtime (or service) role for Glue interaction sessions. AWS Glue uses this role to run statements in a session, and it is required to generate a profile by the dbt-glue adapter. Two policies are attached to the runtime role - the former is related to managing Glue interactive sessions while the latter is for actual data transformation by Glue.\n1# dbt-on-aws/glue/infra/main.tf 2resource \u0026#34;aws_iam_role\u0026#34; \u0026#34;glue_interactive_session\u0026#34; { 3 name = \u0026#34;${local.name}-glue-interactive-session\u0026#34; 4 5 assume_role_policy = data.aws_iam_policy_document.glue_interactive_session_assume_role_policy.json 6 managed_policy_arns = [ 7 aws_iam_policy.glue_interactive_session.arn, 8 aws_iam_policy.glue_dbt.arn 9 ] 10 11 tags = local.tags 12} 13 14data \u0026#34;aws_iam_policy_document\u0026#34; \u0026#34;glue_interactive_session_assume_role_policy\u0026#34; { 15 statement { 16 actions = [\u0026#34;sts:AssumeRole\u0026#34;] 17 18 principals { 19 type = \u0026#34;Service\u0026#34; 20 identifiers = [ 21 \u0026#34;lakeformation.amazonaws.com\u0026#34;, 22 \u0026#34;glue.amazonaws.com\u0026#34; 23 ] 24 } 25 } 26} 27 28resource \u0026#34;aws_iam_policy\u0026#34; \u0026#34;glue_interactive_session\u0026#34; { 29 name = \u0026#34;${local.name}-glue-interactive-session\u0026#34; 30 path = \u0026#34;/\u0026#34; 31 policy = data.aws_iam_policy_document.glue_interactive_session.json 32 tags = local.tags 33} 34 35resource \u0026#34;aws_iam_policy\u0026#34; \u0026#34;glue_dbt\u0026#34; { 36 name = \u0026#34;${local.name}-glue-dbt\u0026#34; 37 path = \u0026#34;/\u0026#34; 38 policy = data.aws_iam_policy_document.glue_dbt.json 39 tags = local.tags 40} 41 42data \u0026#34;aws_iam_policy_document\u0026#34; \u0026#34;glue_interactive_session\u0026#34; { 43 statement { 44 sid = \u0026#34;AllowStatementInASessionToAUser\u0026#34; 45 46 actions = [ 47 \u0026#34;glue:ListSessions\u0026#34;, 48 \u0026#34;glue:GetSession\u0026#34;, 49 \u0026#34;glue:ListStatements\u0026#34;, 50 \u0026#34;glue:GetStatement\u0026#34;, 51 \u0026#34;glue:RunStatement\u0026#34;, 52 \u0026#34;glue:CancelStatement\u0026#34;, 53 \u0026#34;glue:DeleteSession\u0026#34; 54 ] 55 56 resources = [ 57 \u0026#34;arn:aws:glue:${local.region}:${data.aws_caller_identity.current.account_id}:session/*\u0026#34;, 58 ] 59 } 60 61 statement { 62 actions = [\u0026#34;glue:CreateSession\u0026#34;] 63 64 resources = [\u0026#34;*\u0026#34;] 65 } 66 67 statement { 68 actions = [\u0026#34;iam:PassRole\u0026#34;] 69 70 resources = [\u0026#34;arn:aws:iam::*:role/${local.name}-glue-interactive-session*\u0026#34;] 71 72 condition { 73 test = \u0026#34;StringLike\u0026#34; 74 variable = \u0026#34;iam:PassedToService\u0026#34; 75 76 values = [\u0026#34;glue.amazonaws.com\u0026#34;] 77 } 78 } 79 80 statement { 81 actions = [\u0026#34;iam:PassRole\u0026#34;] 82 83 resources = [\u0026#34;arn:aws:iam::*:role/service-role/${local.name}-glue-interactive-session*\u0026#34;] 84 85 condition { 86 test = \u0026#34;StringLike\u0026#34; 87 variable = \u0026#34;iam:PassedToService\u0026#34; 88 89 values = [\u0026#34;glue.amazonaws.com\u0026#34;] 90 } 91 } 92} 93 94data \u0026#34;aws_iam_policy_document\u0026#34; \u0026#34;glue_dbt\u0026#34; { 95 statement { 96 actions = [ 97 \u0026#34;glue:SearchTables\u0026#34;, 98 \u0026#34;glue:BatchCreatePartition\u0026#34;, 99 \u0026#34;glue:CreatePartitionIndex\u0026#34;, 100 \u0026#34;glue:DeleteDatabase\u0026#34;, 101 \u0026#34;glue:GetTableVersions\u0026#34;, 102 \u0026#34;glue:GetPartitions\u0026#34;, 103 \u0026#34;glue:DeleteTableVersion\u0026#34;, 104 \u0026#34;glue:UpdateTable\u0026#34;, 105 \u0026#34;glue:DeleteTable\u0026#34;, 106 \u0026#34;glue:DeletePartitionIndex\u0026#34;, 107 \u0026#34;glue:GetTableVersion\u0026#34;, 108 \u0026#34;glue:UpdateColumnStatisticsForTable\u0026#34;, 109 \u0026#34;glue:CreatePartition\u0026#34;, 110 \u0026#34;glue:UpdateDatabase\u0026#34;, 111 \u0026#34;glue:CreateTable\u0026#34;, 112 \u0026#34;glue:GetTables\u0026#34;, 113 \u0026#34;glue:GetDatabases\u0026#34;, 114 \u0026#34;glue:GetTable\u0026#34;, 115 \u0026#34;glue:GetDatabase\u0026#34;, 116 \u0026#34;glue:GetPartition\u0026#34;, 117 \u0026#34;glue:UpdateColumnStatisticsForPartition\u0026#34;, 118 \u0026#34;glue:CreateDatabase\u0026#34;, 119 \u0026#34;glue:BatchDeleteTableVersion\u0026#34;, 120 \u0026#34;glue:BatchDeleteTable\u0026#34;, 121 \u0026#34;glue:DeletePartition\u0026#34;, 122 \u0026#34;glue:GetUserDefinedFunctions\u0026#34; 123 ] 124 125 resources = [ 126 \u0026#34;arn:aws:glue:${local.region}:${data.aws_caller_identity.current.account_id}:catalog\u0026#34;, 127 \u0026#34;arn:aws:glue:${local.region}:${data.aws_caller_identity.current.account_id}:table/*/*\u0026#34;, 128 \u0026#34;arn:aws:glue:${local.region}:${data.aws_caller_identity.current.account_id}:database/*\u0026#34;, 129 ] 130 } 131 132 statement { 133 actions = [ 134 \u0026#34;lakeformation:UpdateResource\u0026#34;, 135 \u0026#34;lakeformation:ListResources\u0026#34;, 136 \u0026#34;lakeformation:BatchGrantPermissions\u0026#34;, 137 \u0026#34;lakeformation:GrantPermissions\u0026#34;, 138 \u0026#34;lakeformation:GetDataAccess\u0026#34;, 139 \u0026#34;lakeformation:GetTableObjects\u0026#34;, 140 \u0026#34;lakeformation:PutDataLakeSettings\u0026#34;, 141 \u0026#34;lakeformation:RevokePermissions\u0026#34;, 142 \u0026#34;lakeformation:ListPermissions\u0026#34;, 143 \u0026#34;lakeformation:BatchRevokePermissions\u0026#34;, 144 \u0026#34;lakeformation:UpdateTableObjects\u0026#34; 145 ] 146 147 resources = [\u0026#34;*\u0026#34;] 148 } 149 150 statement { 151 actions = [ 152 \u0026#34;s3:GetBucketLocation\u0026#34;, 153 \u0026#34;s3:ListBucket\u0026#34; 154 ] 155 156 resources = [\u0026#34;arn:aws:s3:::${local.default_bucket.name}\u0026#34;] 157 } 158 159 statement { 160 actions = [ 161 \u0026#34;s3:PutObject\u0026#34;, 162 \u0026#34;s3:PutObjectAcl\u0026#34;, 163 \u0026#34;s3:GetObject\u0026#34;, 164 \u0026#34;s3:DeleteObject\u0026#34; 165 ] 166 167 resources = [\u0026#34;arn:aws:s3:::${local.default_bucket.name}/*\u0026#34;] 168 } 169} Project We build a data transformation pipeline using subsets of IMDb data - seven titles and names related datasets are provided as gzipped, tab-separated-values (TSV) formatted files. This results in three tables that can be used for reporting and analysis.\nSave Data to S3 The Axel download accelerator is used to download the data files locally followed by decompressing with the gzip utility. Note that simple retry logic is added as I see download failure from time to time. Finally, the decompressed files are saved into the project S3 bucket using the S3 sync command.\n1# dbt-on-aws/glue/upload-data.sh 2#!/usr/bin/env bash 3 4s3_bucket=$(terraform -chdir=./infra output --raw default_bucket_name) 5hostname=\u0026#34;datasets.imdbws.com\u0026#34; 6declare -a file_names=( 7 \u0026#34;name.basics.tsv.gz\u0026#34; \\ 8 \u0026#34;title.akas.tsv.gz\u0026#34; \\ 9 \u0026#34;title.basics.tsv.gz\u0026#34; \\ 10 \u0026#34;title.crew.tsv.gz\u0026#34; \\ 11 \u0026#34;title.episode.tsv.gz\u0026#34; \\ 12 \u0026#34;title.principals.tsv.gz\u0026#34; \\ 13 \u0026#34;title.ratings.tsv.gz\u0026#34; 14 ) 15 16rm -rf imdb-data 17 18for fn in \u0026#34;${file_names[@]}\u0026#34; 19do 20 download_url=\u0026#34;https://$hostname/$fn\u0026#34; 21 prefix=$(echo ${fn::-7} | tr \u0026#39;.\u0026#39; \u0026#39;_\u0026#39;) 22 echo \u0026#34;download imdb-data/$prefix/$fn from $download_url\u0026#34; 23 while true; 24 do 25 mkdir -p imdb-data/$prefix 26 axel -n 32 -a -o imdb-data/$prefix/$fn $download_url 27 gzip -d imdb-data/$prefix/$fn 28 num_files=$(ls imdb-data/$prefix | wc -l) 29 if [ $num_files == 1 ]; then 30 break 31 fi 32 rm -rf imdb-data/$prefix 33 done 34done 35 36aws s3 sync ./imdb-data s3://$s3_bucket Run Glue Crawlers The Glue crawlers for the seven source tables are executed as shown below.\n1# dbt-on-aws/glue/start-crawlers.sh 2#!/usr/bin/env bash 3 4declare -a crawler_names=( 5 \u0026#34;name_basics\u0026#34; \\ 6 \u0026#34;title_akas\u0026#34; \\ 7 \u0026#34;title_basics\u0026#34; \\ 8 \u0026#34;title_crew\u0026#34; \\ 9 \u0026#34;title_episode\u0026#34; \\ 10 \u0026#34;title_principals\u0026#34; \\ 11 \u0026#34;title_ratings\u0026#34; 12 ) 13 14for cn in \u0026#34;${crawler_names[@]}\u0026#34; 15do 16 echo \u0026#34;start crawler $cn ...\u0026#34; 17 aws glue start-crawler --name $cn 18done Note that the header rows of the source tables are not detected properly by the Glue crawlers, and they have to be filtered out in the stage models of the dbt project.\nSetup dbt Project We need the dbt-core and dbt-glue packages for the main data transformation project. Also the boto3 and aws-glue-sessions packages are necessary for setting up interactive sessions locally. The dbt Glue adapter doesn’t support creating a profile with the dbt init command so that profile creation is skipped when initialising the project.\n1$ pip install --no-cache-dir --upgrade boto3 aws-glue-sessions dbt-core dbt-glue 2$ dbt init --skip-profile-setup 310:04:00 Running with dbt=1.2.1 4Enter a name for your project (letters, digits, underscore): dbt_glue_proj The project profile is manually created as shown below. The attributes are self-explanatory and their details can be checked further in the GitHub repository of the dbt-glue adapter.\n1# dbt-on-aws/glue/set-profile.sh 2#!/usr/bin/env bash 3 4dbt_role_arn=$(terraform -chdir=./infra output --raw glue_interactive_session_role_arn) 5dbt_s3_location=$(terraform -chdir=./infra output --raw default_bucket_name) 6 7cat \u0026lt;\u0026lt; EOF \u0026gt; ~/.dbt/profiles.yml 8dbt_glue_proj: 9 outputs: 10 dev: 11 type: glue 12 role_arn: \u0026#34;${dbt_role_arn}\u0026#34; 13 region: ap-southeast-2 14 workers: 3 15 worker_type: G.1X 16 schema: imdb 17 database: imdb 18 session_provisioning_timeout_in_seconds: 240 19 location: \u0026#34;s3://${dbt_s3_location}\u0026#34; 20 query_timeout_in_seconds: 300 21 idle_timeout: 60 22 glue_version: \u0026#34;3.0\u0026#34; 23 target: dev 24EOF dbt initialises a project in a folder that matches to the project name and generates project boilerplate as shown below. Some of the main objects are dbt_project.yml, and the model folder. The former is required because dbt doesn\u0026rsquo;t know if a folder is a dbt project without it. Also, it contains information that tells dbt how to operate on the project. The latter is for including dbt models, which is basically a set of SQL select statements. See dbt documentation for more details.\n1$ tree glue/dbt_glue_proj/ -L 1 2glue/dbt_glue_proj/ 3├── README.md 4├── analyses 5├── dbt_packages 6├── dbt_project.yml 7├── logs 8├── macros 9├── models 10├── packages.yml 11├── seeds 12├── snapshots 13├── target 14└── tests We can check Glue interactive session connection with the dbt debug command as shown below.\n1$ dbt debug 208:50:58 Running with dbt=1.2.1 3dbt version: 1.2.1 4python version: 3.8.10 5python path: \u0026lt;path-to-python-path\u0026gt; 6os info: Linux-5.4.72-microsoft-standard-WSL2-x86_64-with-glibc2.29 7Using profiles.yml file at /home/\u0026lt;username\u0026gt;/.dbt/profiles.yml 8Using dbt_project.yml file at \u0026lt;path-to-dbt-project\u0026gt;/dbt_project.yml 9 10Configuration: 11 profiles.yml file [OK found and valid] 12 dbt_project.yml file [OK found and valid] 13 14Required dependencies: 15 - git [OK found] 16 17Connection: 18 role_arn: \u0026lt;glue-interactive-session-role-arn\u0026gt; 19 region: ap-southeast-2 20 session_id: None 21 workers: 3 22 worker_type: G.1X 23 session_provisioning_timeout_in_seconds: 240 24 database: imdb 25 schema: imdb 26 location: s3://\u0026lt;s3-bucket-name\u0026gt; 27 extra_jars: None 28 idle_timeout: 60 29 query_timeout_in_seconds: 300 30 glue_version: 3.0 31 security_configuration: None 32 connections: None 33 conf: None 34 extra_py_files: None 35 delta_athena_prefix: None 36 tags: None 37 Connection test: [OK connection ok] 38 39All checks passed! After initialisation, the model configuration is updated. The project materialisation is specified as view although it is the default materialisation. Also tags are added to the entire model folder as well as folders of specific layers - staging, intermediate and marts. As shown below, tags can simplify model execution.\n1# glue/dbt_glue_proj/dbt_project.yml 2name: \u0026#34;dbt_glue_proj\u0026#34; 3 4... 5 6models: 7 dbt_glue_proj: 8 +materialized: view 9 +tags: 10 - \u0026#34;imdb\u0026#34; 11 staging: 12 +tags: 13 - \u0026#34;staging\u0026#34; 14 intermediate: 15 +tags: 16 - \u0026#34;intermediate\u0026#34; 17 marts: 18 +tags: 19 - \u0026#34;marts\u0026#34; The dbt_utils package is installed for adding tests to the final marts models. The packages can be installed by the dbt deps command.\n1# glue/dbt_glue_proj/packages.yml 2packages: 3 - package: dbt-labs/dbt_utils 4 version: 0.9.2 Create dbt Models The models for this post are organised into three layers according to the dbt best practices - staging, intermediate and marts.\nStaging The seven tables that are loaded from S3 are dbt source tables and their details are declared in a YAML file (_imdb_sources.yml). By doing so, we are able to refer to the source tables with the {{ source() }} function. Also we can add tests to source tables. For example below two tests (unique, not_null) are added to the tconst column of the title_basics table below and these tests can be executed by the dbt test command.\n1# glue/dbt_glue_proj/models/staging/imdb/_imdb__sources.yml 2version: 2 3 4sources: 5 - name: imdb 6 description: Subsets of IMDb data, which are available for access to customers for personal and non-commercial use 7 tables: 8 - name: title_basics 9 description: Table that contains basic information of titles 10 columns: 11 - name: tconst 12 description: alphanumeric unique identifier of the title 13 tests: 14 - unique 15 - not_null 16 - name: titletype 17 description: the type/format of the title (e.g. movie, short, tvseries, tvepisode, video, etc) 18 - name: primarytitle 19 description: the more popular title / the title used by the filmmakers on promotional materials at the point of release 20 - name: originaltitle 21 description: original title, in the original language 22 - name: isadult 23 description: flag that indicates whether it is an adult title or not 24 - name: startyear 25 description: represents the release year of a title. In the case of TV Series, it is the series start year 26 - name: endyear 27 description: TV Series end year. NULL for all other title types 28 - name: runtime minutes 29 description: primary runtime of the title, in minutes 30 - name: genres 31 description: includes up to three genres associated with the title 32 ... Based on the source tables, staging models are created. They are created as views, which is the project’s default materialisation. In the SQL statements, column names and data types are modified mainly.\n1# glue/dbt_glue_proj/models/staging/imdb/stg_imdb__title_basics.sql 2with source as ( 3 4 select * from {{ source(\u0026#39;imdb\u0026#39;, \u0026#39;title_basics\u0026#39;) }} 5 6), 7 8renamed as ( 9 10 select 11 tconst as title_id, 12 titletype as title_type, 13 primarytitle as primary_title, 14 originaltitle as original_title, 15 cast(isadult as boolean) as is_adult, 16 cast(startyear as int) as start_year, 17 cast(endyear as int) as end_year, 18 cast(runtimeminutes as int) as runtime_minutes, 19 genres 20 from source 21 where tconst \u0026lt;\u0026gt; \u0026#39;tconst\u0026#39; 22 23) 24 25select * from renamed Below shows the file tree of the staging models. The staging models can be executed using the dbt run command. As we’ve added tags to the staging layer models, we can limit to execute only this layer by dbt run --select staging.\n1$ tree glue/dbt_glue_proj/models/staging/ 2glue/dbt_glue_proj/models/staging/ 3└── imdb 4 ├── _imdb__models.yml 5 ├── _imdb__sources.yml 6 ├── stg_imdb__name_basics.sql 7 ├── stg_imdb__title_akas.sql 8 ├── stg_imdb__title_basics.sql 9 ├── stg_imdb__title_crews.sql 10 ├── stg_imdb__title_episodes.sql 11 ├── stg_imdb__title_principals.sql 12 └── stg_imdb__title_ratings.sql Note that the model materialisation of the staging and intermediate models is view and the dbt project creates VIRTUAL_VIEW tables. Although we are able to reference those tables in other models, they cannot be queried by Athena.\n1$ aws glue get-tables --database imdb \\ 2 --query \u0026#34;TableList[?Name==\u0026#39;stg_imdb__title_basics\u0026#39;].[Name, TableType, StorageDescriptor.Columns]\u0026#34; --output yaml 3- - stg_imdb__title_basics 4 - VIRTUAL_VIEW 5 - - Name: title_id 6 Type: string 7 - Name: title_type 8 Type: string 9 - Name: primary_title 10 Type: string 11 - Name: original_title 12 Type: string 13 - Name: is_adult 14 Type: boolean 15 - Name: start_year 16 Type: int 17 - Name: end_year 18 Type: int 19 - Name: runtime_minutes 20 Type: int 21 - Name: genres 22 Type: string Instead we can use Glue Studio notebooks to query the tables, which is a bit inconvenient.\nIntermediate We can keep intermediate results in this layer so that the models of the final marts layer can be simplified. The source data includes columns where array values are kept as comma separated strings. For example, the genres column of the stg_imdb__title_basics model includes up to three genre values as shown in the previous screenshot. A total of seven columns in three models are columns of comma-separated strings and it is better to flatten them in the intermediate layer. Also, in order to avoid repetition, a dbt macro (f_latten_fields_) is created to share the column-flattening logic.\n1# glue/dbt_glue_proj/macros/flatten_fields.sql 2{% macro flatten_fields(model, field_name, id_field_name) %} 3 select 4 {{ id_field_name }} as id, 5 explode(split({{ field_name }}, \u0026#39;,\u0026#39;)) as field 6 from {{ model }} 7{% endmacro %} The macro function can be added inside a common table expression (CTE) by specifying the relevant model, field name to flatten and id field name.\n1-- glue/dbt_glue_proj/models/intermediate/title/int_genres_flattened_from_title_basics.sql 2with flattened as ( 3 {{ flatten_fields(ref(\u0026#39;stg_imdb__title_basics\u0026#39;), \u0026#39;genres\u0026#39;, \u0026#39;title_id\u0026#39;) }} 4) 5 6select 7 id as title_id, 8 field as genre 9from flattened 10order by id The intermediate models are also materialised as views and we can check the array columns are flattened as expected.\nBelow shows the file tree of the intermediate models. Similar to the staging models, the intermediate models can be executed by dbt run --select intermediate.\n1$ tree glue/dbt_glue_proj/models/intermediate/ glue/dbt_glue_proj/macros/ 2glue/dbt_glue_proj/models/intermediate/ 3├── name 4│ ├── _int_name__models.yml 5│ ├── int_known_for_titles_flattened_from_name_basics.sql 6│ └── int_primary_profession_flattened_from_name_basics.sql 7└── title 8 ├── _int_title__models.yml 9 ├── int_directors_flattened_from_title_crews.sql 10 ├── int_genres_flattened_from_title_basics.sql 11 └── int_writers_flattened_from_title_crews.sql 12 13glue/dbt_glue_proj/macros/ 14└── flatten_fields.sql Marts The models in the marts layer are configured to be materialised as tables in a custom schema. Their materialisation is set to table and the custom schema is specified as analytics while taking _parquet _as the file format. Note that the custom schema name becomes imdb_analytics according to the naming convention of dbt custom schemas. Models of both the staging and intermediate layers are used to create final models to be used for reporting and analytics.\n1-- glue/dbt_glue_proj/models/marts/analytics/titles.sql 2{{ 3 config( 4 schema=\u0026#39;analytics\u0026#39;, 5 materialized=\u0026#39;table\u0026#39;, 6 file_format=\u0026#39;parquet\u0026#39; 7 ) 8}} 9 10with titles as ( 11 12 select * from {{ ref(\u0026#39;stg_imdb__title_basics\u0026#39;) }} 13 14), 15 16principals as ( 17 18 select 19 title_id, 20 count(name_id) as num_principals 21 from {{ ref(\u0026#39;stg_imdb__title_principals\u0026#39;) }} 22 group by title_id 23 24), 25 26names as ( 27 28 select 29 title_id, 30 count(name_id) as num_names 31 from {{ ref(\u0026#39;int_known_for_titles_flattened_from_name_basics\u0026#39;) }} 32 group by title_id 33 34), 35 36ratings as ( 37 38 select 39 title_id, 40 average_rating, 41 num_votes 42 from {{ ref(\u0026#39;stg_imdb__title_ratings\u0026#39;) }} 43 44), 45 46episodes as ( 47 48 select 49 parent_title_id, 50 count(title_id) as num_episodes 51 from {{ ref(\u0026#39;stg_imdb__title_episodes\u0026#39;) }} 52 group by parent_title_id 53 54), 55 56distributions as ( 57 58 select 59 title_id, 60 count(title) as num_distributions 61 from {{ ref(\u0026#39;stg_imdb__title_akas\u0026#39;) }} 62 group by title_id 63 64), 65 66final as ( 67 68 select 69 t.title_id, 70 t.title_type, 71 t.primary_title, 72 t.original_title, 73 t.is_adult, 74 t.start_year, 75 t.end_year, 76 t.runtime_minutes, 77 t.genres, 78 p.num_principals, 79 n.num_names, 80 r.average_rating, 81 r.num_votes, 82 e.num_episodes, 83 d.num_distributions 84 from titles as t 85 left join principals as p on t.title_id = p.title_id 86 left join names as n on t.title_id = n.title_id 87 left join ratings as r on t.title_id = r.title_id 88 left join episodes as e on t.title_id = e.parent_title_id 89 left join distributions as d on t.title_id = d.title_id 90 91) 92 93select * from final The details of the three models can be found in a YAML file (_analytics__models.yml). We can add tests to models and below we see tests of row count matching to their corresponding staging models.\n1# glue/dbt_glue_proj/models/marts/analytics/_analytics__models.yml 2version: 2 3 4models: 5 - name: names 6 description: Table that contains all names with additional details 7 tests: 8 - dbt_utils.equal_rowcount: 9 compare_model: ref(\u0026#39;stg_imdb__name_basics\u0026#39;) 10 - name: titles 11 description: Table that contains all titles with additional details 12 tests: 13 - dbt_utils.equal_rowcount: 14 compare_model: ref(\u0026#39;stg_imdb__title_basics\u0026#39;) 15 - name: genre_titles 16 description: Table that contains basic title details after flattening genres The models of the marts layer can be tested using the dbt test command as shown below.\n1$ dbt test --select marts 209:11:51 Running with dbt=1.2.1 309:11:51 Found 15 models, 17 tests, 0 snapshots, 0 analyses, 521 macros, 0 operations, 0 seed files, 7 sources, 0 exposures, 0 metrics 409:11:51 509:12:28 Concurrency: 1 threads (target=\u0026#39;dev\u0026#39;) 609:12:28 709:12:28 1 of 2 START test dbt_utils_equal_rowcount_names_ref_stg_imdb__name_basics_ .... [RUN] 809:12:53 1 of 2 PASS dbt_utils_equal_rowcount_names_ref_stg_imdb__name_basics_ .......... [PASS in 24.94s] 909:12:53 2 of 2 START test dbt_utils_equal_rowcount_titles_ref_stg_imdb__title_basics_ .. [RUN] 1009:13:04 2 of 2 PASS dbt_utils_equal_rowcount_titles_ref_stg_imdb__title_basics_ ........ [PASS in 11.63s] 1109:13:07 1209:13:07 Finished running 2 tests in 0 hours 1 minutes and 15.74 seconds (75.74s). 1309:13:07 1409:13:07 Completed successfully 1509:13:07 1609:13:07 Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2 Below shows the file tree of the marts models. As with the other layers, the marts models can be executed by dbt run \u0026ndash;select marts.\n$ tree glue/dbt_glue_proj/models/marts/\rglue/dbt_glue_proj/models/marts/\r└── analytics\r├── _analytics__models.yml\r├── genre_titles.sql\r├── names.sql\r└── titles.sql Build Dashboard The models of the marts layer can be consumed by external tools such as Amazon QuickSight. Below shows an example dashboard. The two pie charts on top show proportions of genre and title type. The box plots at the bottom show dispersion of the number of votes and average rating by title type.\nGenerate dbt Documentation A nice feature of dbt is documentation. It provides information about the project and the data warehouse, and it facilitates consumers as well as other developers to discover and understand the datasets better. We can generate the project documents and start a document server as shown below.\n1$ dbt docs generate 2$ dbt docs serve A very useful element of dbt documentation is data lineage, which provides an overall view about how data is transformed and consumed. Below we can see that the final titles model consumes all title-related stating models and an intermediate model from the name basics staging model.\nSummary In this post, we discussed how to build data transformation pipelines using dbt on AWS Glue. Subsets of IMDb data are used as source and data models are developed in multiple layers according to the dbt best practices. dbt can be used as an effective tool for data transformation in a wide range of data projects from data warehousing to data lake to data lakehouse and it supports key AWS analytics services - Redshift, Glue, EMR and Athena. More examples of using dbt will be discussed in subsequent posts.\n","date":"October 9, 2022","img":"/blog/2022-10-09-dbt-on-aws-part-2-glue/featured.png","lang":"en","langName":"English","largeImg":"/blog/2022-10-09-dbt-on-aws-part-2-glue/featured_hu1d701231f4bedda4f795cc28c9478fc5_90647_500x0_resize_box_3.png","permalink":"/blog/2022-10-09-dbt-on-aws-part-2-glue/","series":[{"title":"DBT for Effective Data Transformation on AWS","url":"/series/dbt-for-effective-data-transformation-on-aws/"}],"smallImg":"/blog/2022-10-09-dbt-on-aws-part-2-glue/featured_hu1d701231f4bedda4f795cc28c9478fc5_90647_180x0_resize_box_3.png","tags":[{"title":"AWS","url":"/tags/aws/"},{"title":"AWS Glue","url":"/tags/aws-glue/"},{"title":"Amazon QuickSight","url":"/tags/amazon-quicksight/"},{"title":"Data Build Tool (DBT)","url":"/tags/data-build-tool-dbt/"},{"title":"Apache Spark","url":"/tags/apache-spark/"},{"title":"Terraform","url":"/tags/terraform/"}],"timestamp":1665273600,"title":"Data Build Tool (Dbt) for Effective Data Transformation on AWS – Part 2 Glue"},{"categories":[{"title":"Data Engineering","url":"/categories/data-engineering/"}],"content":"The data build tool (dbt) is an effective data transformation tool and it supports key AWS analytics services - Redshift, Glue, EMR and Athena. In part 1 of the dbt on AWS series, we discuss data transformation pipelines using dbt on Redshift Serverless. Subsets of IMDb data are used as source and data models are developed in multiple layers according to the dbt best practices.\nPart 1 Redshift (this post) Part 2 Glue Part 3 EMR on EC2 Part 4 EMR on EKS Part 5 Athena Motivation In our experience delivering data solutions for our customers, we have observed a desire to move away from a centralised team function, responsible for the data collection, analysis and reporting, towards shifting this responsibility to an organisation\u0026rsquo;s lines of business (LOB) teams. The key driver for this comes from the recognition that LOBs retain the deep data knowledge and business understanding for their respective data domain; which improves the speed with which these teams can develop data solutions and gain customer insights. This shift away from centralised data engineering to LOBs exposed a skills and tooling gap.\nLet\u0026rsquo;s assume as a starting point that the central data engineering team has chosen a project that migrates an on-premise data warehouse into a data lake (spark + iceberg + redshift) on AWS, to provide a cost-effective way to serve data consumers thanks to iceberg\u0026rsquo;s ACID transaction features. The LOB data engineers are new to spark, and they have a little bit of experience in python while the majority of their work is based on SQL. Thanks to their expertise in SQL, however, they are able to get started building data transformation logic on jupyter notebooks using Pyspark. However, they soon find the codebase gets quite bigger even during the minimum valuable product (MVP) phase, which would only amplify the issue as they extend it to cover the entire data warehouse. Additionally the use of notebooks makes development challenging mainly due to lack of modularity and fai,ling to incorporate testing. Upon contacting the central data engineering team for assistance they are advised that the team uses scala and many other tools (e.g. Metorikku) that are successful for them, however cannot be used directly by the engineers of the LOB. Moreover, the engineering team don\u0026rsquo;t even have a suitable data transformation framework that supports iceberg. The LOB data engineering team understand that the data democratisation plan of the enterprise can be more effective if there is a tool or framework that:\ncan be shared across LOBs although they can have different technology stack and practices, fits into various project types from traditional data warehousing to data lakehouse projects, and supports more than a notebook environment by facilitating code modularity and incorporating testing. The data build tool (dbt) is an open-source command line tool, and it does the T in ELT (Extract, Load, Transform) processes well. It supports a wide range of data platforms and the following key AWS analytics services are covered - Redshift, Glue, EMR and Athena. It is one of the most popular tools in the modern data stack that originally covers data warehousing projects. Its scope is extended to data lake projects by the addition of the dbt-spark and dbt-glue adapter where we can develop data lakes with spark SQL. Recently the spark adapter added open source table formats (hudi, iceberg and delta lake) as the supported file formats, and it allows you to work on data lake house projects with it. As discussed in this blog post, dbt has clear advantages compared to spark in terms of\nlow learning curve as SQL is easier than spark better code organisation as there is no correct way of organising transformation pipeline with spark On the other hand, its weaknesses are\nlack of expressiveness as Jinja is quite heavy and verbose, not very readable, and unit-testing is rather tedious limitation of SQL as some logic is much easier to implement with user defined functions rather than SQL Those weaknesses can be overcome by Python models as it allows you to apply transformations as DataFrame operations. Unfortunately the beta feature is not available on any of the AWS services, however it is available on Snowflake, Databricks and BigQuery. Hopefully we can use this feature on Redshift, Glue and EMR in the near future.\nFinally, the following areas are supported by spark, however not supported by DBT:\nE and L of ELT processes real time data processing Overall dbt can be used as an effective tool for data transformation in a wide range of data projects from data warehousing to data lake to data lakehouse. Also it can be more powerful with spark by its Python models feature. Below shows an overview diagram of the scope of this dbt on AWS series. Redshift is highlighted as it is discussed in this post.\nInfrastructure A VPC with 3 public and private subnets is created using the AWS VPC Terraform module. The following Redshift serverless resources are deployed to work with the dbt project. As explained in the Redshift user guide, a namespace is a collection of database objects and users and a workgroup is a collection of compute resources. We also need a Redshift-managed VPC endpoint for a private connection from a client tool. The source can be found in the GitHub repository of this post.\n1# redshift-sls/infra/redshift-sls.tf 2resource \u0026#34;aws_redshiftserverless_namespace\u0026#34; \u0026#34;namespace\u0026#34; { 3 namespace_name = \u0026#34;${local.name}-namespace\u0026#34; 4 5 admin_username = local.redshift.admin_username 6 admin_user_password = local.secrets.redshift_admin_password 7 db_name = local.redshift.db_name 8 default_iam_role_arn = aws_iam_role.redshift_serverless_role.arn 9 iam_roles = [aws_iam_role.redshift_serverless_role.arn] 10 11 tags = local.tags 12} 13 14resource \u0026#34;aws_redshiftserverless_workgroup\u0026#34; \u0026#34;workgroup\u0026#34; { 15 namespace_name = aws_redshiftserverless_namespace.namespace.id 16 workgroup_name = \u0026#34;${local.name}-workgroup\u0026#34; 17 18 base_capacity = local.redshift.base_capacity # 128 19 subnet_ids = module.vpc.private_subnets 20 security_group_ids = [aws_security_group.vpn_redshift_serverless_access.id] 21 22 tags = local.tags 23} 24 25resource \u0026#34;aws_redshiftserverless_endpoint_access\u0026#34; \u0026#34;endpoint_access\u0026#34; { 26 endpoint_name = \u0026#34;${local.name}-endpoint\u0026#34; 27 28 workgroup_name = aws_redshiftserverless_workgroup.workgroup.id 29 subnet_ids = module.vpc.private_subnets 30} As in the previous post, we connect to Redshift via SoftEther VPN to improve developer experience significantly by accessing the database directly from the developer machine. Instead of providing VPN related secrets as Terraform variables in the earlier post, they are created internally and stored to AWS Secrets Manager. Also, the Redshift admin username and password are included so that the secrets can be accessed securely. The details can be found in redshift-sls/infra/secrets.tf and the secret string can be retrieved as shown below.\n1$ aws secretsmanager get-secret-value --secret-id redshift-sls-all-secrets --query \u0026#34;SecretString\u0026#34; --output text 2 { 3 \u0026#34;vpn_pre_shared_key\u0026#34;: \u0026#34;\u0026lt;vpn-pre-shared-key\u0026gt;\u0026#34;, 4 \u0026#34;vpn_admin_password\u0026#34;: \u0026#34;\u0026lt;vpn-admin-password\u0026gt;\u0026#34;, 5 \u0026#34;redshift_admin_username\u0026#34;: \u0026#34;master\u0026#34;, 6 \u0026#34;redshift_admin_password\u0026#34;: \u0026#34;\u0026lt;redshift-admin-password\u0026gt;\u0026#34; 7 } The previous post demonstrates how to create a VPN user and to establish connection in detail. An example of a successful connection is shown below.\nProject We build a data transformation pipeline using subsets of IMDb data - seven titles and names related datasets are provided as gzipped, tab-separated-values (TSV) formatted files. This results in three tables that can be used for reporting and analysis.\nCreate Database Objects The majority of data transformation is performed in the imdb schema, which is configured as the dbt target schema. We create the final three tables in a custom schema named imdb_analytics. Note that its name is according to the naming convention of the dbt custom schema, which is \u0026lt;target_schema\u0026gt;\u0026lt;custom_schema\u0026gt;_. After creating the database schemas, we create a development user (dbt) and a group that the user belongs to, followed by granting necessary permissions of the new schemas to the new group and reassigning schema ownership to the new user.\n1-- redshift-sls/setup-redshift.sql 2-- // create db schemas 3create schema if not exists imdb; 4create schema if not exists imdb_analytics; 5 6-- // create db user and group 7create user dbt with password \u0026#39;\u0026lt;password\u0026gt;\u0026#39;; 8create group dbt with user dbt; 9 10-- // grant permissions to new schemas 11grant usage on schema imdb to group dbt; 12grant create on schema imdb to group dbt; 13grant all on all tables in schema imdb to group dbt; 14 15grant usage on schema imdb_analytics to group dbt; 16grant create on schema imdb_analytics to group dbt; 17grant all on all tables in schema imdb_analytics to group dbt; 18 19-- reassign schema ownership to dbt 20alter schema imdb owner to dbt; 21alter schema imdb_analytics owner to dbt; Save Data to S3 The Axel download accelerator is used to download the data files locally followed by decompressing with the gzip utility. Note that simple retry logic is added as I see download failure from time to time. Finally the decompressed files are saved into the project S3 bucket using the S3 sync command.,\n1# redshift-sls/upload-data.sh 2#!/usr/bin/env bash 3 4s3_bucket=\u0026#34;\u0026lt;s3-bucket-name\u0026gt;\u0026#34; 5hostname=\u0026#34;datasets.imdbws.com\u0026#34; 6declare -a file_names=( 7 \u0026#34;name.basics.tsv.gz\u0026#34; \\ 8 \u0026#34;title.akas.tsv.gz\u0026#34; \\ 9 \u0026#34;title.basics.tsv.gz\u0026#34; \\ 10 \u0026#34;title.crew.tsv.gz\u0026#34; \\ 11 \u0026#34;title.episode.tsv.gz\u0026#34; \\ 12 \u0026#34;title.principals.tsv.gz\u0026#34; \\ 13 \u0026#34;title.ratings.tsv.gz\u0026#34; 14 ) 15 16rm -rf imdb-data 17 18for fn in \u0026#34;${file_names[@]}\u0026#34; 19do 20 download_url=\u0026#34;https://$hostname/$fn\u0026#34; 21 prefix=$(echo ${fn::-7} | tr \u0026#39;.\u0026#39; \u0026#39;_\u0026#39;) 22 echo \u0026#34;download imdb-data/$prefix/$fn from $download_url\u0026#34; 23 # download can fail, retry after removing temporary files if failed 24 while true; 25 do 26 mkdir -p imdb-data/$prefix 27 axel -n 32 -a -o imdb-data/$prefix/$fn $download_url 28 gzip -d imdb-data/$prefix/$fn 29 num_files=$(ls imdb-data/$prefix | wc -l) 30 if [ $num_files == 1 ]; then 31 break 32 fi 33 rm -rf imdb-data/$prefix 34 done 35done 36 37aws s3 sync ./imdb-data s3://$s3_bucket Copy Data The data files in S3 are loaded into Redshift using the COPY command as shown below.\n1-- redshift-sls/setup-redshift.sql 2-- // copy data to tables 3-- name_basics 4drop table if exists imdb.name_basics; 5create table imdb.name_basics ( 6 nconst text, 7 primary_name text, 8 birth_year text, 9 death_year text, 10 primary_profession text, 11 known_for_titles text 12); 13 14copy imdb.name_basics 15from \u0026#39;s3://\u0026lt;s3-bucket-name\u0026gt;/name_basics\u0026#39; 16iam_role default 17delimiter \u0026#39;\\t\u0026#39; 18region \u0026#39;ap-southeast-2\u0026#39; 19ignoreheader 1; 20 21-- title_akas 22drop table if exists imdb.title_akas; 23create table imdb.title_akas ( 24 title_id text, 25 ordering int, 26 title varchar(max), 27 region text, 28 language text, 29 types text, 30 attributes text, 31 is_original_title boolean 32); 33 34copy imdb.title_akas 35from \u0026#39;s3://\u0026lt;s3-bucket-name\u0026gt;/title_akas\u0026#39; 36iam_role default 37delimiter \u0026#39;\\t\u0026#39; 38region \u0026#39;ap-southeast-2\u0026#39; 39ignoreheader 1; 40 41-- title_basics 42drop table if exists imdb.title_basics; 43create table imdb.title_basics ( 44 tconst text, 45 title_type text, 46 primary_title varchar(max), 47 original_title varchar(max), 48 is_adult boolean, 49 start_year text, 50 end_year text, 51 runtime_minutes text, 52 genres text 53); 54 55copy imdb.title_basics 56from \u0026#39;s3://\u0026lt;s3-bucket-name\u0026gt;/title_basics\u0026#39; 57iam_role default 58delimiter \u0026#39;\\t\u0026#39; 59region \u0026#39;ap-southeast-2\u0026#39; 60ignoreheader 1; 61 62-- title_crews 63drop table if exists imdb.title_crews; 64create table imdb.title_crews ( 65 tconst text, 66 directors varchar(max), 67 writers varchar(max) 68); 69 70copy imdb.title_crews 71from \u0026#39;s3://\u0026lt;s3-bucket-name\u0026gt;/title_crew\u0026#39; 72iam_role default 73delimiter \u0026#39;\\t\u0026#39; 74region \u0026#39;ap-southeast-2\u0026#39; 75ignoreheader 1; 76 77-- title_episodes 78drop table if exists imdb.title_episodes; 79create table imdb.title_episodes ( 80 tconst text, 81 parent_tconst text, 82 season_number int, 83 episode_number int 84); 85 86copy imdb.title_episodes 87from \u0026#39;s3://\u0026lt;s3-bucket-name\u0026gt;/title_episode\u0026#39; 88iam_role default 89delimiter \u0026#39;\\t\u0026#39; 90region \u0026#39;ap-southeast-2\u0026#39; 91ignoreheader 1; 92 93-- title_principals 94drop table if exists imdb.title_principals; 95create table imdb.title_principals ( 96 tconst text, 97 ordering int, 98 nconst text, 99 category text, 100 job varchar(max), 101 characters varchar(max) 102); 103 104copy imdb.title_principals 105from \u0026#39;s3://\u0026lt;s3-bucket-name\u0026gt;/title_principals\u0026#39; 106iam_role default 107delimiter \u0026#39;\\t\u0026#39; 108region \u0026#39;ap-southeast-2\u0026#39; 109ignoreheader 1; 110 111-- title_ratings 112drop table if exists imdb.title_ratings; 113create table imdb.title_ratings ( 114 tconst text, 115 average_rating float, 116 num_votes int 117); 118 119copy imdb.title_ratings 120from \u0026#39;s3://\u0026lt;s3-bucket-name\u0026gt;/title_ratings\u0026#39; 121iam_role default 122delimiter \u0026#39;\\t\u0026#39; 123region \u0026#39;ap-southeast-2\u0026#39; 124ignoreheader 1; Initialise dbt Project We need the dbt-core and dbt-redshift. Once installed, we can initialise a dbt project with the dbt init command. We are required to specify project details such as project name, database adapter and database connection info. Note dbt creates the project profile to .dbt/profile.yml of the user home directory by default.\n1$ dbt init 207:07:16 Running with dbt=1.2.1 3Enter a name for your project (letters, digits, underscore): dbt_redshift_sls 4Which database would you like to use? 5[1] postgres 6[2] redshift 7 8(Don\u0026#39;t see the one you want? https://docs.getdbt.com/docs/available-adapters) 9 10Enter a number: 2 11host (hostname.region.redshift.amazonaws.com): \u0026lt;redshift-endpoint-url\u0026gt; 12port [5439]: 13user (dev username): dbt 14[1] password 15[2] iam 16Desired authentication method option (enter a number): 1 17password (dev password): 18dbname (default database that dbt will build objects in): main 19schema (default schema that dbt will build objects in): gdelt 20threads (1 or more) [1]: 4 2107:08:13 Profile dbt_redshift_sls written to /home/\u0026lt;username\u0026gt;/.dbt/profiles.yml using target\u0026#39;s profile_template.yml and your supplied values. Run \u0026#39;dbt debug\u0026#39; to validate the connection. 2207:08:13 23Your new dbt project \u0026#34;dbt_redshift_sls\u0026#34; was created! 24 25For more information on how to configure the profiles.yml file, 26please consult the dbt documentation here: 27 28 https://docs.getdbt.com/docs/configure-your-profile 29 30One more thing: 31 32Need help? Don\u0026#39;t hesitate to reach out to us via GitHub issues or on Slack: 33 34 https://community.getdbt.com/ 35 36Happy modeling! dbt initialises a project in a folder that matches to the project name and generates project boilerplate as shown below. Some of the main objects are dbt_project.yml, and the model folder. The former is required because dbt doesn\u0026rsquo;t know if a folder is a dbt project without it. Also it contains information that tells dbt how to operate on the project. The latter is for including dbt models, which is basically a set of SQL select statements. See dbt documentation for more details.\n1$ tree dbt_redshift_sls/ -L 1 2dbt_redshift_sls/ 3├── README.md 4├── analyses 5├── dbt_project.yml 6├── macros 7├── models 8├── seeds 9├── snapshots 10└── tests We can check the database connection with the dbt debug command. Do not forget to connect to VPN as mentioned earlier.\n1$ dbt debug 203:50:58 Running with dbt=1.2.1 3dbt version: 1.2.1 4python version: 3.8.10 5python path: \u0026lt;path-to-python-path\u0026gt; 6os info: Linux-5.4.72-microsoft-standard-WSL2-x86_64-with-glibc2.29 7Using profiles.yml file at /home/\u0026lt;username\u0026gt;/.dbt/profiles.yml 8Using dbt_project.yml file at \u0026lt;path-to-dbt-project\u0026gt;/dbt_project.yml 9 10Configuration: 11 profiles.yml file [OK found and valid] 12 dbt_project.yml file [OK found and valid] 13 14Required dependencies: 15 - git [OK found] 16 17Connection: 18 host: \u0026lt;redshift-endpoint-url\u0026gt; 19 port: 5439 20 user: dbt 21 database: main 22 schema: imdb 23 search_path: None 24 keepalives_idle: 240 25 sslmode: None 26 method: database 27 cluster_id: None 28 iam_profile: None 29 iam_duration_seconds: 900 30 Connection test: [OK connection ok] 31 32All checks passed! After initialisation, the model configuration is updated. The project materialisation is specified as view although it is the default materialisation. Also tags are added to the entire model folder as well as folders of specific layers - staging, intermediate and marts. As shown below, tags can simplify model execution.\n1# redshift-sls/dbt_redshift_sls/dbt_project.yml 2name: \u0026#34;dbt_redshift_sls\u0026#34; 3... 4 5models: 6 dbt_redshift_sls: 7 +materialized: view 8 +tags: 9 - \u0026#34;imdb\u0026#34; 10 staging: 11 +tags: 12 - \u0026#34;staging\u0026#34; 13 intermediate: 14 +tags: 15 - \u0026#34;intermediate\u0026#34; 16 marts: 17 +tags: 18 - \u0026#34;marts\u0026#34; Two dbt packages are used in this project. The dbt-labs/codegen is used to save typing when generating the source and base models while dbt-labda/dbt_utils for adding tests to the final marts models. The packages can be installed by the dbt deps command.\n1# redshift-sls/dbt_redshift_sls/packages.yml 2packages: 3 - package: dbt-labs/codegen 4 version: 0.8.0 5 - package: dbt-labs/dbt_utils 6 version: 0.9.2 Create dbt Models The models for this post are organised into three layers according to the dbt best practices - staging, intermediate and marts.\nStaging The seven tables that are loaded from S3 are dbt source tables and their details are declared in a YAML file (_imdb_sources.yml). By doing so, we are able to refer to the source tables with the {{ source() }} function. Also we can add tests to source tables. For example below two tests (unique, not_null) are added to the tconst column of the title_basics table below and these tests can be executed by the dbt test command.\n1# redshift-sls/dbt_redshift_sls/models/staging/imdb/_imdb__sources.yml 2version: 2 3 4sources: 5 - name: imdb 6 description: Subsets of IMDb data, which are available for access to customers for personal and non-commercial use 7 tables: 8 ... 9 - name: title_basics 10 description: Table that contains basic information of titles 11 columns: 12 - name: tconst 13 description: alphanumeric unique identifier of the title 14 tests: 15 - unique 16 - not_null 17 - name: title_type 18 description: the type/format of the title (e.g. movie, short, tvseries, tvepisode, video, etc) 19 - name: primary_title 20 description: the more popular title / the title used by the filmmakers on promotional materials at the point of release 21 - name: original_title 22 description: original title, in the original language 23 - name: is_adult 24 description: flag that indicates whether it is an adult title or not 25 - name: start_year 26 description: represents the release year of a title. In the case of TV Series, it is the series start year 27 - name: end_year 28 description: TV Series end year. NULL for all other title types 29 - name: runtime_minutes 30 description: primary runtime of the title, in minutes 31 - name: genres 32 description: includes up to three genres associated with the title 33 ... Based on the source tables, staging models are created. They are created as views, which is the project\u0026rsquo;s default materialisation. In the SQL statements, column names and data types are modified mainly.\n1-- // redshift-sls/dbt_redshift_sls/models/staging/imdb/stg_imdb__title_basics.sql 2with source as ( 3 4 select * from {{ source(\u0026#39;imdb\u0026#39;, \u0026#39;title_basics\u0026#39;) }} 5 6), 7 8renamed as ( 9 10 select 11 tconst as title_id, 12 title_type, 13 primary_title, 14 original_title, 15 is_adult, 16 start_year::int as start_year, 17 end_year::int as end_year, 18 runtime_minutes::int as runtime_minutes, 19 genres 20 from source 21 22) 23 24select * from renamed Below shows the file tree of the staging models. The staging models can be executed using the dbt run command. As we\u0026rsquo;ve added tags to the staging layer models, we can limit to execute only this layer by dbt run --select staging.\n1redshift-sls/dbt_redshift_sls/models/staging/ 2└── imdb 3 ├── _imdb__models.yml 4 ├── _imdb__sources.yml 5 ├── stg_imdb__name_basics.sql 6 ├── stg_imdb__title_akas.sql 7 ├── stg_imdb__title_basics.sql 8 ├── stg_imdb__title_crews.sql 9 ├── stg_imdb__title_episodes.sql 10 ├── stg_imdb__title_principals.sql 11 └── stg_imdb__title_ratings.sql Intermediate We can keep intermediate results in this layer so that the models of the final marts layer can be simplified. The source data includes columns where array values are kept as comma separated strings. For example, the genres column of the stg_imdb__title_basics model includes up to 3 genre values as shown below.\nA total of seven columns in three models are columns of comma-separated strings and it is better to flatten them in the intermediate layer. Also, in order to avoid repetition, a dbt macro (f_latten_fields_) is created to share the column-flattening logic.\n1# redshift-sls/dbt_redshift_sls/macros/flatten_fields.sql 2{% macro flatten_fields(model, field_name, id_field_name) %} 3 with subset as ( 4 select 5 {{ id_field_name }} as id, 6 regexp_count({{ field_name }}, \u0026#39;,\u0026#39;) + 1 AS num_fields, 7 {{ field_name }} as fields 8 from {{ model }} 9 ) 10 select 11 id, 12 1 as idx, 13 split_part(fields, \u0026#39;,\u0026#39;, 1) as field 14 from subset 15 union all 16 select 17 s.id, 18 idx + 1 as idx, 19 split_part(s.fields, \u0026#39;,\u0026#39;, idx + 1) 20 from subset s 21 join cte on s.id = cte.id 22 where idx \u0026lt; num_fields 23{% endmacro %} The macro function can be added inside a recursive cte by specifying the relevant model, field name to flatten and ID field name.\n1-- dbt_redshift_sls/models/intermediate/title/int_genres_flattened_from_title_basics.sql 2with recursive cte (id, idx, field) as ( 3 {{ flatten_fields(ref(\u0026#39;stg_imdb__title_basics\u0026#39;), \u0026#39;genres\u0026#39;, \u0026#39;title_id\u0026#39;) }} 4) 5 6select 7 id as title_id, 8 field as genre 9from cte 10order by id The intermediate models are also materialised as views, and we can check the array columns are flattened as expected.\nBelow shows the file tree of the intermediate models. Similar to the staging models, the intermediate models can be executed by dbt run --select intermediate.\n1redshift-sls/dbt_redshift_sls/models/intermediate/ 2├── name 3│ ├── _int_name__models.yml 4│ ├── int_known_for_titles_flattened_from_name_basics.sql 5│ └── int_primary_profession_flattened_from_name_basics.sql 6└── title 7 ├── _int_title__models.yml 8 ├── int_directors_flattened_from_title_crews.sql 9 ├── int_genres_flattened_from_title_basics.sql 10 └── int_writers_flattened_from_title_crews.sql 11 12redshift-sls/dbt_redshift_sls/macros/ 13└── flatten_fields.sql Marts The models in the marts layer are configured to be materialised as tables in a custom schema. Their materialisation is set to table and the custom schema is specified as analytics. Note that the custom schema name becomes imdb_analytics according to the naming convention of dbt custom schemas. Models of both the staging and intermediate layers are used to create final models to be used for reporting and analytics.\n1-- redshift-sls/dbt_redshift_sls/models/marts/analytics/titles.sql 2{{ 3 config( 4 schema=\u0026#39;analytics\u0026#39;, 5 materialized=\u0026#39;table\u0026#39;, 6 sort=\u0026#39;title_id\u0026#39;, 7 dist=\u0026#39;title_id\u0026#39; 8 ) 9}} 10 11with titles as ( 12 13 select * from {{ ref(\u0026#39;stg_imdb__title_basics\u0026#39;) }} 14 15), 16 17principals as ( 18 19 select 20 title_id, 21 count(name_id) as num_principals 22 from {{ ref(\u0026#39;stg_imdb__title_principals\u0026#39;) }} 23 group by title_id 24 25), 26 27names as ( 28 29 select 30 title_id, 31 count(name_id) as num_names 32 from {{ ref(\u0026#39;int_known_for_titles_flattened_from_name_basics\u0026#39;) }} 33 group by title_id 34 35), 36 37ratings as ( 38 39 select 40 title_id, 41 average_rating, 42 num_votes 43 from {{ ref(\u0026#39;stg_imdb__title_ratings\u0026#39;) }} 44 45), 46 47episodes as ( 48 49 select 50 parent_title_id, 51 count(title_id) as num_episodes 52 from {{ ref(\u0026#39;stg_imdb__title_episodes\u0026#39;) }} 53 group by parent_title_id 54 55), 56 57distributions as ( 58 59 select 60 title_id, 61 count(title) as num_distributions 62 from {{ ref(\u0026#39;stg_imdb__title_akas\u0026#39;) }} 63 group by title_id 64 65), 66 67final as ( 68 69 select 70 t.title_id, 71 t.title_type, 72 t.primary_title, 73 t.original_title, 74 t.is_adult, 75 t.start_year, 76 t.end_year, 77 t.runtime_minutes, 78 t.genres, 79 p.num_principals, 80 n.num_names, 81 r.average_rating, 82 r.num_votes, 83 e.num_episodes, 84 d.num_distributions 85 from titles as t 86 left join principals as p on t.title_id = p.title_id 87 left join names as n on t.title_id = n.title_id 88 left join ratings as r on t.title_id = r.title_id 89 left join episodes as e on t.title_id = e.parent_title_id 90 left join distributions as d on t.title_id = d.title_id 91 92) 93 94select * from final The details of the three models can be found in a YAML file (_analytics__models.yml). We can add tests to models and below we see tests of row count matching to their corresponding staging models.\n1# redshift-sls/dbt_redshift_sls/models/marts/analytics/_analytics__models.yml 2version: 2 3 4models: 5 - name: names 6 description: Table that contains all names with additional details 7 tests: 8 - dbt_utils.equal_rowcount: 9 compare_model: ref(\u0026#39;stg_imdb__name_basics\u0026#39;) 10 - name: titles 11 description: Table that contains all titles with additional details 12 tests: 13 - dbt_utils.equal_rowcount: 14 compare_model: ref(\u0026#39;stg_imdb__title_basics\u0026#39;) 15 - name: genre_titles 16 description: Table that contains basic title details after flattening genres Below shows the file tree of the marts models. As with the other layers, the marts models can be executed by dbt run \u0026ndash;select marts.\n1redshift-sls/dbt_redshift_sls/models/marts/ 2└── analytics 3 ├── _analytics__models.yml 4 ├── genre_titles.sql 5 ├── names.sql 6 └── titles.sql Using the Redshift query editor v2, we can quickly create charts with the final models. The example below shows a pie chart and we see about 50% of titles are from the top 5 genres.\nGenerate dbt Documentation A nice feature of dbt is documentation. It provides information about the project and the data warehouse, and it facilitates consumers as well as other developers to discover and understand the datasets better. We can generate the project documents and start a document server as shown below.\n1$ dbt docs generate 2$ dbt docs serve A very useful element of dbt documentation is data lineage, which provides an overall view about how data is transformed and consumed. Below we can see that the final titles model consumes all title-related stating models and an intermediate model from the name basics staging model.\nSummary In this post, we discussed how to build data transformation pipelines using dbt on Redshift Serverless. Subsets of IMDb data are used as source and data models are developed in multiple layers according to the dbt best practices. dbt can be used as an effective tool for data transformation in a wide range of data projects from data warehousing to data lake to data lakehouse, and it supports key AWS analytics services - Redshift, Glue, EMR and Athena. More examples of using dbt will be discussed in subsequent posts.\n","date":"September 28, 2022","img":"/blog/2022-09-28-dbt-on-aws-part-1-redshift/featured.png","lang":"en","langName":"English","largeImg":"/blog/2022-09-28-dbt-on-aws-part-1-redshift/featured_hu35d67be5daab492ffba00e48de3bbd29_97234_500x0_resize_box_3.png","permalink":"/blog/2022-09-28-dbt-on-aws-part-1-redshift/","series":[{"title":"DBT for Effective Data Transformation on AWS","url":"/series/dbt-for-effective-data-transformation-on-aws/"}],"smallImg":"/blog/2022-09-28-dbt-on-aws-part-1-redshift/featured_hu35d67be5daab492ffba00e48de3bbd29_97234_180x0_resize_box_3.png","tags":[{"title":"AWS","url":"/tags/aws/"},{"title":"Amazon Redshift","url":"/tags/amazon-redshift/"},{"title":"Data Build Tool (DBT)","url":"/tags/data-build-tool-dbt/"},{"title":"Terraform","url":"/tags/terraform/"}],"timestamp":1664323200,"title":"Data Build Tool (Dbt) for Effective Data Transformation on AWS – Part 1 Redshift"},{"categories":[{"title":"Data Engineering","url":"/categories/data-engineering/"}],"content":"When we develop a Spark application on EMR, we can use docker for local development or notebooks via EMR Studio (or EMR Notebooks). However, the local development option is not viable if the size of data is large. Also, I am not a fan of notebooks as it is not possible to utilise the features my editor supports such as syntax highlighting, autocomplete and code formatting. Moreover, it is not possible to organise code into modules and to perform unit testing properly with that option. In this post, We will discuss how to set up a remote development environment on an EMR cluster deployed in a private subnet with VPN and the VS Code remote SSH extension. Typical Spark development examples will be illustrated while sharing the cluster with multiple users. Overall it brings another effective way of developing Spark apps on EMR, which improves developer experience significantly.\nArchitecture An EMR cluster is deployed in a private subnet and, by default, it is not possible to access it from the developer machine. We can construct a PC-to-PC VPN with SoftEther VPN to establish connection to the master node of the cluster. The VPN server runs in a public subnet, and it is managed by an autoscaling group where only a single instance is maintained. An elastic IP address is associated with the instance so that its public IP doesn\u0026rsquo;t change even if the EC2 instance is recreated. Access from the VPN server to the master node is allowed by an additional security group where the VPN\u0026rsquo;s security group is granted access to the master node. The infrastructure is built using Terraform and the source can be found in the post\u0026rsquo;s GitHub repository.\nSoftEther VPN provides the server and client manager programs and they can be downloaded from the download centre page. We can create a VPN user using the server manager and the user can establish connection using the client manager. In this way a developer can access an EMR cluster deployed in a private subnet from the developer machine. Check one of my earlier posts titled Simplify Your Development on AWS with Terraform for a step-by-step illustration of creating a user and making a connection. The VS Code Remote - SSH extension is used to open a folder in the master node of an EMR cluster. In this way, developer experience can be improved significantly while making use of the full feature set of VS Code. The architecture of the remote development environment is shown below.\nInfrastructure The infrastructure of this post is an extension that I illustrated in the previous post. The resources covered there (VPC, subnets, auto scaling group for VPN etc.) won\u0026rsquo;t be repeated. The main resource in this post is an EMR cluster and the latest EMR 6.7.0 release is deployed with single master and core node instances. It is set up to use the AWS Glue Data Catalog as the metastore for Hive and Spark SQL by updating the corresponding configuration classification. Additionally, a managed scaling policy is created so that up to 5 instances are added to the core node. Note the additional security group of the master and slave by which the VPN server is granted access to the master and core node instances - the details of that security group is shown below.\n1# infra/emr.tf 2resource \u0026#34;aws_emr_cluster\u0026#34; \u0026#34;emr_cluster\u0026#34; { 3 name = \u0026#34;${local.name}-emr-cluster\u0026#34; 4 release_label = local.emr.release_label # emr-6.7.0 5 service_role = aws_iam_role.emr_service_role.arn 6 autoscaling_role = aws_iam_role.emr_autoscaling_role.arn 7 applications = local.emr.applications # [\u0026#34;Spark\u0026#34;, \u0026#34;Livy\u0026#34;, \u0026#34;JupyterEnterpriseGateway\u0026#34;, \u0026#34;Hive\u0026#34;] 8 ebs_root_volume_size = local.emr.ebs_root_volume_size 9 log_uri = \u0026#34;s3n://${aws_s3_bucket.default_bucket[0].id}/elasticmapreduce/\u0026#34; 10 step_concurrency_level = 256 11 keep_job_flow_alive_when_no_steps = true 12 termination_protection = false 13 14 ec2_attributes { 15 key_name = aws_key_pair.emr_key_pair.key_name 16 instance_profile = aws_iam_instance_profile.emr_ec2_instance_profile.arn 17 subnet_id = element(tolist(module.vpc.private_subnets), 0) 18 emr_managed_master_security_group = aws_security_group.emr_master.id 19 emr_managed_slave_security_group = aws_security_group.emr_slave.id 20 service_access_security_group = aws_security_group.emr_service_access.id 21 additional_master_security_groups = aws_security_group.emr_vpn_access.id # grant access to VPN server 22 additional_slave_security_groups = aws_security_group.emr_vpn_access.id # grant access to VPN server 23 } 24 25 master_instance_group { 26 instance_type = local.emr.instance_type # m5.xlarge 27 instance_count = local.emr.instance_count # 1 28 } 29 core_instance_group { 30 instance_type = local.emr.instance_type # m5.xlarge 31 instance_count = local.emr.instance_count # 1 32 } 33 34 configurations_json = \u0026lt;\u0026lt;EOF 35 [ 36 { 37 \u0026#34;Classification\u0026#34;: \u0026#34;hive-site\u0026#34;, 38 \u0026#34;Properties\u0026#34;: { 39 \u0026#34;hive.metastore.client.factory.class\u0026#34;: \u0026#34;com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory\u0026#34; 40 } 41 }, 42 { 43 \u0026#34;Classification\u0026#34;: \u0026#34;spark-hive-site\u0026#34;, 44 \u0026#34;Properties\u0026#34;: { 45 \u0026#34;hive.metastore.client.factory.class\u0026#34;: \u0026#34;com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory\u0026#34; 46 } 47 } 48 ] 49 EOF 50 51 tags = local.tags 52 53 depends_on = [module.vpc] 54} 55 56resource \u0026#34;aws_emr_managed_scaling_policy\u0026#34; \u0026#34;emr_scaling_policy\u0026#34; { 57 cluster_id = aws_emr_cluster.emr_cluster.id 58 59 compute_limits { 60 unit_type = \u0026#34;Instances\u0026#34; 61 minimum_capacity_units = 1 62 maximum_capacity_units = 5 63 } 64} The following security group is created to enable access from the VPN server to the EMR instances. Note that the inbound rule is created only when the local.vpn.to_create variable value is true while the security group is created always - if the value is false, the security group has no inbound rule.\n1# infra/emr.tf 2resource \u0026#34;aws_security_group\u0026#34; \u0026#34;emr_vpn_access\u0026#34; { 3 name = \u0026#34;${local.name}-emr-vpn-access\u0026#34; 4 vpc_id = module.vpc.vpc_id 5 6 lifecycle { 7 create_before_destroy = true 8 } 9 10 tags = local.tags 11} 12 13resource \u0026#34;aws_security_group_rule\u0026#34; \u0026#34;emr_vpn_inbound\u0026#34; { 14 count = local.vpn.to_create ? 1 : 0 15 type = \u0026#34;ingress\u0026#34; 16 description = \u0026#34;VPN access\u0026#34; 17 security_group_id = aws_security_group.emr_vpn_access.id 18 protocol = \u0026#34;tcp\u0026#34; 19 from_port = 0 20 to_port = 65535 21 source_security_group_id = aws_security_group.vpn[0].id 22} Change to Secret Generation For configuring the VPN server, we need a IPsec pre-shared key and admin password. While those are specified as variables earlier, they are generated internally in this post for simplicity. The Terraform shell resource module generates and concatenates them with double dashes (\u0026ndash;). The corresponding values are parsed into the user data of the VPN instance and the string is saved into a file to be used for configuring the VPN server manager.\n1## create VPN secrets - IPsec Pre-Shared Key and admin password for VPN 2## see https://cloud.google.com/network-connectivity/docs/vpn/how-to/generating-pre-shared-key 3module \u0026#34;vpn_secrets\u0026#34; { 4 source = \u0026#34;Invicton-Labs/shell-resource/external\u0026#34; 5 6 # generate \u0026lt;IPsec Pre-Shared Key\u0026gt;--\u0026lt;admin password\u0026gt; and parsed in vpn module 7 command_unix = \u0026#34;echo $(openssl rand -base64 24)--$(openssl rand -base64 24)\u0026#34; 8} 9 10resource \u0026#34;local_file\u0026#34; \u0026#34;vpn_secrets\u0026#34; { 11 content = module.vpn_secrets.stdout 12 filename = \u0026#34;${path.module}/secrets/vpn_secrets\u0026#34; 13} 14 15module \u0026#34;vpn\u0026#34; { 16 source = \u0026#34;terraform-aws-modules/autoscaling/aws\u0026#34; 17 version = \u0026#34;~\u0026gt; 6.5\u0026#34; 18 count = local.vpn.to_create ? 1 : 0 19 20 name = \u0026#34;${local.name}-vpn-asg\u0026#34; 21 22 key_name = local.vpn.to_create ? aws_key_pair.key_pair[0].key_name : null 23 vpc_zone_identifier = module.vpc.public_subnets 24 min_size = 1 25 max_size = 1 26 desired_capacity = 1 27 28 image_id = data.aws_ami.amazon_linux_2.id 29 instance_type = element([for s in local.vpn.spot_override : s.instance_type], 0) 30 security_groups = [aws_security_group.vpn[0].id] 31 iam_instance_profile_arn = aws_iam_instance_profile.vpn[0].arn 32 33 # Launch template 34 create_launch_template = true 35 update_default_version = true 36 37 user_data = base64encode(join(\u0026#34;\\n\u0026#34;, [ 38 \u0026#34;#cloud-config\u0026#34;, 39 yamlencode({ 40 # https://cloudinit.readthedocs.io/en/latest/topics/modules.html 41 write_files : [ 42 { 43 path : \u0026#34;/opt/vpn/bootstrap.sh\u0026#34;, 44 content : templatefile(\u0026#34;${path.module}/scripts/bootstrap.sh\u0026#34;, { 45 aws_region = local.region, 46 allocation_id = aws_eip.vpn[0].allocation_id, 47 vpn_psk = split(\u0026#34;--\u0026#34;, replace(module.vpn_secrets.stdout, \u0026#34;\\n\u0026#34;, \u0026#34;\u0026#34;))[0], # specify IPsec pre-shared key 48 admin_password = split(\u0026#34;--\u0026#34;, replace(module.vpn_secrets.stdout, \u0026#34;\\n\u0026#34;, \u0026#34;\u0026#34;))[1] # specify admin password 49 }), 50 permissions : \u0026#34;0755\u0026#34;, 51 } 52 ], 53 runcmd : [ 54 [\u0026#34;/opt/vpn/bootstrap.sh\u0026#34;], 55 ], 56 }) 57 ])) 58 59 ... 60 61 tags = local.tags 62} After deploying all the resources, it is good to go to the next section if we\u0026rsquo;re able to connect to the VPN server as shown below.\nPreparation User Creation While we are able to use the default hadoop user for development, we can add additional users to share the cluster as well. First let\u0026rsquo;s access the master node via ssh as shown below. Note the access key is stored in the infra/key-pair folder and the master private DNS name can be obtained from the _emr_cluster_master_dns _output value.\n1# access to the master node via ssh 2jaehyeon@cevo$ export EMR_MASTER_DNS=$(terraform -chdir=./infra output --raw emr_cluster_master_dns) 3jaehyeon@cevo$ ssh -i infra/key-pair/emr-remote-dev-emr-key.pem hadoop@$EMR_MASTER_DNS 4The authenticity of host \u0026#39;ip-10-0-113-113.ap-southeast-2.compute.internal (10.0.113.113)\u0026#39; can\u0026#39;t be established. 5ECDSA key fingerprint is SHA256:mNdgPWnDkCG/6IsUDdHAETe/InciOatb8jwELnwfWR4. 6Are you sure you want to continue connecting (yes/no/[fingerprint])? yes 7Warning: Permanently added \u0026#39;ip-10-0-113-113.ap-southeast-2.compute.internal,10.0.113.113\u0026#39; (ECDSA) to the list of known hosts. 8 9 __| __|_ ) 10 _| ( / Amazon Linux 2 AMI 11 ___|\\___|___| 12 13https://aws.amazon.com/amazon-linux-2/ 1417 package(s) needed for security, out of 38 available 15Run \u0026#34;sudo yum update\u0026#34; to apply all updates. 16 17EEEEEEEEEEEEEEEEEEEE MMMMMMMM MMMMMMMM RRRRRRRRRRRRRRR 18E::::::::::::::::::E M:::::::M M:::::::M R::::::::::::::R 19EE:::::EEEEEEEEE:::E M::::::::M M::::::::M R:::::RRRRRR:::::R 20 E::::E EEEEE M:::::::::M M:::::::::M RR::::R R::::R 21 E::::E M::::::M:::M M:::M::::::M R:::R R::::R 22 E:::::EEEEEEEEEE M:::::M M:::M M:::M M:::::M R:::RRRRRR:::::R 23 E::::::::::::::E M:::::M M:::M:::M M:::::M R:::::::::::RR 24 E:::::EEEEEEEEEE M:::::M M:::::M M:::::M R:::RRRRRR::::R 25 E::::E M:::::M M:::M M:::::M R:::R R::::R 26 E::::E EEEEE M:::::M MMM M:::::M R:::R R::::R 27EE:::::EEEEEEEE::::E M:::::M M:::::M R:::R R::::R 28E::::::::::::::::::E M:::::M M:::::M RR::::R R::::R 29EEEEEEEEEEEEEEEEEEEE MMMMMMM MMMMMMM RRRRRR RRRRRR 30 31[hadoop@ip-10-0-113-113 ~]$ A user can be created as shown below. Optionally the user is added to the sudoers file so that the user is allowed to run a command as the root user without specifying the password. Note this is a shortcut only, and please check this page for proper usage of editing the sudoers file.\n1# create a user and add to sudoers 2[hadoop@ip-10-0-113-113 ~]$ sudo adduser jaehyeon 3[hadoop@ip-10-0-113-113 ~]$ ls /home/ 4ec2-user emr-notebook hadoop jaehyeon 5[hadoop@ip-10-0-113-113 ~]$ sudo su 6[root@ip-10-0-113-113 hadoop]# chmod +w /etc/sudoers 7[root@ip-10-0-113-113 hadoop]# echo \u0026#34;jaehyeon ALL=(ALL) NOPASSWD:ALL\u0026#34; \u0026gt;\u0026gt; /etc/sudoers Also, as described in the EMR documentation, we must add the HDFS user directory for the user account and grant ownership of the directory so that the user is allowed to log in to the cluster to run Hadoop jobs.\n1[root@ip-10-0-113-113 hadoop]# sudo su - hdfs 2# create user directory 3-bash-4.2$ hdfs dfs -mkdir /user/jaehyeon 4SLF4J: Class path contains multiple SLF4J bindings. 5SLF4J: Found binding in [jar:file:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class] 6SLF4J: Found binding in [jar:file:/usr/lib/tez/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class] 7SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation. 8SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory] 9# update directory ownership 10-bash-4.2$ hdfs dfs -chown jaehyeon:jaehyeon /user/jaehyeon 11SLF4J: Class path contains multiple SLF4J bindings. 12SLF4J: Found binding in [jar:file:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class] 13SLF4J: Found binding in [jar:file:/usr/lib/tez/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class] 14SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation. 15SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory] 16# check directories in /user 17-bash-4.2$ hdfs dfs -ls /user 18SLF4J: Class path contains multiple SLF4J bindings. 19SLF4J: Found binding in [jar:file:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class] 20SLF4J: Found binding in [jar:file:/usr/lib/tez/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class] 21SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation. 22SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory] 23Found 7 items 24drwxrwxrwx - hadoop hdfsadmingroup 0 2022-09-03 10:19 /user/hadoop 25drwxr-xr-x - mapred mapred 0 2022-09-03 10:19 /user/history 26drwxrwxrwx - hdfs hdfsadmingroup 0 2022-09-03 10:19 /user/hive 27drwxr-xr-x - jaehyeon jaehyeon 0 2022-09-03 10:39 /user/jaehyeon 28drwxrwxrwx - livy livy 0 2022-09-03 10:19 /user/livy 29drwxrwxrwx - root hdfsadmingroup 0 2022-09-03 10:19 /user/root 30drwxrwxrwx - spark spark 0 2022-09-03 10:19 /user/spark Finally, we need to add the public key to the .ssh/authorized_keys file in order to set up public key authentication for SSH access.\n1# add public key 2[hadoop@ip-10-0-113-113 ~]$ sudo su - jaehyeon 3[jaehyeon@ip-10-0-113-113 ~]$ mkdir .ssh 4[jaehyeon@ip-10-0-113-113 ~]$ chmod 700 .ssh 5[jaehyeon@ip-10-0-113-113 ~]$ touch .ssh/authorized_keys 6[jaehyeon@ip-10-0-113-113 ~]$ chmod 600 .ssh/authorized_keys 7[jaehyeon@ip-10-0-113-113 ~]$ PUBLIC_KEY=\u0026#34;\u0026lt;SSH-PUBLIC-KEY\u0026gt;\u0026#34; 8[jaehyeon@ip-10-0-113-113 ~]$ echo $PUBLIC_KEY \u0026gt; .ssh/authorized_keys Clone Repository As we can open a folder in the master node, the GitHub repository is cloned to each user\u0026rsquo;s home folder to open later.\n1[hadoop@ip-10-0-113-113 ~]$ sudo yum install git 2[hadoop@ip-10-0-113-113 ~]$ git clone https://github.com/jaehyeon-kim/emr-remote-dev.git 3[hadoop@ip-10-0-113-113 ~]$ ls 4emr-remote-dev 5[hadoop@ip-10-0-113-113 ~]$ sudo su - jaehyeon 6[jaehyeon@ip-10-0-113-113 ~]$ git clone https://github.com/jaehyeon-kim/emr-remote-dev.git 7[jaehyeon@ip-10-0-113-113 ~]$ ls 8emr-remote-dev Access to EMR Cluster Now we have two users that have access to the EMR cluster and their connection details are saved into an SSH configuration file as shown below.\n1Host emr-hadoop 2 HostName ip-10-0-113-113.ap-southeast-2.compute.internal 3 User hadoop 4 ForwardAgent yes 5 IdentityFile C:\\Users\\\u0026lt;username\u0026gt;\\.ssh\\emr-remote-dev-emr-key.pem 6Host emr-jaehyeon 7 HostName ip-10-0-113-113.ap-southeast-2.compute.internal 8 User jaehyeon 9 ForwardAgent yes 10 IdentityFile C:\\Users\\\u0026lt;username\u0026gt;\\.ssh\\id_rsa Then we can see the connection details in the remote explorer menu of VS Code. Note the remote SSH extension should be installed for it. On right-clicking the mouse on the emr-hadoop connection, we can select the option to connect to the host in a new window.\nIn a new window, a menu pops up to select the platform of the remote host.\nIf it\u0026rsquo;s the first time connecting to the server, it requests to confirm whether you trust and want to continue connecting to the host. We can hit Continue.\nOnce we are connected, we can open a folder in the server. On selecting File \u0026gt; Open Folder… menu, we can see a list of folders that we can open. Let\u0026rsquo;s open the repository folder we cloned earlier.\nVS Code asks whether we trust the authors of the files in this folder and we can hit Yes.\nNow access to the server with the remote SSH extension is complete and we can check it by opening a terminal where it shows the typical EMR shell.\nPython Configuration We can install the Python extension at minimum and it indicates the extension will be installed in the remote server (emr-hadoop).\nWe\u0026rsquo;ll use the Pyspark and py4j packages that are included in the existing spark distribution. It can be done simply by creating an .env file that adds the relevant paths to the PYTHONPATH variable. In the following screenshot, you see that there is no warning to import SparkSession.\nRemote Development Transform Data It is a simple Spark application that reads a sample NY taxi trip dataset from a public S3 bucket. Once loaded, it converts the pick-up and drop-off datetime columns from string to timestamp followed by writing the transformed data to a destination S3 bucket. It finishes by creating a Glue table with the transformed data.\n1# tripdata_write.py 2from pyspark.sql import SparkSession 3 4from utils import to_timestamp_df 5 6if __name__ == \u0026#34;__main__\u0026#34;: 7 spark = SparkSession.builder.appName(\u0026#34;Trip Data\u0026#34;).enableHiveSupport().getOrCreate() 8 9 dbname = \u0026#34;tripdata\u0026#34; 10 tblname = \u0026#34;ny_taxi\u0026#34; 11 bucket_name = \u0026#34;emr-remote-dev-590312749310-ap-southeast-2\u0026#34; 12 dest_path = f\u0026#34;s3://{bucket_name}/{tblname}/\u0026#34; 13 src_path = \u0026#34;s3://aws-data-analytics-workshops/shared_datasets/tripdata/\u0026#34; 14 # read csv 15 ny_taxi = spark.read.option(\u0026#34;inferSchema\u0026#34;, \u0026#34;true\u0026#34;).option(\u0026#34;header\u0026#34;, \u0026#34;true\u0026#34;).csv(src_path) 16 ny_taxi = to_timestamp_df(ny_taxi, [\u0026#34;lpep_pickup_datetime\u0026#34;, \u0026#34;lpep_dropoff_datetime\u0026#34;]) 17 ny_taxi.printSchema() 18 # write parquet 19 ny_taxi.write.mode(\u0026#34;overwrite\u0026#34;).parquet(dest_path) 20 # create glue table 21 ny_taxi.createOrReplaceTempView(tblname) 22 spark.sql(f\u0026#34;CREATE DATABASE IF NOT EXISTS {dbname}\u0026#34;) 23 spark.sql(f\u0026#34;USE {dbname}\u0026#34;) 24 spark.sql( 25 f\u0026#34;\u0026#34;\u0026#34;CREATE TABLE IF NOT EXISTS {tblname} 26 USING PARQUET 27 LOCATION \u0026#39;{dest_path}\u0026#39; 28 AS SELECT * FROM {tblname} 29 \u0026#34;\u0026#34;\u0026#34; 30 ) As the spark application should run in a cluster, we need to copy it into HDFS. For simplicity, I copied the current folder into the /user/hadoop/emr-remote-dev directory.\n1# copy current folder into /user/hadoop/emr-remote-dev 2[hadoop@ip-10-0-113-113 emr-remote-dev]$ hdfs dfs -put . /user/hadoop/emr-remote-dev 3SLF4J: Class path contains multiple SLF4J bindings. 4SLF4J: Found binding in [jar:file:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class] 5SLF4J: Found binding in [jar:file:/usr/lib/tez/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class] 6SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation. 7SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory] 8# check contents in /user/hadoop/emr-remote-dev 9[hadoop@ip-10-0-113-113 emr-remote-dev]$ hdfs dfs -ls /user/hadoop/emr-remote-dev 10SLF4J: Class path contains multiple SLF4J bindings. 11SLF4J: Found binding in [jar:file:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class] 12SLF4J: Found binding in [jar:file:/usr/lib/tez/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class] 13SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation. 14SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory] 15Found 10 items 16-rw-r--r-- 1 hadoop hdfsadmingroup 93 2022-09-03 11:11 /user/hadoop/emr-remote-dev/.env 17drwxr-xr-x - hadoop hdfsadmingroup 0 2022-09-03 11:11 /user/hadoop/emr-remote-dev/.git 18-rw-r--r-- 1 hadoop hdfsadmingroup 741 2022-09-03 11:11 /user/hadoop/emr-remote-dev/.gitignore 19drwxr-xr-x - hadoop hdfsadmingroup 0 2022-09-03 11:11 /user/hadoop/emr-remote-dev/.vscode 20-rw-r--r-- 1 hadoop hdfsadmingroup 25 2022-09-03 11:11 /user/hadoop/emr-remote-dev/README.md 21drwxr-xr-x - hadoop hdfsadmingroup 0 2022-09-03 11:11 /user/hadoop/emr-remote-dev/infra 22-rw-r--r-- 1 hadoop hdfsadmingroup 873 2022-09-03 11:11 /user/hadoop/emr-remote-dev/test_utils.py 23-rw-r--r-- 1 hadoop hdfsadmingroup 421 2022-09-03 11:11 /user/hadoop/emr-remote-dev/tripdata_read.sh 24-rw-r--r-- 1 hadoop hdfsadmingroup 1066 2022-09-03 11:11 /user/hadoop/emr-remote-dev/tripdata_write.py 25-rw-r--r-- 1 hadoop hdfsadmingroup 441 2022-09-03 11:11 /user/hadoop/emr-remote-dev/utils.py The app can be submitted by specifying the HDFS locations of the app and source file. It is deployed to the YARN cluster with the client deployment mode. In this way, data processing can be performed by executors in the core node and we are able to check execution details in the same terminal.\n1[hadoop@ip-10-0-113-113 emr-remote-dev]$ spark-submit \\ 2 --master yarn \\ 3 --deploy-mode client \\ 4 --py-files hdfs:/user/hadoop/emr-remote-dev/utils.py \\ 5 hdfs:/user/hadoop/emr-remote-dev/tripdata_write.py Once the app completes, we can see that a Glue database named tripdata is created, and it includes a table named ny_taxi.\nRead Data We can connect to the cluster with the other user account as well. Below shows an example of the PySpark shell that reads data from the table created earlier. It just reads the Glue table and adds a column of trip duration followed by showing the summary statistics of key columns.\nUnit Test The Spark application uses a custom function that converts the data type of one or more columns from string to timestamp - to_timestamp_df(). The source of the function and the testing script can be found below.\n1# utils.py 2from typing import List, Union 3from pyspark.sql import DataFrame 4from pyspark.sql.functions import col, to_timestamp 5 6def to_timestamp_df( 7 df: DataFrame, fields: Union[List[str], str], format: str = \u0026#34;M/d/yy H:mm\u0026#34; 8) -\u0026gt; DataFrame: 9 fields = [fields] if isinstance(fields, str) else fields 10 for field in fields: 11 df = df.withColumn(field, to_timestamp(col(field), format)) 12 return df 13 14# test_utils.py 15import pytest 16import datetime 17from pyspark.sql import SparkSession 18from py4j.protocol import Py4JError 19 20from utils import to_timestamp_df 21 22 23@pytest.fixture(scope=\u0026#34;session\u0026#34;) 24def spark(): 25 return ( 26 SparkSession.builder.master(\u0026#34;local\u0026#34;) 27 .appName(\u0026#34;test\u0026#34;) 28 .config(\u0026#34;spark.submit.deployMode\u0026#34;, \u0026#34;client\u0026#34;) 29 .getOrCreate() 30 ) 31 32 33def test_to_timestamp_success(spark): 34 raw_df = spark.createDataFrame( 35 [(\u0026#34;1/1/17 0:01\u0026#34;,)], 36 [\u0026#34;date\u0026#34;], 37 ) 38 39 test_df = to_timestamp_df(raw_df, \u0026#34;date\u0026#34;, \u0026#34;M/d/yy H:mm\u0026#34;) 40 for row in test_df.collect(): 41 assert row[\u0026#34;date\u0026#34;] == datetime.datetime(2017, 1, 1, 0, 1) 42 43 44def test_to_timestamp_bad_format(spark): 45 raw_df = spark.createDataFrame( 46 [(\u0026#34;1/1/17 0:01\u0026#34;,)], 47 [\u0026#34;date\u0026#34;], 48 ) 49 50 with pytest.raises(Py4JError): 51 to_timestamp_df(raw_df, \u0026#34;date\u0026#34;, \u0026#34;M/d/yy HH:mm\u0026#34;).collect() For unit testing, we need to install the Pytest package and export the PYTHONPATH variable that can be found in the .env file. Note, as testing can be run with a local Spark session, the testing package can only be installed in the master node. Below shows an example test run output.\nSummary In this post, we discussed how to set up a remote development environment on an EMR cluster. A cluster is deployed in a private subnet, access from a developer machine is established via PC-to-PC VPN and the VS Code Remote - SSH extension is used to perform remote development. Aside from the default hadoop user, an additional user account is created to show how to share the cluster with multiple users and spark development examples are illustrated with those user accounts. Overall the remote development brings another effective option to develop spark applications on EMR, which improves developer experience significantly.\n","date":"September 7, 2022","img":"/blog/2022-09-07-emr-remote-dev/featured.png","lang":"en","langName":"English","largeImg":"/blog/2022-09-07-emr-remote-dev/featured_huab2404d3988c555f66e1beb603c1cd6b_72448_500x0_resize_box_3.png","permalink":"/blog/2022-09-07-emr-remote-dev/","series":[],"smallImg":"/blog/2022-09-07-emr-remote-dev/featured_huab2404d3988c555f66e1beb603c1cd6b_72448_180x0_resize_box_3.png","tags":[{"title":"AWS","url":"/tags/aws/"},{"title":"Amazon EMR","url":"/tags/amazon-emr/"},{"title":"Apache Spark","url":"/tags/apache-spark/"},{"title":"PySpark","url":"/tags/pyspark/"},{"title":"Visual Studio Code","url":"/tags/visual-studio-code/"},{"title":"Terraform","url":"/tags/terraform/"}],"timestamp":1662508800,"title":"Develop and Test Apache Spark Apps for EMR Remotely Using Visual Studio Code"},{"categories":[],"content":"This guide show you how to install on Arch Linux.\n","date":"September 6, 2022","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/docs/installation/linux/archlinux/","series":[],"smallImg":"","tags":[],"timestamp":1662475343,"title":"Install on Arch Linux"},{"categories":[],"content":"A fast, responsive and feature-rich Hugo theme for blog and documentations site.\n","date":"September 6, 2022","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/docs/introduction/","series":[],"smallImg":"","tags":[],"timestamp":1662475343,"title":"Introduction"},{"categories":[],"content":"This guide show you how to install on Ubuntu.\n","date":"September 6, 2022","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/docs/installation/linux/ubuntu/","series":[],"smallImg":"","tags":[],"timestamp":1662475343,"title":"Install on Ubuntu"},{"categories":[],"content":"This guide show you how to install on Windows.\n","date":"September 6, 2022","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/docs/installation/windows/","series":[],"smallImg":"","tags":[],"timestamp":1662475343,"title":"Install on Windows"},{"categories":[{"title":"Data Engineering","url":"/categories/data-engineering/"}],"content":"Amazon EMR on EKS is a deployment option for Amazon EMR that allows you to automate the provisioning and management of open-source big data frameworks on EKS. While eksctl is popular for working with Amazon EKS clusters, it has limitations when it comes to building infrastructure that integrates multiple AWS services. Also, it is not straightforward to update EKS cluster resources incrementally with it. On the other hand Terraform can be an effective tool for managing infrastructure that includes not only EKS and EMR virtual clusters but also other AWS resources. Moreover, Terraform has a wide range of modules, and it can even be simpler to build and manage infrastructure using those compared to the CLI tool. In this post, we’ll discuss how to provision and manage Spark jobs on EMR on EKS with Terraform. Amazon EKS Blueprints for Terraform will be used for provisioning EKS, EMR virtual cluster and related resources. Also, Spark job autoscaling will be managed by Karpenter where two Spark jobs with and without Dynamic Resource Allocation (DRA) will be compared.\n[Update 2023-12-15] Amazon EKS Blueprints for Terraform is upgraded into the version 5 while this post is based on the version 4.7.0. Some links don\u0026rsquo;t exist in the new GitHub page.\nInfrastructure When a user submits a Spark job, multiple Pods (controller, driver and executors) will be deployed to the EKS cluster that is registered with EMR. In general, Karpenter provides just-in-time capacity for unschedulable Pods by creating (and terminating afterwards) additional nodes. We can configure the pod templates of a Spark job so that all the Pods are managed by Karpenter. In this way, we are able to run it only in transient nodes. Karpenter simplifies autoscaling by provisioning just-in-time capacity, and it also reduces scheduling latency. The source can be found in the post\u0026rsquo;s GitHub repository.\nVPC Both private and public subnets are created in three availability zones using the AWS VPC module. The first two subnet tags are in relation to the subnet requirements and considerations of Amazon EKS. The last one of the private subnet tags (karpenter.sh/discovery) is added so that Karpenter can discover the relevant subnets when provisioning a node for Spark jobs.\n1# infra/main.tf 2module \u0026#34;vpc\u0026#34; { 3 source = \u0026#34;terraform-aws-modules/vpc/aws\u0026#34; 4 version = \u0026#34;~\u0026gt; 3.14\u0026#34; 5 6 name = \u0026#34;${local.name}-vpc\u0026#34; 7 cidr = local.vpc.cidr 8 9 azs = local.vpc.azs 10 public_subnets = [for k, v in local.vpc.azs : cidrsubnet(local.vpc.cidr, 3, k)] 11 private_subnets = [for k, v in local.vpc.azs : cidrsubnet(local.vpc.cidr, 3, k + 3)] 12 13 enable_nat_gateway = true 14 single_nat_gateway = true 15 enable_dns_hostnames = true 16 create_igw = true 17 18 public_subnet_tags = { 19 \u0026#34;kubernetes.io/cluster/${local.name}\u0026#34; = \u0026#34;shared\u0026#34; 20 \u0026#34;kubernetes.io/role/elb\u0026#34; = 1 21 } 22 23 private_subnet_tags = { 24 \u0026#34;kubernetes.io/cluster/${local.name}\u0026#34; = \u0026#34;shared\u0026#34; 25 \u0026#34;kubernetes.io/role/internal-elb\u0026#34; = 1 26 \u0026#34;karpenter.sh/discovery\u0026#34; = local.name 27 } 28 29 tags = local.tags 30} EKS Cluster Amazon EKS Blueprints for Terraform extends the AWS EKS module, and it simplifies to create EKS clusters and Kubenetes add-ons. When it comes to EMR on EKS, it deploys the necessary resources to run EMR Spark jobs. Specifically it automates steps 4 to 7 of the setup documentation and it is possible to configure multiple teams (namespaces) as well. In the module configuration, only one managed node group (managed-ondemand) is created, and it’ll be used to deploy all the critical add-ons. Note that Spark jobs will run in transient nodes, which are managed by Karpenter. Therefore, we don’t need to create node groups for them.\n1# infra/main.tf 2module \u0026#34;eks_blueprints\u0026#34; { 3 source = \u0026#34;github.com/aws-ia/terraform-aws-eks-blueprints?ref=v4.7.0\u0026#34; 4 5 cluster_name = local.name 6 cluster_version = local.eks.cluster_version 7 8 # EKS network config 9 vpc_id = module.vpc.vpc_id 10 private_subnet_ids = module.vpc.private_subnets 11 12 cluster_endpoint_private_access = true 13 cluster_endpoint_public_access = true 14 15 node_security_group_additional_rules = { 16 ingress_self_all = { 17 description = \u0026#34;Node to node all ports/protocols, recommended and required for Add-ons\u0026#34; 18 protocol = \u0026#34;-1\u0026#34; 19 from_port = 0 20 to_port = 0 21 type = \u0026#34;ingress\u0026#34; 22 self = true 23 } 24 egress_all = { 25 description = \u0026#34;Node all egress, recommended outbound traffic for Node groups\u0026#34; 26 protocol = \u0026#34;-1\u0026#34; 27 from_port = 0 28 to_port = 0 29 type = \u0026#34;egress\u0026#34; 30 cidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;] 31 ipv6_cidr_blocks = [\u0026#34;::/0\u0026#34;] 32 } 33 ingress_cluster_to_node_all_traffic = { 34 description = \u0026#34;Cluster API to Nodegroup all traffic, can be restricted further eg, spark-operator 8080...\u0026#34; 35 protocol = \u0026#34;-1\u0026#34; 36 from_port = 0 37 to_port = 0 38 type = \u0026#34;ingress\u0026#34; 39 source_cluster_security_group = true 40 } 41 } 42 43 # EKS manage node groups 44 managed_node_groups = { 45 ondemand = { 46 node_group_name = \u0026#34;managed-ondemand\u0026#34; 47 instance_types = [\u0026#34;m5.xlarge\u0026#34;] 48 subnet_ids = module.vpc.private_subnets 49 max_size = 5 50 min_size = 1 51 desired_size = 1 52 create_launch_template = true 53 launch_template_os = \u0026#34;amazonlinux2eks\u0026#34; 54 update_config = [{ 55 max_unavailable_percentage = 30 56 }] 57 } 58 } 59 60 # EMR on EKS 61 enable_emr_on_eks = true 62 emr_on_eks_teams = { 63 analytics = { 64 namespace = \u0026#34;analytics\u0026#34; 65 job_execution_role = \u0026#34;analytics-job-execution-role\u0026#34; 66 additional_iam_policies = [aws_iam_policy.emr_on_eks.arn] 67 } 68 } 69 70 tags = local.tags 71} EMR Virtual Cluster Terraform has the EMR virtual cluster resource and the EKS cluster can be registered with the associating namespace (analytics). It’ll complete the last step of the setup documentation.\n1# infra/main.tf 2resource \u0026#34;aws_emrcontainers_virtual_cluster\u0026#34; \u0026#34;analytics\u0026#34; { 3 name = \u0026#34;${module.eks_blueprints.eks_cluster_id}-analytics\u0026#34; 4 5 container_provider { 6 id = module.eks_blueprints.eks_cluster_id 7 type = \u0026#34;EKS\u0026#34; 8 9 info { 10 eks_info { 11 namespace = \u0026#34;analytics\u0026#34; 12 } 13 } 14 } 15} Kubernetes Add-ons The Blueprints include the kubernetes-addons module that simplifies deployment of Amazon EKS add-ons as well as Kubernetes add-ons. For scaling Spark jobs in transient nodes, Karpenter and AWS Node Termination Handler add-ons will be used mainly.\n1# infra/main.tf 2module \u0026#34;eks_blueprints_kubernetes_addons\u0026#34; { 3 source = \u0026#34;github.com/aws-ia/terraform-aws-eks-blueprints//modules/kubernetes-addons?ref=v4.7.0\u0026#34; 4 5 eks_cluster_id = module.eks_blueprints.eks_cluster_id 6 eks_cluster_endpoint = module.eks_blueprints.eks_cluster_endpoint 7 eks_oidc_provider = module.eks_blueprints.oidc_provider 8 eks_cluster_version = module.eks_blueprints.eks_cluster_version 9 10 # EKS add-ons 11 enable_amazon_eks_vpc_cni = true 12 enable_amazon_eks_coredns = true 13 enable_amazon_eks_kube_proxy = true 14 15 # K8s add-ons 16 enable_coredns_autoscaler = true 17 enable_metrics_server = true 18 enable_cluster_autoscaler = true 19 enable_karpenter = true 20 enable_aws_node_termination_handler = true 21 22 tags = local.tags 23} Karpenter According to the AWS News Blog,\nKarpenter is an open-source, flexible, high-performance Kubernetes cluster autoscaler built with AWS. It helps improve your application availability and cluster efficiency by rapidly launching right-sized compute resources in response to changing application load. Karpenter also provides just-in-time compute resources to meet your application’s needs and will soon automatically optimize a cluster’s compute resource footprint to reduce costs and improve performance.\nSimply put, Karpeter adds nodes to handle unschedulable pods, schedules pods on those nodes, and removes the nodes when they are not needed. To configure Karpenter, we need to create provisioners that define how Karpenter manages unschedulable pods and expired nodes. For Spark jobs, we can deploy separate provisioners for the driver and executor programs.\nSpark Driver Provisioner The labels contain arbitrary key-value pairs. As shown later, we can add it to the nodeSelector field of the Spark pod template. Then Karpenter provisions a node (if not existing) as defined by this Provisioner object. The requirements define which nodes to provision. Here 3 well-known labels are specified - availability zone, instance family and capacity type. The provider section is specific to cloud providers and, for AWS, we need to indicate InstanceProfile, LaunchTemplate, SubnetSelector or SecurityGroupSelector. Here we’ll use a launch template that keeps the instance group and security group ids. SubnetSelector is added separately as it is not covered by the launch template. Recall that we added a tag to private subnets (\u0026ldquo;karpenter.sh/discovery\u0026rdquo; = local.name) and we can use it here so that Karpenter discovers the relevant subnets when provisioning a node.\n1# infra/provisioners/spark-driver.yaml 2apiVersion: karpenter.sh/v1alpha5 3kind: Provisioner 4metadata: 5 name: spark-driver 6spec: 7 labels: 8 type: karpenter 9 provisioner: spark-driver 10 ttlSecondsAfterEmpty: 30 11 requirements: 12 - key: \u0026#34;topology.kubernetes.io/zone\u0026#34; 13 operator: In 14 values: [${az}] 15 - key: karpenter.k8s.aws/instance-family 16 operator: In 17 values: [m4, m5] 18 - key: \u0026#34;karpenter.sh/capacity-type\u0026#34; 19 operator: In 20 values: [\u0026#34;on-demand\u0026#34;] 21 limits: 22 resources: 23 cpu: \u0026#34;1000\u0026#34; 24 memory: 1000Gi 25 provider: 26 launchTemplate: \u0026#34;karpenter-${cluster_name}\u0026#34; 27 subnetSelector: 28 karpenter.sh/discovery: ${cluster_name} Spark Executor Provisioner The executor provisioner configuration is similar except that it allows more instance family values and the capacity type value is changed into spot.\n1# infra/provisioners/spark-executor.yaml 2apiVersion: karpenter.sh/v1alpha5 3kind: Provisioner 4metadata: 5 name: spark-executor 6spec: 7 labels: 8 type: karpenter 9 provisioner: spark-executor 10 ttlSecondsAfterEmpty: 30 11 requirements: 12 - key: \u0026#34;topology.kubernetes.io/zone\u0026#34; 13 operator: In 14 values: [${az}] 15 - key: karpenter.k8s.aws/instance-family 16 operator: In 17 values: [m4, m5, r4, r5] 18 - key: \u0026#34;karpenter.sh/capacity-type\u0026#34; 19 operator: In 20 values: [\u0026#34;spot\u0026#34;] 21 limits: 22 resources: 23 cpu: \u0026#34;1000\u0026#34; 24 memory: 1000Gi 25 provider: 26 launchTemplate: \u0026#34;karpenter-${cluster_name}\u0026#34; 27 subnetSelector: 28 karpenter.sh/discovery: ${cluster_name} Terraform Resources As mentioned earlier, a launch template is created for the provisioners, and it includes the instance profile, security group ID and additional configuration. The provisioner resources are created from the YAML manifests. Note we only select a single available zone in order to save cost and improve performance of Spark jobs.\n1# infra/main.tf 2module \u0026#34;karpenter_launch_templates\u0026#34; { 3 source = \u0026#34;github.com/aws-ia/terraform-aws-eks-blueprints//modules/launch-templates?ref=v4.7.0\u0026#34; 4 5 eks_cluster_id = module.eks_blueprints.eks_cluster_id 6 7 launch_template_config = { 8 linux = { 9 ami = data.aws_ami.eks.id 10 launch_template_prefix = \u0026#34;karpenter\u0026#34; 11 iam_instance_profile = module.eks_blueprints.managed_node_group_iam_instance_profile_id[0] 12 vpc_security_group_ids = [module.eks_blueprints.worker_node_security_group_id] 13 block_device_mappings = [ 14 { 15 device_name = \u0026#34;/dev/xvda\u0026#34; 16 volume_type = \u0026#34;gp3\u0026#34; 17 volume_size = 100 18 } 19 ] 20 } 21 } 22 23 tags = merge(local.tags, { Name = \u0026#34;karpenter\u0026#34; }) 24} 25 26# deploy spark provisioners for Karpenter autoscaler 27data \u0026#34;kubectl_path_documents\u0026#34; \u0026#34;karpenter_provisioners\u0026#34; { 28 pattern = \u0026#34;${path.module}/provisioners/spark*.yaml\u0026#34; 29 vars = { 30 az = join(\u0026#34;,\u0026#34;, slice(local.vpc.azs, 0, 1)) 31 cluster_name = local.name 32 } 33} 34 35resource \u0026#34;kubectl_manifest\u0026#34; \u0026#34;karpenter_provisioner\u0026#34; { 36 for_each = toset(data.kubectl_path_documents.karpenter_provisioners.documents) 37 yaml_body = each.value 38 39 depends_on = [module.eks_blueprints_kubernetes_addons] 40} Now we can deploy the infrastructure. Be patient until it completes.\nSpark Job A test spark app and pod templates are uploaded to a S3 bucket. The spark app is for testing autoscaling, and it creates multiple parallel threads and waits for a few seconds - it is obtained from EKS Workshop. The pod templates basically select the relevant provisioners for the driver and executor programs. Two Spark jobs will run with and without Dynamic Resource Allocation (DRA). DRA is a Spark feature where the initial number of executors are spawned, and then it is increased until the maximum number of executors is met to process the pending tasks. Idle executors are terminated when there are no pending tasks. This feature is particularly useful if we are not sure how many executors are necessary.\n1## upload.sh 2#!/usr/bin/env bash 3 4# write test script 5mkdir -p scripts/src 6cat \u0026lt;\u0026lt; EOF \u0026gt; scripts/src/threadsleep.py 7import sys 8from time import sleep 9from pyspark.sql import SparkSession 10spark = SparkSession.builder.appName(\u0026#34;threadsleep\u0026#34;).getOrCreate() 11def sleep_for_x_seconds(x):sleep(x*20) 12sc=spark.sparkContext 13sc.parallelize(range(1,6), 5).foreach(sleep_for_x_seconds) 14spark.stop() 15EOF 16 17# write pod templates 18mkdir -p scripts/config 19cat \u0026lt;\u0026lt; EOF \u0026gt; scripts/config/driver-template.yaml 20apiVersion: v1 21kind: Pod 22spec: 23 nodeSelector: 24 type: \u0026#39;karpenter\u0026#39; 25 provisioner: \u0026#39;spark-driver\u0026#39; 26 tolerations: 27 - key: \u0026#39;spark-driver\u0026#39; 28 operator: \u0026#39;Exists\u0026#39; 29 effect: \u0026#39;NoSchedule\u0026#39; 30 containers: 31 - name: spark-kubernetes-driver 32EOF 33 34cat \u0026lt;\u0026lt; EOF \u0026gt; scripts/config/executor-template.yaml 35apiVersion: v1 36kind: Pod 37spec: 38 nodeSelector: 39 type: \u0026#39;karpenter\u0026#39; 40 provisioner: \u0026#39;spark-executor\u0026#39; 41 tolerations: 42 - key: \u0026#39;spark-executor\u0026#39; 43 operator: \u0026#39;Exists\u0026#39; 44 effect: \u0026#39;NoSchedule\u0026#39; 45 containers: 46 - name: spark-kubernetes-executor 47EOF 48 49# sync to S3 50DEFAULT_BUCKET_NAME=$(terraform -chdir=./infra output --raw default_bucket_name) 51aws s3 sync . s3://$DEFAULT_BUCKET_NAME --exclude \u0026#34;*\u0026#34; --include \u0026#34;scripts/*\u0026#34; Without Dynamic Resource Allocation (DRA) 15 executors are configured to run for the Spark job without DRA. The application configuration is overridden to disable DRA and maps pod templates for the diver and executor programs.\n1export VIRTUAL_CLUSTER_ID=$(terraform -chdir=./infra output --raw emrcontainers_virtual_cluster_id) 2export EMR_ROLE_ARN=$(terraform -chdir=./infra output --json emr_on_eks_role_arn | jq \u0026#39;.[0]\u0026#39; -r) 3export DEFAULT_BUCKET_NAME=$(terraform -chdir=./infra output --raw default_bucket_name) 4export AWS_REGION=$(aws ec2 describe-availability-zones --query \u0026#39;AvailabilityZones[0].[RegionName]\u0026#39; --output text) 5 6## without DRA 7aws emr-containers start-job-run \\ 8--virtual-cluster-id $VIRTUAL_CLUSTER_ID \\ 9--name threadsleep-karpenter-wo-dra \\ 10--execution-role-arn $EMR_ROLE_ARN \\ 11--release-label emr-6.7.0-latest \\ 12--region $AWS_REGION \\ 13--job-driver \u0026#39;{ 14 \u0026#34;sparkSubmitJobDriver\u0026#34;: { 15 \u0026#34;entryPoint\u0026#34;: \u0026#34;s3://\u0026#39;${DEFAULT_BUCKET_NAME}\u0026#39;/scripts/src/threadsleep.py\u0026#34;, 16 \u0026#34;sparkSubmitParameters\u0026#34;: \u0026#34;--conf spark.executor.instances=15 --conf spark.executor.memory=1G --conf spark.executor.cores=1 --conf spark.driver.cores=1\u0026#34; 17 } 18 }\u0026#39; \\ 19--configuration-overrides \u0026#39;{ 20 \u0026#34;applicationConfiguration\u0026#34;: [ 21 { 22 \u0026#34;classification\u0026#34;: \u0026#34;spark-defaults\u0026#34;, 23 \u0026#34;properties\u0026#34;: { 24 \u0026#34;spark.dynamicAllocation.enabled\u0026#34;:\u0026#34;false\u0026#34;, 25 \u0026#34;spark.kubernetes.executor.deleteOnTermination\u0026#34;: \u0026#34;true\u0026#34;, 26 \u0026#34;spark.kubernetes.driver.podTemplateFile\u0026#34;:\u0026#34;s3://\u0026#39;${DEFAULT_BUCKET_NAME}\u0026#39;/scripts/config/driver-template.yaml\u0026#34;, 27 \u0026#34;spark.kubernetes.executor.podTemplateFile\u0026#34;:\u0026#34;s3://\u0026#39;${DEFAULT_BUCKET_NAME}\u0026#39;/scripts/config/executor-template.yaml\u0026#34; 28 } 29 } 30 ] 31}\u0026#39; As indicated earlier, Karpenter can provide just-in-time compute resources to meet the Spark job\u0026rsquo;s requirements, and we see that 3 new nodes are added accordingly. Note that, unlike cluster autoscaler, Karpenter provision nodes without creating a node group.\nOnce the job completes, the new nodes are terminated as expected.\nBelow shows the event timeline of the Spark job. It adds all the 15 executors regardless of whether there are pending tasks or not. The DRA feature of Spark can be beneficial in this situation, and it’ll be discussed in the next section.\nWith Dynamic Resource Allocation (DRA) Here the initial number of executors is set to 1. With DRA enabled, the driver is expected to scale up the executors until it reaches the maximum number of executors if there are pending tasks.\n1export VIRTUAL_CLUSTER_ID=$(terraform -chdir=./infra output --raw emrcontainers_virtual_cluster_id) 2export EMR_ROLE_ARN=$(terraform -chdir=./infra output --json emr_on_eks_role_arn | jq \u0026#39;.[0]\u0026#39; -r) 3export DEFAULT_BUCKET_NAME=$(terraform -chdir=./infra output --raw default_bucket_name) 4export AWS_REGION=$(aws ec2 describe-availability-zones --query \u0026#39;AvailabilityZones[0].[RegionName]\u0026#39; --output text) 5 6## with DRA 7aws emr-containers start-job-run \\ 8--virtual-cluster-id $VIRTUAL_CLUSTER_ID \\ 9--name threadsleep-karpenter-w-dra \\ 10--execution-role-arn $EMR_ROLE_ARN \\ 11--release-label emr-6.7.0-latest \\ 12--region $AWS_REGION \\ 13--job-driver \u0026#39;{ 14 \u0026#34;sparkSubmitJobDriver\u0026#34;: { 15 \u0026#34;entryPoint\u0026#34;: \u0026#34;s3://\u0026#39;${DEFAULT_BUCKET_NAME}\u0026#39;/scripts/src/threadsleep.py\u0026#34;, 16 \u0026#34;sparkSubmitParameters\u0026#34;: \u0026#34;--conf spark.executor.instances=1 --conf spark.executor.memory=1G --conf spark.executor.cores=1 --conf spark.driver.cores=1\u0026#34; 17 } 18 }\u0026#39; \\ 19--configuration-overrides \u0026#39;{ 20 \u0026#34;applicationConfiguration\u0026#34;: [ 21 { 22 \u0026#34;classification\u0026#34;: \u0026#34;spark-defaults\u0026#34;, 23 \u0026#34;properties\u0026#34;: { 24 \u0026#34;spark.dynamicAllocation.enabled\u0026#34;:\u0026#34;true\u0026#34;, 25 \u0026#34;spark.dynamicAllocation.shuffleTracking.enabled\u0026#34;:\u0026#34;true\u0026#34;, 26 \u0026#34;spark.dynamicAllocation.minExecutors\u0026#34;:\u0026#34;1\u0026#34;, 27 \u0026#34;spark.dynamicAllocation.maxExecutors\u0026#34;:\u0026#34;10\u0026#34;, 28 \u0026#34;spark.dynamicAllocation.initialExecutors\u0026#34;:\u0026#34;1\u0026#34;, 29 \u0026#34;spark.dynamicAllocation.schedulerBacklogTimeout\u0026#34;: \u0026#34;1s\u0026#34;, 30 \u0026#34;spark.dynamicAllocation.executorIdleTimeout\u0026#34;: \u0026#34;5s\u0026#34;, 31 \u0026#34;spark.kubernetes.driver.podTemplateFile\u0026#34;:\u0026#34;s3://\u0026#39;${DEFAULT_BUCKET_NAME}\u0026#39;/scripts/config/driver-template.yaml\u0026#34;, 32 \u0026#34;spark.kubernetes.executor.podTemplateFile\u0026#34;:\u0026#34;s3://\u0026#39;${DEFAULT_BUCKET_NAME}\u0026#39;/scripts/config/executor-template.yaml\u0026#34; 33 } 34 } 35 ] 36}\u0026#39; As expected, the executors are added dynamically and removed subsequently as they are not needed.\nSummary In this post, it is discussed how to provision and manage Spark jobs on EMR on EKS with Terraform. Amazon EKS Blueprints for Terraform is used for provisioning EKS, EMR virtual cluster and related resources. Also, Karpenter is used to manage Spark job autoscaling and two Spark jobs with and without Dynamic Resource Allocation (DRA) are used for comparison. It is found that Karpenter manages transient nodes for Spark jobs to meet their scaling requirements effectively.\n","date":"August 26, 2022","img":"/blog/2022-08-26-emr-on-eks-with-terraform/featured.png","lang":"en","langName":"English","largeImg":"/blog/2022-08-26-emr-on-eks-with-terraform/featured_hu0db6c869a471dd7000f2d35dcf0e8ab0_67936_500x0_resize_box_3.png","permalink":"/blog/2022-08-26-emr-on-eks-with-terraform/","series":[],"smallImg":"/blog/2022-08-26-emr-on-eks-with-terraform/featured_hu0db6c869a471dd7000f2d35dcf0e8ab0_67936_180x0_resize_box_3.png","tags":[{"title":"AWS","url":"/tags/aws/"},{"title":"Amazon EKS","url":"/tags/amazon-eks/"},{"title":"Amazon EMR","url":"/tags/amazon-emr/"},{"title":"Apache Spark","url":"/tags/apache-spark/"},{"title":"Kubernetes","url":"/tags/kubernetes/"},{"title":"Terraform","url":"/tags/terraform/"}],"timestamp":1661472000,"title":"Manage EMR on EKS With Terraform"},{"categories":[{"title":"Data Engineering","url":"/categories/data-engineering/"}],"content":"Apache Airflow is a popular workflow management platform. A wide range of AWS services are integrated with the platform by Amazon AWS Operators. AWS Lambda is one of the integrated services, and it can be used to develop workflows efficiently. The current Lambda Operator, however, just invokes a Lambda function, and it can fail to report the invocation result of a function correctly and to record the exact error message from failure. In this post, we’ll discuss a custom Lambda operator that handles those limitations.\nArchitecture We’ll discuss a custom Lambda operator, and it extends the Lambda operator provided by AWS. When a DAG creates a task that invokes a Lambda function, it updates the Lambda payload with a correlation ID that uniquely identifies the task. The correlation ID is added to every log message that the Lambda function generates. Finally, the custom operator filters the associating CloudWatch log events, prints the log messages and raises a runtime error when an error message is found. In this setup, we are able to correctly identify the function invocation result and to point to the exact error message if it fails. The source of this post can be found in a GitHub repository.\nLambda Setup Lambda Function The Logger utility of the Lambda Powertools Python package is used to record log messages. The correlation ID is added to the event payload, and it is set to be injected with log messages by the logger.inject_lambda_context decorator. Note the Lambda Context would be a better place to add a correlation ID as we can add a custom client context object. However, it is not recognised when an invocation is made asynchronously, and we have to add it to the event payload. We use another decorator (middleware_before_after) and it logs messages before and after the function invocation. The latter message that indicates the end of a function is important as we can rely on it in order to identify whether a function is completed without an error. If a function finishes with an error, the last log message won’t be recorded. Also, we can check if a function fails by checking a log message where its level is ERROR, and it is created by the logger.exception method. The Lambda event payload has two extra attributes - n for setting-up the number of iteration and to_fail for determining whether to raise an error.\n1# lambda/src/lambda_function.py 2import time 3from aws_lambda_powertools import Logger 4from aws_lambda_powertools.utilities.typing import LambdaContext 5from aws_lambda_powertools.middleware_factory import lambda_handler_decorator 6 7logger = Logger(log_record_order=[\u0026#34;correlation_id\u0026#34;, \u0026#34;level\u0026#34;, \u0026#34;message\u0026#34;, \u0026#34;location\u0026#34;]) 8 9 10@lambda_handler_decorator 11def middleware_before_after(handler, event, context): 12 logger.info(\u0026#34;Function started\u0026#34;) 13 response = handler(event, context) 14 logger.info(\u0026#34;Function ended\u0026#34;) 15 return response 16 17 18@logger.inject_lambda_context(correlation_id_path=\u0026#34;correlation_id\u0026#34;) 19@middleware_before_after 20def lambda_handler(event: dict, context: LambdaContext): 21 num_iter = event.get(\u0026#34;n\u0026#34;, 10) 22 to_fail = event.get(\u0026#34;to_fail\u0026#34;, False) 23 logger.info(f\u0026#34;num_iter - {num_iter}, fail - {to_fail}\u0026#34;) 24 try: 25 for n in range(num_iter): 26 logger.info(f\u0026#34;iter - {n + 1}...\u0026#34;) 27 time.sleep(1) 28 if to_fail: 29 raise Exception 30 except Exception as e: 31 logger.exception(\u0026#34;Function invocation failed...\u0026#34;) 32 raise RuntimeError(\u0026#34;Unable to finish loop\u0026#34;) from e SAM Template The Serverless Application Model (SAM) framework is used to deploy the Lambda function. The Lambda Powertools Python package is added as a Lambda layer. The Lambda log group is configured so that messages are kept only for 1 day, and it can help reduce time to filter log events. By default, a Lambda function is invoked twice more on error when it is invoked asynchronously - the default retry attempts equals to 2. It is set to 0 as retry behaviour can be controlled by Airflow if necessary, and it can make it easier to track function invocation status.\n1# lambda/template.yml 2AWSTemplateFormatVersion: \u0026#34;2010-09-09\u0026#34; 3Transform: AWS::Serverless-2016-10-31 4Description: Lambda functions used to demonstrate Lambda invoke operator with S3 log extension 5 6Globals: 7 Function: 8 MemorySize: 128 9 Timeout: 30 10 Runtime: python3.8 11 Tracing: Active 12 Environment: 13 Variables: 14 POWERTOOLS_SERVICE_NAME: airflow 15 LOG_LEVEL: INFO 16 Tags: 17 Application: LambdaInvokeOperatorDemo 18Resources: 19 ExampleFunction: 20 Type: AWS::Serverless::Function 21 Properties: 22 FunctionName: example-lambda-function 23 Description: Example lambda function 24 CodeUri: src/ 25 Handler: lambda_function.lambda_handler 26 Layers: 27 - !Sub arn:aws:lambda:${AWS::Region}:017000801446:layer:AWSLambdaPowertoolsPython:26 28 ExampleFunctionAsyncConfig: 29 Type: AWS::Lambda::EventInvokeConfig 30 Properties: 31 FunctionName: !Ref ExampleFunction 32 MaximumRetryAttempts: 0 33 Qualifier: \u0026#34;$LATEST\u0026#34; 34 LogGroup: 35 Type: AWS::Logs::LogGroup 36 Properties: 37 LogGroupName: !Sub \u0026#34;/aws/lambda/${ExampleFunction}\u0026#34; 38 RetentionInDays: 1 39 40Outputs: 41 ExampleFunction: 42 Value: !Ref ExampleFunction 43 Description: Example lambda function ARN Lambda Operator Lambda Invoke Function Operator Below shows the source of the Lambda invoke function operator. After invoking a Lambda function, the _execute _method checks if the response status code indicates success and whether _FunctionError _is found in the response payload. When an invocation is made synchronously (RequestResponse invocation type), it can identify whether the invocation is successful or not because the response is returned after it finishes. However it reports a generic error message when it fails and we have to visit CloudWatch Logs if we want to check the exact error. It gets worse when it is invoked asynchronously (Event invocation type) because the response is made before the invocation finishes. In this case it is not even possible to check whether the invocation is successful.\n1... 2 3class AwsLambdaInvokeFunctionOperator(BaseOperator): 4 def __init__( 5 self, 6 *, 7 function_name: str, 8 log_type: Optional[str] = None, 9 qualifier: Optional[str] = None, 10 invocation_type: Optional[str] = None, 11 client_context: Optional[str] = None, 12 payload: Optional[str] = None, 13 aws_conn_id: str = \u0026#39;aws_default\u0026#39;, 14 **kwargs, 15 ): 16 super().__init__(**kwargs) 17 self.function_name = function_name 18 self.payload = payload 19 self.log_type = log_type 20 self.qualifier = qualifier 21 self.invocation_type = invocation_type 22 self.client_context = client_context 23 self.aws_conn_id = aws_conn_id 24 25 def execute(self, context: \u0026#39;Context\u0026#39;): 26 hook = LambdaHook(aws_conn_id=self.aws_conn_id) 27 success_status_codes = [200, 202, 204] 28 self.log.info(\u0026#34;Invoking AWS Lambda function: %s with payload: %s\u0026#34;, self.function_name, self.payload) 29 response = hook.invoke_lambda( 30 function_name=self.function_name, 31 invocation_type=self.invocation_type, 32 log_type=self.log_type, 33 client_context=self.client_context, 34 payload=self.payload, 35 qualifier=self.qualifier, 36 ) 37 self.log.info(\u0026#34;Lambda response metadata: %r\u0026#34;, response.get(\u0026#34;ResponseMetadata\u0026#34;)) 38 if response.get(\u0026#34;StatusCode\u0026#34;) not in success_status_codes: 39 raise ValueError(\u0026#39;Lambda function did not execute\u0026#39;, json.dumps(response.get(\u0026#34;ResponseMetadata\u0026#34;))) 40 payload_stream = response.get(\u0026#34;Payload\u0026#34;) 41 payload = payload_stream.read().decode() 42 if \u0026#34;FunctionError\u0026#34; in response: 43 raise ValueError( 44 \u0026#39;Lambda function execution resulted in error\u0026#39;, 45 {\u0026#34;ResponseMetadata\u0026#34;: response.get(\u0026#34;ResponseMetadata\u0026#34;), \u0026#34;Payload\u0026#34;: payload}, 46 ) 47 self.log.info(\u0026#39;Lambda function invocation succeeded: %r\u0026#39;, response.get(\u0026#34;ResponseMetadata\u0026#34;)) 48 return payload Custom Lambda Operator The custom Lambda operator extends the Lambda invoke function operator. It updates the Lambda payload by adding a correlation ID. The _execute _method is extended by the _log_processor _decorator function. As the name suggests, the decorator function filters all log messages that include the correlation ID and print them. This process loops over the lifetime of the invocation. While processing log events, it raises an error if an error message is found. And log event processing gets stopped when a message that indicates the end of the invocation is encountered. Finally, in order to handle the case where an invocation doesn’t finish within the timeout seconds, it raises an error at the end of the loop.\nThe main benefits of this approach are\nwe don’t have to rewrite Lambda invocation logic as we extend the Lambda invoke function operator, we can track a lambda invocation status regardless of its invocation type, and we are able to record all relevant log messages of an invocation 1# airflow/dags/lambda_operator.py 2... 3 4class CustomLambdaFunctionOperator(AwsLambdaInvokeFunctionOperator): 5 def __init__( 6 self, 7 *, 8 function_name: str, 9 log_type: Optional[str] = None, 10 qualifier: Optional[str] = None, 11 invocation_type: Optional[str] = None, 12 client_context: Optional[str] = None, 13 payload: Optional[str] = None, 14 aws_conn_id: str = \u0026#34;aws_default\u0026#34;, 15 correlation_id: str = str(uuid4()), 16 **kwargs, 17 ): 18 super().__init__( 19 function_name=function_name, 20 log_type=log_type, 21 qualifier=qualifier, 22 invocation_type=invocation_type, 23 client_context=client_context, 24 payload=json.dumps( 25 {**json.loads((payload or \u0026#34;{}\u0026#34;)), **{\u0026#34;correlation_id\u0026#34;: correlation_id}} 26 ), 27 aws_conn_id=aws_conn_id, 28 **kwargs, 29 ) 30 self.correlation_id = correlation_id 31 32 def log_processor(func): 33 @functools.wraps(func) 34 def wrapper_decorator(self, *args, **kwargs): 35 payload = func(self, *args, **kwargs) 36 function_timeout = self.get_function_timeout() 37 self.process_log_events(function_timeout) 38 return payload 39 40 return wrapper_decorator 41 42 @log_processor 43 def execute(self, context: \u0026#34;Context\u0026#34;): 44 return super().execute(context) 45 46 def get_function_timeout(self): 47 resp = boto3.client(\u0026#34;lambda\u0026#34;).get_function_configuration(FunctionName=self.function_name) 48 return resp[\u0026#34;Timeout\u0026#34;] 49 50 def process_log_events(self, function_timeout: int): 51 start_time = 0 52 for _ in range(function_timeout): 53 response_iterator = self.get_response_iterator( 54 self.function_name, self.correlation_id, start_time 55 ) 56 for page in response_iterator: 57 for event in page[\u0026#34;events\u0026#34;]: 58 start_time = event[\u0026#34;timestamp\u0026#34;] 59 message = json.loads(event[\u0026#34;message\u0026#34;]) 60 print(message) 61 if message[\u0026#34;level\u0026#34;] == \u0026#34;ERROR\u0026#34;: 62 raise RuntimeError(\u0026#34;ERROR found in log\u0026#34;) 63 if message[\u0026#34;message\u0026#34;] == \u0026#34;Function ended\u0026#34;: 64 return 65 time.sleep(1) 66 raise RuntimeError(\u0026#34;Lambda function end message not found after function timeout\u0026#34;) 67 68 @staticmethod 69 def get_response_iterator(function_name: str, correlation_id: str, start_time: int): 70 paginator = boto3.client(\u0026#34;logs\u0026#34;).get_paginator(\u0026#34;filter_log_events\u0026#34;) 71 return paginator.paginate( 72 logGroupName=f\u0026#34;/aws/lambda/{function_name}\u0026#34;, 73 filterPattern=f\u0026#39;\u0026#34;{correlation_id}\u0026#34;\u0026#39;, 74 startTime=start_time + 1, 75 ) Unit Testing Unit testing is performed for the main log processing function (process_log_events). Log events fixture is created by a closure function. Depending on the case argument, it returns a log events list that covers success, error or timeout error. It is used as the mock response of the _get_response_iterator _method. The 3 testing cases cover each of the possible scenarios.\n1# airflow/tests/test_lambda_operator.py 2import json 3import pytest 4from unittest.mock import MagicMock 5from dags.lambda_operator import CustomLambdaFunctionOperator 6 7 8@pytest.fixture 9def log_events(): 10 def _(case): 11 events = [ 12 { 13 \u0026#34;timestamp\u0026#34;: 1659296879605, 14 \u0026#34;message\u0026#34;: \u0026#39;{\u0026#34;correlation_id\u0026#34;:\u0026#34;2850fda4-9005-4375-aca8-88dfdda222ba\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;Function started\u0026#34;,\u0026#34;location\u0026#34;:\u0026#34;middleware_before_after:12\u0026#34;,\u0026#34;timestamp\u0026#34;:\u0026#34;2022-07-31 19:47:59,605+0000\u0026#34;,\u0026#34;service\u0026#34;:\u0026#34;airflow\u0026#34;, ...}\\n\u0026#39;, 15 }, 16 { 17 \u0026#34;timestamp\u0026#34;: 1659296879605, 18 \u0026#34;message\u0026#34;: \u0026#39;{\u0026#34;correlation_id\u0026#34;:\u0026#34;2850fda4-9005-4375-aca8-88dfdda222ba\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;num_iter - 10, fail - False\u0026#34;,\u0026#34;location\u0026#34;:\u0026#34;lambda_handler:23\u0026#34;,\u0026#34;timestamp\u0026#34;:\u0026#34;2022-07-31 19:47:59,605+0000\u0026#34;,\u0026#34;service\u0026#34;:\u0026#34;airflow\u0026#34;, ...}\\n\u0026#39;, 19 }, 20 { 21 \u0026#34;timestamp\u0026#34;: 1659296879605, 22 \u0026#34;message\u0026#34;: \u0026#39;{\u0026#34;correlation_id\u0026#34;:\u0026#34;2850fda4-9005-4375-aca8-88dfdda222ba\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;iter - 1...\u0026#34;,\u0026#34;location\u0026#34;:\u0026#34;lambda_handler:26\u0026#34;,\u0026#34;timestamp\u0026#34;:\u0026#34;2022-07-31 19:47:59,605+0000\u0026#34;,\u0026#34;service\u0026#34;:\u0026#34;airflow\u0026#34;, ...}\\n\u0026#39;, 23 }, 24 ] 25 if case == \u0026#34;success\u0026#34;: 26 events.append( 27 { 28 \u0026#34;timestamp\u0026#34;: 1659296889620, 29 \u0026#34;message\u0026#34;: \u0026#39;{\u0026#34;correlation_id\u0026#34;:\u0026#34;2850fda4-9005-4375-aca8-88dfdda222ba\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;Function ended\u0026#34;,\u0026#34;location\u0026#34;:\u0026#34;middleware_before_after:14\u0026#34;,\u0026#34;timestamp\u0026#34;:\u0026#34;2022-07-31 19:48:09,619+0000\u0026#34;,\u0026#34;service\u0026#34;:\u0026#34;airflow\u0026#34;, ...}\\n\u0026#39;, 30 } 31 ) 32 elif case == \u0026#34;error\u0026#34;: 33 events.append( 34 { 35 \u0026#34;timestamp\u0026#34;: 1659296889629, 36 \u0026#34;message\u0026#34;: \u0026#39;{\u0026#34;correlation_id\u0026#34;:\u0026#34;2850fda4-9005-4375-aca8-88dfdda222ba\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;ERROR\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;Function invocation failed...\u0026#34;,\u0026#34;location\u0026#34;:\u0026#34;lambda_handler:31\u0026#34;,\u0026#34;timestamp\u0026#34;:\u0026#34;2022-07-31 19:48:09,628+0000\u0026#34;,\u0026#34;service\u0026#34;:\u0026#34;airflow\u0026#34;, ..., \u0026#34;exception\u0026#34;:\u0026#34;Traceback (most recent call last):\\\\n File \\\\\u0026#34;/var/task/lambda_function.py\\\\\u0026#34;, line 29, in lambda_handler\\\\n raise Exception\\\\nException\u0026#34;,\u0026#34;exception_name\u0026#34;:\u0026#34;Exception\u0026#34;,\u0026#34;xray_trace_id\u0026#34;:\u0026#34;1-62e6dc6f-30b8e51d000de0ee5a22086b\u0026#34;}\\n\u0026#39;, 37 }, 38 ) 39 return [{\u0026#34;events\u0026#34;: events}] 40 41 return _ 42 43 44def test_process_log_events_success(log_events): 45 success_resp = log_events(\u0026#34;success\u0026#34;) 46 operator = CustomLambdaFunctionOperator( 47 task_id=\u0026#34;sync_w_error\u0026#34;, 48 function_name=\u0026#34;\u0026#34;, 49 invocation_type=\u0026#34;RequestResponse\u0026#34;, 50 payload=json.dumps({\u0026#34;n\u0026#34;: 1, \u0026#34;to_fail\u0026#34;: True}), 51 aws_conn_id=None, 52 ) 53 operator.get_response_iterator = MagicMock(return_value=success_resp) 54 assert operator.process_log_events(1) == None 55 56 57def test_process_log_events_fail_with_error(log_events): 58 fail_resp = log_events(\u0026#34;error\u0026#34;) 59 operator = CustomLambdaFunctionOperator( 60 task_id=\u0026#34;sync_w_error\u0026#34;, 61 function_name=\u0026#34;\u0026#34;, 62 invocation_type=\u0026#34;RequestResponse\u0026#34;, 63 payload=json.dumps({\u0026#34;n\u0026#34;: 1, \u0026#34;to_fail\u0026#34;: True}), 64 aws_conn_id=None, 65 ) 66 operator.get_response_iterator = MagicMock(return_value=fail_resp) 67 with pytest.raises(RuntimeError) as e: 68 operator.process_log_events(1) 69 assert \u0026#34;ERROR found in log\u0026#34; == str(e.value) 70 71 72def test_process_log_events_fail_by_timeout(log_events): 73 fail_resp = log_events(None) 74 operator = CustomLambdaFunctionOperator( 75 task_id=\u0026#34;sync_w_error\u0026#34;, 76 function_name=\u0026#34;\u0026#34;, 77 invocation_type=\u0026#34;RequestResponse\u0026#34;, 78 payload=json.dumps({\u0026#34;n\u0026#34;: 1, \u0026#34;to_fail\u0026#34;: True}), 79 aws_conn_id=None, 80 ) 81 operator.get_response_iterator = MagicMock(return_value=fail_resp) 82 with pytest.raises(RuntimeError) as e: 83 operator.process_log_events(1) 84 assert \u0026#34;Lambda function end message not found after function timeout\u0026#34; == str(e.value) Below shows the results of the testing.\n1$ pytest airflow/tests/test_lambda_operator.py -v 2============================================ test session starts ============================================= 3platform linux -- Python 3.8.10, pytest-7.1.2, pluggy-1.0.0 -- /home/jaehyeon/personal/revisit-lambda-operator/venv/bin/python3 4cachedir: .pytest_cache 5rootdir: /home/jaehyeon/personal/revisit-lambda-operator 6plugins: anyio-3.6.1 7collected 3 items 8 9airflow/tests/test_lambda_operator.py::test_process_log_events_success PASSED [ 33%] 10airflow/tests/test_lambda_operator.py::test_process_log_events_fail_with_error PASSED [ 66%] 11airflow/tests/test_lambda_operator.py::test_process_log_events_fail_by_timeout PASSED [100%] 12 13============================================= 3 passed in 1.34s ============================================== Compare Operators Docker Compose In order to compare the two operators, the Airflow Docker quick start guide is simplified into using the Local Executor. In this setup, both scheduling and task execution are handled by the airflow scheduler service. Instead of creating an AWS connection for invoking Lambda functions, the host AWS configuration is shared by volume-mapping (${HOME}/.aws to /home/airflow/.aws). Also, as I don’t use the default AWS profile but a profile named cevo, it is added to the scheduler service as an environment variable (AWS_PROFILE: \u0026ldquo;cevo\u0026rdquo;).\n1# airflow/docker-compose.yaml 2--- 3version: \u0026#34;3\u0026#34; 4x-airflow-common: \u0026amp;airflow-common 5 image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:2.3.3} 6 environment: airflow-common-env 7 AIRFLOW__CORE__EXECUTOR: LocalExecutor 8 AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow 9 # For backward compatibility, with Airflow \u0026lt;2.3 10 AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow 11 AIRFLOW__CORE__FERNET_KEY: \u0026#34;\u0026#34; 12 AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: \u0026#34;true\u0026#34; 13 AIRFLOW__CORE__LOAD_EXAMPLES: \u0026#34;false\u0026#34; 14 AIRFLOW__API__AUTH_BACKENDS: \u0026#34;airflow.api.auth.backend.basic_auth\u0026#34; 15 _PIP_ADDITIONAL_REQUIREMENTS: ${_PIP_ADDITIONAL_REQUIREMENTS:-} 16 volumes: 17 - ./dags:/opt/airflow/dags 18 - ./logs:/opt/airflow/logs 19 - ./plugins:/opt/airflow/plugins 20 - ${HOME}/.aws:/home/airflow/.aws 21 user: \u0026#34;${AIRFLOW_UID:-50000}:0\u0026#34; 22 depends_on: \u0026amp;airflow-common-depends-on 23 postgres: 24 condition: service_healthy 25 26services: 27 postgres: 28 image: postgres:13 29 ports: 30 - 5432:5432 31 environment: 32 POSTGRES_USER: airflow 33 POSTGRES_PASSWORD: airflow 34 POSTGRES_DB: airflow 35 volumes: 36 - postgres-db-volume:/var/lib/postgresql/data 37 healthcheck: 38 test: [\u0026#34;CMD\u0026#34;, \u0026#34;pg_isready\u0026#34;, \u0026#34;-U\u0026#34;, \u0026#34;airflow\u0026#34;] 39 interval: 5s 40 retries: 5 41 42 airflow-webserver: 43 \u0026lt;\u0026lt;: *airflow-common 44 command: webserver 45 ports: 46 - 8080:8080 47 depends_on: 48 \u0026lt;\u0026lt;: *airflow-common-depends-on 49 airflow-init: 50 condition: service_completed_successfully 51 52 airflow-scheduler: 53 \u0026lt;\u0026lt;: *airflow-common 54 command: scheduler 55 environment: 56 \u0026lt;\u0026lt;: *airflow-common-env 57 AWS_PROFILE: \u0026#34;cevo\u0026#34; 58 depends_on: 59 \u0026lt;\u0026lt;: *airflow-common-depends-on 60 airflow-init: 61 condition: service_completed_successfully 62 63 airflow-init: 64 \u0026lt;\u0026lt;: *airflow-common 65 entrypoint: /bin/bash 66 # yamllint disable rule:line-length 67 command: 68 - -c 69 - | 70 if [[ -z \u0026#34;${AIRFLOW_UID}\u0026#34; ]]; then 71 echo 72 echo -e \u0026#34;\\033[1;33mWARNING!!!: AIRFLOW_UID not set!\\e[0m\u0026#34; 73 echo \u0026#34;If you are on Linux, you SHOULD follow the instructions below to set \u0026#34; 74 echo \u0026#34;AIRFLOW_UID environment variable, otherwise files will be owned by root.\u0026#34; 75 echo \u0026#34;For other operating systems you can get rid of the warning with manually created .env file:\u0026#34; 76 echo \u0026#34; See: https://airflow.apache.org/docs/apache-airflow/stable/start/docker.html#setting-the-right-airflow-user\u0026#34; 77 echo 78 fi 79 mkdir -p /sources/logs /sources/dags /sources/plugins 80 chown -R \u0026#34;${AIRFLOW_UID}:0\u0026#34; /sources/{logs,dags,plugins} 81 exec /entrypoint airflow version 82 # yamllint enable rule:line-length 83 environment: 84 \u0026lt;\u0026lt;: *airflow-common-env 85 _AIRFLOW_DB_UPGRADE: \u0026#34;true\u0026#34; 86 _AIRFLOW_WWW_USER_CREATE: \u0026#34;true\u0026#34; 87 _AIRFLOW_WWW_USER_USERNAME: ${_AIRFLOW_WWW_USER_USERNAME:-airflow} 88 _AIRFLOW_WWW_USER_PASSWORD: ${_AIRFLOW_WWW_USER_PASSWORD:-airflow} 89 _PIP_ADDITIONAL_REQUIREMENTS: \u0026#34;\u0026#34; 90 user: \u0026#34;0:0\u0026#34; 91 volumes: 92 - .:/sources 93 94volumes: 95 postgres-db-volume: The quick start guide requires a number of steps to initialise an environment before starting the services and they are added to a single shell script shown below.\n1# airflow/init.sh 2#!/usr/bin/env bash 3 4## initialising environment 5# remove docker-compose services 6docker-compose down --volumes 7# create folders to mount 8rm -rf ./logs 9mkdir -p ./dags ./logs ./plugins ./tests 10# setting the right airflow user 11echo -e \u0026#34;AIRFLOW_UID=$(id -u)\u0026#34; \u0026gt; .env 12# initialise database 13docker-compose up airflow-init After finishing the initialisation steps, the docker compose services can be started by docker-compose up -d.\nLambda Invoke Function Operator Two tasks are created with the Lambda invoke function operator. The first is invoked synchronously (RequestResponse) while the latter is asynchronously (Event). Both are configured to raise an error after 10 seconds.\n1# airflow/dags/example_without_logging.py 2import os 3import json 4from datetime import datetime 5 6from airflow import DAG 7from airflow.providers.amazon.aws.operators.aws_lambda import AwsLambdaInvokeFunctionOperator 8 9LAMBDA_FUNCTION_NAME = os.getenv(\u0026#34;LAMBDA_FUNCTION_NAME\u0026#34;, \u0026#34;example-lambda-function\u0026#34;) 10 11 12def _set_payload(n: int = 10, to_fail: bool = True): 13 return json.dumps({\u0026#34;n\u0026#34;: n, \u0026#34;to_fail\u0026#34;: to_fail}) 14 15 16with DAG( 17 dag_id=\u0026#34;example_without_logging\u0026#34;, 18 schedule_interval=None, 19 start_date=datetime(2022, 1, 1), 20 max_active_runs=2, 21 concurrency=2, 22 tags=[\u0026#34;logging\u0026#34;], 23 catchup=False, 24) as dag: 25 [ 26 AwsLambdaInvokeFunctionOperator( 27 task_id=\u0026#34;sync_w_error\u0026#34;, 28 function_name=LAMBDA_FUNCTION_NAME, 29 invocation_type=\u0026#34;RequestResponse\u0026#34;, 30 payload=_set_payload(), 31 aws_conn_id=None, 32 ), 33 AwsLambdaInvokeFunctionOperator( 34 task_id=\u0026#34;async_w_error\u0026#34;, 35 function_name=LAMBDA_FUNCTION_NAME, 36 invocation_type=\u0026#34;Event\u0026#34;, 37 payload=_set_payload(), 38 aws_conn_id=None, 39 ), 40 ] As shown below the task by asynchronous invocation is incorrectly marked as success. It is because practically only the response status code is checked as it doesn\u0026rsquo;t wait until the invocation finishes. On the other hand, the task by synchronous invocation is indicated as failed. However, it doesn’t show the exact error that fails the invocation - see below for further details.\nThe error message is Lambda function execution resulted in error, and it is the generic message constructed by the Lambda invoke function operator.\nCustom Lambda Operator Five tasks are created with the custom Lambda operator The first four tasks cover success and failure by synchronous and asynchronous invocations. The last task is to check failure due to timeout.\n1# airflow/dags/example_with_logging.py 2import os 3import json 4from datetime import datetime 5 6from airflow import DAG 7from lambda_operator import CustomLambdaFunctionOperator 8 9LAMBDA_FUNCTION_NAME = os.getenv(\u0026#34;LAMBDA_FUNCTION_NAME\u0026#34;, \u0026#34;example-lambda-function\u0026#34;) 10 11 12def _set_payload(n: int = 10, to_fail: bool = True): 13 return json.dumps({\u0026#34;n\u0026#34;: n, \u0026#34;to_fail\u0026#34;: to_fail}) 14 15 16with DAG( 17 dag_id=\u0026#34;example_with_logging\u0026#34;, 18 schedule_interval=None, 19 start_date=datetime(2022, 1, 1), 20 max_active_runs=2, 21 concurrency=5, 22 tags=[\u0026#34;logging\u0026#34;], 23 catchup=False, 24) as dag: 25 [ 26 CustomLambdaFunctionOperator( 27 task_id=\u0026#34;sync_w_error\u0026#34;, 28 function_name=LAMBDA_FUNCTION_NAME, 29 invocation_type=\u0026#34;RequestResponse\u0026#34;, 30 payload=_set_payload(), 31 aws_conn_id=None, 32 ), 33 CustomLambdaFunctionOperator( 34 task_id=\u0026#34;async_w_error\u0026#34;, 35 function_name=LAMBDA_FUNCTION_NAME, 36 invocation_type=\u0026#34;Event\u0026#34;, 37 payload=_set_payload(), 38 aws_conn_id=None, 39 ), 40 CustomLambdaFunctionOperator( 41 task_id=\u0026#34;sync_wo_error\u0026#34;, 42 function_name=LAMBDA_FUNCTION_NAME, 43 invocation_type=\u0026#34;RequestResponse\u0026#34;, 44 payload=_set_payload(to_fail=False), 45 aws_conn_id=None, 46 ), 47 CustomLambdaFunctionOperator( 48 task_id=\u0026#34;async_wo_error\u0026#34;, 49 function_name=LAMBDA_FUNCTION_NAME, 50 invocation_type=\u0026#34;Event\u0026#34;, 51 payload=_set_payload(to_fail=False), 52 aws_conn_id=None, 53 ), 54 CustomLambdaFunctionOperator( 55 task_id=\u0026#34;async_timeout_error\u0026#34;, 56 function_name=LAMBDA_FUNCTION_NAME, 57 invocation_type=\u0026#34;Event\u0026#34;, 58 payload=_set_payload(n=40, to_fail=False), 59 aws_conn_id=None, 60 ), 61 ] As expected we see two success tasks and three failure tasks. The custom Lambda operator tracks Lambda function invocation status correctly.\nBelow shows log messages of the success task by asynchronous invocation. Each message includes the same correlation ID and the last message from the Lambda function is Function ended.\nThe failed task by asynchronous invocation also shows all log messages, and it is possible to check what caused the invocation to fail.\nThe case of failure due to timeout doesn’t show an error message from the Lambda invocation. However, we can treat it as failure because we don’t see the message of the function invocation ended within the function timeout.\nStill the failure by synchronous invocation doesn’t show the exact error message, and it is because an error is raised before the process log events function is executed. Because of this, I advise to invoke a Lambda function asynchronously.\nSummary In this post, we discussed limitations of the Lambda invoke function operator and created a custom Lambda operator. The custom operator reports the invocation result of a function correctly and records the exact error message from failure. A number of tasks are created to compare the results between the two operators, and it is shown that the custom operator handles those limitations successfully.\n","date":"August 6, 2022","img":"/blog/2022-08-06-revisit-lambda-operator/featured.png","lang":"en","langName":"English","largeImg":"/blog/2022-08-06-revisit-lambda-operator/featured_hudaeb25d8660dc3c0c83176a3034f7899_24814_500x0_resize_box_3.png","permalink":"/blog/2022-08-06-revisit-lambda-operator/","series":[],"smallImg":"/blog/2022-08-06-revisit-lambda-operator/featured_hudaeb25d8660dc3c0c83176a3034f7899_24814_180x0_resize_box_3.png","tags":[{"title":"AWS","url":"/tags/aws/"},{"title":"AWS Lambda","url":"/tags/aws-lambda/"},{"title":"Apache Airflow","url":"/tags/apache-airflow/"},{"title":"Python","url":"/tags/python/"},{"title":"Docker","url":"/tags/docker/"},{"title":"Docker Compose","url":"/tags/docker-compose/"}],"timestamp":1659744000,"title":"Revisit AWS Lambda Invoke Function Operator of Apache Airflow"},{"categories":[{"title":"Development","url":"/categories/development/"}],"content":"AWS Lambda provides serverless computing capabilities, and it can be used for performing validation or light processing/transformation of data. Moreover, with its integration with more than 140 AWS services, it facilitates building complex systems employing event-driven architectures. There are many ways to build serverless applications and one of the most efficient ways is using specialised frameworks such as the AWS Serverless Application Model (SAM) and Serverless Framework. In this post, I’ll demonstrate how to build a serverless data processing application using SAM.\nArchitecture When we create an application or pipeline with AWS Lambda, most likely we’ll include its event triggers and destinations. The AWS Serverless Application Model (SAM) facilitates building serverless applications by providing shorthand syntax with a number of custom resource types. Also, the AWS SAM CLI supports an execution environment that helps build, test, debug and deploy applications easily. Furthermore, the CLI can be integrated with full-pledged IaC tools such as the AWS Cloud Development Kit (CDK) and Terraform - note integration with the latter is in its roadmap. With the integration, serverless application development can be a lot easier with capabilities of local testing and building. An alternative tool is the Serverless Framework. It supports multiple cloud providers and broader event sources out-of-box but its integration with IaC tools is practically non-existent.\nIn this post, we’ll build a simple data pipeline using SAM where a Lambda function is triggered when an object (CSV file) is created in a S3 bucket. The Lambda function converts the object into parquet and AVRO files and saves to a destination S3 bucket. For simplicity, we’ll use a single bucket for the source and destination.\nSAM Application After installing the SAM CLI, I initialised an app with the Python 3.8 Lambda runtime from the hello world template (sam init --runtime python3.8). Then it is modified for the data pipeline app. The application is defined in the template.yaml and the source of the main Lambda function is placed in the _transform _folder. We need 3rd party packages for converting source files into the parquet and AVRO formats - AWS Data Wrangler and fastavro. Instead of packaging them together with the Lambda function, they are made available as Lambda layers. While using the AWS managed Lambda layer for the former, we only need to build the Lambda layer for the _fastavro _package, and it is located in the _fastavro _folder. The source of the app can be found in the GitHub repository of this post.\n1fastavro 2└── requirements.txt 3transform 4├── __init__.py 5├── app.py 6└── requirements.txt 7tests 8├── __init__.py 9└── unit 10 ├── __init__.py 11 └── test_handler.py 12template.yaml 13requirements-dev.txt 14test.csv In the resources section of the template, the Lambda layer for AVRO transformation (FastAvro), the main Lambda function (TransformFunction) and the source (and destination) S3 bucket (SourceBucket) are added. The layer can be built simply by adding the pip package name to the requirements.txt file. It is set to be compatible with Python 3.7 to 3.9. For the Lambda function, its source is configured to be built from the _transform _folder and the ARNs of the custom and AWS managed Lambda layers are added to the layers property. Also, an S3 bucket event is configured so that this Lambda function is triggered whenever a new object is created to the bucket. Finally, as it needs to have permission to read and write objects to the S3 bucket, its invocation policies are added from ready-made policy templates - _S3ReadPolicy _and S3WritePolicy.\n1# template.yaml 2AWSTemplateFormatVersion: \u0026#34;2010-09-09\u0026#34; 3Transform: AWS::Serverless-2016-10-31 4Description: \u0026gt; 5 sam-for-data-professionals 6 7 Sample SAM Template for sam-for-data-professionals 8 9Globals: 10 Function: 11 MemorySize: 256 12 Timeout: 20 13 14Resources: 15 FastAvro: 16 Type: AWS::Serverless::LayerVersion 17 Properties: 18 LayerName: fastavro-layer-py3 19 ContentUri: fastavro/ 20 CompatibleRuntimes: 21 - python3.7 22 - python3.8 23 - python3.9 24 Metadata: 25 BuildMethod: python3.8 26 TransformFunction: 27 Type: AWS::Serverless::Function 28 Properties: 29 CodeUri: transform/ 30 Handler: app.lambda_handler 31 Runtime: python3.8 32 Layers: 33 - !Ref FastAvro 34 - arn:aws:lambda:ap-southeast-2:336392948345:layer:AWSDataWrangler-Python38:8 35 Policies: 36 - S3ReadPolicy: 37 BucketName: sam-for-data-professionals-cevo 38 - S3WritePolicy: 39 BucketName: sam-for-data-professionals-cevo 40 Events: 41 BucketEvent: 42 Type: S3 43 Properties: 44 Bucket: !Ref SourceBucket 45 Events: 46 - \u0026#34;s3:ObjectCreated:*\u0026#34; 47 SourceBucket: 48 Type: AWS::S3::Bucket 49 Properties: 50 BucketName: sam-for-data-professionals-cevo 51 52Outputs: 53 FastAvro: 54 Description: \u0026#34;ARN of fastavro-layer-py3\u0026#34; 55 Value: !Ref FastAvro 56 TransformFunction: 57 Description: \u0026#34;Transform Lambda Function ARN\u0026#34; 58 Value: !GetAtt TransformFunction.Arn Lambda Function The transform function reads an input file from the S3 bucket and saves the records as the parquet and AVRO formats. Thanks to the Lambda layers, we can access the necessary 3rd party packages as well as reduce the size of uploaded deployment packages and make it faster to deploy it.\n1# transform/app.py 2import re 3import io 4from fastavro import writer, parse_schema 5import awswrangler as wr 6import pandas as pd 7import boto3 8 9s3 = boto3.client(\u0026#34;s3\u0026#34;) 10 11avro_schema = { 12 \u0026#34;doc\u0026#34;: \u0026#34;User details\u0026#34;, 13 \u0026#34;name\u0026#34;: \u0026#34;User\u0026#34;, 14 \u0026#34;namespace\u0026#34;: \u0026#34;user\u0026#34;, 15 \u0026#34;type\u0026#34;: \u0026#34;record\u0026#34;, 16 \u0026#34;fields\u0026#34;: [{\u0026#34;name\u0026#34;: \u0026#34;name\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;}, {\u0026#34;name\u0026#34;: \u0026#34;age\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;int\u0026#34;}], 17} 18 19 20def check_fields(df: pd.DataFrame, schema: dict): 21 if schema.get(\u0026#34;fields\u0026#34;) is None: 22 raise Exception(\u0026#34;missing fields in schema keys\u0026#34;) 23 if len(set(df.columns) - set([f[\u0026#34;name\u0026#34;] for f in schema[\u0026#34;fields\u0026#34;]])) \u0026gt; 0: 24 raise Exception(\u0026#34;missing columns in schema key of fields\u0026#34;) 25 26 27def check_data_types(df: pd.DataFrame, schema: dict): 28 dtypes = df.dtypes.to_dict() 29 for field in schema[\u0026#34;fields\u0026#34;]: 30 match_type = \u0026#34;object\u0026#34; if field[\u0026#34;type\u0026#34;] == \u0026#34;string\u0026#34; else field[\u0026#34;type\u0026#34;] 31 if re.search(match_type, str(dtypes[field[\u0026#34;name\u0026#34;]])) is None: 32 raise Exception(f\u0026#34;incorrect column type - {field[\u0026#39;name\u0026#39;]}\u0026#34;) 33 34 35def generate_avro_file(df: pd.DataFrame, schema: dict): 36 check_fields(df, schema) 37 check_data_types(df, schema) 38 buffer = io.BytesIO() 39 writer(buffer, parse_schema(schema), df.to_dict(\u0026#34;records\u0026#34;)) 40 buffer.seek(0) 41 return buffer 42 43 44def lambda_handler(event, context): 45 # get bucket and key values 46 record = next(iter(event[\u0026#34;Records\u0026#34;])) 47 bucket = record[\u0026#34;s3\u0026#34;][\u0026#34;bucket\u0026#34;][\u0026#34;name\u0026#34;] 48 key = record[\u0026#34;s3\u0026#34;][\u0026#34;object\u0026#34;][\u0026#34;key\u0026#34;] 49 file_name = re.sub(\u0026#34;.csv$\u0026#34;, \u0026#34;\u0026#34;, key.split(\u0026#34;/\u0026#34;)[-1]) 50 # read input csv as a data frame 51 input_path = f\u0026#34;s3://{bucket}/{key}\u0026#34; 52 input_df = wr.s3.read_csv([input_path]) 53 # write to s3 as a parquet file 54 wr.s3.to_parquet(df=input_df, path=f\u0026#34;s3://{bucket}/output/{file_name}.parquet\u0026#34;) 55 # write to s3 as an avro file 56 s3.upload_fileobj(generate_avro_file(input_df, avro_schema), bucket, f\u0026#34;output/{file_name}.avro\u0026#34;) Unit Testing We use a custom function to create AVRO files (generate_avro_file) while relying on the AWS Data Wrangler package for reading input files and writing to parquet files. Therefore, unit testing is performed for the custom function only. Mainly it tests whether the AVRO schema matches the input data fields and data types.\n1# tests/unit/test_handler.py 2import pytest 3import pandas as pd 4from transform import app 5 6 7@pytest.fixture 8def input_df(): 9 return pd.DataFrame.from_dict({\u0026#34;name\u0026#34;: [\u0026#34;Vrinda\u0026#34;, \u0026#34;Tracy\u0026#34;], \u0026#34;age\u0026#34;: [22, 28]}) 10 11 12def test_generate_avro_file_success(input_df): 13 avro_schema = { 14 \u0026#34;doc\u0026#34;: \u0026#34;User details\u0026#34;, 15 \u0026#34;name\u0026#34;: \u0026#34;User\u0026#34;, 16 \u0026#34;namespace\u0026#34;: \u0026#34;user\u0026#34;, 17 \u0026#34;type\u0026#34;: \u0026#34;record\u0026#34;, 18 \u0026#34;fields\u0026#34;: [{\u0026#34;name\u0026#34;: \u0026#34;name\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;}, {\u0026#34;name\u0026#34;: \u0026#34;age\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;int\u0026#34;}], 19 } 20 app.generate_avro_file(input_df, avro_schema) 21 assert True 22 23 24def test_generate_avro_file_fail_missing_fields(input_df): 25 avro_schema = { 26 \u0026#34;doc\u0026#34;: \u0026#34;User details\u0026#34;, 27 \u0026#34;name\u0026#34;: \u0026#34;User\u0026#34;, 28 \u0026#34;namespace\u0026#34;: \u0026#34;user\u0026#34;, 29 \u0026#34;type\u0026#34;: \u0026#34;record\u0026#34;, 30 } 31 with pytest.raises(Exception) as e: 32 app.generate_avro_file(input_df, avro_schema) 33 assert \u0026#34;missing fields in schema keys\u0026#34; == str(e.value) 34 35 36def test_generate_avro_file_fail_missing_columns(input_df): 37 avro_schema = { 38 \u0026#34;doc\u0026#34;: \u0026#34;User details\u0026#34;, 39 \u0026#34;name\u0026#34;: \u0026#34;User\u0026#34;, 40 \u0026#34;namespace\u0026#34;: \u0026#34;user\u0026#34;, 41 \u0026#34;type\u0026#34;: \u0026#34;record\u0026#34;, 42 \u0026#34;fields\u0026#34;: [{\u0026#34;name\u0026#34;: \u0026#34;name\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;}], 43 } 44 with pytest.raises(Exception) as e: 45 app.generate_avro_file(input_df, avro_schema) 46 assert \u0026#34;missing columns in schema key of fields\u0026#34; == str(e.value) 47 48 49def test_generate_avro_file_fail_incorrect_age_type(input_df): 50 avro_schema = { 51 \u0026#34;doc\u0026#34;: \u0026#34;User details\u0026#34;, 52 \u0026#34;name\u0026#34;: \u0026#34;User\u0026#34;, 53 \u0026#34;namespace\u0026#34;: \u0026#34;user\u0026#34;, 54 \u0026#34;type\u0026#34;: \u0026#34;record\u0026#34;, 55 \u0026#34;fields\u0026#34;: [{\u0026#34;name\u0026#34;: \u0026#34;name\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;}, {\u0026#34;name\u0026#34;: \u0026#34;age\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;}], 56 } 57 with pytest.raises(Exception) as e: 58 app.generate_avro_file(input_df, avro_schema) 59 assert f\u0026#34;incorrect column type - age\u0026#34; == str(e.value) Build and Deploy The app has to be built before deployment. It can be done by sam build.\nThe deployment can be done with and without a guide. For the latter, we need to specify additional parameters such as the Cloudformation stack name, capabilities (as we create an IAM role for Lambda) and a flag to automatically determine an S3 bucket to store build artifacts.\n1sam deploy \\ 2 --stack-name sam-for-data-professionals \\ 3 --capabilities CAPABILITY_IAM \\ 4 --resolve-s3 Trigger Lambda Function We can simply trigger the Lambda function by uploading a source file to the S3 bucket. Once it is uploaded, we are able to see that the output parquet and AVRO files are saved as expected.\n1$ aws s3 cp test.csv s3://sam-for-data-professionals-cevo/input/ 2upload: ./test.csv to s3://sam-for-data-professionals-cevo/input/test.csv 3 4$ aws s3 ls s3://sam-for-data-professionals-cevo/output/ 52022-07-17 17:33:21 403 test.avro 62022-07-17 17:33:21 2112 test.parquet Summary In this post, it is illustrated how to build a serverless data processing application using SAM. A Lambda function is developed, which is triggered whenever an object is created in a S3 bucket. It converts input CSV files into the parquet and AVRO formats before saving into the destination bucket. For the format conversion, it uses 3rd party packages, and they are made available by Lambda layers. The application is built and deployed and the function triggering is checked.\n","date":"July 18, 2022","img":"/blog/2022-07-18-sam-for-data-professionals/featured.png","lang":"en","langName":"English","largeImg":"/blog/2022-07-18-sam-for-data-professionals/featured_hu896f95b0db9041cf59826714af2f73f7_22838_500x0_resize_box_3.png","permalink":"/blog/2022-07-18-sam-for-data-professionals/","series":[],"smallImg":"/blog/2022-07-18-sam-for-data-professionals/featured_hu896f95b0db9041cf59826714af2f73f7_22838_180x0_resize_box_3.png","tags":[{"title":"AWS","url":"/tags/aws/"},{"title":"AWS Lambda","url":"/tags/aws-lambda/"},{"title":"AWS SAM","url":"/tags/aws-sam/"}],"timestamp":1658102400,"title":"Serverless Application Model (SAM) for Data Professionals"},{"categories":[],"content":"Since v1.0.0-alpha.1, HBS supports much more image processing methods. Such as Crop, Fit and Fill images. You can also apply filters on an image.\nSee also Image Processing.\n","date":"July 8, 2022","img":"/news/2022/07/more-image-processing-methods/featured-sample.webp","lang":"en","langName":"English","largeImg":"/news/2022/07/more-image-processing-methods/featured-sample_huc6bcc14d597e300fd9ab4aae536c68a5_498412_500x0_resize_q75_h2_box_2.webp","permalink":"/news/2022/07/more-image-processing-methods/","series":[],"smallImg":"/news/2022/07/more-image-processing-methods/featured-sample_huc6bcc14d597e300fd9ab4aae536c68a5_498412_180x0_resize_q75_h2_box_2.webp","tags":[],"timestamp":1657251287,"title":"More Image Processing Methods"},{"categories":[{"title":"Data Engineering","url":"/categories/data-engineering/"}],"content":"Unlike traditional Data Lake, new table formats (Iceberg, Hudi and Delta Lake) support features that can be used to apply data warehousing patterns, which can bring a way to be rescued from Data Swamp. In this post, we\u0026rsquo;ll discuss how to implement ETL using retail analytics data. It has two dimension data (user and product) and a single fact data (order). The dimension data sets have different ETL strategies depending on whether to track historical changes. For the fact data, the primary keys of the dimension data are added to facilitate later queries. We\u0026rsquo;ll use Iceberg for data storage/management and Spark for data processing. Instead of provisioning an EMR cluster, a local development environment will be used. Finally, the ETL results will be queried by Athena for verification.\nEMR Local Environment In one of my earlier posts, we discussed how to develop and test Apache Spark apps for EMR locally using Docker (and/or VSCode). Instead of provisioning an EMR cluster, we can quickly build an ETL app using the local environment. For this post, a new local environment is created based on the Docker image of the latest EMR 6.6.0 release. Check the GitHub repository for this post for further details.\nApache Iceberg is supported by EMR 6.5.0 or later, and it requires iceberg-defaults configuration classification that enables Iceberg. The latest EMR Docker release (emr-6.6.0-20220411), however, doesn\u0026rsquo;t support that configuration classification and I didn\u0026rsquo;t find the _iceberg _folder (/usr/share/aws/iceberg) within the Docker container. Therefore, the project\u0026rsquo;s AWS integration example is used instead and the following script (run.sh) is an update of the example script that allows to launch the Pyspark shell or to submit a Spark application.\n1# run.sh 2#!/usr/bin/env bash 3 4# add Iceberg dependency 5ICEBERG_VERSION=0.13.2 6DEPENDENCIES=\u0026#34;org.apache.iceberg:iceberg-spark3-runtime:$ICEBERG_VERSION\u0026#34; 7 8# add AWS dependency 9AWS_SDK_VERSION=2.17.131 10AWS_MAVEN_GROUP=software.amazon.awssdk 11AWS_PACKAGES=( 12 \u0026#34;bundle\u0026#34; 13 \u0026#34;url-connection-client\u0026#34; 14) 15for pkg in \u0026#34;${AWS_PACKAGES[@]}\u0026#34;; do 16 DEPENDENCIES+=\u0026#34;,$AWS_MAVEN_GROUP:$pkg:$AWS_SDK_VERSION\u0026#34; 17done 18 19# execute pyspark or spark-submit 20execution=$1 21app_path=$2 22if [ -z $execution ]; then 23 echo \u0026#34;missing execution type. specify either pyspark or spark-submit\u0026#34; 24 exit 1 25fi 26 27if [ $execution == \u0026#34;pyspark\u0026#34; ]; then 28 pyspark --packages $DEPENDENCIES \\ 29 --conf spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions \\ 30 --conf spark.sql.catalog.demo=org.apache.iceberg.spark.SparkCatalog \\ 31 --conf spark.sql.catalog.demo.warehouse=s3://iceberg-etl-demo \\ 32 --conf spark.sql.catalog.demo.catalog-impl=org.apache.iceberg.aws.glue.GlueCatalog \\ 33 --conf spark.sql.catalog.demo.io-impl=org.apache.iceberg.aws.s3.S3FileIO 34elif [ $execution == \u0026#34;spark-submit\u0026#34; ]; then 35 if [ -z $app_path ]; then 36 echo \u0026#34;pyspark application is mandatory\u0026#34; 37 exit 1 38 else 39 spark-submit --packages $DEPENDENCIES \\ 40 --deploy-mode client \\ 41 --master local[*] \\ 42 --conf spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions \\ 43 --conf spark.sql.catalog.demo=org.apache.iceberg.spark.SparkCatalog \\ 44 --conf spark.sql.catalog.demo.warehouse=s3://iceberg-etl-demo \\ 45 --conf spark.sql.catalog.demo.catalog-impl=org.apache.iceberg.aws.glue.GlueCatalog \\ 46 --conf spark.sql.catalog.demo.io-impl=org.apache.iceberg.aws.s3.S3FileIO \\ 47 $app_path 48 fi 49fi Here is an example of using the script.\n1# launch pyspark shell 2$ ./run.sh pyspark 3 4# execute spark-submit, requires pyspark application (etl.py) as the second argument 5$ ./run.sh spark-submit path-to-app.py ETL Strategy Sample Data We use the retail analytics sample database from YugaByteDB to get the ETL sample data. Records from the following 3 tables are used to run ETL on its own ETL strategy.\nThe main focus of the demo ETL application is to show how to track product price changes over time and to apply those changes to the order data. Normally ETL is performed daily, but it\u0026rsquo;ll be time-consuming to execute daily incremental ETL with the order data because it includes records spanning for 5 calendar years. Moreover, as it is related to the user and product data, splitting the corresponding dimension records will be quite difficult. Instead, I chose to run yearly incremental ETL. I first grouped orders in 4 groups where the first group (year 0) includes orders in 2016 and 2017. And each of the remaining groups (year 1 to 3) keeps records of a whole year from 2018 to 2020. Then I created 4 product groups in order to match the order groups and to execute incremental ETL together with the order data. The first group (year 0) keeps the original data and the product price is set to be increased by 5% in the following years until the last group (year 3). Note, with this setup, it is expected that orders for a given product tend to be mapped to a higher product price over time. On the other hand, the ETL strategy of the user data is not to track historical data so that it is used as it is. The sample data files used for the ETL app are listed below, and they can be found in the data folder of the GitHub repository.\n1$ tree data 2data 3├── orders_year_0.csv 4├── orders_year_1.csv 5├── orders_year_2.csv 6├── orders_year_3.csv 7├── products_year_0.csv 8├── products_year_1.csv 9├── products_year_2.csv 10├── products_year_3.csv 11└── users.csv Users Slowly changing dimension (SCD) type 1 is implemented for the user data. This method basically _upsert_s records by comparing the primary key values and therefore doesn\u0026rsquo;t track historical data. The data has the natural key of _id _and its md5 hash is used as the surrogate key named user_sk - this column is used as the primary key of the table. The table is configured to be partitioned by its surrogate key in 20 buckets. The table creation statement can be found below.\n1CREATE TABLE demo.dwh.users ( 2\tuser_sk string, 3\tid bigint, 4\tname string, 5\temail string, 6\taddress string, 7\tcity string, 8\tstate string, 9\tzip string, 10\tbirth_date date, 11\tsource string, 12\tcreated_at timestamp) 13USING iceberg 14PARTITIONED BY (bucket(20, user_sk)) Products Slowly changing dimension (SCD) type 2 is taken for product data. This method tracks historical data by adding multiple records for a given natural key. Same as the user data, the id column is the natural key. Each record for the same natural key will be given a different surrogate key and the md5 hash of a combination of the id and _created_at _columns is used as the surrogate key named prod_sk. Each record has its own effect period and it is determined by the eff_from and _eff_to _columns and the latest record is marked as 1 for its curr_flag value. The table is also configured to be partitioned by its surrogate key in 20 buckets. The table creation statement is shown below.\n1CREATE TABLE demo.dwh.products ( 2\tprod_sk string, 3\tid bigint, 4\tcategory string, 5\tprice decimal(6,3), 6\ttitle string, 7\tvendor string, 8\tcurr_flag int, 9\teff_from timestamp, 10\teff_to timestamp, 11\tcreated_at timestamp) 12USING iceberg 13PARTITIONED BY (bucket(20, prod_sk)) Orders The orders table has a composite primary key of the surrogate keys of the dimension tables - users_sk and prod_sk. Those columns don\u0026rsquo;t exist in the source data and are added during transformation. The table is configured to be partitioned by the date part of the created_at column. The table creation statement can be found below.\n1CREATE TABLE demo.dwh.orders ( 2\tuser_sk string, 3\tprod_sk string, 4\tid bigint, 5\tdiscount decimal(4,2), 6\tquantity integer, 7\tcreated_at timestamp) 8USING iceberg 9PARTITIONED BY (days(created_at)) ETL Implementation Users In the transformation phase, a source dataframe is created by creating the surrogate key (user_sk), changing data types of relevant columns and selecting columns in the same order as the table is created. Then a view (users_tbl) is created from the source dataframe and it is used to execute MERGE operation by comparing the surrogate key values of the source view with those of the target users table.\n1# src.py 2def etl_users(file_path: str, spark_session: SparkSession): 3 print(\u0026#34;users - transform records...\u0026#34;) 4 src_df = ( 5 spark_session.read.option(\u0026#34;header\u0026#34;, \u0026#34;true\u0026#34;) 6 .csv(file_path) 7 .withColumn(\u0026#34;user_sk\u0026#34;, md5(\u0026#34;id\u0026#34;)) 8 .withColumn(\u0026#34;id\u0026#34;, expr(\u0026#34;CAST(id AS bigint)\u0026#34;)) 9 .withColumn(\u0026#34;created_at\u0026#34;, to_timestamp(\u0026#34;created_at\u0026#34;)) 10 .withColumn(\u0026#34;birth_date\u0026#34;, to_date(\u0026#34;birth_date\u0026#34;)) 11 .select( 12 \u0026#34;user_sk\u0026#34;, 13 \u0026#34;id\u0026#34;, 14 \u0026#34;name\u0026#34;, 15 \u0026#34;email\u0026#34;, 16 \u0026#34;address\u0026#34;, 17 \u0026#34;city\u0026#34;, 18 \u0026#34;state\u0026#34;, 19 \u0026#34;zip\u0026#34;, 20 \u0026#34;birth_date\u0026#34;, 21 \u0026#34;source\u0026#34;, 22 \u0026#34;created_at\u0026#34;, 23 ) 24 ) 25 print(\u0026#34;users - upsert records...\u0026#34;) 26 src_df.createOrReplaceTempView(\u0026#34;users_tbl\u0026#34;) 27 spark_session.sql( 28 \u0026#34;\u0026#34;\u0026#34; 29 MERGE INTO demo.dwh.users t 30 USING (SELECT * FROM users_tbl ORDER BY user_sk) s 31 ON s.user_sk = t.user_sk 32 WHEN MATCHED THEN UPDATE SET * 33 WHEN NOT MATCHED THEN INSERT * 34 \u0026#34;\u0026#34;\u0026#34; 35 ) Products The source dataframe is created by adding the surrogate key while concatenating the id and _created_at _columns, followed by changing data types of relevant columns and selecting columns in the same order as the table is created. The view (products_tbl) that is created from the source dataframe is used to query all the records that have the product ids in the source table - see products_to_update. Note we need data from the products table in order to update eff_from, _eff_to _and _current_flag _column values. Then eff_lead is added to the result set, which is the next record\u0026rsquo;s created_at value for a given product id - see products_updated. The final result set is created by determining the _curr_flag _and _eff_to _column value. Note that the _eff_to _value of the last record for a product is set to ‘9999-12-31 00:00:00\u0026rsquo; in order to make it easy to query the relevant records. The updated records are updated/inserted by executing MERGE operation by comparing the surrogate key values to those of the target products table\n1# src.py 2def etl_products(file_path: str, spark_session: SparkSession): 3 print(\u0026#34;products - transform records...\u0026#34;) 4 src_df = ( 5 spark_session.read.option(\u0026#34;header\u0026#34;, \u0026#34;true\u0026#34;) 6 .csv(file_path) 7 .withColumn(\u0026#34;prod_sk\u0026#34;, md5(concat(\u0026#34;id\u0026#34;, \u0026#34;created_at\u0026#34;))) 8 .withColumn(\u0026#34;id\u0026#34;, expr(\u0026#34;CAST(id AS bigint)\u0026#34;)) 9 .withColumn(\u0026#34;price\u0026#34;, expr(\u0026#34;CAST(price AS decimal(6,3))\u0026#34;)) 10 .withColumn(\u0026#34;created_at\u0026#34;, to_timestamp(\u0026#34;created_at\u0026#34;)) 11 .withColumn(\u0026#34;curr_flag\u0026#34;, expr(\u0026#34;CAST(NULL AS int)\u0026#34;)) 12 .withColumn(\u0026#34;eff_from\u0026#34;, col(\u0026#34;created_at\u0026#34;)) 13 .withColumn(\u0026#34;eff_to\u0026#34;, expr(\u0026#34;CAST(NULL AS timestamp)\u0026#34;)) 14 .select( 15 \u0026#34;prod_sk\u0026#34;, 16 \u0026#34;id\u0026#34;, 17 \u0026#34;category\u0026#34;, 18 \u0026#34;price\u0026#34;, 19 \u0026#34;title\u0026#34;, 20 \u0026#34;vendor\u0026#34;, 21 \u0026#34;curr_flag\u0026#34;, 22 \u0026#34;eff_from\u0026#34;, 23 \u0026#34;eff_to\u0026#34;, 24 \u0026#34;created_at\u0026#34;, 25 ) 26 ) 27 print(\u0026#34;products - upsert records...\u0026#34;) 28 src_df.createOrReplaceTempView(\u0026#34;products_tbl\u0026#34;) 29 products_update_qry = \u0026#34;\u0026#34;\u0026#34; 30 WITH products_to_update AS ( 31 SELECT l.* 32 FROM demo.dwh.products AS l 33 JOIN products_tbl AS r ON l.id = r.id 34 UNION 35 SELECT * 36 FROM products_tbl 37 ), products_updated AS ( 38 SELECT *, 39 LEAD(created_at) OVER (PARTITION BY id ORDER BY created_at) AS eff_lead 40 FROM products_to_update 41 ) 42 SELECT prod_sk, 43 id, 44 category, 45 price, 46 title, 47 vendor, 48 (CASE WHEN eff_lead IS NULL THEN 1 ELSE 0 END) AS curr_flag, 49 eff_from, 50 COALESCE(eff_lead, to_timestamp(\u0026#39;9999-12-31 00:00:00\u0026#39;)) AS eff_to, 51 created_at 52 FROM products_updated 53 ORDER BY prod_sk 54 \u0026#34;\u0026#34;\u0026#34; 55 spark_session.sql( 56 f\u0026#34;\u0026#34;\u0026#34; 57 MERGE INTO demo.dwh.products t 58 USING ({products_update_qry}) s 59 ON s.prod_sk = t.prod_sk 60 WHEN MATCHED THEN UPDATE SET * 61 WHEN NOT MATCHED THEN INSERT * 62 \u0026#34;\u0026#34;\u0026#34; 63 ) Orders After transformation, a view (orders_tbl) is created from the source dataframe. The relevant user (user_sk) and product (prod_sk) surrogate keys are added to source data by joining the users and products dimension tables. The users table is SCD type 1 so matching the _user_id _alone is enough for the join condition. On the other hand, additional join condition based on the _eff_from _and _eff_to _columns is necessary for the products table as it is SCD type 2 and records in that table have their own effective periods. Note that ideally we should be able to apply INNER JOIN but the sample data is not clean and some product records are not matched by that operation. For example, an order whose id is 15 is made at 2018-06-26 02:24:38 with a product whose id is 116. However the earliest record of that product is created at 2018-09-12 15:23:05 and it\u0026rsquo;ll be missed by INNER JOIN. Therefore LEFT JOIN is applied to create the initial result set (orders_updated) and, for those products that are not matched, the surrogate keys of the earliest records are added instead. Finally the updated order records are appended using the DataFrameWriterV2 API.\n1# src.py 2def etl_orders(file_path: str, spark_session: SparkSession): 3 print(\u0026#34;orders - transform records...\u0026#34;) 4 src_df = ( 5 spark_session.read.option(\u0026#34;header\u0026#34;, \u0026#34;true\u0026#34;) 6 .csv(file_path) 7 .withColumn(\u0026#34;id\u0026#34;, expr(\u0026#34;CAST(id AS bigint)\u0026#34;)) 8 .withColumn(\u0026#34;user_id\u0026#34;, expr(\u0026#34;CAST(user_id AS bigint)\u0026#34;)) 9 .withColumn(\u0026#34;product_id\u0026#34;, expr(\u0026#34;CAST(product_id AS bigint)\u0026#34;)) 10 .withColumn(\u0026#34;discount\u0026#34;, expr(\u0026#34;CAST(discount AS decimal(4,2))\u0026#34;)) 11 .withColumn(\u0026#34;quantity\u0026#34;, expr(\u0026#34;CAST(quantity AS int)\u0026#34;)) 12 .withColumn(\u0026#34;created_at\u0026#34;, to_timestamp(\u0026#34;created_at\u0026#34;)) 13 ) 14 print(\u0026#34;orders - append records...\u0026#34;) 15 src_df.createOrReplaceTempView(\u0026#34;orders_tbl\u0026#34;) 16 spark_session.sql( 17 \u0026#34;\u0026#34;\u0026#34; 18 WITH src_products AS ( 19 SELECT * FROM demo.dwh.products 20 ), orders_updated AS ( 21 SELECT o.*, u.user_sk, p.prod_sk 22 FROM orders_tbl o 23 LEFT JOIN demo.dwh.users u 24 ON o.user_id = u.id 25 LEFT JOIN src_products p 26 ON o.product_id = p.id 27 AND o.created_at \u0026gt;= p.eff_from 28 AND o.created_at \u0026lt; p.eff_to 29 ), products_tbl AS ( 30 SELECT prod_sk, 31 id, 32 ROW_NUMBER() OVER (PARTITION BY id ORDER BY eff_from) AS rn 33 FROM src_products 34 ) 35 SELECT o.user_sk, 36 COALESCE(o.prod_sk, p.prod_sk) AS prod_sk, 37 o.id, 38 o.discount, 39 o.quantity, 40 o.created_at 41 FROM orders_updated AS o 42 JOIN products_tbl AS p ON o.product_id = p.id 43 WHERE p.rn = 1 44 ORDER BY o.created_at 45 \u0026#34;\u0026#34;\u0026#34; 46 ).writeTo(\u0026#34;demo.dwh.orders\u0026#34;).append() Run ETL The ETL script begins with creating all the tables - users, products and orders. Then the ETL for the users table is executed. Note that, although it is executed as initial loading, the code can also be applied to incremental ETL. Finally, incremental ETL is executed for the products and orders tables. The application can be submitted by ./run.sh spark-submit etl.py. Note to create the following environment variables before submitting the application.\nAWS_ACCESS_KEY_ID AWS_SECRET_ACCESS_KEY AWS_SESSION_TOKEN Note it is optional and required if authentication is made via assume role AWS_REGION Note it is NOT AWS_DEFAULT_REGION 1# etl.py 2from pyspark.sql import SparkSession 3from src import create_tables, etl_users, etl_products, etl_orders 4 5spark = SparkSession.builder.appName(\u0026#34;Iceberg ETL Demo\u0026#34;).getOrCreate() 6 7## create all tables - demo.dwh.users, demo.dwh.products and demo.dwh.orders 8create_tables(spark_session=spark) 9 10## users etl - assuming SCD type 1 11etl_users(\u0026#34;./data/users.csv\u0026#34;, spark) 12 13## incremental ETL 14for yr in range(0, 4): 15 print(f\u0026#34;processing year {yr}\u0026#34;) 16 ## products etl - assuming SCD type 2 17 etl_products(f\u0026#34;./data/products_year_{yr}.csv\u0026#34;, spark) 18 ## orders etl - relevant user_sk and prod_sk are added during transformation 19 etl_orders(f\u0026#34;./data/orders_year_{yr}.csv\u0026#34;, spark) Once the application completes, we\u0026rsquo;re able to query the iceberg tables on Athena. The following query returns all products whose id is 1. It is shown that the price increases over time and the relevant columns (curr_flag, eff_from and eff_to) for SCD type 2 are created as expected.\n1SELECT * 2FROM dwh.products 3WHERE id = 1 4ORDER BY eff_from The following query returns sample order records that bought the product. It can be checked that the product surrogate key matches the products dimension records.\n1WITH src_orders AS ( 2 SELECT o.user_sk, o.prod_sk, o.id, p.title, p.price, o.discount, o.quantity, o.created_at, 3 ROW_NUMBER() OVER (PARTITION BY p.price ORDER BY o.created_at) AS rn 4 FROM dwh.orders AS o 5 JOIN dwh.products AS p ON o.prod_sk = p.prod_sk 6 WHERE p.id = 1 7) 8SELECT * 9FROM src_orders 10WHERE rn = 1 11ORDER BY created_at Summary In this post, we discussed how to implement ETL using retail analytics data. In transformation, SCD type 1 and SCD type 2 are applied to the user and product data respectively. For the order data, the corresponding surrogate keys of the user and product data are added. A Pyspark application that implements ETL against Iceberg tables is used for demonstration in an EMR location environment. Finally, the ETL results will be queried by Athena for verification.\n","date":"June 26, 2022","img":"/blog/2022-06-26-iceberg-etl-demo/featured.png","lang":"en","langName":"English","largeImg":"/blog/2022-06-26-iceberg-etl-demo/featured_hu593bea0bbd3278708b1d2f8e8019b3ec_43604_500x0_resize_box_3.png","permalink":"/blog/2022-06-26-iceberg-etl-demo/","series":[],"smallImg":"/blog/2022-06-26-iceberg-etl-demo/featured_hu593bea0bbd3278708b1d2f8e8019b3ec_43604_180x0_resize_box_3.png","tags":[{"title":"AWS","url":"/tags/aws/"},{"title":"Amazon EMR","url":"/tags/amazon-emr/"},{"title":"Apache Spark","url":"/tags/apache-spark/"},{"title":"PySpark","url":"/tags/pyspark/"},{"title":"Apache Iceberg","url":"/tags/apache-iceberg/"},{"title":"ETL","url":"/tags/etl/"},{"title":"SCD","url":"/tags/scd/"},{"title":"Slowly Changing Dimension","url":"/tags/slowly-changing-dimension/"},{"title":"Docker","url":"/tags/docker/"},{"title":"Docker Compose","url":"/tags/docker-compose/"},{"title":"Visual Studio Code","url":"/tags/visual-studio-code/"}],"timestamp":1656201600,"title":"Data Warehousing ETL Demo With Apache Iceberg on EMR Local Environment"},{"categories":[{"title":"Data Engineering","url":"/categories/data-engineering/"}],"content":"[UPDATE 2023-12-07]\nI wrote a new post that simplifies the Spark configuration dramatically. Besides, the log configuration is based on Log4J2, which applies to newer Spark versions. Moreover, the container is configured to run the Spark History Server, and it allows us to debug and diagnose completed and running Spark applications. I recommend referring to the new post. Amazon EMR is a managed service that simplifies running Apache Spark on AWS. It has multiple deployment options that cover EC2, EKS, Outposts and Serverless. For development and testing, EMR Notebooks or EMR Studio can be an option. Both provide a Jupyter Notebook environment and the former is only available for EMR on EC2. There are cases, however, that development (and learning) is performed in a local environment more efficiently. The AWS Glue team understands this demand, and they illustrate how to make use of a custom Docker image for Glue in a recent blog post. However, we don’t hear similar news from the EMR team. In order to fill the gap, we’ll discuss how to create a Spark local development environment for EMR using Docker and/or VSCode. Typical Spark development examples will be demonstrated, which covers Spark Submit, Pytest, PySpark shell, Jupyter Notebook and Spark Structured Streaming. For the Spark Submit and Jupyter Notebook examples, Glue Catalog integration will be illustrated as well. And both the cases of utilising Visual Studio Code Remote - Containers extension and running as an isolated container will be covered in some key examples.\nCustom Docker Image While we may build a custom Spark Docker image from scratch, it’ll be tricky to configure the AWS Glue Data Catalog as the metastore for Spark SQL. Note that it is important to set up this feature because it can be used to integrate other AWS services such as Athena, Glue, Redshift Spectrum and so on. For example, with this feature, we can create a Glue table using a Spark application and the table can be queried by Athena or Redshift Spectrum.\nInstead, we can use one of the Docker images for EMR on EKS as a base image and build a custom image from it. As indicated in the EMR on EKS document, we can pull an EMR release image from ECR. Note to select the right AWS account ID as it is different from one region to another. After authenticating to the ECR repository, I pulled the latest EMR 6.5.0 release image.\n1## different aws region has a different account id 2$ aws ecr get-login-password --region ap-southeast-2 \\ 3 | docker login --username AWS --password-stdin 038297999601.dkr.ecr.ap-southeast-2.amazonaws.com 4## download the latest release (6.5.0) 5$ docker pull 038297999601.dkr.ecr.ap-southeast-2.amazonaws.com/spark/emr-6.5.0:20211119 In the Dockerfile, I updated the default user (hadoop) to have the admin privilege as it can be handy to modify system configuration if necessary. Then spark-defaults.conf and log4j.properties are copied to the Spark configuration folder - they’ll be discussed in detail below. Finally a number of python packages are installed. Among those, the ipykernel and python-dotenv packages are installed to work on Jupyter Notebooks and the pytest and pytest-cov packages are for testing. The custom Docker image is built with the following command: docker build -t=emr-6.5.0:20211119 .devcontainer/.\n1# .devcontainer/Dockerfile 2FROM 038297999601.dkr.ecr.ap-southeast-2.amazonaws.com/spark/emr-6.5.0:20211119 3 4USER root 5 6## Add hadoop to sudo 7RUN yum install -y sudo git \\ 8 \u0026amp;\u0026amp; echo \u0026#34;hadoop ALL=(ALL) NOPASSWD:ALL\u0026#34; \u0026gt;\u0026gt; /etc/sudoers 9 10## Update spark config and log4j properties 11COPY ./spark/spark-defaults.conf /usr/lib/spark/conf/spark-defaults.conf 12COPY ./spark/log4j.properties /usr/lib/spark/conf/log4j.properties 13 14## Install python packages 15COPY ./pkgs /tmp/pkgs 16RUN pip3 install -r /tmp/pkgs/requirements.txt 17 18USER hadoop:hadoop In the default spark configuration file (spark-defaults.conf) shown below, I commented out the following properties that are strictly related to EMR on EKS.\nspark.master spark.submit.deployMode spark.kubernetes.container.image.pullPolicy spark.kubernetes.pyspark.pythonVersion Then I changed the custom AWS credentials provider class from WebIdentityTokenCredentialsProvider to EnvironmentVariableCredentialsProvider. Note EMR jobs are run by a service account on EKS and authentication is managed by web identity token credentials. In a local environment, however, we don’t have an identity provider to authenticate so that access via environment variables can be an easy alternative option. We need the following environment variables to access AWS resources.\nAWS_ACCESS_KEY_ID AWS_SECRET_ACCESS_KEY AWS_SESSION_TOKEN note it is optional and required if authentication is made via assume role AWS_REGION note it is NOT AWS_DEFAULT_REGION Finally, I enabled Hive support and set AWSGlueDataCatalogHiveClientFactory as the Hive metastore factory class. When we start an EMR job, we can override application configuration to use AWS Glue Data Catalog as the metastore for Spark SQL and these are the relevant configuration changes for it.\n1# .devcontainer/spark/spark-defaults.conf 2 3... 4 5#spark.master k8s://https://kubernetes.default.svc:443 6#spark.submit.deployMode cluster 7spark.hadoop.fs.defaultFS file:/// 8spark.shuffle.service.enabled false 9spark.dynamicAllocation.enabled false 10#spark.kubernetes.container.image.pullPolicy Always 11#spark.kubernetes.pyspark.pythonVersion 3 12spark.hadoop.fs.s3.customAWSCredentialsProvider com.amazonaws.auth.EnvironmentVariableCredentialsProvider 13spark.hadoop.dynamodb.customAWSCredentialsProvider com.amazonaws.auth.EnvironmentVariableCredentialsProvider 14spark.authenticate true 15## for Glue catalog 16spark.sql.catalogImplementation hive 17spark.hadoop.hive.metastore.client.factory.class com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory Even if the credentials provider class is changed, it keeps showing long warning messages while fetching EC2 metadata. The following lines are added to the Log4j properties in order to disable those messages.\n1# .devcontainer/spark/log4j.properties 2 3... 4 5## Ignore warn messages related to EC2 metadata access failure 6log4j.logger.com.amazonaws.internal.InstanceMetadataServiceResourceFetcher=FATAL 7log4j.logger.com.amazonaws.util.EC2MetadataUtils=FATAL VSCode Development Container We are able to run Spark Submit, pytest, PySpark shell examples as an isolated container using the custom Docker image. However it can be much more convenient if we are able to perform development inside the Docker container where our app is executed. The Visual Studio Code Remote - Containers extension allows you to open a folder inside a container and to use VSCode’s feature sets. It supports both a standalone container and Docker Compose. In this post, we’ll use the latter as we’ll discuss an example Spark Structured Streaming application and multiple services should run and linked together for it.\nDocker Compose The main service (container) is named spark and its command prevents it from being terminated. The current working directory is mapped to /home/hadoop/repo and it’ll be the container folder that we’ll open for development. The aws configuration folder is volume-mapped to the container user’s home directory. It is an optional configuration to access AWS services without relying on AWS credentials via environment variables. The remaining services are related to Kafka. The kafka and zookeeper services are to run a Kafka cluster and the kafka-ui allows us to access the cluster on a browser. The services share the same Docker network named spark. Note that the compose file includes other Kafka related services and their details can be found in one of my earlier posts.\n1# .devcontainer/docker-compose.yml 2version: \u0026#34;2\u0026#34; 3 4services: 5 spark: 6 image: emr-6.5.0:20211119 7 container_name: spark 8 command: /bin/bash -c \u0026#34;while sleep 1000; do :; done\u0026#34; 9 networks: 10 - spark 11 volumes: 12 - ${PWD}:/home/hadoop/repo 13 - ${HOME}/.aws:/home/hadoop/.aws 14 zookeeper: 15 image: bitnami/zookeeper:3.7.0 16 container_name: zookeeper 17 ports: 18 - \u0026#34;2181:2181\u0026#34; 19 networks: 20 - spark 21 environment: 22 - ALLOW_ANONYMOUS_LOGIN=yes 23 kafka: 24 image: bitnami/kafka:2.8.1 25 container_name: kafka 26 ports: 27 - \u0026#34;9092:9092\u0026#34; 28 networks: 29 - spark 30 environment: 31 - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181 32 - ALLOW_PLAINTEXT_LISTENER=yes 33 depends_on: 34 - zookeeper 35 kafka-ui: 36 image: provectuslabs/kafka-ui:0.3.3 37 container_name: kafka-ui 38 ports: 39 - \u0026#34;8080:8080\u0026#34; 40 networks: 41 - spark 42 environment: 43 KAFKA_CLUSTERS_0_NAME: local 44 KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:9092 45 KAFKA_CLUSTERS_0_ZOOKEEPER: zookeeper:2181 46 ... 47 depends_on: 48 - zookeeper 49 - kafka 50... 51 52networks: 53 spark: 54 name: spark Development Container The development container is configured to connect the _spark _service among the Docker Compose services. The _AWS_PROFILE _environment variable is optionally set for AWS configuration and additional folders are added to PYTHONPATH, which is to use the bundled pyspark and py4j packages of the Spark distribution. The port 4040 for Spark History Server is added to the forwarded ports array - I guess it’s optional as the port is made accessible in the compose file. The remaining sections are for installing VSCode extensions and adding editor configuration. Note we need the Python extension (ms-python.python) not only for code formatting but also for working on Jupyter Notebooks.\n1# .devcontainer/devcontainer.json 2{ 3 \u0026#34;name\u0026#34;: \u0026#34;Spark Development\u0026#34;, 4 \u0026#34;dockerComposeFile\u0026#34;: \u0026#34;docker-compose.yml\u0026#34;, 5 \u0026#34;service\u0026#34;: \u0026#34;spark\u0026#34;, 6 \u0026#34;runServices\u0026#34;: [ 7 \u0026#34;spark\u0026#34;, 8 \u0026#34;zookeeper\u0026#34;, 9 \u0026#34;kafka\u0026#34;, 10 \u0026#34;kafka-ui\u0026#34; 11 ], 12 \u0026#34;remoteEnv\u0026#34;: { 13 \u0026#34;AWS_PROFILE\u0026#34;: \u0026#34;cevo\u0026#34;, 14 \u0026#34;PYTHONPATH\u0026#34;: \u0026#34;/usr/lib/spark/python/lib/py4j-0.10.9-src.zip:/usr/lib/spark/python/\u0026#34; 15 }, 16 \u0026#34;workspaceFolder\u0026#34;: \u0026#34;/home/hadoop/repo\u0026#34;, 17 \u0026#34;extensions\u0026#34;: [\u0026#34;ms-python.python\u0026#34;, \u0026#34;esbenp.prettier-vscode\u0026#34;], 18 \u0026#34;forwardPorts\u0026#34;: [4040], 19 \u0026#34;settings\u0026#34;: { 20 \u0026#34;terminal.integrated.profiles.linux\u0026#34;: { 21 \u0026#34;bash\u0026#34;: { 22 \u0026#34;path\u0026#34;: \u0026#34;/bin/bash\u0026#34; 23 } 24 }, 25 \u0026#34;terminal.integrated.defaultProfile.linux\u0026#34;: \u0026#34;bash\u0026#34;, 26 \u0026#34;editor.formatOnSave\u0026#34;: true, 27 \u0026#34;editor.defaultFormatter\u0026#34;: \u0026#34;esbenp.prettier-vscode\u0026#34;, 28 \u0026#34;editor.tabSize\u0026#34;: 2, 29 \u0026#34;python.defaultInterpreterPath\u0026#34;: \u0026#34;python3\u0026#34;, 30 \u0026#34;python.testing.pytestEnabled\u0026#34;: true, 31 \u0026#34;python.linting.enabled\u0026#34;: true, 32 \u0026#34;python.linting.pylintEnabled\u0026#34;: false, 33 \u0026#34;python.linting.flake8Enabled\u0026#34;: false, 34 \u0026#34;python.formatting.provider\u0026#34;: \u0026#34;black\u0026#34;, 35 \u0026#34;python.formatting.blackPath\u0026#34;: \u0026#34;black\u0026#34;, 36 \u0026#34;python.formatting.blackArgs\u0026#34;: [\u0026#34;--line-length\u0026#34;, \u0026#34;100\u0026#34;], 37 \u0026#34;[python]\u0026#34;: { 38 \u0026#34;editor.tabSize\u0026#34;: 4, 39 \u0026#34;editor.defaultFormatter\u0026#34;: \u0026#34;ms-python.python\u0026#34; 40 } 41 } 42} We can open the current folder in the development container after launching the Docker Compose services by executing the following command in the command palette.\nRemote-Containers: Open Folder in Container\u0026hellip; Once the development container is ready, the current folder will be open within the spark service container. We are able to check the container’s current folder is /home/hadoop/repo and the container user is hadoop.\nFile Permission Management I use Ubuntu in WSL 2 for development and the user ID and group ID of my WSL user are 1000. On the other hand, the container user is hadoop and its user ID and group ID are 999 and 1000 respectively. When you create a file in the host, the user has the read and write permissions of the file while the group only has the read permission. Therefore, you can read the file inside the development container by the container user, but it is not possible to modify it due to lack of the write permission. This file permission issue will happen when a file is created by the container user and the WSL user tries to modify it in the host. A quick search shows this is a typical behaviour applicable only to Linux (not Mac or Windows).\nIn order to handle this file permission issue, we can update the file permission so that the read and write permissions are given to both the user and group. Note the host (WSL) user and container user have the same group ID and writing activities will be allowed at least by the group permission. Below shows an example. The read and write permissions for files in the project folder are given to both the user and group. Those that are created by the container user indicate the username while there are 2 files that are created by the WSL user, and it is indicated by the user ID because there is no user whose user ID is 1000 in the container.\n1bash-4.2$ ls -al | grep \u0026#39;^-\u0026#39; 2-rw-rw-r-- 1 hadoop hadoop 1086 Apr 12 22:23 .env 3-rw-rw-r-- 1 1000 hadoop 1855 Apr 12 19:45 .gitignore 4-rw-rw-r-- 1 1000 hadoop 66 Mar 30 22:39 README.md 5-rw-rw-r-- 1 hadoop hadoop 874 Apr 5 11:14 test_utils.py 6-rw-rw-r-- 1 hadoop hadoop 3882 Apr 12 22:24 tripdata.ipynb 7-rw-rw-r-- 1 hadoop hadoop 1653 Apr 24 13:09 tripdata_notify.py 8-rw-rw-r-- 1 hadoop hadoop 1101 Apr 24 01:22 tripdata.py 9-rw-rw-r-- 1 hadoop hadoop 664 Apr 12 19:45 utils.py Below is the same file list that is printed in the host. Note that the group name is changed into the WSL user’s group and those that are created by the container user are marked by the user ID.\n1jaehyeon@cevo:~/personal/emr-local-dev$ ls -al | grep \u0026#39;^-\u0026#39; 2-rw-rw-r-- 1 999 jaehyeon 1086 Apr 13 08:23 .env 3-rw-rw-r-- 1 jaehyeon jaehyeon 1855 Apr 13 05:45 .gitignore 4-rw-rw-r-- 1 jaehyeon jaehyeon 66 Mar 31 09:39 README.md 5-rw-rw-r-- 1 999 jaehyeon 874 Apr 5 21:14 test_utils.py 6-rw-rw-r-- 1 999 jaehyeon 3882 Apr 13 08:24 tripdata.ipynb 7-rw-rw-r-- 1 999 jaehyeon 1101 Apr 24 11:22 tripdata.py 8-rw-rw-r-- 1 999 jaehyeon 1653 Apr 24 23:09 tripdata_notify.py 9-rw-rw-r-- 1 999 jaehyeon 664 Apr 13 05:45 utils.py We can add the read or write permission of a single file or a folder easily as shown below - g+rw. Note the last example is for the AWS configuration folder and only the read access is given to the group. Note also that file permission change is not affected if the repository is cloned into a new place, and thus it only affects the local development environment.\n1# add write access of a file to the group 2sudo chmod g+rw /home/hadoop/repo/\u0026lt;file-name\u0026gt; 3# add write access of a folder to the group 4sudo chmod -R g+rw /home/hadoop/repo/\u0026lt;folder-name\u0026gt; 5# add read access of the .aws folder to the group 6sudo chmod -R g+r /home/hadoop/.aws Examples In this section, I’ll demonstrate typical Spark development examples. They’ll cover Spark Submit, Pytest, PySpark shell, Jupyter Notebook and Spark Structured Streaming. For the Spark Submit and Jupyter Notebook examples, Glue Catalog integration will be illustrated as well. And both the cases of utilising Visual Studio Code Remote - Containers extension and running as an isolated container will be covered in some key examples.\nSpark Submit It is a simple Spark application that reads a sample NY taxi trip dataset from a public S3 bucket. Once loaded, it converts the pick-up and drop-off datetime columns from string to timestamp followed by writing the transformed data to a destination S3 bucket. The destination bucket name (bucket_name) can be specified by a system argument or its default value is taken. It finishes by creating a Glue table and, similar to the destination bucket name, the table name (tblname) can be specified as well.\n1# tripdata.py 2import sys 3from pyspark.sql import SparkSession 4 5from utils import to_timestamp_df 6 7if __name__ == \u0026#34;__main__\u0026#34;: 8 spark = SparkSession.builder.appName(\u0026#34;Trip Data\u0026#34;).getOrCreate() 9 10 dbname = \u0026#34;tripdata\u0026#34; 11 tblname = \u0026#34;ny_taxi\u0026#34; if len(sys.argv) \u0026lt;= 1 else sys.argv[1] 12 bucket_name = \u0026#34;emr-local-dev\u0026#34; if len(sys.argv) \u0026lt;= 2 else sys.argv[2] 13 dest_path = f\u0026#34;s3://{bucket_name}/{tblname}/\u0026#34; 14 src_path = \u0026#34;s3://aws-data-analytics-workshops/shared_datasets/tripdata/\u0026#34; 15 # read csv 16 ny_taxi = spark.read.option(\u0026#34;inferSchema\u0026#34;, \u0026#34;true\u0026#34;).option(\u0026#34;header\u0026#34;, \u0026#34;true\u0026#34;).csv(src_path) 17 ny_taxi = to_timestamp_df(ny_taxi, [\u0026#34;lpep_pickup_datetime\u0026#34;, \u0026#34;lpep_dropoff_datetime\u0026#34;]) 18 ny_taxi.printSchema() 19 # write parquet 20 ny_taxi.write.mode(\u0026#34;overwrite\u0026#34;).parquet(dest_path) 21 # create glue table 22 ny_taxi.registerTempTable(tblname) 23 spark.sql(f\u0026#34;CREATE DATABASE IF NOT EXISTS {dbname}\u0026#34;) 24 spark.sql(f\u0026#34;USE {dbname}\u0026#34;) 25 spark.sql( 26 f\u0026#34;\u0026#34;\u0026#34;CREATE TABLE IF NOT EXISTS {tblname} 27 USING PARQUET 28 LOCATION \u0026#39;{dest_path}\u0026#39; 29 AS SELECT * FROM {tblname} 30 \u0026#34;\u0026#34;\u0026#34; 31 ) The Spark application can be submitted as shown below.\n1export AWS_ACCESS_KEY_ID=\u0026lt;AWS-ACCESS-KEY-ID\u0026gt; 2export AWS_SECRET_ACCESS_KEY=\u0026lt;AWS-SECRET-ACCESS-KEY\u0026gt; 3export AWS_REGION=\u0026lt;AWS-REGION\u0026gt; 4# optional 5export AWS_SESSION_TOKEN=\u0026lt;AWS-SESSION-TOKEN\u0026gt; 6 7$SPARK_HOME/bin/spark-submit \\ 8 --deploy-mode client \\ 9 --master local[*] \\ 10 tripdata.py Once it completes, the Glue table will be created, and we can query it using Athena as shown below.\nIf we want to submit the application as an isolated container, we can use the custom image directly. Below shows the equivalent Docker run command.\n1docker run --rm \\ 2 -e AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID \\ 3 -e AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY \\ 4 -e AWS_SESSION_TOKEN=$AWS_SESSION_TOKEN \\ # optional 5 -e AWS_REGION=$AWS_REGION \\ 6 -v $PWD:/usr/hadoop \\ 7 emr-6.5.0:20211119 \\ 8 /usr/lib/spark/bin/spark-submit --deploy-mode client --master local[*] /usr/hadoop/tripdata.py taxi emr-local-dev Pytest The Spark application in the earlier example uses a custom function that converts the data type of one or more columns from string to timestamp - to_timestamp_df(). The source of the function and the testing script of it can be found below.\n1# utils.py 2from typing import List, Union 3from pyspark.sql import DataFrame 4from pyspark.sql.functions import col, to_timestamp 5 6def to_timestamp_df( 7 df: DataFrame, fields: Union[List[str], str], format: str = \u0026#34;M/d/yy H:mm\u0026#34; 8) -\u0026gt; DataFrame: 9 fields = [fields] if isinstance(fields, str) else fields 10 for field in fields: 11 df = df.withColumn(field, to_timestamp(col(field), format)) 12 return df 13# test_utils.py 14import pytest 15import datetime 16from pyspark.sql import SparkSession 17from py4j.protocol import Py4JError 18 19from utils import to_timestamp_df 20 21@pytest.fixture(scope=\u0026#34;session\u0026#34;) 22def spark(): 23 return ( 24 SparkSession.builder.master(\u0026#34;local\u0026#34;) 25 .appName(\u0026#34;test\u0026#34;) 26 .config(\u0026#34;spark.submit.deployMode\u0026#34;, \u0026#34;client\u0026#34;) 27 .getOrCreate() 28 ) 29 30 31def test_to_timestamp_success(spark): 32 raw_df = spark.createDataFrame( 33 [(\u0026#34;1/1/17 0:01\u0026#34;,)], 34 [\u0026#34;date\u0026#34;], 35 ) 36 37 test_df = to_timestamp_df(raw_df, \u0026#34;date\u0026#34;, \u0026#34;M/d/yy H:mm\u0026#34;) 38 for row in test_df.collect(): 39 assert row[\u0026#34;date\u0026#34;] == datetime.datetime(2017, 1, 1, 0, 1) 40 41 42def test_to_timestamp_bad_format(spark): 43 raw_df = spark.createDataFrame( 44 [(\u0026#34;1/1/17 0:01\u0026#34;,)], 45 [\u0026#34;date\u0026#34;], 46 ) 47 48 with pytest.raises(Py4JError): 49 to_timestamp_df(raw_df, \u0026#34;date\u0026#34;, \u0026#34;M/d/yy HH:mm\u0026#34;).collect() As the test cases don’t access AWS services, they can be executed simply by the Pytest command (e.g. pytest -v).\nTesting can also be made in an isolated container as shown below. Note that we need to add the PYTHONPATH environment variable because we use the bundled Pyspark package.\n1docker run --rm \\ 2 -e PYTHONPATH=\u0026#34;/usr/lib/spark/python/lib/py4j-0.10.9-src.zip:/usr/lib/spark/python/\u0026#34; \\ 3 -v $PWD:/usr/hadoop \\ 4 emr-6.5.0:20211119 \\ 5 pytest /usr/hadoop -v PySpark Shell The PySpark shell can be launched as shown below.\n1$SPARK_HOME/bin/pyspark \\ 2 --deploy-mode client \\ 3 --master local[*] Also, below shows an example of launching it as an isolated container.\n1docker run --rm -it \\ 2 -e AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID \\ 3 -e AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY \\ 4 -e AWS_SESSION_TOKEN=$AWS_SESSION_TOKEN \\ # optional 5 -e AWS_REGION=$AWS_REGION \\ 6 -v $PWD:/usr/hadoop \\ 7 emr-6.5.0:20211119 \\ 8 /usr/lib/spark/bin/pyspark --deploy-mode client --master local[*] Jupyter Notebook Jupyter Notebook is a popular Spark application authoring tool, and we can create a notebook simply by creating a file with the ipynb extension in VSCode. Note we need the ipykernel package in order to run code cells, and it is already installed in the custom Docker image. For accessing AWS resources, we need the environment variables of AWS credentials mentioned earlier. We can use the python-dotenv package. Specifically we can create an .env file and add AWS credentials to it. Then we can add a code cell that loads the .env file at the beginning of the notebook.\nIn the next code cell, the app reads the Glue table and adds a column of trip duration followed by showing the summary statistics of key columns. We see some puzzling records that show zero trip duration or negative total amount. Among those, we find negative total amount records should be reported immediately and a Spark Structured Streaming application turns out to be a good option.\nSpark Streaming We need sample data that can be read by the Spark application. In order to generate it, the individual records are taken from the source CSV file and saved locally after being converted into json. Below script creates those json files in the data/json folder. Inside the development container, it can be executed as python3 data/generate.py.\n1# data/generate.py 2import shutil 3import io 4import json 5import csv 6from pathlib import Path 7import boto3 8 9BUCKET_NAME = \u0026#34;aws-data-analytics-workshops\u0026#34; 10KEY_NAME = \u0026#34;shared_datasets/tripdata/tripdata.csv\u0026#34; 11DATA_PATH = Path.joinpath(Path(__file__).parent, \u0026#34;json\u0026#34;) 12 13 14def recreate_data_path_if(data_path: Path, recreate: bool = True): 15 if recreate: 16 shutil.rmtree(data_path, ignore_errors=True) 17 data_path.mkdir() 18 19 20def write_to_json(bucket_name: str, key_name: str, data_path: Path, recreate: bool = True): 21 s3 = boto3.resource(\u0026#34;s3\u0026#34;) 22 data = io.BytesIO() 23 bucket = s3.Bucket(bucket_name) 24 bucket.download_fileobj(key_name, data) 25 contents = data.getvalue().decode(\u0026#34;utf-8\u0026#34;) 26 print(\u0026#34;download complete\u0026#34;) 27 reader = csv.DictReader(contents.split(\u0026#34;\\n\u0026#34;)) 28 recreate_data_path_if(data_path, recreate) 29 for c, row in enumerate(reader): 30 record_id = str(c).zfill(5) 31 data_path.joinpath(f\u0026#34;{record_id}.json\u0026#34;).write_text( 32 json.dumps({**{\u0026#34;record_id\u0026#34;: record_id}, **row}) 33 ) 34 35 36if __name__ == \u0026#34;__main__\u0026#34;: 37 write_to_json(BUCKET_NAME, KEY_NAME, DATA_PATH, True) In the Spark streaming application, the steam reader loads JSON files in the data/json folder and the data schema is provided by DDL statements. Then it generates the target dataframe that filters records whose total amount is negative. Note the target dataframe is structured to have the key and value columns, which is required by Kafka. Finally, it writes the records of the target dataframe to the _notifications _topics of the Kafka cluster.\n1# tripdata_notify.py 2from pyspark.sql import SparkSession 3from pyspark.sql.functions import col, to_json, struct 4from utils import remove_checkpoint 5 6if __name__ == \u0026#34;__main__\u0026#34;: 7 remove_checkpoint() 8 9 spark = ( 10 SparkSession.builder.appName(\u0026#34;Trip Data Notification\u0026#34;) 11 .config(\u0026#34;spark.streaming.stopGracefullyOnShutdown\u0026#34;, \u0026#34;true\u0026#34;) 12 .config(\u0026#34;spark.sql.shuffle.partitions\u0026#34;, 3) 13 .getOrCreate() 14 ) 15 16 tripdata_ddl = \u0026#34;\u0026#34;\u0026#34; 17 record_id STRING, 18 VendorID STRING, 19 lpep_pickup_datetime STRING, 20 lpep_dropoff_datetime STRING, 21 store_and_fwd_flag STRING, 22 RatecodeID STRING, 23 PULocationID STRING, 24 DOLocationID STRING, 25 passenger_count STRING, 26 trip_distance STRING, 27 fare_amount STRING, 28 extra STRING, 29 mta_tax STRING, 30 tip_amount STRING, 31 tolls_amount STRING, 32 ehail_fee STRING, 33 improvement_surcharge STRING, 34 total_amount STRING, 35 payment_type STRING, 36 trip_type STRING 37 \u0026#34;\u0026#34;\u0026#34; 38 39 ny_taxi = ( 40 spark.readStream.format(\u0026#34;json\u0026#34;) 41 .option(\u0026#34;path\u0026#34;, \u0026#34;data/json\u0026#34;) 42 .option(\u0026#34;maxFilesPerTrigger\u0026#34;, \u0026#34;1000\u0026#34;) 43 .schema(tripdata_ddl) 44 .load() 45 ) 46 47 target_df = ny_taxi.filter(col(\u0026#34;total_amount\u0026#34;) \u0026lt;= 0).select( 48 col(\u0026#34;record_id\u0026#34;).alias(\u0026#34;key\u0026#34;), to_json(struct(\u0026#34;*\u0026#34;)).alias(\u0026#34;value\u0026#34;) 49 ) 50 51 notification_writer_query = ( 52 target_df.writeStream.format(\u0026#34;kafka\u0026#34;) 53 .queryName(\u0026#34;notifications\u0026#34;) 54 .option(\u0026#34;kafka.bootstrap.servers\u0026#34;, \u0026#34;kafka:9092\u0026#34;) 55 .option(\u0026#34;topic\u0026#34;, \u0026#34;notifications\u0026#34;) 56 .outputMode(\u0026#34;append\u0026#34;) 57 .option(\u0026#34;checkpointLocation\u0026#34;, \u0026#34;.checkpoint\u0026#34;) 58 .start() 59 ) 60 61 notification_writer_query.awaitTermination() The streaming application can be submitted as shown below. Note the Kafak 0.10+ Source for Structured Streaming and its dependencies are added directly to the spark submit command as indicated by the official document.\n1$SPARK_HOME/bin/spark-submit \\ 2 --deploy-mode client \\ 3 --master local[*] \\ 4 --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2 \\ 5 tripdata_notify.py We can check the topic via Kafka UI on port 8080. We see the notifications topic has 50 messages, which matches to the number that we obtained from the notebook.\nWe can check the individual messages via the UI as well.\nSummary In this post, we discussed how to create a Spark local development environment for EMR using Docker and/or VSCode. A range of Spark development examples are demonstrated, and Glue Catalog integration is illustrated in some of them. And both the cases of utilising Visual Studio Code Remote - Containers extension and running as an isolated container are covered in some key examples.\n","date":"May 8, 2022","img":"/blog/2022-05-08-emr-local-dev/featured.png","lang":"en","langName":"English","largeImg":"/blog/2022-05-08-emr-local-dev/featured_hu653aa93a98a5139fededca231658be70_25693_500x0_resize_box_3.png","permalink":"/blog/2022-05-08-emr-local-dev/","series":[],"smallImg":"/blog/2022-05-08-emr-local-dev/featured_hu653aa93a98a5139fededca231658be70_25693_180x0_resize_box_3.png","tags":[{"title":"AWS","url":"/tags/aws/"},{"title":"Amazon EMR","url":"/tags/amazon-emr/"},{"title":"Apache Spark","url":"/tags/apache-spark/"},{"title":"PySpark","url":"/tags/pyspark/"},{"title":"Docker","url":"/tags/docker/"},{"title":"Docker Compose","url":"/tags/docker-compose/"},{"title":"Visual Studio Code","url":"/tags/visual-studio-code/"}],"timestamp":1651968000,"title":"Develop and Test Apache Spark Apps for EMR Locally Using Docker"},{"categories":[{"title":"Data Streaming","url":"/categories/data-streaming/"}],"content":"In the previous post, we discussed a Change Data Capture (CDC) solution with a schema registry. A local development environment is set up using Docker Compose. The Debezium and Confluent S3 connectors are deployed with the Confluent Avro converter and the Apicurio registry is used as the schema registry service. A quick example is shown to illustrate how schema evolution can be managed by the schema registry. In this post, we\u0026rsquo;ll build the solution on AWS using MSK, MSK Connect, Aurora PostgreSQL and ECS.\nPart 1 Local Development Part 2 MSK Deployment (this post) Architecture Below shows an updated CDC architecture with a schema registry. The Debezium connector talks to the schema registry first and checks if the schema is available. If it doesn\u0026rsquo;t exist, it is registered and cached in the schema registry. Then the producer serializes the data with the schema and sends it to the topic with the schema ID. When the sink connector consumes the message, it\u0026rsquo;ll read the schema with the ID and deserializes it. The schema registry uses a PostgreSQL database as an artifact store where multiple versions of schemas are kept. In this post, we\u0026rsquo;ll build it on AWS. An MSK cluster will be created and data will be pushed from a database deployed using Aurora PostgreSQL. The database has a schema called registry and schema metadata will be stored in it. The Apicurio registry will be deployed as an ECS service behind an internal load balancer.\nInfrastructure The main AWS resources will be deployed to private subnets of a VPC and connection between those will be managed by updating security group inbound rules. For example, the MSK connectors should have access to the registry service and the connectors\u0026rsquo; security group ID should be added to the inbound rule of the registry service. As multiple resources are deployed to private subnets, it\u0026rsquo;ll be convenient to set up VPN so that access to them can be made from the developer machine. It can improve developer experience significantly. We\u0026rsquo;ll use Terraform for managing the resources on AWS and how to set up VPC, VPN and Aurora PostgreSQL is discussed in detail in one of my earlier posts. In this post, I\u0026rsquo;ll illustrate those that are not covered in the article. The Terraform source can be found in the GitHub repository for this post.\nMSK Cluster As discussed in one of the earlier posts, we\u0026rsquo;ll create an MSK cluster with 2 brokers of the kafka.m5.large instance type in order to prevent the failed authentication error. 2 inbound rules are configured for the MSK\u0026rsquo;s security group. The first one is allowing all access from its own security group, and it is required for MSK connectors to have access to the MKS cluster. Note, when we create a connector from the AWS console, the cluster\u0026rsquo;s subnets and security group are selected for the connector by default. The second inbound rule is allowing the VPN\u0026rsquo;s security group at port 9098, which is the port of bootstrap servers for IAM authentication. Also, an IAM role is created, which can be assumed by MSK connectors in order to have permission on the cluster, topic and group. The Terraform file for the MSK cluster and related resources can be found in infra/msk.tf.\nSchema Registry The schema registry is deployed via ECS as a Fargate task. 2 tasks are served by an ECS service, and it can be accessed by an internal load balancer. The load balancer is configured to allow inbound traffic from the MSK cluster and VPN, and it has access to the individual tasks. Normally inbound traffic to the tasks should be allowed to the load balancer only but, for testing, it is set that they accept inbound traffic from VPN as well. The Terraform file for the schema registry and related resources can be found in infra/registry.tf.\nSetup Database In order for the schema registry to work properly, the database should have the appropriate schema named registry. Also, the database needs to have sample data loaded into the ods schema. Therefore, it is not possible to create all resources at once, and we need to skip creating the registry service at first. It can be done by setting the registry_create variable to false.\n1# infra/variables.tf 2variable \u0026#34;registry_create\u0026#34; { 3 description = \u0026#34;Whether to create an Apicurio registry service\u0026#34; 4 default = false 5} A simple python application is created to set up the database, and it can be run as shown below. Note do not forget to connect the VPN before executing the command.\n1(venv) $ python connect/data/load/main.py --help 2Usage: main.py [OPTIONS] 3 4Options: 5 -h, --host TEXT Database host [required] 6 -p, --port INTEGER Database port [default: 5432] 7 -d, --dbname TEXT Database name [required] 8 -u, --user TEXT Database user name [required] 9 --password TEXT Database user password [required] 10 --install-completion Install completion for the current shell. 11 --show-completion Show completion for the current shell, to copy it or 12 customize the installation. 13 --help Show this message and exit. 14 15(venv) $ python connect/data/load/main.py --help -h \u0026lt;db-host-name-or-ip\u0026gt; -d \u0026lt;dbname\u0026gt; -u \u0026lt;username\u0026gt; 16Password: 17To create database? [y/N]: y 18Database connection created 19Northwind SQL scripts executed Deploy Schema Registry Once the database setup is complete, we can apply the Terraform stack with the registry_create variable to true. When it\u0026rsquo;s deployed, we can check the APIs that the registry service supports as shown below. In line with the previous post, we\u0026rsquo;ll use the Confluent schema registry compatible API.\nKafka UI The Kafka UI supports MSK IAM Authentication and we can use it to monitor and manage MSK clusters and related objects/resources. My AWS credentials are mapped to the container and my AWS profile (cevo) is added to the SASL config environment variable. Note environment variables are used for the bootstrap server endpoint and registry host. It can be started as docker-compose -f kafka-ui.yml up.\n1# kafka-ui.yml 2version: \u0026#34;2\u0026#34; 3services: 4 kafka-ui: 5 image: provectuslabs/kafka-ui:0.3.3 6 container_name: kafka-ui 7 ports: 8 - \u0026#34;8080:8080\u0026#34; 9 # restart: always 10 volumes: 11 - $HOME/.aws:/root/.aws 12 environment: 13 KAFKA_CLUSTERS_0_NAME: msk 14 KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: $BS_SERVERS 15 KAFKA_CLUSTERS_0_PROPERTIES_SECURITY_PROTOCOL: SASL_SSL 16 KAFKA_CLUSTERS_0_PROPERTIES_SASL_MECHANISM: AWS_MSK_IAM 17 KAFKA_CLUSTERS_0_PROPERTIES_SASL_CLIENT_CALLBACK_HANDLER_CLASS: software.amazon.msk.auth.iam.IAMClientCallbackHandler 18 KAFKA_CLUSTERS_0_PROPERTIES_SASL_JAAS_CONFIG: software.amazon.msk.auth.iam.IAMLoginModule required awsProfileName=\u0026#34;cevo\u0026#34;; 19 KAFKA_CLUSTERS_0_SCHEMAREGISTRY: http://$REGISTRY_HOST/apis/ccompat/v6 The UI can be checked on a browser as shown below.\nCreate Connectors Creating custom plugins and connectors is illustrated in detail in one of my earlier posts. Here I\u0026rsquo;ll sketch key points only. The custom plugins for the source and sink connectors should include the Kafka Connect Avro Converter as well. The version 6.0.3 is used, and plugin packaging can be checked in connect/local/download-connectors.sh.\nThe Debezium Postgres Connector is used as the source connector. Here the main difference from the earlier post is using the Confluent Avro Converter class for key and value converter properties and adding the schema registry URL.\n1# connect/msk/debezium.properties 2connector.class=io.debezium.connector.postgresql.PostgresConnector 3tasks.max=1 4plugin.name=pgoutput 5publication.name=cdc_publication 6slot.name=orders 7database.hostname=analytics-db-cluster.cluster-ctrfy31kg8iq.ap-southeast-2.rds.amazonaws.com 8database.port=5432 9database.user=master 10database.password=\u0026lt;database-user-password\u0026gt; 11database.dbname=main 12database.server.name=ord 13schema.include=ods 14table.include.list=ods.cdc_events 15key.converter=io.confluent.connect.avro.AvroConverter 16key.converter.schema.registry.url=http://internal-analytics-registry-lb-754693167.ap-southeast-2.elb.amazonaws.com/apis/ccompat/v6 17value.converter=io.confluent.connect.avro.AvroConverter 18value.converter.schema.registry.url=http://internal-analytics-registry-lb-754693167.ap-southeast-2.elb.amazonaws.com/apis/ccompat/v6 19transforms=unwrap 20transforms.unwrap.type=io.debezium.transforms.ExtractNewRecordState 21transforms.unwrap.drop.tombstones=false 22transforms.unwrap.delete.handling.mode=rewrite 23transforms.unwrap.add.fields=op,db,table,schema,lsn,source.ts_ms The sink connector also uses the Confluent Avro Converter class for key and value converter properties and the schema registry URL is added accordingly.\n1# connect/msk/confluent.properties 2connector.class=io.confluent.connect.s3.S3SinkConnector 3storage.class=io.confluent.connect.s3.storage.S3Storage 4format.class=io.confluent.connect.s3.format.avro.AvroFormat 5tasks.max=1 6topics=ord.ods.cdc_events 7s3.bucket.name=analytics-data-590312749310-ap-southeast-2 8s3.region=ap-southeast-2 9flush.size=100 10rotate.schedule.interval.ms=60000 11timezone=Australia/Sydney 12partitioner.class=io.confluent.connect.storage.partitioner.DefaultPartitioner 13key.converter=io.confluent.connect.avro.AvroConverter 14key.converter.schema.registry.url=http://internal-analytics-registry-lb-754693167.ap-southeast-2.elb.amazonaws.com/apis/ccompat/v6 15value.converter=io.confluent.connect.avro.AvroConverter 16value.converter.schema.registry.url=http://internal-analytics-registry-lb-754693167.ap-southeast-2.elb.amazonaws.com/apis/ccompat/v6 17errors.log.enable=true As with the previous post, we can check the key and value schemas are created once the source connector is deployed. Note we can check the details of the schemas by clicking the relevant schema items.\nWe can see the messages (key and value) are properly deserialized within the UI as we added the schema registry URL as an environment variable and it can be accessed from it.\nSchema Evolution The schema registry keeps multiple versions of schemas and we can check it by adding a column to the table and updating records.\n1--// add a column with a default value 2ALTER TABLE ods.cdc_events 3 ADD COLUMN employee_id int DEFAULT -1; 4 5--// update employee ID 6UPDATE ods.cdc_events 7 SET employee_id = (employee -\u0026gt;\u0026gt; \u0026#39;employee_id\u0026#39;)::INT 8WHERE customer_id = \u0026#39;VINET\u0026#39; Once the above queries are executed, we see a new version is added to the topic\u0026rsquo;s value schema, and it includes the new field.\nSummary In this post, we continued the discussion of a Change Data Capture (CDC) solution with a schema registry, and it is deployed to AWS. Multiple services including MSK, MSK Connect, Aurora PostgreSQL and ECS are used to build the solution. All major resources are deployed in private subnets and VPN is used to access them in order to improve developer experience. The Apicurio registry is used as the schema registry service, and it is deployed as an ECS service. In order for the connectors to have access to the registry, the Confluent Avro Converter is packaged together with the connector sources. The post ends with illustrating how schema evolution is managed by the schema registry.\n","date":"April 3, 2022","img":"/blog/2022-04-03-schema-registry-part2/featured.png","lang":"en","langName":"English","largeImg":"/blog/2022-04-03-schema-registry-part2/featured_hu55ef0aeef397b55768e961367d004bbc_59689_500x0_resize_box_3.png","permalink":"/blog/2022-04-03-schema-registry-part2/","series":[{"title":"Integrate Schema Registry With MSK Connect","url":"/series/integrate-schema-registry-with-msk-connect/"}],"smallImg":"/blog/2022-04-03-schema-registry-part2/featured_hu55ef0aeef397b55768e961367d004bbc_59689_180x0_resize_box_3.png","tags":[{"title":"AWS","url":"/tags/aws/"},{"title":"Amazon ECS","url":"/tags/amazon-ecs/"},{"title":"Amazon MSK","url":"/tags/amazon-msk/"},{"title":"Amazon MSK Connect","url":"/tags/amazon-msk-connect/"},{"title":"Apache Kafka","url":"/tags/apache-kafka/"},{"title":"Kafka Connect","url":"/tags/kafka-connect/"},{"title":"Terraform","url":"/tags/terraform/"},{"title":"Docker","url":"/tags/docker/"},{"title":"Docker Compose","url":"/tags/docker-compose/"}],"timestamp":1648944000,"title":"Use External Schema Registry With MSK Connect – Part 2 MSK Deployment"},{"categories":[{"title":"Data Streaming","url":"/categories/data-streaming/"}],"content":"When we discussed a Change Data Capture (CDC) solution in one of the earlier posts, we used the JSON converter that comes with Kafka Connect. We optionally enabled the key and value schemas and the topic messages include those schemas together with payload. It seems to be convenient at first as the messages are saved into S3 on their own. However, it became cumbersome when we tried to use the DeltaStreamer utility. Specifically it requires the scheme of the files, but unfortunately we cannot use the schema that is generated by the default JSON converter - it returns the struct type, which is not supported by the Hudi utility. In order to handle this issue, we created a schema with the record type using the Confluent Avro converter and used it after saving on S3. However, as we aimed to manage a long-running process, generating a schema manually was not an optimal solution because, for example, we\u0026rsquo;re not able to handle schema evolution effectively. In this post, we\u0026rsquo;ll discuss an improved architecture that makes use of a schema registry that resides outside the Kafka cluster and allows the producers and consumers to reference the schemas externally.\nPart 1 Local Development (this post) Part 2 MSK Deployment Architecture Below shows an updated CDC architecture with a schema registry. The Debezium connector talks to the schema registry first and checks if the schema is available. If it doesn\u0026rsquo;t exist, it is registered and cached in the schema registry. Then the producer serializes the data with the schema and sends it to the topic with the schema ID. When the sink connector consumes the message, it\u0026rsquo;ll read the schema with the ID and deserializes it. The schema registry uses a PostgreSQL database as an artifact store where multiple versions of schemas are kept. In this post, we\u0026rsquo;ll build it locally using Docker Compose.\nLocal Services Earlier we discussed a local development environment for a Change Data Capture (CDC) solution using the Confluent platform - see this post for details. While it provides a quick and easy way of developing Kafka locally, it doesn\u0026rsquo;t seem to match MSK well. For example, its docker image already includes the Avro converter and schema registry client libraries. Because of that, while Kafka connectors with Avro serialization work in the platform without modification, they\u0026rsquo;ll fail on MSK Connect if they are deployed with the same configuration. Therefore, it\u0026rsquo;ll be better to use docker images from other open source projects instead of the Confluent platform while we can still use docker-compose to build a local development environment. The associating docker-compose file can be found in the GitHub repository for this post.\nKafka Cluster We can create a single node Kafka cluster as a docker-compose service with Zookeeper, which is used to store metadata about the cluster. The Bitnami images are used to build the services as shown below.\n1# docker-compose.yml 2services: 3 zookeeper: 4 image: bitnami/zookeeper:3.7.0 5 container_name: zookeeper 6 ports: 7 - \u0026#34;2181:2181\u0026#34; 8 environment: 9 - ALLOW_ANONYMOUS_LOGIN=yes 10 kafka: 11 image: bitnami/kafka:2.8.1 12 container_name: kafka 13 ports: 14 - \u0026#34;9092:9092\u0026#34; 15 environment: 16 - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181 17 - ALLOW_PLAINTEXT_LISTENER=yes 18 depends_on: 19 - zookeeper 20 ... Kafka Connect The same Bitnami image can be used to create a Kafka connect service. It is set to run in the distributed mode so that multiple connectors can be deployed together. Three Kafka connector sources are mapped to the /opt/connectors folder of the container - Debezium, Confluent S3 Sink and Voluble. Note that this folder is added to the plugin path of the connector configuration file (connect-distributed.properties) so that they can be discovered when it is requested to create those. Also, a script is created to download the connector sources (and related libraries) to the connect/local/src folder - it\u0026rsquo;ll be illustrated below. Finally, my AWS account is configured by AWS SSO so that temporary AWS credentials are passed as environment variables - it is necessary for the S3 sink connector.\n1# docker-compose.yml 2services: 3 ... 4 kafka-connect: 5 image: bitnami/kafka:2.8.1 6 container_name: connect 7 command: \u0026gt; 8 /opt/bitnami/kafka/bin/connect-distributed.sh 9 /opt/bitnami/kafka/config/connect-distributed.properties 10 ports: 11 - \u0026#34;8083:8083\u0026#34; 12 environment: 13 AWS_ACCESS_KEY_ID: $AWS_ACCESS_KEY_ID 14 AWS_SECRET_ACCESS_KEY: $AWS_SECRET_ACCESS_KEY 15 AWS_SESSION_TOKEN: $AWS_SESSION_TOKEN 16 volumes: 17 - \u0026#34;./connect/local/src/debezium-connector-postgres:/opt/connectors/debezium-postgres\u0026#34; 18 - \u0026#34;./connect/local/src/confluent-s3/lib:/opt/connectors/confluent-s3\u0026#34; 19 - \u0026#34;./connect/local/src/voluble/lib:/opt/connectors/voluble\u0026#34; 20 - \u0026#34;./connect/local/config/connect-distributed.properties:/opt/bitnami/kafka/config/connect-distributed.properties\u0026#34; 21 depends_on: 22 - zookeeper 23 - kafka 24 ... When the script (download-connectors.sh) runs, it downloads connector sources from Maven Central and Confluent Hub and decompresses. And the Kafka Connect Avro Converter is packaged together with connector sources, which is necessary for Avro serialization of messages and schema registry integration. Note that, if we run our own Kafka connect, we\u0026rsquo;d add it to one of the folders of the connect service and update its plugin path to enable class discovery. However, we don\u0026rsquo;t have such control on MSK Connect, and we should add the converter source to the individual connectors.\n1# connect/local/download-connectors.sh 2#!/usr/bin/env bash 3 4echo \u0026#34;Add avro converter? (Y/N)\u0026#34; 5read WITH_AVRO 6 7SCRIPT_DIR=\u0026#34;$(cd $(dirname \u0026#34;$0\u0026#34;); pwd)\u0026#34; 8 9SRC_PATH=${SCRIPT_DIR}/src 10rm -rf ${SCRIPT_DIR}/src \u0026amp;\u0026amp; mkdir -p ${SRC_PATH} 11 12## Debezium Source Connector 13echo \u0026#34;downloading debezium postgres connector...\u0026#34; 14DOWNLOAD_URL=https://repo1.maven.org/maven2/io/debezium/debezium-connector-postgres/1.8.1.Final/debezium-connector-postgres-1.8.1.Final-plugin.tar.gz 15 16curl -S -L ${DOWNLOAD_URL} | tar -C ${SRC_PATH} --warning=no-unknown-keyword -xzf - 17 18## Confluent S3 Sink Connector 19echo \u0026#34;downloading confluent s3 connector...\u0026#34; 20DOWNLOAD_URL=https://d1i4a15mxbxib1.cloudfront.net/api/plugins/confluentinc/kafka-connect-s3/versions/10.0.5/confluentinc-kafka-connect-s3-10.0.5.zip 21 22curl ${DOWNLOAD_URL} -o ${SRC_PATH}/confluent.zip \\ 23 \u0026amp;\u0026amp; unzip -qq ${SRC_PATH}/confluent.zip -d ${SRC_PATH} \\ 24 \u0026amp;\u0026amp; rm ${SRC_PATH}/confluent.zip \\ 25 \u0026amp;\u0026amp; mv ${SRC_PATH}/$(ls ${SRC_PATH} | grep confluentinc-kafka-connect-s3) ${SRC_PATH}/confluent-s3 26 27## Voluble Source Connector 28echo \u0026#34;downloading voluble connector...\u0026#34; 29DOWNLOAD_URL=https://d1i4a15mxbxib1.cloudfront.net/api/plugins/mdrogalis/voluble/versions/0.3.1/mdrogalis-voluble-0.3.1.zip 30 31curl ${DOWNLOAD_URL} -o ${SRC_PATH}/voluble.zip \\ 32 \u0026amp;\u0026amp; unzip -qq ${SRC_PATH}/voluble.zip -d ${SRC_PATH} \\ 33 \u0026amp;\u0026amp; rm ${SRC_PATH}/voluble.zip \\ 34 \u0026amp;\u0026amp; mv ${SRC_PATH}/$(ls ${SRC_PATH} | grep mdrogalis-voluble) ${SRC_PATH}/voluble 35 36if [ ${WITH_AVRO} == \u0026#34;Y\u0026#34; ]; then 37 echo \u0026#34;downloading kafka connect avro converter...\u0026#34; 38 DOWNLOAD_URL=https://d1i4a15mxbxib1.cloudfront.net/api/plugins/confluentinc/kafka-connect-avro-converter/versions/6.0.3/confluentinc-kafka-connect-avro-converter-6.0.3.zip 39 40 curl ${DOWNLOAD_URL} -o ${SRC_PATH}/avro.zip \\ 41 \u0026amp;\u0026amp; unzip -qq ${SRC_PATH}/avro.zip -d ${SRC_PATH} \\ 42 \u0026amp;\u0026amp; rm ${SRC_PATH}/avro.zip \\ 43 \u0026amp;\u0026amp; mv ${SRC_PATH}/$(ls ${SRC_PATH} | grep confluentinc-kafka-connect-avro-converter) ${SRC_PATH}/avro 44 45 echo \u0026#34;copying to connectors...\u0026#34; 46 cp -r ${SRC_PATH}/avro/lib/* ${SRC_PATH}/debezium-connector-postgres 47 cp -r ${SRC_PATH}/avro/lib/* ${SRC_PATH}/confluent-s3/lib 48 cp -r ${SRC_PATH}/avro/lib/* ${SRC_PATH}/voluble/lib 49fi Schema Registry The Confluent schema registry uses Kafka as the storage backend, and it is not sure whether it supports IAM authentication. Therefore, the Apicurio Registry is used instead as it supports a SQL database as storage as well. It provides multiple APIs and one of them is compatible with the Confluent schema registry (/apis/ccompat/v6). We\u0026rsquo;ll use this API as we plan to use the Confluent version of Avro converter and schema registry client. The PostgreSQL database will be used for the artifact store of the registry as well as the source database of the Debezium connector. For Debezium, the pgoutput plugin is used so that logical replication is enabled (wal_level=logical). The NorthWind database is used as the source database - see this post for details. The registry service expects database connection details from environment variables, and it is set to wait until the source database is up and running.\n1# docker-compose.yml 2services: 3 ... 4 postgres: 5 image: postgres:13 6 container_name: postgres 7 command: [\u0026#34;postgres\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;wal_level=logical\u0026#34;] 8 ports: 9 - 5432:5432 10 volumes: 11 - ./connect/data/sql:/docker-entrypoint-initdb.d 12 environment: 13 - POSTGRES_DB=main 14 - POSTGRES_USER=master 15 - POSTGRES_PASSWORD=password 16 registry: 17 image: apicurio/apicurio-registry-sql:2.2.0.Final 18 container_name: registry 19 command: bash -c \u0026#39;while !\u0026lt;/dev/tcp/postgres/5432; do sleep 1; done; /usr/local/s2i/run\u0026#39; 20 ports: 21 - \u0026#34;9090:8080\u0026#34; 22 environment: 23 REGISTRY_DATASOURCE_URL: \u0026#34;jdbc:postgresql://postgres/main?currentSchema=registry\u0026#34; 24 REGISTRY_DATASOURCE_USERNAME: master 25 REGISTRY_DATASOURCE_PASSWORD: password 26 depends_on: 27 - zookeeper 28 - kafka 29 - postgres 30 ... Once started, we see the Confluent schema registry compatible API from the API list, and we\u0026rsquo;ll use it for creating Kafka connectors.\nKafka UI Having a good user interface can make development much easier and pleasant. The Kafka UI is an open source application where we can monitor and manage Kafka brokers, related objects and resources. It supports MSK IAM authentication as well, and it is a good choice for developing applications on MSK. It allows adding details of one or more Kafka clusters as environment variables. We only have a single Kafka cluster and details of the cluster and related resources are added as shown below.\n1# docker-compose.yml 2services: 3 ... 4 kafka-ui: 5 image: provectuslabs/kafka-ui:0.3.3 6 container_name: kafka-ui 7 ports: 8 - \u0026#34;8080:8080\u0026#34; 9 environment: 10 KAFKA_CLUSTERS_0_NAME: local 11 KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:9092 12 KAFKA_CLUSTERS_0_ZOOKEEPER: zookeeper:2181 13 KAFKA_CLUSTERS_0_SCHEMAREGISTRY: http://registry:8080/apis/ccompat/v6 14 KAFKA_CLUSTERS_0_KAFKACONNECT_0_NAME: local 15 KAFKA_CLUSTERS_0_KAFKACONNECT_0_ADDRESS: http://kafka-connect:8083 16 depends_on: 17 - zookeeper 18 - kafka The UI is quite intuitive, and we can monitor (and manage) the Kafka cluster and related objects/resources comprehensively.\nCreate Connectors The Debezium source connector can be created as shown below. The configuration details can be found in one of the earlier posts. Here the main difference is the key and value converters are set to the Confluent Avro converter in order for the key and value to be serialized into the Avro format. Note the Confluent compatible API is added to the registry URL.\n1curl -i -X POST -H \u0026#34;Accept:application/json\u0026#34; -H \u0026#34;Content-Type:application/json\u0026#34; \\ 2 http://localhost:8083/connectors/ \\ 3 -d \u0026#39;{ 4 \u0026#34;name\u0026#34;: \u0026#34;orders-source\u0026#34;, 5 \u0026#34;config\u0026#34;: { 6 \u0026#34;connector.class\u0026#34;: \u0026#34;io.debezium.connector.postgresql.PostgresConnector\u0026#34;, 7 \u0026#34;tasks.max\u0026#34;: \u0026#34;1\u0026#34;, 8 \u0026#34;plugin.name\u0026#34;: \u0026#34;pgoutput\u0026#34;, 9 \u0026#34;publication.name\u0026#34;: \u0026#34;cdc_publication\u0026#34;, 10 \u0026#34;slot.name\u0026#34;: \u0026#34;orders\u0026#34;, 11 \u0026#34;database.hostname\u0026#34;: \u0026#34;postgres\u0026#34;, 12 \u0026#34;database.port\u0026#34;: \u0026#34;5432\u0026#34;, 13 \u0026#34;database.user\u0026#34;: \u0026#34;master\u0026#34;, 14 \u0026#34;database.password\u0026#34;: \u0026#34;password\u0026#34;, 15 \u0026#34;database.dbname\u0026#34;: \u0026#34;main\u0026#34;, 16 \u0026#34;database.server.name\u0026#34;: \u0026#34;ord\u0026#34;, 17 \u0026#34;schema.include\u0026#34;: \u0026#34;ods\u0026#34;, 18 \u0026#34;table.include.list\u0026#34;: \u0026#34;ods.cdc_events\u0026#34;, 19 \u0026#34;key.converter\u0026#34;: \u0026#34;io.confluent.connect.avro.AvroConverter\u0026#34;, 20 \u0026#34;key.converter.schema.registry.url\u0026#34;: \u0026#34;http://registry:8080/apis/ccompat/v6\u0026#34;, 21 \u0026#34;value.converter\u0026#34;: \u0026#34;io.confluent.connect.avro.AvroConverter\u0026#34;, 22 \u0026#34;value.converter.schema.registry.url\u0026#34;: \u0026#34;http://registry:8080/apis/ccompat/v6\u0026#34;, 23 \u0026#34;transforms\u0026#34;: \u0026#34;unwrap\u0026#34;, 24 \u0026#34;transforms.unwrap.type\u0026#34;: \u0026#34;io.debezium.transforms.ExtractNewRecordState\u0026#34;, 25 \u0026#34;transforms.unwrap.drop.tombstones\u0026#34;: \u0026#34;false\u0026#34;, 26 \u0026#34;transforms.unwrap.delete.handling.mode\u0026#34;: \u0026#34;rewrite\u0026#34;, 27 \u0026#34;transforms.unwrap.add.fields\u0026#34;: \u0026#34;op,db,table,schema,lsn,source.ts_ms\u0026#34; 28 } 29 }\u0026#39; The Confluent S3 sink connector is used instead of the Lenses S3 sink connector because the Lenses connector doesn\u0026rsquo;t work with the Kafka Connect Avro Converter. Here the key and value converters are updated to the Confluent Avro converter with the corresponding schema registry URL.\n1curl -i -X POST -H \u0026#34;Accept:application/json\u0026#34; -H \u0026#34;Content-Type:application/json\u0026#34; \\ 2 http://localhost:8083/connectors/ \\ 3 -d \u0026#39;{ 4 \u0026#34;name\u0026#34;: \u0026#34;orders-sink\u0026#34;, 5 \u0026#34;config\u0026#34;: { 6 \u0026#34;connector.class\u0026#34;: \u0026#34;io.confluent.connect.s3.S3SinkConnector\u0026#34;, 7 \u0026#34;storage.class\u0026#34;: \u0026#34;io.confluent.connect.s3.storage.S3Storage\u0026#34;, 8 \u0026#34;format.class\u0026#34;: \u0026#34;io.confluent.connect.s3.format.avro.AvroFormat\u0026#34;, 9 \u0026#34;tasks.max\u0026#34;: \u0026#34;1\u0026#34;, 10 \u0026#34;topics\u0026#34;:\u0026#34;ord.ods.cdc_events\u0026#34;, 11 \u0026#34;s3.bucket.name\u0026#34;: \u0026#34;analytics-data-590312749310-ap-southeast-2\u0026#34;, 12 \u0026#34;s3.region\u0026#34;: \u0026#34;ap-southeast-2\u0026#34;, 13 \u0026#34;flush.size\u0026#34;: \u0026#34;100\u0026#34;, 14 \u0026#34;rotate.schedule.interval.ms\u0026#34;: \u0026#34;60000\u0026#34;, 15 \u0026#34;timezone\u0026#34;: \u0026#34;Australia/Sydney\u0026#34;, 16 \u0026#34;partitioner.class\u0026#34;: \u0026#34;io.confluent.connect.storage.partitioner.DefaultPartitioner\u0026#34;, 17 \u0026#34;key.converter\u0026#34;: \u0026#34;io.confluent.connect.avro.AvroConverter\u0026#34;, 18 \u0026#34;key.converter.schema.registry.url\u0026#34;: \u0026#34;http://registry:8080/apis/ccompat/v6\u0026#34;, 19 \u0026#34;value.converter\u0026#34;: \u0026#34;io.confluent.connect.avro.AvroConverter\u0026#34;, 20 \u0026#34;value.converter.schema.registry.url\u0026#34;: \u0026#34;http://registry:8080/apis/ccompat/v6\u0026#34;, 21 \u0026#34;errors.log.enable\u0026#34;: \u0026#34;true\u0026#34; 22 } 23 }\u0026#39; Once the source connector is created, we can check that the key and value schemas are created as shown below. Note we can check the details of the schemas by clicking the relevant items.\nAs we added the schema registry URL as an environment variable, we see the records (key and value) are properly deserialized within the UI.\nSchema Evolution The schema registry keeps multiple versions of schemas and we can check it by adding a column to the table and updating records.\n1--// add a column with a default value 2ALTER TABLE ods.cdc_events 3 ADD COLUMN employee_id int DEFAULT -1; 4 5--// update employee ID 6UPDATE ods.cdc_events 7 SET employee_id = (employee -\u0026gt;\u0026gt; \u0026#39;employee_id\u0026#39;)::INT 8WHERE customer_id = \u0026#39;VINET\u0026#39; Once the above queries are executed, we see a new version is added to the topic\u0026rsquo;s value schema, and it includes the new field.\nSummary In this post, we discussed an improved architecture of a Change Data Capture (CDC) solution with a schema registry. A local development environment is set up using Docker Compose. The Debezium and Confluent S3 connectors are deployed with the Confluent Avro converter and the Apicurio registry is used as the schema registry service. A quick example is shown to illustrate how schema evolution can be managed by the schema registry. In the next post, it\u0026rsquo;ll be deployed to AWS mainly using MSK Connect, Aurora PostgreSQL and ECS.\n","date":"March 7, 2022","img":"/blog/2022-03-07-schema-registry-part1/featured.png","lang":"en","langName":"English","largeImg":"/blog/2022-03-07-schema-registry-part1/featured_hu55ef0aeef397b55768e961367d004bbc_59689_500x0_resize_box_3.png","permalink":"/blog/2022-03-07-schema-registry-part1/","series":[{"title":"Integrate Schema Registry With MSK Connect","url":"/series/integrate-schema-registry-with-msk-connect/"}],"smallImg":"/blog/2022-03-07-schema-registry-part1/featured_hu55ef0aeef397b55768e961367d004bbc_59689_180x0_resize_box_3.png","tags":[{"title":"AWS","url":"/tags/aws/"},{"title":"Amazon MSK","url":"/tags/amazon-msk/"},{"title":"Amazon MSK Connect","url":"/tags/amazon-msk-connect/"},{"title":"Apache Kafka","url":"/tags/apache-kafka/"},{"title":"Kafka Connect","url":"/tags/kafka-connect/"},{"title":"Docker","url":"/tags/docker/"},{"title":"Docker Compose","url":"/tags/docker-compose/"}],"timestamp":1646611200,"title":"Use External Schema Registry With MSK Connect – Part 1 Local Development"},{"categories":[{"title":"Development","url":"/categories/development/"}],"content":"When I wrote my data lake demo series (part 1, part 2 and part 3) recently, I used an Aurora PostgreSQL, MSK and EMR cluster. All of them were deployed to private subnets and dedicated infrastructure was created using CloudFormation. Using the infrastructure as code (IaC) tool helped a lot, but it resulted in creating 7 CloudFormation stacks, which was a bit harder to manage in the end. Then I looked into how to simplify building infrastructure and managing resources on AWS and decided to use Terraform instead. I find it has useful constructs (e.g. meta-arguments) to make it simpler to create and manage resources. It also has a wide range of useful modules that facilitate development significantly. In this post, we’ll build an infrastructure for development on AWS with Terraform. A VPN server will also be included in order to improve developer experience by accessing resources in private subnets from developer machines.\n[UPDATE 2023-10-13]\nIn later projects, the VPN admin password and VPN pre shared key are auto-generated and saved as a secret in AWS Secrets Manager. The changes are added to VPN section. Architecture The infrastructure that we’ll discuss in this post is shown below. The database is deployed in a private subnet, and it is not possible to access it from the developer machine. We can construct a PC-to-PC VPN with SoftEther VPN. The VPN server runs in a public subnet, and it is managed by an autoscaling group where only a single instance will be maintained. An elastic IP address is associated by a bootstrap script so that its public IP doesn\u0026rsquo;t change even if the EC2 instance is recreated. We can add users with the server manager program, and they can access the server with the client program. Access from the VPN server to the database is allowed by adding an inbound rule where the source security group ID is set to the VPN server’s security group ID. Note that another option is AWS Client VPN, but it is way more expensive. We’ll create 2 private subnets, and it’ll cost $0.30/hour for endpoint association in the Sydney region. It also charges $0.05/hour for each connection and the minimum charge will be $0.35/hour. On the other hand, the SorftEther VPN server runs in the t3.nano instance and its cost is only $0.0066/hour.\nEven developing a single database can result in a stack of resources and Terraform can be of great help to create and manage those resources. Also, VPN can improve developer experience significantly as it helps access them from developer machines. In this post, it’ll be illustrated how to access a database but access to other resources such as MSK, EMR, ECS and EKS can also be made.\nInfrastructure Terraform can be installed in multiple ways and the CLI has intuitive commands to manage AWS infrastructure. Key commands are\ninit - It is used to initialize a working directory containing Terraform configuration files. plan - It creates an execution plan, which lets you preview the changes that Terraform plans to make to your infrastructure. apply - It executes the actions proposed in a Terraform plan. destroy - It is a convenient way to destroy all remote objects managed by a particular Terraform configuration. The GitHub repository for this post has the following directory structure. Terraform resources are grouped into 4 files, and they’ll be discussed further below. The remaining files are supporting elements and their details can be found in the language reference.\n1$ tree 2. 3├── README.md 4├── _data.tf 5├── _outputs.tf 6├── _providers.tf 7├── _variables.tf 8├── aurora.tf 9├── keypair.tf 10├── scripts 11│ └── bootstrap.sh 12├── vpc.tf 13└── vpn.tf 14 151 directory, 10 files VPC We can use the AWS VPC module to construct a VPC. A Terraform module is a container for multiple resources, and it makes it easier to manage related resources. A VPC with 2 availability zones is defined and private/public subnets are configured to each of them. Optionally a NAT gateway is added only to a single availability zone.\n1# vpc.tf 2module \u0026#34;vpc\u0026#34; { 3 source = \u0026#34;terraform-aws-modules/vpc/aws\u0026#34; 4 5 name = \u0026#34;${local.resource_prefix}-vpc\u0026#34; 6 cidr = \u0026#34;10.${var.class_b}.0.0/16\u0026#34; 7 8 azs = [\u0026#34;${var.aws_region}a\u0026#34;, \u0026#34;${var.aws_region}b\u0026#34;] 9 private_subnets = [\u0026#34;10.${var.class_b}.0.0/19\u0026#34;, \u0026#34;10.${var.class_b}.32.0/19\u0026#34;] 10 public_subnets = [\u0026#34;10.${var.class_b}.64.0/19\u0026#34;, \u0026#34;10.${var.class_b}.96.0/19\u0026#34;] 11 12 enable_nat_gateway = true 13 single_nat_gateway = true 14 one_nat_gateway_per_az = false 15} Key Pair An optional key pair is created. It can be used to access an EC2 instance via SSH. The PEM file will be saved to the key-pair folder once created.\n1# keypair.tf 2resource \u0026#34;tls_private_key\u0026#34; \u0026#34;pk\u0026#34; { 3 count = var.key_pair_create ? 1 : 0 4 algorithm = \u0026#34;RSA\u0026#34; 5 rsa_bits = 4096 6} 7 8resource \u0026#34;aws_key_pair\u0026#34; \u0026#34;key_pair\u0026#34; { 9 count = var.key_pair_create ? 1 : 0 10 key_name = \u0026#34;${local.resource_prefix}-key\u0026#34; 11 public_key = tls_private_key.pk[0].public_key_openssh 12} 13 14resource \u0026#34;local_file\u0026#34; \u0026#34;pem_file\u0026#34; { 15 count = var.key_pair_create ? 1 : 0 16 filename = pathexpand(\u0026#34;${path.module}/key-pair/${local.resource_prefix}-key.pem\u0026#34;) 17 file_permission = \u0026#34;0400\u0026#34; 18 sensitive_content = tls_private_key.pk[0].private_key_pem 19} VPN The AWS Auto Scaling Group (ASG) module is used to manage the SoftEther VPN server. The ASG maintains a single EC2 instance in one of the public subnets. The user data script (bootstrap.sh) is configured to run at launch and it’ll be discussed below. Note that there are other resources that are necessary to make the VPN server to work correctly and those can be found in the vpn.tf. Also note that the VPN resource requires a number of configuration values. While most of them have default values or are automatically determined, the IPsec Pre-Shared key (vpn_psk) and administrator password (admin_password) do not have default values. They need to be specified while running the plan, apply and _destroy _commands. Finally, if the variable vpn_limit_ingress is set to true, the inbound rules of the VPN security group is limited to the running machine’s IP address.\n1# _variables.tf 2variable \u0026#34;vpn_create\u0026#34; { 3 description = \u0026#34;Whether to create a VPN instance\u0026#34; 4 default = true 5} 6 7variable \u0026#34;vpn_limit_ingress\u0026#34; { 8 description = \u0026#34;Whether to limit the CIDR block of VPN security group inbound rules.\u0026#34; 9 default = true 10} 11 12variable \u0026#34;vpn_use_spot\u0026#34; { 13 description = \u0026#34;Whether to use spot or on-demand EC2 instance\u0026#34; 14 default = false 15} 16 17variable \u0026#34;vpn_psk\u0026#34; { 18 description = \u0026#34;The IPsec Pre-Shared Key\u0026#34; 19 type = string 20 sensitive = true 21} 22 23variable \u0026#34;admin_password\u0026#34; { 24 description = \u0026#34;SoftEther VPN admin / database master password\u0026#34; 25 type = string 26 sensitive = true 27} 28 29locals { 30 ... 31 local_ip_address = \u0026#34;${chomp(data.http.local_ip_address.body)}/32\u0026#34; 32 vpn_ingress_cidr = var.vpn_limit_ingress ? local.local_ip_address : \u0026#34;0.0.0.0/0\u0026#34; 33 vpn_spot_override = [ 34 { instance_type: \u0026#34;t3.nano\u0026#34; }, 35 { instance_type: \u0026#34;t3a.nano\u0026#34; }, 36 ] 37} 38 39# vpn.tf 40module \u0026#34;vpn\u0026#34; { 41 source = \u0026#34;terraform-aws-modules/autoscaling/aws\u0026#34; 42 count = var.vpn_create ? 1 : 0 43 44 name = \u0026#34;${local.resource_prefix}-vpn-asg\u0026#34; 45 46 key_name = var.key_pair_create ? aws_key_pair.key_pair[0].key_name : null 47 vpc_zone_identifier = module.vpc.public_subnets 48 min_size = 1 49 max_size = 1 50 desired_capacity = 1 51 52 image_id = data.aws_ami.amazon_linux_2.id 53 instance_type = element([for s in local.vpn_spot_override: s.instance_type], 0) 54 security_groups = [aws_security_group.vpn[0].id] 55 iam_instance_profile_arn = aws_iam_instance_profile.vpn[0].arn 56 57 # Launch template 58 create_lt = true 59 update_default_version = true 60 61 user_data_base64 = base64encode(join(\u0026#34;\\n\u0026#34;, [ 62 \u0026#34;#cloud-config\u0026#34;, 63 yamlencode({ 64 # https://cloudinit.readthedocs.io/en/latest/topics/modules.html 65 write_files : [ 66 { 67 path : \u0026#34;/opt/vpn/bootstrap.sh\u0026#34;, 68 content : templatefile(\u0026#34;${path.module}/scripts/bootstrap.sh\u0026#34;, { 69 aws_region = var.aws_region, 70 allocation_id = aws_eip.vpn[0].allocation_id, 71 vpn_psk = var.vpn_psk, 72 admin_password = var.admin_password 73 }), 74 permissions : \u0026#34;0755\u0026#34;, 75 } 76 ], 77 runcmd : [ 78 [\u0026#34;/opt/vpn/bootstrap.sh\u0026#34;], 79 ], 80 }) 81 ])) 82 83 # Mixed instances 84 use_mixed_instances_policy = true 85 mixed_instances_policy = { 86 instances_distribution = { 87 on_demand_base_capacity = var.vpn_use_spot ? 0 : 1 88 on_demand_percentage_above_base_capacity = var.vpn_use_spot ? 0 : 100 89 spot_allocation_strategy = \u0026#34;capacity-optimized\u0026#34; 90 } 91 override = local.vpn_spot_override 92 } 93 94 tags_as_map = { 95 \u0026#34;Name\u0026#34; = \u0026#34;${local.resource_prefix}-vpn-asg\u0026#34; 96 } 97} 98 99resource \u0026#34;aws_eip\u0026#34; \u0026#34;vpn\u0026#34; { 100 count = var.vpn_create ? 1 : 0 101 tags = { 102 \u0026#34;Name\u0026#34; = \u0026#34;${local.resource_prefix}-vpn-eip\u0026#34; 103 } 104} 105 106... The bootstrap script associates the elastic IP address followed by starting the SoftEther VPN server by a Docker container. It accepts the pre-shared key (vpn_psk) and administrator password (admin_password) as environment variables. Also, the Virtual Hub name is set to DEFAULT.\n1# scripts/bootstrap.sh 2#!/bin/bash -ex 3 4## Allocate elastic IP and disable source/destination checks 5TOKEN=$(curl --silent --max-time 60 -X PUT http://169.254.169.254/latest/api/token -H \u0026#34;X-aws-ec2-metadata-token-ttl-seconds: 30\u0026#34;) 6INSTANCEID=$(curl --silent --max-time 60 -H \u0026#34;X-aws-ec2-metadata-token: $TOKEN\u0026#34; http://169.254.169.254/latest/meta-data/instance-id) 7aws --region ${aws_region} ec2 associate-address --instance-id $INSTANCEID --allocation-id ${allocation_id} 8aws --region ${aws_region} ec2 modify-instance-attribute --instance-id $INSTANCEID --source-dest-check \u0026#34;{\\\u0026#34;Value\\\u0026#34;: false}\u0026#34; 9 10## Start SoftEther VPN server 11yum update -y \u0026amp;\u0026amp; yum install docker -y 12systemctl enable docker.service \u0026amp;\u0026amp; systemctl start docker.service 13 14docker pull siomiz/softethervpn:debian 15docker run -d \\ 16 --cap-add NET_ADMIN \\ 17 --name softethervpn \\ 18 --restart unless-stopped \\ 19 -p 500:500/udp -p 4500:4500/udp -p 1701:1701/tcp -p 1194:1194/udp -p 5555:5555/tcp -p 443:443/tcp \\ 20 -e PSK=${vpn_psk} \\ 21 -e SPW=${admin_password} \\ 22 -e HPW=DEFAULT \\ 23 siomiz/softethervpn:debian [UPDATE 2023-10-13] In later projects, the VPN admin password and VPN pre shared key are auto-generated and saved as a secret in AWS Secrets Manager. The secret is named as \u0026quot;${local.name}-vpn-secrets\u0026quot; and can be obtained on AWS Console. Or the VPN secret ID are added to a Terraform output value, it can be obtained as shown below.\n1aws secretsmanager get-secret-value \\ 2 --secret-id $(terraform output -raw vpn_secret_id) | jq -c \u0026#39;.SecretString | fromjson\u0026#39; 3# {\u0026#34;vpn_pre_shared_key\u0026#34;:\u0026#34;xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\u0026#34;,\u0026#34;vpn_admin_password\u0026#34;:\u0026#34;xxxxxxxxxxxxxxxx\u0026#34;} Below shows relevant Terraform changes.\n1# vpn.tf 2module \u0026#34;vpn\u0026#34; { 3 source = \u0026#34;terraform-aws-modules/autoscaling/aws\u0026#34; 4 version = \u0026#34;~\u0026gt; 6.5\u0026#34; 5 count = local.vpn.to_create ? 1 : 0 6 7 name = \u0026#34;${local.name}-vpn-asg\u0026#34; 8 9 ... 10 11 # Launch template 12 create_launch_template = true 13 update_default_version = true 14 15 user_data = base64encode(join(\u0026#34;\\n\u0026#34;, [ 16 \u0026#34;#cloud-config\u0026#34;, 17 yamlencode({ 18 # https://cloudinit.readthedocs.io/en/latest/topics/modules.html 19 write_files : [ 20 { 21 path : \u0026#34;/opt/vpn/bootstrap.sh\u0026#34;, 22 content : templatefile(\u0026#34;${path.module}/scripts/bootstrap.sh\u0026#34;, { 23 aws_region = local.region, 24 allocation_id = aws_eip.vpn[0].allocation_id, 25 vpn_psk = \u0026#34;${random_password.vpn_pre_shared_key[0].result}\u0026#34;, # \u0026lt;- internally generated value 26 admin_password = \u0026#34;${random_password.vpn_admin_pw[0].result}\u0026#34; # \u0026lt;- internally generated value 27 }), 28 permissions : \u0026#34;0755\u0026#34;, 29 } 30 ], 31 runcmd : [ 32 [\u0026#34;/opt/vpn/bootstrap.sh\u0026#34;], 33 ], 34 }) 35 ])) 36 37 ... 38 39 tags = local.tags 40} 41 42... 43 44## create VPN secrets - IPsec Pre-Shared Key and admin password for VPN 45## see https://cloud.google.com/network-connectivity/docs/vpn/how-to/generating-pre-shared-key 46resource \u0026#34;random_password\u0026#34; \u0026#34;vpn_pre_shared_key\u0026#34; { 47 count = local.vpn.to_create ? 1 : 0 48 length = 32 49 override_special = \u0026#34;/+\u0026#34; 50} 51 52resource \u0026#34;random_password\u0026#34; \u0026#34;vpn_admin_pw\u0026#34; { 53 count = local.vpn.to_create ? 1 : 0 54 length = 16 55 special = false 56} 57 58resource \u0026#34;aws_secretsmanager_secret\u0026#34; \u0026#34;vpn_secrets\u0026#34; { 59 count = local.vpn.to_create ? 1 : 0 60 name = \u0026#34;${local.name}-vpn-secrets\u0026#34; 61 description = \u0026#34;Service Account Password for the API\u0026#34; 62 recovery_window_in_days = 0 63 64 tags = local.tags 65} 66 67resource \u0026#34;aws_secretsmanager_secret_version\u0026#34; \u0026#34;vpn_secrets\u0026#34; { 68 count = local.vpn.to_create ? 1 : 0 69 secret_id = aws_secretsmanager_secret.vpn_secrets[0].id 70 secret_string = \u0026lt;\u0026lt;EOF 71 { 72 \u0026#34;vpn_pre_shared_key\u0026#34;: \u0026#34;${random_password.vpn_pre_shared_key[0].result}\u0026#34;, 73 \u0026#34;vpn_admin_password\u0026#34;: \u0026#34;${random_password.vpn_admin_pw[0].result}\u0026#34; 74 } 75EOF 76} 77 78resource \u0026#34;tls_private_key\u0026#34; \u0026#34;pk\u0026#34; { 79 count = local.vpn.to_create ? 1 : 0 80 algorithm = \u0026#34;RSA\u0026#34; 81 rsa_bits = 4096 82} 83 84resource \u0026#34;aws_key_pair\u0026#34; \u0026#34;key_pair\u0026#34; { 85 count = local.vpn.to_create ? 1 : 0 86 key_name = \u0026#34;${local.name}-vpn-key\u0026#34; 87 public_key = tls_private_key.pk[0].public_key_openssh 88} 89 90... 91 92# outputs.tf 93... 94 95output \u0026#34;vpn_secret_id\u0026#34; { 96 description = \u0026#34;VPN secret ID\u0026#34; 97 value = local.vpn.to_create ? aws_secretsmanager_secret.vpn_secrets[0].id : null 98} 99 100output \u0026#34;vpn_secret_version\u0026#34; { 101 description = \u0026#34;VPN secret version ID\u0026#34; 102 value = local.vpn.to_create ? aws_secretsmanager_secret_version.vpn_secrets[0].version_id : null 103} 104... Database An Aurora PostgreSQL cluster is created using the AWS RDS Aurora module. It is set to have only a single instance and is deployed to a private subnet. Note that a security group (vpn_access) is created that allows access from the VPN server, and it is added to vpc_security_group_ids.\n1# aurora.tf 2module \u0026#34;aurora\u0026#34; { 3 source = \u0026#34;terraform-aws-modules/rds-aurora/aws\u0026#34; 4 5 name = \u0026#34;${local.resource_prefix}-db-cluster\u0026#34; 6 engine = \u0026#34;aurora-postgresql\u0026#34; 7 engine_version = \u0026#34;13\u0026#34; 8 auto_minor_version_upgrade = false 9 10 instances = { 11 1 = { 12 instance_class = \u0026#34;db.t3.medium\u0026#34; 13 } 14 } 15 16 vpc_id = module.vpc.vpc_id 17 db_subnet_group_name = aws_db_subnet_group.aurora.id 18 create_db_subnet_group = false 19 create_security_group = true 20 vpc_security_group_ids = [aws_security_group.vpn_access.id] 21 22 iam_database_authentication_enabled = false 23 create_random_password = false 24 master_password = var.admin_password 25 database_name = local.database_name 26 27 apply_immediately = true 28 skip_final_snapshot = true 29 30 db_cluster_parameter_group_name = aws_rds_cluster_parameter_group.aurora.id 31 enabled_cloudwatch_logs_exports = [\u0026#34;postgresql\u0026#34;] 32 33 tags = { 34 Name = \u0026#34;${local.resource_prefix}-db-cluster\u0026#34; 35 } 36} 37 38resource \u0026#34;aws_db_subnet_group\u0026#34; \u0026#34;aurora\u0026#34; { 39 name = \u0026#34;${local.resource_prefix}-db-subnet-group\u0026#34; 40 subnet_ids = module.vpc.private_subnets 41 42 tags = { 43 Name = \u0026#34;${local.resource_prefix}-db-subnet-group\u0026#34; 44 } 45} 46 47... 48 49resource \u0026#34;aws_security_group\u0026#34; \u0026#34;vpn_access\u0026#34; { 50 name = \u0026#34;${local.resource_prefix}-db-security-group\u0026#34; 51 vpc_id = module.vpc.vpc_id 52 53 lifecycle { 54 create_before_destroy = true 55 } 56} 57 58resource \u0026#34;aws_security_group_rule\u0026#34; \u0026#34;aurora_vpn_inbound\u0026#34; { 59 count = var.vpn_create ? 1 : 0 60 type = \u0026#34;ingress\u0026#34; 61 description = \u0026#34;VPN access\u0026#34; 62 security_group_id = aws_security_group.vpn_access.id 63 protocol = \u0026#34;tcp\u0026#34; 64 from_port = \u0026#34;5432\u0026#34; 65 to_port = \u0026#34;5432\u0026#34; 66 source_security_group_id = aws_security_group.vpn[0].id 67} VPN Configuration Both the VPN Server Manager and Client can be obtained from the download centre. The server and client configuration are illustrated below.\nVPN Server We can begin with adding a new setting.\nWe need to fill in the input fields in the red boxes below. It’s possible to use the elastic IP address as the host name and the administrator password should match to what is used for Terraform.\nThen we can make a connection to the server by clicking the connect button.\nIf it’s the first attempt, we’ll see the following pop-up message and we can click yes to set up the IPsec.\nIn the dialog, we just need to enter the IPsec Pre-Shared key and click ok.\nOnce a connection is made successfully, we can manage the Virtual Hub by clicking the manage virtual hub button. Note that we created a Virtual Hub named DEFAULT and the session will be established on that Virtual Hub.\nWe can create a new user by clicking the manage users button.\nAnd clicking the new button.\nFor simplicity, we can use Password Authentication as the auth type and enter the username and password.\nA new user is created, and we can use the credentials on the client program to make a connection to the server.\nVPN Client We can add a VPN connection by clicking the menu shown below.\nWe’ll need to create a Virtual Network Adapter and should click the yes button.\nIn the new dialog, we can add the adapter name and hit ok. Note we should have the administrator privilege to create a new adapter.\nThen a new dialog box will be shown. We can add a connection by entering the input fields in the red boxes below. The VPN server details should match to what are created by Terraform and the user credentials that are created in the previous section can be used.\nOnce a connection is added, we can make a connection to the VPN server by right-clicking the item and clicking the connect menu.\nWe can see that the status is changed into connected.\nOnce the VPN server is connected, we can access the database that is deployed in the private subnet. A connection is tested by a database client, and it is shown that the connection is successful.\nSummary In this post, we discussed how to set up a development infrastructure on AWS with Terraform. Terraform is used as an effective way of managing resources on AWS. An Aurora PostgreSQL cluster is created in a private subnet and SoftEther VPN is configured to access the database from the developer machine.\n","date":"February 6, 2022","img":"/blog/2022-02-06-dev-infra-terraform/featured.png","lang":"en","langName":"English","largeImg":"/blog/2022-02-06-dev-infra-terraform/featured_hube8ddf9cf5c302b7f1d964b6331dc37c_46253_500x0_resize_box_3.png","permalink":"/blog/2022-02-06-dev-infra-terraform/","series":[],"smallImg":"/blog/2022-02-06-dev-infra-terraform/featured_hube8ddf9cf5c302b7f1d964b6331dc37c_46253_180x0_resize_box_3.png","tags":[{"title":"AWS","url":"/tags/aws/"},{"title":"Terraform","url":"/tags/terraform/"},{"title":"VPN","url":"/tags/vpn/"}],"timestamp":1644105600,"title":"Simplify Your Development on AWS With Terraform"},{"categories":[{"title":"Data Engineering","url":"/categories/data-engineering/"}],"content":"EMR on EKS provides a deployment option for Amazon EMR that allows you to automate the provisioning and management of open-source big data frameworks on Amazon EKS. While a wide range of open source big data components are available in EMR on EC2, only Apache Spark is available in EMR on EKS. It is more flexible, however, that applications of different EMR versions can be run in multiple availability zones on either EC2 or Fargate. Also, other types of containerized applications can be deployed on the same EKS cluster. Therefore, if you have or plan to have, for example, Apache Airflow, Apache Superset or Kubeflow as your analytics toolkits, it can be an effective way to manage big data (as well as non-big data) workloads. While Glue is more for ETL, EMR on EKS can also be used for other types of tasks such as machine learning. Moreover, it allows you to build a Spark application, not a Gluish Spark application. For example, while you have to use custom connectors for Hudi or Iceberg for Glue, you can use their native libraries with EMR on EKS. In this post, we\u0026rsquo;ll discuss EMR on EKS with simple and elaborated examples.\nCluster setup and configuration We\u0026rsquo;ll use command line utilities heavily. The following tools are required.\nAWS CLI V2 - it is the official command line interface that enables users to interact with AWS services. eksctl - it is a CLI tool for creating and managing clusters on EKS. kubectl - it is a command line utility for communicating with the cluster API server. Upload preliminary resources to S3 We need supporting files, and they are created/downloaded into the config and manifests folders using a setup script - the script can be found in the project GitHub repository. The generated files will be illustrated below.\n1export OWNER=jaehyeon 2export AWS_REGION=ap-southeast-2 3export CLUSTER_NAME=emr-eks-example 4export EMR_ROLE_NAME=${CLUSTER_NAME}-job-execution 5export S3_BUCKET_NAME=${CLUSTER_NAME}-${AWS_REGION} 6export LOG_GROUP_NAME=/${CLUSTER_NAME} 7 8## run setup script 9# - create config files, sample scripts and download necessary files 10./setup.sh 11 12tree -p config -p manifests 13# config 14# ├── [-rw-r--r--] cdc_events.avsc 15# ├── [-rw-r--r--] cdc_events_s3.properties 16# ├── [-rw-r--r--] driver_pod_template.yml 17# ├── [-rw-r--r--] executor_pod_template.yml 18# ├── [-rw-r--r--] food_establishment_data.csv 19# ├── [-rw-r--r--] health_violations.py 20# └── [-rw-r--r--] hudi-utilities-bundle_2.12-0.10.0.jar 21# manifests 22# ├── [-rw-r--r--] cluster.yaml 23# ├── [-rw-r--r--] nodegroup-spot.yaml 24# └── [-rw-r--r--] nodegroup.yaml We\u0026rsquo;ll configure logging on S3 and CloudWatch so that a S3 bucket and CloudWatch log group are created. Also a Glue database is created as I encountered an error to create a Glue table when the database doesn\u0026rsquo;t exist. Finally the files in the config folder are uploaded to S3.\n1#### create S3 bucket/log group/glue database and upload files to S3 2aws s3 mb s3://${S3_BUCKET_NAME} 3aws logs create-log-group --log-group-name=${LOG_GROUP_NAME} 4aws glue create-database --database-input \u0026#39;{\u0026#34;Name\u0026#34;: \u0026#34;datalake\u0026#34;}\u0026#39; 5 6## upload files to S3 7for f in $(ls ./config/) 8 do 9 aws s3 cp ./config/${f} s3://${S3_BUCKET_NAME}/config/ 10 done 11# upload: config/cdc_events.avsc to s3://emr-eks-example-ap-southeast-2/config/cdc_events.avsc 12# upload: config/cdc_events_s3.properties to s3://emr-eks-example-ap-southeast-2/config/cdc_events_s3.properties 13# upload: config/driver_pod_template.yml to s3://emr-eks-example-ap-southeast-2/config/driver_pod_template.yml 14# upload: config/executor_pod_template.yml to s3://emr-eks-example-ap-southeast-2/config/executor_pod_template.yml 15# upload: config/food_establishment_data.csv to s3://emr-eks-example-ap-southeast-2/config/food_establishment_data.csv 16# upload: config/health_violations.py to s3://emr-eks-example-ap-southeast-2/config/health_violations.py 17# upload: config/hudi-utilities-bundle_2.12-0.10.0.jar to s3://emr-eks-example-ap-southeast-2/config/hudi-utilities-bundle_2.12-0.10.0.jar Create EKS cluster and node group We can use either command line options or a config file when creating a cluster or node group using eksctl. We\u0026rsquo;ll use config files and below shows the corresponding config files.\n1# ./config/cluster.yaml 2--- 3apiVersion: eksctl.io/v1alpha5 4kind: ClusterConfig 5 6metadata: 7 name: emr-eks-example 8 region: ap-southeast-2 9 tags: 10 Owner: jaehyeon 11# ./config/nodegroup.yaml 12--- 13apiVersion: eksctl.io/v1alpha5 14kind: ClusterConfig 15 16metadata: 17 name: emr-eks-example 18 region: ap-southeast-2 19 tags: 20 Owner: jaehyeon 21 22managedNodeGroups: 23- name: nodegroup 24 desiredCapacity: 2 25 instanceType: m5.xlarge _eksctl _creates a cluster or node group via CloudFormation. Each command will create a dedicated CloudFormation stack and it\u0026rsquo;ll take about 15 minutes. Also it generates the default kubeconfig file in the $HOME/.kube folder. Once the node group is created, we can check it using the kubectl command.\n1#### create cluster, node group and configure 2eksctl create cluster -f ./manifests/cluster.yaml 3eksctl create nodegroup -f ./manifests/nodegroup.yaml 4 5kubectl get nodes 6# NAME STATUS ROLES AGE VERSION 7# ip-192-168-33-60.ap-southeast-2.compute.internal Ready \u0026lt;none\u0026gt; 5m52s v1.21.5-eks-bc4871b 8# ip-192-168-95-68.ap-southeast-2.compute.internal Ready \u0026lt;none\u0026gt; 5m49s v1.21.5-eks-bc4871b Set up Amazon EMR on EKS As described in the Amazon EMR on EKS development guide, Amazon EKS uses Kubernetes namespaces to divide cluster resources between multiple users and applications. A virtual cluster is a Kubernetes namespace that Amazon EMR is registered with. Amazon EMR uses virtual clusters to run jobs and host endpoints. The following steps are taken in order to set up for EMR on EKS.\nEnable cluster access for Amazon EMR on EKS After creating a Kubernetes namespace for EMR (spark), it is necessary to allow Amazon EMR on EKS to access the namespace. It can be automated by eksctl and specifically the following actions are performed.\nsetting up RBAC authorization by creating a Kubernetes role and binding the role to a Kubernetes user mapping the Kubernetes user to the EMR on EKS service-linked role 1kubectl create namespace spark 2eksctl create iamidentitymapping --cluster ${CLUSTER_NAME} \\ 3 --namespace spark --service-name \u0026#34;emr-containers\u0026#34; While the details of the role and role binding can be found in the development guide, we can see that the aws-auth ConfigMap is updated with the new Kubernetes user.\n1kubectl describe cm aws-auth -n kube-system 2# Name: aws-auth 3# Namespace: kube-system 4# Labels: \u0026lt;none\u0026gt; 5# Annotations: \u0026lt;none\u0026gt; 6 7# Data 8# ==== 9# mapRoles: 10# ---- 11# - groups: 12# - system:bootstrappers 13# - system:nodes 14# rolearn: arn:aws:iam::\u0026lt;AWS-ACCOUNT-ID\u0026gt;:role/eksctl-emr-eks-example-nodegroup-NodeInstanceRole-15J26FPOYH0AL 15# username: system:node:{{EC2PrivateDNSName}} 16# - rolearn: arn:aws:iam::\u0026lt;AWS-ACCOUNT-ID\u0026gt;:role/AWSServiceRoleForAmazonEMRContainers 17# username: emr-containers 18 19# mapUsers: 20# ---- 21# [] 22 23# Events: \u0026lt;none\u0026gt; Create an IAM OIDC identity provider for the EKS cluster We can associate an IAM role with a Kubernetes service account. This service account can then provide AWS permissions to the containers in any pod that uses that service account. Simply put, the service account for EMR will be allowed to assume the EMR job execution role by OIDC federation - see EKS user guide for details. The job execution role will be created below. In order for the OIDC federation to work, we need to set up an IAM OIDC provider for the EKS cluster.\n1eksctl utils associate-iam-oidc-provider \\ 2 --cluster ${CLUSTER_NAME} --approve 3 4aws iam list-open-id-connect-providers --query \u0026#34;OpenIDConnectProviderList[1]\u0026#34; 5# { 6# \u0026#34;Arn\u0026#34;: \u0026#34;arn:aws:iam::\u0026lt;AWS-ACCOUNT-ID\u0026gt;:oidc-provider/oidc.eks.ap-southeast-2.amazonaws.com/id/6F3C18F00D8610088272FEF11013B8C5\u0026#34; 7# } Create a job execution role The following job execution role is created for the examples of this post. The permissions are set up to perform tasks on S3 and Glue. We\u0026rsquo;ll also enable logging on S3 and CloudWatch so that the necessary permissions are added as well.\n1aws iam create-role \\ 2 --role-name ${EMR_ROLE_NAME} \\ 3 --assume-role-policy-document \u0026#39;{ 4 \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, 5 \u0026#34;Statement\u0026#34;: [ 6 { 7 \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, 8 \u0026#34;Principal\u0026#34;: { 9 \u0026#34;Service\u0026#34;: \u0026#34;elasticmapreduce.amazonaws.com\u0026#34; 10 }, 11 \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRole\u0026#34; 12 } 13 ] 14}\u0026#39; 15 16aws iam put-role-policy \\ 17 --role-name ${EMR_ROLE_NAME} \\ 18 --policy-name ${EMR_ROLE_NAME}-policy \\ 19 --policy-document \u0026#39;{ 20 \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, 21 \u0026#34;Statement\u0026#34;: [ 22 { 23 \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, 24 \u0026#34;Action\u0026#34;: [ 25 \u0026#34;s3:PutObject\u0026#34;, 26 \u0026#34;s3:GetObject\u0026#34;, 27 \u0026#34;s3:ListBucket\u0026#34;, 28 \u0026#34;s3:DeleteObject\u0026#34; 29 ], 30 \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; 31 }, 32 { 33 \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, 34 \u0026#34;Action\u0026#34;: [ 35 \u0026#34;glue:*\u0026#34; 36 ], 37 \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; 38 }, 39 { 40 \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, 41 \u0026#34;Action\u0026#34;: [ 42 \u0026#34;logs:PutLogEvents\u0026#34;, 43 \u0026#34;logs:CreateLogStream\u0026#34;, 44 \u0026#34;logs:DescribeLogGroups\u0026#34;, 45 \u0026#34;logs:DescribeLogStreams\u0026#34; 46 ], 47 \u0026#34;Resource\u0026#34;: [ 48 \u0026#34;arn:aws:logs:*:*:*\u0026#34; 49 ] 50 } 51 ] 52}\u0026#39; Update the trust policy of the job execution role As mentioned earlier, the EMR service account is allowed to assume the job execution role by OIDC federation. In order to enable it, we need to update the trust relationship of the role. We can update it as shown below.\n1aws emr-containers update-role-trust-policy \\ 2 --cluster-name ${CLUSTER_NAME} \\ 3 --namespace spark \\ 4 --role-name ${EMR_ROLE_NAME} 5 6aws iam get-role --role-name ${EMR_ROLE_NAME} --query \u0026#34;Role.AssumeRolePolicyDocument.Statement[1]\u0026#34; 7# { 8# \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, 9# \u0026#34;Principal\u0026#34;: { 10# \u0026#34;Federated\u0026#34;: \u0026#34;arn:aws:iam::\u0026lt;AWS-ACCOUNT-ID\u0026gt;:oidc-provider/oidc.eks.ap-southeast-2.amazonaws.com/id/6F3C18F00D8610088272FEF11013B8C5\u0026#34; 11# }, 12# \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRoleWithWebIdentity\u0026#34;, 13# \u0026#34;Condition\u0026#34;: { 14# \u0026#34;StringLike\u0026#34;: { 15# \u0026#34;oidc.eks.ap-southeast-2.amazonaws.com/id/6F3C18F00D8610088272FEF11013B8C5:sub\u0026#34;: \u0026#34;system:serviceaccount:spark:emr-containers-sa-*-*-\u0026lt;AWS-ACCOUNT-ID\u0026gt;-93ztm12b8wi73z7zlhtudeipd0vpa8b60gchkls78cj1q\u0026#34; 16# } 17# } 18# } Register Amazon EKS Cluster with Amazon EMR We can register the Amazon EKS cluster with Amazon EMR as shown below. We need to provide the EKS cluster name and namespace.\n1## register EKS cluster with EMR 2aws emr-containers create-virtual-cluster \\ 3 --name ${CLUSTER_NAME} \\ 4 --container-provider \u0026#39;{ 5 \u0026#34;id\u0026#34;: \u0026#34;\u0026#39;${CLUSTER_NAME}\u0026#39;\u0026#34;, 6 \u0026#34;type\u0026#34;: \u0026#34;EKS\u0026#34;, 7 \u0026#34;info\u0026#34;: { 8 \u0026#34;eksInfo\u0026#34;: { 9 \u0026#34;namespace\u0026#34;: \u0026#34;spark\u0026#34; 10 } 11 } 12}\u0026#39; 13 14aws emr-containers list-virtual-clusters --query \u0026#34;sort_by(virtualClusters, \u0026amp;createdAt)[-1]\u0026#34; 15# { 16# \u0026#34;id\u0026#34;: \u0026#34;9wvd1yhms5tk1k8chrn525z34\u0026#34;, 17# \u0026#34;name\u0026#34;: \u0026#34;emr-eks-example\u0026#34;, 18# \u0026#34;arn\u0026#34;: \u0026#34;arn:aws:emr-containers:ap-southeast-2:\u0026lt;AWS-ACCOUNT-ID\u0026gt;:/virtualclusters/9wvd1yhms5tk1k8chrn525z34\u0026#34;, 19# \u0026#34;state\u0026#34;: \u0026#34;RUNNING\u0026#34;, 20# \u0026#34;containerProvider\u0026#34;: { 21# \u0026#34;type\u0026#34;: \u0026#34;EKS\u0026#34;, 22# \u0026#34;id\u0026#34;: \u0026#34;emr-eks-example\u0026#34;, 23# \u0026#34;info\u0026#34;: { 24# \u0026#34;eksInfo\u0026#34;: { 25# \u0026#34;namespace\u0026#34;: \u0026#34;spark\u0026#34; 26# } 27# } 28# }, 29# \u0026#34;createdAt\u0026#34;: \u0026#34;2022-01-07T01:26:37+00:00\u0026#34;, 30# \u0026#34;tags\u0026#34;: {} 31# } We can also check the virtual cluster on the EMR console.\nExamples Food Establishment Inspection This example is from the getting started tutorial of the Amazon EMR management guide. The PySpark script executes a simple SQL statement that counts the top 10 restaurants with the most Red violations and saves the output to S3. The script and its data source are saved to S3.\nIn the job request, we specify the job name, virtual cluster ID and job execution role. Also, the spark submit details are specified in the job driver option where the entrypoint is set to the S3 location of the PySpark script, entry point arguments and spark submit parameters. Finally, S3 and CloudWatch monitoring configuration is specified.\n1export VIRTUAL_CLUSTER_ID=$(aws emr-containers list-virtual-clusters --query \u0026#34;sort_by(virtualClusters, \u0026amp;createdAt)[-1].id\u0026#34; --output text) 2export EMR_ROLE_ARN=$(aws iam get-role --role-name ${EMR_ROLE_NAME} --query Role.Arn --output text) 3 4## create job request 5cat \u0026lt;\u0026lt; EOF \u0026gt; ./request-health-violations.json 6{ 7 \u0026#34;name\u0026#34;: \u0026#34;health-violations\u0026#34;, 8 \u0026#34;virtualClusterId\u0026#34;: \u0026#34;${VIRTUAL_CLUSTER_ID}\u0026#34;, 9 \u0026#34;executionRoleArn\u0026#34;: \u0026#34;${EMR_ROLE_ARN}\u0026#34;, 10 \u0026#34;releaseLabel\u0026#34;: \u0026#34;emr-6.2.0-latest\u0026#34;, 11 \u0026#34;jobDriver\u0026#34;: { 12 \u0026#34;sparkSubmitJobDriver\u0026#34;: { 13 \u0026#34;entryPoint\u0026#34;: \u0026#34;s3://${S3_BUCKET_NAME}/config/health_violations.py\u0026#34;, 14 \u0026#34;entryPointArguments\u0026#34;: [ 15 \u0026#34;--data_source\u0026#34;, \u0026#34;s3://${S3_BUCKET_NAME}/config/food_establishment_data.csv\u0026#34;, 16 \u0026#34;--output_uri\u0026#34;, \u0026#34;s3://${S3_BUCKET_NAME}/output\u0026#34; 17 ], 18 \u0026#34;sparkSubmitParameters\u0026#34;: \u0026#34;--conf spark.executor.instances=2 \\ 19 --conf spark.executor.memory=2G \\ 20 --conf spark.executor.cores=1 \\ 21 --conf spark.driver.cores=1 \\ 22 --conf spark.driver.memory=2G\u0026#34; 23 } 24 }, 25 \u0026#34;configurationOverrides\u0026#34;: { 26 \u0026#34;monitoringConfiguration\u0026#34;: { 27 \u0026#34;cloudWatchMonitoringConfiguration\u0026#34;: { 28 \u0026#34;logGroupName\u0026#34;: \u0026#34;${LOG_GROUP_NAME}\u0026#34;, 29 \u0026#34;logStreamNamePrefix\u0026#34;: \u0026#34;health\u0026#34; 30 }, 31 \u0026#34;s3MonitoringConfiguration\u0026#34;: { 32 \u0026#34;logUri\u0026#34;: \u0026#34;s3://${S3_BUCKET_NAME}/logs/\u0026#34; 33 } 34 } 35 } 36} 37EOF 38 39aws emr-containers start-job-run \\ 40 --cli-input-json file://./request-health-violations.json Once a job run is started, it can be checked under the virtual cluster section of the EMR console.\nWhen we click the View logs link, it launches the Spark History Server on a new tab.\nAs configured, the container logs of the job can be found in CloudWatch.\nAlso, the logs for the containers (spark driver and executor) and control-logs (job runner) can be found in S3.\nOnce the job completes, we can check the output from S3 as shown below.\n1export OUTPUT_FILE=$(aws s3 ls s3://${S3_BUCKET_NAME}/output/ | grep .csv | awk \u0026#39;{print $4}\u0026#39;) 2aws s3 cp s3://${S3_BUCKET_NAME}/output/${OUTPUT_FILE} - | head -n 15 3# name,total_red_violations 4# SUBWAY,322 5# T-MOBILE PARK,315 6# WHOLE FOODS MARKET,299 7# PCC COMMUNITY MARKETS,251 8# TACO TIME,240 9# MCDONALD\u0026#39;S,177 10# THAI GINGER,153 11# SAFEWAY INC #1508,143 12# TAQUERIA EL RINCONSITO,134 13# HIMITSU TERIYAKI,128 Hudi DeltaStreamer In an earlier post, we discussed a Hudi table generation using the DeltaStreamer utility as part of a CDC-based data ingestion solution. In that exercise, we executed the spark job in an EMR cluster backed by EC2 instances. We can run the spark job in our EKS cluster.\nWe can configure to run the executors in spot instances in order to save cost. A spot node group can be created by the following configuration file.\n1# ./manifests/nodegroup-spot.yaml 2--- 3apiVersion: eksctl.io/v1alpha5 4kind: ClusterConfig 5 6metadata: 7 name: emr-eks-example 8 region: ap-southeast-2 9 tags: 10 Owner: jaehyeon 11 12managedNodeGroups: 13- name: nodegroup-spot 14 desiredCapacity: 3 15 instanceTypes: 16 - m5.xlarge 17 - m5a.xlarge 18 - m4.xlarge 19 spot: true Once the spot node group is created, we can see 3 instances are added to the EKS node with the SPOT capacity type.\n1eksctl create nodegroup -f ./manifests/nodegroup-spot.yaml 2 3kubectl get nodes \\ 4 --label-columns=eks.amazonaws.com/nodegroup,eks.amazonaws.com/capacityType \\ 5 --sort-by=.metadata.creationTimestamp 6# NAME STATUS ROLES AGE VERSION NODEGROUP CAPACITYTYPE 7# ip-192-168-33-60.ap-southeast-2.compute.internal Ready \u0026lt;none\u0026gt; 52m v1.21.5-eks-bc4871b nodegroup ON_DEMAND 8# ip-192-168-95-68.ap-southeast-2.compute.internal Ready \u0026lt;none\u0026gt; 51m v1.21.5-eks-bc4871b nodegroup ON_DEMAND 9# ip-192-168-79-20.ap-southeast-2.compute.internal Ready \u0026lt;none\u0026gt; 114s v1.21.5-eks-bc4871b nodegroup-spot SPOT 10# ip-192-168-1-57.ap-southeast-2.compute.internal Ready \u0026lt;none\u0026gt; 112s v1.21.5-eks-bc4871b nodegroup-spot SPOT 11# ip-192-168-34-249.ap-southeast-2.compute.internal Ready \u0026lt;none\u0026gt; 97s v1.21.5-eks-bc4871b nodegroup-spot SPOT The driver and executor pods should be created in different nodes, and it can be controlled by Pod Template. Below the driver and executor have a different node selector, and they\u0026rsquo;ll be assigned based on the capacity type label specified in the node selector.\n1# ./config/driver_pod_template.yml 2apiVersion: v1 3kind: Pod 4spec: 5 nodeSelector: 6 eks.amazonaws.com/capacityType: ON_DEMAND 7 8# ./config/executor_pod_template.yml 9apiVersion: v1 10kind: Pod 11spec: 12 nodeSelector: 13 eks.amazonaws.com/capacityType: SPOT The job request for the DeltaStreamer job can be found below. Note that, in the entrypoint, we specified the latest Hudi utilities bundle (0.10.0) from S3 instead of the pre-installed Hudi 0.8.0. It is because Hudi 0.8.0 supports JDBC based Hive sync only while Hudi 0.9.0+ supports multiple Hive sync modes including Hive metastore. EMR on EKS doesn\u0026rsquo;t run _HiveServer2 _so that JDBC based Hive sync doesn\u0026rsquo;t work. Instead, we can specify Hive sync based on Hive metastore because Glue data catalog can be used as Hive metastore. Therefore, we need a newer version of the Hudi library in order to register the resulting Hudi table to Glue data catalog. Also, in the application configuration, we configured to use Glue data catalog as the Hive metastore and the driver/executor pod template files are specified.\n1export VIRTUAL_CLUSTER_ID=$(aws emr-containers list-virtual-clusters --query \u0026#34;sort_by(virtualClusters, \u0026amp;createdAt)[-1].id\u0026#34; --output text) 2export EMR_ROLE_ARN=$(aws iam get-role --role-name ${EMR_ROLE_NAME} --query Role.Arn --output text) 3 4## create job request 5cat \u0026lt;\u0026lt; EOF \u0026gt; ./request-cdc-events.json 6{ 7 \u0026#34;name\u0026#34;: \u0026#34;cdc-events\u0026#34;, 8 \u0026#34;virtualClusterId\u0026#34;: \u0026#34;${VIRTUAL_CLUSTER_ID}\u0026#34;, 9 \u0026#34;executionRoleArn\u0026#34;: \u0026#34;${EMR_ROLE_ARN}\u0026#34;, 10 \u0026#34;releaseLabel\u0026#34;: \u0026#34;emr-6.4.0-latest\u0026#34;, 11 \u0026#34;jobDriver\u0026#34;: { 12 \u0026#34;sparkSubmitJobDriver\u0026#34;: { 13 \u0026#34;entryPoint\u0026#34;: \u0026#34;s3://${S3_BUCKET_NAME}/config/hudi-utilities-bundle_2.12-0.10.0.jar\u0026#34;, 14 \u0026#34;entryPointArguments\u0026#34;: [ 15 \u0026#34;--table-type\u0026#34;, \u0026#34;COPY_ON_WRITE\u0026#34;, 16 \u0026#34;--source-ordering-field\u0026#34;, \u0026#34;__source_ts_ms\u0026#34;, 17 \u0026#34;--props\u0026#34;, \u0026#34;s3://${S3_BUCKET_NAME}/config/cdc_events_s3.properties\u0026#34;, 18 \u0026#34;--source-class\u0026#34;, \u0026#34;org.apache.hudi.utilities.sources.JsonDFSSource\u0026#34;, 19 \u0026#34;--target-base-path\u0026#34;, \u0026#34;s3://${S3_BUCKET_NAME}/hudi/cdc-events/\u0026#34;, 20 \u0026#34;--target-table\u0026#34;, \u0026#34;datalake.cdc_events\u0026#34;, 21 \u0026#34;--schemaprovider-class\u0026#34;, \u0026#34;org.apache.hudi.utilities.schema.FilebasedSchemaProvider\u0026#34;, 22 \u0026#34;--enable-hive-sync\u0026#34;, 23 \u0026#34;--min-sync-interval-seconds\u0026#34;, \u0026#34;60\u0026#34;, 24 \u0026#34;--continuous\u0026#34;, 25 \u0026#34;--op\u0026#34;, \u0026#34;UPSERT\u0026#34; 26 ], 27 \u0026#34;sparkSubmitParameters\u0026#34;: \u0026#34;--class org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer \\ 28 --jars local:///usr/lib/spark/external/lib/spark-avro_2.12-3.1.2-amzn-0.jar,s3://${S3_BUCKET_NAME}/config/hudi-utilities-bundle_2.12-0.10.0.jar \\ 29 --conf spark.driver.cores=1 \\ 30 --conf spark.driver.memory=2G \\ 31 --conf spark.executor.instances=2 \\ 32 --conf spark.executor.memory=2G \\ 33 --conf spark.executor.cores=1 \\ 34 --conf spark.sql.catalogImplementation=hive \\ 35 --conf spark.serializer=org.apache.spark.serializer.KryoSerializer\u0026#34; 36 } 37 }, 38 \u0026#34;configurationOverrides\u0026#34;: { 39 \u0026#34;applicationConfiguration\u0026#34;: [ 40 { 41 \u0026#34;classification\u0026#34;: \u0026#34;spark-defaults\u0026#34;, 42 \u0026#34;properties\u0026#34;: { 43 \u0026#34;spark.hadoop.hive.metastore.client.factory.class\u0026#34;:\u0026#34;com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory\u0026#34;, 44 \u0026#34;spark.kubernetes.driver.podTemplateFile\u0026#34;:\u0026#34;s3://${S3_BUCKET_NAME}/config/driver_pod_template.yml\u0026#34;, 45 \u0026#34;spark.kubernetes.executor.podTemplateFile\u0026#34;:\u0026#34;s3://${S3_BUCKET_NAME}/config/executor_pod_template.yml\u0026#34; 46 } 47 } 48 ], 49 \u0026#34;monitoringConfiguration\u0026#34;: { 50 \u0026#34;cloudWatchMonitoringConfiguration\u0026#34;: { 51 \u0026#34;logGroupName\u0026#34;: \u0026#34;${LOG_GROUP_NAME}\u0026#34;, 52 \u0026#34;logStreamNamePrefix\u0026#34;: \u0026#34;cdc\u0026#34; 53 }, 54 \u0026#34;s3MonitoringConfiguration\u0026#34;: { 55 \u0026#34;logUri\u0026#34;: \u0026#34;s3://${S3_BUCKET_NAME}/logs/\u0026#34; 56 } 57 } 58 } 59} 60EOF 61 62aws emr-containers start-job-run \\ 63 --cli-input-json file://./request-cdc-events.json Once the job run is started, we can check it as shown below.\n1aws emr-containers list-job-runs --virtual-cluster-id ${VIRTUAL_CLUSTER_ID} --query \u0026#34;jobRuns[?name==\u0026#39;cdc-events\u0026#39;]\u0026#34; 2# [ 3# { 4# \u0026#34;id\u0026#34;: \u0026#34;00000002vhi9hivmjk5\u0026#34;, 5# \u0026#34;name\u0026#34;: \u0026#34;cdc-events\u0026#34;, 6# \u0026#34;virtualClusterId\u0026#34;: \u0026#34;9wvd1yhms5tk1k8chrn525z34\u0026#34;, 7# \u0026#34;arn\u0026#34;: \u0026#34;arn:aws:emr-containers:ap-southeast-2:\u0026lt;AWS-ACCOUNT-ID\u0026gt;:/virtualclusters/9wvd1yhms5tk1k8chrn525z34/jobruns/00000002vhi9hivmjk5\u0026#34;, 8# \u0026#34;state\u0026#34;: \u0026#34;RUNNING\u0026#34;, 9# \u0026#34;clientToken\u0026#34;: \u0026#34;63a707e4-e5bc-43e4-b11a-5dcfb4377fd3\u0026#34;, 10# \u0026#34;executionRoleArn\u0026#34;: \u0026#34;arn:aws:iam::\u0026lt;AWS-ACCOUNT-ID\u0026gt;:role/emr-eks-example-job-execution\u0026#34;, 11# \u0026#34;releaseLabel\u0026#34;: \u0026#34;emr-6.4.0-latest\u0026#34;, 12# \u0026#34;createdAt\u0026#34;: \u0026#34;2022-01-07T02:09:34+00:00\u0026#34;, 13# \u0026#34;createdBy\u0026#34;: \u0026#34;arn:aws:sts::\u0026lt;AWS-ACCOUNT-ID\u0026gt;:assumed-role/AWSReservedSSO_AWSFullAccountAdmin_fb6fa00561d5e1c2/jaehyeon.kim@cevo.com.au\u0026#34;, 14# \u0026#34;tags\u0026#34;: {} 15# } 16# ] With kubectl, we can check there are 1 driver, 2 executors and 1 job runner pods.\n1kubectl get pod -n spark 2# NAME READY STATUS RESTARTS AGE 3# pod/00000002vhi9hivmjk5-wf8vp 3/3 Running 0 14m 4# pod/delta-streamer-datalake-cdcevents-5397917e324dea27-exec-1 2/2 Running 0 12m 5# pod/delta-streamer-datalake-cdcevents-5397917e324dea27-exec-2 2/2 Running 0 12m 6# pod/spark-00000002vhi9hivmjk5-driver 2/2 Running 0 13m Also, we can see the driver pod runs in the on-demand node group while the executor and job runner pods run in the spot node group.\n1## driver runs in the on demand node 2for n in $(kubectl get nodes -l eks.amazonaws.com/capacityType=ON_DEMAND --no-headers | cut -d \u0026#34; \u0026#34; -f1) 3 do echo \u0026#34;Pods on instance ${n}:\u0026#34;;kubectl get pods -n spark --no-headers --field-selector spec.nodeName=${n} 4 echo 5 done 6# Pods on instance ip-192-168-33-60.ap-southeast-2.compute.internal: 7# No resources found in spark namespace. 8 9# Pods on instance ip-192-168-95-68.ap-southeast-2.compute.internal: 10# spark-00000002vhi9hivmjk5-driver 2/2 Running 0 17m 11 12## executor and job runner run in the spot node 13for n in $(kubectl get nodes -l eks.amazonaws.com/capacityType=SPOT --no-headers | cut -d \u0026#34; \u0026#34; -f1) 14 do echo \u0026#34;Pods on instance ${n}:\u0026#34;;kubectl get pods -n spark --no-headers --field-selector spec.nodeName=${n} 15 echo 16 done 17# Pods on instance ip-192-168-1-57.ap-southeast-2.compute.internal: 18# delta-streamer-datalake-cdcevents-5397917e324dea27-exec-2 2/2 Running 0 16m 19 20# Pods on instance ip-192-168-34-249.ap-southeast-2.compute.internal: 21# 00000002vhi9hivmjk5-wf8vp 3/3 Running 0 18m 22 23# Pods on instance ip-192-168-79-20.ap-southeast-2.compute.internal: 24# delta-streamer-datalake-cdcevents-5397917e324dea27-exec-1 2/2 Running 0 16m The Hudi utility will register a table in the Glue data catalog and it can be checked as shown below.\n1aws glue get-table --database-name datalake --name cdc_events \\ 2 --query \u0026#34;Table.[DatabaseName, Name, StorageDescriptor.Location, CreateTime, CreatedBy]\u0026#34; 3# [ 4# \u0026#34;datalake\u0026#34;, 5# \u0026#34;cdc_events\u0026#34;, 6# \u0026#34;s3://emr-eks-example-ap-southeast-2/hudi/cdc-events\u0026#34;, 7# \u0026#34;2022-01-07T13:18:49+11:00\u0026#34;, 8# \u0026#34;arn:aws:sts::590312749310:assumed-role/emr-eks-example-job-execution/aws-sdk-java-1641521928075\u0026#34; 9# ] Finally, the details of the table can be queried in Athena.\nClean up The resources that are created for this post can be deleted using aws cli and eksctl as shown below.\n1## delete virtual cluster 2export JOB_RUN_ID=$(aws emr-containers list-job-runs --virtual-cluster-id ${VIRTUAL_CLUSTER_ID} --query \u0026#34;jobRuns[?name==\u0026#39;cdc-events\u0026#39;].id\u0026#34; --output text) 3aws emr-containers cancel-job-run --id ${JOB_RUN_ID} \\ 4 --virtual-cluster-id ${VIRTUAL_CLUSTER_ID} 5aws emr-containers delete-virtual-cluster --id ${VIRTUAL_CLUSTER_ID} 6## delete s3 7aws s3 rm s3://${S3_BUCKET_NAME} --recursive 8aws s3 rb s3://${S3_BUCKET_NAME} --force 9## delete log group 10aws logs delete-log-group --log-group-name ${LOG_GROUP_NAME} 11## delete glue table/database 12aws glue delete-table --database-name datalake --name cdc_events 13aws glue delete-database --name datalake 14## delete iam role/policy 15aws iam delete-role-policy --role-name ${EMR_ROLE_NAME} --policy-name ${EMR_ROLE_NAME}-policy 16aws iam delete-role --role-name ${EMR_ROLE_NAME} 17## delete eks cluster 18eksctl delete cluster --name ${CLUSTER_NAME} Summary In this post, we discussed how to run spark jobs on EKS. First we created an EKS cluster and a node group using eksctl. Then we set up EMR on EKS. A simple PySpark job that shows the basics of EMR on EKS is illustrated and a more realistic example of running Hudi DeltaStreamer utility is demonstrated where the driver and executors are assigned in different node groups.\n","date":"January 17, 2022","img":"/blog/2022-01-17-emr-on-eks-by-example/featured.png","lang":"en","langName":"English","largeImg":"/blog/2022-01-17-emr-on-eks-by-example/featured_hu7346e7fea9bf42a33719962b2d46c84d_76740_500x0_resize_box_3.png","permalink":"/blog/2022-01-17-emr-on-eks-by-example/","series":[],"smallImg":"/blog/2022-01-17-emr-on-eks-by-example/featured_hu7346e7fea9bf42a33719962b2d46c84d_76740_180x0_resize_box_3.png","tags":[{"title":"AWS","url":"/tags/aws/"},{"title":"Amazon EMR","url":"/tags/amazon-emr/"},{"title":"Amazon EKS","url":"/tags/amazon-eks/"},{"title":"Apache Spark","url":"/tags/apache-spark/"},{"title":"Kubernetes","url":"/tags/kubernetes/"}],"timestamp":1642377600,"title":"EMR on EKS by Example"},{"categories":[{"title":"Data Engineering","url":"/categories/data-engineering/"}],"content":"In the previous post, we created a VPC that has private and public subnets in 2 availability zones in order to build and deploy the data lake solution on AWS. NAT instances are created to forward outbound traffic to the internet and a VPN bastion host is set up to facilitate deployment. An Aurora PostgreSQL cluster is deployed to host the source database and a Python command line app is used to create the database. To develop data ingestion using CDC, an Amazon MSK cluster is deployed and the Debezium source and Lenses S3 sink connectors are created on MSK Connect. We also confirmed the order creation and update events are captured as expected. As the last part of this series, we\u0026rsquo;ll build an Apache Hudi DeltaStreamer app on Amazon EMR and use the resulting Hudi table with Amazon Athena and Amazon Quicksight to build a dashboard.\nPart 1 Local Development Part 2 Implement CDC Part 3 Implement Data Lake (this post) Architecture As described in a Red Hat IT topics article, change data capture (CDC) is a proven data integration pattern to track when and what changes occur in data then alert other systems and services that must respond to those changes. Change data capture helps maintain consistency and functionality across all systems that rely on data.\nThe primary use of CDC is to enable applications to respond almost immediately whenever data in databases change. Specifically its use cases cover microservices integration, data replication with up-to-date data, building time-sensitive analytics dashboards, auditing and compliance, cache invalidation, full-text search and so on. There are a number of approaches for CDC - polling, dual writes and log-based CDC. Among those, log-based CDC has advantages to other approaches.\nBoth Amazon DMS and Debezium implement log-based CDC. While the former is a managed service, the latter can be deployed to a Kafka cluster as a (source) connector. It uses Apache Kafka as a messaging service to deliver database change notifications to the applicable systems and applications. Note that Kafka Connect is a tool for streaming data between Apache Kafka and other data systems by connectors in a scalable and reliable way. In AWS, we can use Amazon MSK and MSK Connect for building a Debezium based CDC solution.\nData replication to data lakes using CDC can be much more effective if data is stored to a format that supports atomic transactions and consistent updates. Popular choices are Apache Hudi, Apache Iceberg and Delta Lake. Among those, Apache Hudi can be a good option as it is well-integrated with AWS services.\nBelow shows the architecture of the data lake solution that we will be building in this series of posts.\nEmploying the transactional outbox pattern, the source database publishes change event records to the CDC event table. The event records are generated by triggers that listen to insert and update events on source tables. See the Source Database section of the first post of this series for details. CDC is implemented in a streaming environment and Amazon MSK is used to build the streaming infrastructure. In order to process the real-time CDC event records, a source and sink connectors are set up in Amazon MSK Connect. The Debezium connector for PostgreSQL is used as the source connector and the Lenses S3 connector is used as the sink connector. The sink connector pushes messages to a S3 bucket. Hudi DeltaStreamer is run on Amazon EMR. As a spark application, it reads files from the S3 bucket and upserts Hudi records to another S3 bucket. The Hudi table is created in the AWS Glue Data Catalog. The Hudi table is queried in Amazon Athena while the table is registered in the AWS Glue Data Catalog. Dashboards are created in Amazon Quicksight where the dataset is created using Amazon Athena. In this post, we\u0026rsquo;ll build a Hudi DeltaStreamer app on Amazon EMR and use the resulting Hudi table with Athena and Quicksight to build a dashboard.\nInfrastructure In the previous post, we created a VPC in the Sydney region, which has private and public subnets in 2 availability zones. We also created NAT instances in each availability zone to forward outbound traffic to the internet and a VPN bastion host to access resources in the private subnets. An EMR cluster will be deployed to one of the private subnets of the VPC.\nEMR Cluster We\u0026rsquo;ll create the EMR cluster with the following configurations.\nIt is created with the latest EMR release - semr-6.4.0. It has 1 master and 2 core instance groups - their instance types are m4.large. Both the instance groups have additional security groups that allow access from the VPN bastion host. It installs Hadoop, Hive, Spark, Presto, Hue and Livy. It uses the AWS Glue Data Catalog as the metastore for Hive and Spark. The last configuration is important to register the Hudi table to the Glue Data Catalog so that it can be accessed from other AWS services such as Athena and Quicksight. The cluster is created by CloudFormation and the template also creates a Glue database (datalake) in which the Hudi table will be created. The template can be found in the project GitHub repository.\nOnce the EMR cluster is ready, we can access the master instance as shown below. Note don\u0026rsquo;t forget to connect the VPN bastion host using the SoftEther VPN client program.\nHudi Table Source Schema Kafka only transfers data in byte format and data verification is not performed at the cluster level. As producers and consumers do not communicate with each other, we need a schema registry that sits outside a Kafka cluster and handles distribution of schemas. Although it is recommended to associate with a schema registry, we avoid using it because it requires either an external service or a custom server to host a schema registry. Ideally it\u0026rsquo;ll be good if we\u0026rsquo;re able to use the AWS Glue Schema Registry, but unfortunately it doesn\u0026rsquo;t support a REST interface and cannot be used at the moment.\nIn order to avoid having a schema registry, we use the built-in JSON converter (org.apache.kafka.connect.json.JsonConverter) as the key and value converters for the Debezium source and S3 sink connectors. The resulting value schema of our CDC event message is of the struct type, and it can be found in the project GitHub repository. However, it is not supported by the DeltaStreamer utility that we\u0026rsquo;ll be using to generate the Hudi table. A quick fix is replacing it with the Avro schema, and we can generate it with the local docker-compose environment that we discussed in the first post of this series. Once the local environment is up and running, we can create the Debezium source connector with the Avro converter (io.confluent.connect.avro.AvroConverter) as shown below.\n1curl -i -X POST -H \u0026#34;Accept:application/json\u0026#34; -H \u0026#34;Content-Type:application/json\u0026#34; \\ 2 http://localhost:8083/connectors/ -d @connector/local/source-debezium-avro.json Then we can download the value schema in the Schema tab of the topic.\nThe schema file of the CDC event messages can be found in the project GitHub repository. Note that, although we use it as the source schema file for the DeltaStreamer app, we keep using the JSON converter for the Kafka connectors as we don\u0026rsquo;t set up a schema registry\nDeltaStreamer The HoodieDeltaStreamer utility (part of hudi-utilities-bundle) provides the way to ingest from different sources such as DFS or Kafka, with the following capabilities.\nExactly once ingestion of new events from Kafka, incremental imports from Sqoop or output of HiveIncrementalPuller or files under a DFS folder Support JSON, AVRO or a custom record types for the incoming data Manage checkpoints, rollback \u0026amp; recovery Leverage AVRO schemas from DFS or Confluent schema registry. Support for plugging in transformations As shown below, it runs as a Spark application. Some important options are illustrated below.\nHudi-related jar files are specified directly because Amazon EMR release version 5.28.0 and later installs Hudi components by default. Hudi 0.8.0 is installed for EMR release 6.4.0. It is deployed by the cluster deploy mode where the driver and executor have 2G and 4G of memory respectively. Copy on Write (CoW) is configured as the storage type. Additional Hudi properties are saved in S3 (cdc_events_deltastreamer_s3.properties) - it\u0026rsquo;ll be discussed below. The JSON type is configured as the source file type - note we use the built-in JSON converter for the Kafka connectors. The S3 target base path indicates the place where the Hudi data is stored, and the target table configures the resulting table. As we enable the AWS Glue Data Catalog as the Hive metastore, it can be accessed in Glue. The file-based schema provider is configured. The Avro schema file is referred to as the source schema file in the additional Hudi property file. Hive sync is enabled, and the minimum sync interval is set to 5 seconds. It is set to run continuously and the default UPSERT operation is chosen. 1spark-submit --jars /usr/lib/spark/external/lib/spark-avro.jar,/usr/lib/hudi/hudi-utilities-bundle.jar \\ 2 --master yarn \\ 3 --deploy-mode cluster \\ 4 --driver-memory 2g \\ 5 --executor-memory 4g \\ 6 --conf spark.sql.catalogImplementation=hive \\ 7 --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \\ 8 --class org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer /usr/lib/hudi/hudi-utilities-bundle.jar \\ 9 --table-type COPY_ON_WRITE \\ 10 --source-ordering-field __source_ts_ms \\ 11 --props \u0026#34;s3://data-lake-demo-cevo/hudi/config/cdc_events_deltastreamer_s3.properties\u0026#34; \\ 12 --source-class org.apache.hudi.utilities.sources.JsonDFSSource \\ 13 --target-base-path \u0026#34;s3://data-lake-demo-cevo/hudi/cdc-events/\u0026#34; \\ 14 --target-table datalake.cdc_events \\ 15 --schemaprovider-class org.apache.hudi.utilities.schema.FilebasedSchemaProvider \\ 16 --enable-sync \\ 17 --min-sync-interval-seconds 5 \\ 18 --continuous \\ 19 --op UPSERT Below shows the additional Hudi properties. For Hive sync, the database, table, partition fields and JDBC URL are specified. Note that the private IP address of the master instance is added to the host of the JDBC URL. It is required when submitting the application by the cluster deploy mode. By default, the host is set to localhost and the connection failure error will be thrown if the app doesn\u0026rsquo;t run in the master. The remaining Hudi datasource properties are to configure the primary key of the Hudi table - every record in Hudi is uniquely identified by a pair of record key and partition path fields. The Hudi DeltaStreamer properties specify the source schema file and the S3 location where the source data files exist. More details about the configurations can be found in the Hudi website.\n1# ./hudi/config/cdc_events_deltastreamer_s3.properties 2## base properties 3hoodie.upsert.shuffle.parallelism=2 4hoodie.insert.shuffle.parallelism=2 5hoodie.delete.shuffle.parallelism=2 6hoodie.bulkinsert.shuffle.parallelism=2 7 8## datasource properties 9hoodie.datasource.hive_sync.database=datalake 10hoodie.datasource.hive_sync.table=cdc_events 11hoodie.datasource.hive_sync.partition_fields=customer_id,order_id 12hoodie.datasource.hive_sync.partition_extractor_class=org.apache.hudi.hive.MultiPartKeysValueExtractor 13hoodie.datasource.hive_sync.jdbcurl=jdbc:hive2://10.100.22.160:10000 14hoodie.datasource.write.recordkey.field=order_id 15hoodie.datasource.write.partitionpath.field=customer_id,order_id 16hoodie.datasource.write.keygenerator.class=org.apache.hudi.keygen.ComplexKeyGenerator 17hoodie.datasource.write.hive_style_partitioning=true 18# only supported in Hudi 0.9.0+ 19# hoodie.datasource.write.drop.partition.columns=true 20 21## deltastreamer properties 22hoodie.deltastreamer.schemaprovider.source.schema.file=s3://data-lake-demo-cevo/hudi/config/schema-msk.datalake.cdc_events.avsc 23hoodie.deltastreamer.source.dfs.root=s3://data-lake-demo-cevo/cdc-events/ 24 25## file properties 26# 1,024 * 1,024 * 128 = 134,217,728 (128 MB) 27hoodie.parquet.small.file.limit=134217728 EMR Steps While it is possible to submit the application in the master instance, it can also be submitted as an EMR step. As an example, a simple version of the app is submitted as shown below. The JSON file that configures the step can be found in the project GitHub repository.\n1aws emr add-steps \\ 2 --cluster-id \u0026lt;cluster-id\u0026gt; \\ 3 --steps file://hudi/steps/cdc-events-simple.json Once the step is added, we can see its details on the EMR console.\nGlue Table Below shows the Glue tables that are generated by the DeltaStreamer apps. The main table (cdc_events) is by submitting the app on the master instance while the simple version is by the EMR step.\nWe can check the details of the table in Athena. When we run the describe query, it shows column and partition information. As expected, it has 2 partition columns and additional Hudi table meta information is included as extra columns.\nDashboard Using the Glue table, we can create dashboards in Quicksight. As an example, a dataset of order items is created using Athena as illustrated in the Quicksight documentation. As the order items column in the source table is JSON, custom SQL is chosen so that it can be preprocessed in a more flexible way. The following SQL statement is used to create the dataset. First it parses the order items column and then flattens the array elements into rows. Also, the revenue column is added as a calculated field.\n1WITH raw_data AS ( 2 SELECT 3 customer_id, 4 order_id, 5 transform( 6 CAST(json_parse(order_items) AS ARRAY(MAP(varchar, varchar))), 7 x -\u0026gt; CAST(ROW(x[\u0026#39;discount\u0026#39;], x[\u0026#39;quantity\u0026#39;], x[\u0026#39;unit_price\u0026#39;]) 8 AS ROW(discount decimal(6,2), quantity decimal(6,2), unit_price decimal(6,2))) 9 ) AS order_items 10 from datalake.cdc_events 11), flat_data AS ( 12\tSELECT customer_id, 13\torder_id, 14\titem 15\tFROM raw_data 16\tCROSS JOIN UNNEST(order_items) AS t(item) 17) 18SELECT customer_id, 19 order_id, 20 item.discount, 21 item.quantity, 22 item.unit_price 23FROM flat_data A demo dashboard is created using the order items dataset as shown below. The pie chart on the left indicates there are 3 big customers and the majority of revenue is earned by the top 20 customers. The scatter plot on the right shows a more interesting story. It marks the average quantity and revenue by customers and the dots are scaled by the number of orders - the more orders, the larger the size of the dot. While the 3 big customers occupy the top right area, 5 potentially profitable customers are identified. They do not purchase frequently but tend to buy expensive items, resulting in the average revenue being higher. We may investigate them further if a promotional event may be appropriate to make them purchase more frequently in the future.\nConclusion In this post, we created an EMR cluster and developed a DeltaStreamer app that can be used to upsert records to a Hudi table. Being sourced as an Athena dataset, the records of the table are used by a Quicksight dashboard. Over the series of posts we have built an effective end-to-end data lake solution while combining various AWS services and open source tools. The source database is hosted in an Aurora PostgreSQL cluster and a change data capture (CDC) solution is built on Amazon MSK and MSK Connect. With the CDC output files in S3, a DeltaStreamer app is developed on Amazon EMR to build a Hudi table. The resulting table is used to create a dashboard with Amazon Athena and Quicksight.\n","date":"December 19, 2021","img":"/blog/2021-12-19-datalake-demo-part3/featured.png","lang":"en","langName":"English","largeImg":"/blog/2021-12-19-datalake-demo-part3/featured_hua25eccd3824300d3b1ed87f56797248c_164526_500x0_resize_box_3.png","permalink":"/blog/2021-12-19-datalake-demo-part3/","series":[{"title":"Data Lake Demo Using Change Data Capture","url":"/series/data-lake-demo-using-change-data-capture/"}],"smallImg":"/blog/2021-12-19-datalake-demo-part3/featured_hua25eccd3824300d3b1ed87f56797248c_164526_180x0_resize_box_3.png","tags":[{"title":"AWS","url":"/tags/aws/"},{"title":"Amazon EMR","url":"/tags/amazon-emr/"},{"title":"Amazon MSK","url":"/tags/amazon-msk/"},{"title":"Amazon MSK Connect","url":"/tags/amazon-msk-connect/"},{"title":"Apache Spark","url":"/tags/apache-spark/"},{"title":"Apache Hudi","url":"/tags/apache-hudi/"},{"title":"Apache Kafka","url":"/tags/apache-kafka/"},{"title":"Kafka Connect","url":"/tags/kafka-connect/"},{"title":"Change Data Capture","url":"/tags/change-data-capture/"},{"title":"Data Lake","url":"/tags/data-lake/"},{"title":"Docker","url":"/tags/docker/"},{"title":"Terraform","url":"/tags/terraform/"}],"timestamp":1639872000,"title":"Data Lake Demo Using Change Data Capture (CDC) on AWS – Part 3 Implement Data Lake"},{"categories":[{"title":"Data Engineering","url":"/categories/data-engineering/"}],"content":"In the previous post, we discussed a data lake solution where data ingestion is performed using change data capture (CDC) and the output files are upserted to an Apache Hudi table. Being registered to Glue Data Catalog, it can be used for ad-hoc queries and report/dashboard creation. The Northwind database is used as the source database and, following the transactional outbox pattern, order-related changes are _upserted _to an outbox table by triggers. The data ingestion is developed using Kafka connectors in the local Confluent platform where the Debezium for PostgreSQL is used as the source connector and the Lenses S3 sink connector is used as the sink connector. We confirmed the order creation and update events are captured as expected, and it is ready for production deployment. In this post, we\u0026rsquo;ll build the CDC part of the solution on AWS using Amazon MSK and MSK Connect.\nPart 1 Local Development Part 2 Implement CDC (this post) Part 3 Implement Data Lake Architecture As described in a Red Hat IT topics article, change data capture (CDC) is a proven data integration pattern to track when and what changes occur in data then alert other systems and services that must respond to those changes. Change data capture helps maintain consistency and functionality across all systems that rely on data.\nThe primary use of CDC is to enable applications to respond almost immediately whenever data in databases change. Specifically its use cases cover microservices integration, data replication with up-to-date data, building time-sensitive analytics dashboards, auditing and compliance, cache invalidation, full-text search and so on. There are a number of approaches for CDC - polling, dual writes and log-based CDC. Among those, log-based CDC has advantages to other approaches.\nBoth Amazon DMS and Debezium implement log-based CDC. While the former is a managed service, the latter can be deployed to a Kafka cluster as a (source) connector. It uses Apache Kafka as a messaging service to deliver database change notifications to the applicable systems and applications. Note that Kafka Connect is a tool for streaming data between Apache Kafka and other data systems by connectors in a scalable and reliable way. In AWS, we can use Amazon MSK and MSK Connect for building a Debezium based CDC solution.\nData replication to data lakes using CDC can be much more effective if data is stored to a format that supports atomic transactions and consistent updates. Popular choices are Apache Hudi, Apache Iceberg and Delta Lake. Among those, Apache Hudi can be a good option as it is well-integrated with AWS services.\nBelow shows the architecture of the data lake solution that we will be building in this series of posts.\nEmploying the transactional outbox pattern, the source database publishes change event records to the CDC event table. The event records are generated by triggers that listen to insert and update events on source tables. See the Source Database section of the previous post for details. CDC is implemented in a streaming environment and Amazon MSK is used to build the streaming infrastructure. In order to process the real-time CDC event records, a source and sink connectors are set up in Amazon MSK Connect. The Debezium connector for PostgreSQL is used as the source connector and the Lenses S3 connector is used as the sink connector. The sink connector pushes messages to a S3 bucket. Hudi DeltaStreamer is run on Amazon EMR. As a spark application, it reads files from the S3 bucket and upserts Hudi records to another S3 bucket. The Hudi table is created in the AWS Glue Data Catalog. The Hudi table is queried in Amazon Athena while the table is registered in the AWS Glue Data Catalog. Dashboards are created in Amazon Quicksight where the dataset is created using Amazon Athena. In this post, we\u0026rsquo;ll build the CDC part of the solution on AWS using Amazon MSK and MSK Connect.\nInfrastructure VPC We\u0026rsquo;ll build the data lake solution in a dedicated VPC. The VPC is created in the Sydney region, and it has private and public subnets in 2 availability zones. Note the source database, MSK cluster/connectors and EMR cluster will be deployed to the private subnets. The CloudFormation template can be found in the Free Templates for AWS CloudFormation- see the VPC section for details.\nNAT Instances NAT instances are created in each of the availability zone to forward outbound traffic to the internet. The CloudFormation template can be found in the VPC section of the site as well.\nVPN Bastion Host We can use a VPN bastion host to access resources in the private subnets. The free template site provides both SSH and VPN bastion host templates and I find the latter is more convenient. It can be used to access other resources via different ports as well as be used as an SSH bastion host. The CloudFormation template creates an EC2 instance in one of the public subnets and installs a SoftEther VPN server. The template requires you to add the pre-shared key and the VPN admin password, which can be used to manage the server and create connection settings for the VPN server. After creating the CloudFormation stack, we need to download the server manager from the download page and to install the admin tools.\nAfter that, we can create connection settings for the VPN server by providing the necessary details as marked in red boxes below. We should add the public IP address of the EC2 instance and the VPN admin password.\nThe template also has VPN username and password parameters and those are used to create a user account in the VPN bastion server. Using the credentials we can set up a VPN connection using the SoftEther VPN client program - it can be downloaded on the same download page. We should add the public IP address of the EC2 instance to the host name and select DEFAULT as the virtual hub name.\nAfter that, we can connect to the VPN bastion host and access resources in the private subnets.\nAurora PostgreSQL We create the source database with Aurora PostgreSQL. The CloudFormation template from the free template site creates database instances in multiple availability zones. We can simplify it by creating only a single instance. Also, as we\u0026rsquo;re going to use the pgoutput plugin for the Debezium source connector, we need to set rds:logical_replication value to \u0026ldquo;1\u0026rdquo; in the database cluster parameter group. Note to add the CloudFormation stack name of the VPN bastion host to the ParentSSHBastionStack parameter value so that the database can be accessed by the VPN bastion host. Also note that the template doesn\u0026rsquo;t include an inbound rule from the MSK cluster that\u0026rsquo;ll be created below. For now, we need to add the inbound rule manually. The updated template can be found in the project GitHub repository.\nSource Database As with the local development, we can create the source database by executing the db creation SQL scripts. A function is created in Python. After creating the datalake schema and setting the search path of the database (devdb) to it, it executes the SQL scripts.\n1# ./data/load/src.py 2def create_northwind_db(): 3 \u0026#34;\u0026#34;\u0026#34; 4 Create Northwind database by executing SQL scripts 5 \u0026#34;\u0026#34;\u0026#34; 6 try: 7 global conn 8 with conn: 9 with conn.cursor() as curs: 10 curs.execute(\u0026#34;CREATE SCHEMA datalake;\u0026#34;) 11 curs.execute(\u0026#34;SET search_path TO datalake;\u0026#34;) 12 curs.execute(\u0026#34;ALTER database devdb SET search_path TO datalake;\u0026#34;) 13 curs.execute(set_script_path(\u0026#34;01_northwind_ddl.sql\u0026#34;).open(\u0026#34;r\u0026#34;).read()) 14 curs.execute(set_script_path(\u0026#34;02_northwind_data.sql\u0026#34;).open(\u0026#34;r\u0026#34;).read()) 15 curs.execute(set_script_path(\u0026#34;03_cdc_events.sql\u0026#34;).open(\u0026#34;r\u0026#34;).read()) 16 conn.commit() 17 typer.echo(\u0026#34;Northwind SQL scripts executed\u0026#34;) 18 except (psycopg2.OperationalError, psycopg2.DatabaseError, FileNotFoundError) as err: 19 typer.echo(create_northwind_db.__name__, err) 20 close_conn() 21 exit(1) In order to facilitate the db creation, a simple command line application is created using the Typer library.\n1# ./data/load/main.py 2import typer 3from src import set_connection, create_northwind_db 4 5def main( 6 host: str = typer.Option(..., \u0026#34;--host\u0026#34;, \u0026#34;-h\u0026#34;, help=\u0026#34;Database host\u0026#34;), 7 port: int = typer.Option(5432, \u0026#34;--port\u0026#34;, \u0026#34;-p\u0026#34;, help=\u0026#34;Database port\u0026#34;), 8 dbname: str = typer.Option(..., \u0026#34;--dbname\u0026#34;, \u0026#34;-d\u0026#34;, help=\u0026#34;Database name\u0026#34;), 9 user: str = typer.Option(..., \u0026#34;--user\u0026#34;, \u0026#34;-u\u0026#34;, help=\u0026#34;Database user name\u0026#34;), 10 password: str = typer.Option(..., prompt=True, hide_input=True, help=\u0026#34;Database user password\u0026#34;), 11): 12 to_create = typer.confirm(\u0026#34;To create database?\u0026#34;) 13 if to_create: 14 params = {\u0026#34;host\u0026#34;: host, \u0026#34;port\u0026#34;: port, \u0026#34;dbname\u0026#34;: dbname, \u0026#34;user\u0026#34;: user, \u0026#34;password\u0026#34;: password} 15 set_connection(params) 16 create_northwind_db() 17 18if __name__ == \u0026#34;__main__\u0026#34;: 19 typer.run(main) It has a set of options to specify - database host, post, database name and username. The database password is set to be prompted, and an additional confirmation is required. If all options are provided, the app runs and creates the source database.\n1(venv) jaehyeon@cevo:~/data-lake-demo$ python data/load/main.py --help 2Usage: main.py [OPTIONS] 3 4Options: 5 -h, --host TEXT Database host [required] 6 -p, --port INTEGER Database port [default: 5432] 7 -d, --dbname TEXT Database name [required] 8 -u, --user TEXT Database user name [required] 9 --password TEXT Database user password [required] 10 --install-completion Install completion for the current shell. 11 --show-completion Show completion for the current shell, to copy it or 12 customize the installation. 13 --help Show this message and exit. 14 15(venv) jaehyeon@cevo:~/data-lake-demo$ python data/load/main.py -h \u0026lt;db-host-name-or-ip\u0026gt; -d \u0026lt;dbname\u0026gt; -u \u0026lt;username\u0026gt; 16Password: 17To create database? [y/N]: y 18Database connection created 19Northwind SQL scripts executed As illustrated thoroughly in the previous post, it inserts 829 order event records to the cdc_events table.\nMSK Cluster We\u0026rsquo;ll create an MSK cluster with 2 brokers (servers). The instance type of brokers is set to kafka.m5.large. Note that the smallest instance type of kafka.t3.small may look better for development, but we\u0026rsquo;ll have a failed authentication error when IAM Authentication is used for access control and connectors are created on MSK Connect. It is because the T3 instance type is limited to 1 TCP connection per broker per second and if the frequency is higher than the limit, that error is thrown. Note, while it\u0026rsquo;s possible to avoid it by updating reconnect.backoff.ms to 1000, it is not allowed on MSK Connect.\nWhen it comes to inbound rules of the cluster security group, we need to configure that all inbound traffic is allowed from its own security group. This is because connectors on MSK Connect are deployed with the same security group of the MSK cluster, and they should have access to it. Also, we need to allow port 9098 from the security group of the VPN bastion host - 9098 is the port for establishing the initial connection to the cluster when IAM Authentication is used.\nFinally, a cluster configuration is created manually as it\u0026rsquo;s not supported by CloudFormation and its ARN and revision number are added as parameters. The configuration is shown below.\n1auto.create.topics.enable = true 2delete.topic.enable = true The CloudFormation template can be found in the project GitHub repository.\nMSK Connect Role As we use IAM Authentication for access control, the connectors need to have permission on the cluster, topic and group. Also, the sink connector should have access to put output files to the S3 bucket. For simplicity, a single connector role is created for both source and sink connectors in the same template.\nCDC Development In order to create a connector on MSK Connect, we need to create a custom plugin and the connector itself. A custom plugin is a set of JAR files containing the implementation of one or more connectors, transforms, or converters and installed on the workers of the connect cluster where the connector is running. Both the resources are not supported by CloudFormation and can be created on AWS Console or using AWS SDK.\nCustom Plugins We need plugins for the Debezium source and S3 sink connectors. Plugin objects can be saved to S3 in either JAR or ZIP file format. We can download them from the relevant release/download pages of Debezium and Lenses Stream Reactor. Note to put contents at the root level of the zip archives. Once they are saved to S3, we can create them simply by specifying their S3 URIs.\nMSK Connectors Source Connector _Debezium\u0026rsquo;s PostgreSQL connector captures row-level changes in the schemas of a PostgreSQL database. PostgreSQL versions 9.6, 10, 11, 12 and 13 are supported. The first time it connects to a PostgreSQL server or cluster, the connector takes a consistent snapshot of all schemas. After that snapshot is complete, the connector continuously captures row-level changes that insert, update, and delete database content and that were committed to a PostgreSQL database. The connector generates data change event records and streams them to Kafka topics. For each table, the default behavior is that the connector streams all generated events to a separate Kafka topic for that table. Applications and services consume data change event records from that topic.\nThe connector has a number of connector properties including name, connector class, database connection details, key/value converter and so on - the full list of properties can be found in this page. The properties that need explanation are listed below.\nplugin.name - Using the logical decoding feature, an output plug-in enables clients to consume changes to the transaction log in a user-friendly manner. Debezium supports decoderbufs, wal2json and pgoutput plug-ins. Both wal2json and pgoutput are available in Amazon RDS for PostgreSQL and Amazon Aurora PostgreSQL. decoderbufs requires a separate installation, and it is excluded from the option. Among the 2 supported plug-ins, pgoutput is selected because it is the standard logical decoding output plug-in in PostgreSQL 10+ and has better performance for large transactions. publication.name - With the pgoutput plug-in, the Debezium connector creates a publication (if not exists) and sets publication.autocreate.mode to all_tables. It can cause an issue to update a record to a table that doesn\u0026rsquo;t have the primary key or replica identity. We can set the value to filtered where the connector adjusts the applicable tables by other property values. Alternatively we can create a publication on our own and add the name to publication.name property. I find creating a publication explicitly is easier to maintain. Note a publication alone is not sufficient to handle the issue. All affected tables by the publication should have the primary key or replica identity. In our example, the _orders _and _order_details _tables should meet the condition. In short, creating an explicit publication can prevent the event generation process from interrupting other processes by limiting the scope of CDC event generation. key.converter/value.converter - Although Avro serialization is recommended, JSON is a format that can be generated without schema registry and can be read by DeltaStreamer. transforms - A Debezium event data has a complex structure that provides a wealth of information. It can be quite difficult to process such a structure using DeltaStreamer. Debezium\u0026rsquo;s event flattening single message transformation (SMT) is configured to flatten the output payload. Note once the connector is deployed, the CDC event records will be published to msk.datalake.cdc_events topic.\n1# ./connector/msk/source-debezium.properties 2connector.class=io.debezium.connector.postgresql.PostgresConnector 3tasks.max=1 4plugin.name=pgoutput 5publication.name=cdc_publication 6database.hostname=\u0026lt;database-hostname-or-ip-address\u0026gt; 7database.port=5432 8database.user=\u0026lt;database-user\u0026gt; 9database.password=\u0026lt;database-user-password\u0026gt; 10database.dbname=devdb 11database.server.name=msk 12schema.include=datalake 13table.include.list=datalake.cdc_events 14key.converter=org.apache.kafka.connect.json.JsonConverter 15value.converter=org.apache.kafka.connect.json.JsonConverter 16transforms=unwrap 17transforms.unwrap.type=io.debezium.transforms.ExtractNewRecordState 18transforms.unwrap.drop.tombstones=false 19transforms.unwrap.delete.handling.mode=rewrite 20transforms.unwrap.add.fields=op,db,table,schema,lsn,source.ts_ms The connector can be created in multiple steps. The first step is to select the relevant custom plugin from the custom plugins list.\nIn the second step, we can configure connector properties. Applicable properties are\nConnector name and description Apache Kafka cluster (MSK or self-managed) with an authentication method Connector configuration Connector capacity - Autoscaled or Provisioned Worker configuration IAM role of the connector - it is created in the CloudFormation template In the third step, we configure cluster security. As we selected IAM Authentication, only TLS encryption is available.\nFinally, we can configure logging. A log group is created in the CloudFormation, and we can add its ARN.\nAfter reviewing, we can create the connector.\nSink Connector Lenses S3 Connector is a Kafka Connect sink connector for writing records from Kafka to AWS S3 Buckets. It extends the standard connect config adding a parameter for a SQL command (Lenses Kafka Connect Query Language or \u0026ldquo;KCQL\u0026rdquo;). This defines how to map data from the source (in this case Kafka) to the target (S3). Importantly, it also includes how data should be partitioned into S3, the bucket names and the serialization format (support includes JSON, Avro, Parquet, Text, CSV and binary).\nI find the Lenses S3 connector is more straightforward to configure than the Confluent S3 sink connector for its SQL-like syntax. The KCQL configuration indicates that object files are set to be\nmoved from a Kafka topic (msk.datalake.cdc_events) to an S3 bucket (data-lake-demo-cevo) with object prefix of _cdc-events-local, partitioned by customer_id and order_id e.g. customer_id=\u0026lt;customer-id\u0026gt;/order_id=\u0026lt;order-id\u0026gt;, stored as the JSON format and, flushed every 60 seconds or when there are 50 records. 1# ./connector/msk/sink-s3-lenses.properties 2connector.class=io.lenses.streamreactor.connect.aws.s3.sink.S3SinkConnector 3tasks.max=1 4connect.s3.kcql=INSERT INTO data-lake-demo-cevo:cdc-events SELECT * FROM msk.datalake.cdc_events PARTITIONBY customer_id,order_id STOREAS `json` WITH_FLUSH_INTERVAL = 60 WITH_FLUSH_COUNT = 50 5aws.region=ap-southeast-2 6aws.custom.endpoint=https://s3.ap-southeast-2.amazonaws.com/ 7topics=msk.datalake.cdc_events 8key.converter.schemas.enable=false 9schema.enable=false 10errors.log.enable=true 11key.converter=org.apache.kafka.connect.json.JsonConverter 12value.converter=org.apache.kafka.connect.json.JsonConverter We can create the sink connector on AWS Console using the same steps to the source connector.\nUpdate/Insert Examples Kafka UI When we used the Confluent platform for local development in the previous post, we checked topics and messages on the control tower UI. For MSK, we can use Kafka UI. Below shows a docker-compose file for it. Note that the MSK cluster is secured by IAM Authentication so that _AWS_MSK_IAM _is specified as the SASL mechanism. Under the hood, it uses Amazon MSK Library for AWS IAM for authentication and AWS credentials are provided via volume mapping. Also don\u0026rsquo;t forget to connect the VPN bastion host using the SoftEther VPN client program.\n1# ./kafka-ui/docker-compose.yml 2version: \u0026#34;2\u0026#34; 3services: 4 kafka-ui: 5 image: provectuslabs/kafka-ui 6 container_name: kafka-ui 7 ports: 8 - \u0026#34;8080:8080\u0026#34; 9 # restart: always 10 volumes: 11 - $HOME/.aws:/root/.aws 12 environment: 13 KAFKA_CLUSTERS_0_NAME: msk 14 KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: $BS_SERVERS 15 KAFKA_CLUSTERS_0_PROPERTIES_SECURITY_PROTOCOL: \u0026#34;SASL_SSL\u0026#34; 16 KAFKA_CLUSTERS_0_PROPERTIES_SASL_MECHANISM: \u0026#34;AWS_MSK_IAM\u0026#34; 17 KAFKA_CLUSTERS_0_PROPERTIES_SASL_CLIENT_CALLBACK_HANDLER_CLASS: \u0026#34;software.amazon.msk.auth.iam.IAMClientCallbackHandler\u0026#34; 18 KAFKA_CLUSTERS_0_PROPERTIES_SASL_JAAS_CONFIG: \u0026#34;software.amazon.msk.auth.iam.IAMLoginModule required;\u0026#34; Once started, the UI can be accessed via http://localhost:8080. We see that the CDC topic is found in the topics list. There are 829 messages in the topic, and it matches the number of records in the cdc_events table at creation.\nWhen we click the topic name, it shows further details of it. When clicking the messages tab, we can see individual messages within the topic. The UI also has some other options, and it can be convenient to manage topics and messages.\nThe message records can be expanded, and we can check a message\u0026rsquo;s key, content and headers. Also, we can either copy the value to clipboard or save as a file.\nUpdate Event The order 11077 initially didn\u0026rsquo;t have the _shipped_date _value. When the value is updated later, a new output file will be generated with the updated value.\n1BEGIN TRANSACTION; 2 UPDATE orders 3 SET shipped_date = \u0026#39;1998-06-15\u0026#39;::date 4 WHERE order_id = 11077; 5COMMIT TRANSACTION; 6END; In S3, we see 2 JSON objects are included in the output file for the order entry and the shipped_date value is updated as expected. Note that the Debezium connector converts the DATE type to the INT32 type, which represents the number of days since the epoch.\nInsert Event When a new order is created, it\u0026rsquo;ll insert a record to the _orders _table as well as one or more order items to the _order_details _table. Therefore, we expect multiple event records will be created when a new order is created. We can check it by inserting an order and related order details items.\n1BEGIN TRANSACTION; 2 INSERT INTO orders VALUES (11075, \u0026#39;RICSU\u0026#39;, 8, \u0026#39;1998-05-06\u0026#39;, \u0026#39;1998-06-03\u0026#39;, NULL, 2, 6.19000006, \u0026#39;Richter Supermarkt\u0026#39;, \u0026#39;Starenweg 5\u0026#39;, \u0026#39;Genève\u0026#39;, NULL, \u0026#39;1204\u0026#39;, \u0026#39;Switzerland\u0026#39;); 3 INSERT INTO order_details VALUES (11075, 2, 19, 10, 0.150000006); 4 INSERT INTO order_details VALUES (11075, 46, 12, 30, 0.150000006); 5 INSERT INTO order_details VALUES (11075, 76, 18, 2, 0.150000006); 6COMMIT TRANSACTION; 7END; We can see the output file includes 4 JSON objects where the first object has NULL _order_items _and _products _value. We can also see that those values are expanded gradually in subsequent event records\nConclusion We created a VPC that has private and public subnets in 2 availability zones in order to build and deploy the data lake solution on AWS. NAT instances are created to forward outbound traffic to the internet and a VPN bastion host is set up to facilitate deployment. An Aurora PostgreSQL cluster is deployed to host the source database and a Python command line app is used to create the database. To develop data ingestion using CDC, an MSK cluster is deployed and the Debezium source and Lenses S3 sink connectors are created on MSK Connect. We also confirmed the order creation and update events are captured as expected with the scenarios used by local development. Using CDC event output files in S3, we are able to build an Apache Hudi table on EMR and use it for ad-hoc queries and report/dashboard generation. It\u0026rsquo;ll be covered in the next post.\n","date":"December 12, 2021","img":"/blog/2021-12-12-datalake-demo-part2/featured.png","lang":"en","langName":"English","largeImg":"/blog/2021-12-12-datalake-demo-part2/featured_hua25eccd3824300d3b1ed87f56797248c_164526_500x0_resize_box_3.png","permalink":"/blog/2021-12-12-datalake-demo-part2/","series":[{"title":"Data Lake Demo Using Change Data Capture","url":"/series/data-lake-demo-using-change-data-capture/"}],"smallImg":"/blog/2021-12-12-datalake-demo-part2/featured_hua25eccd3824300d3b1ed87f56797248c_164526_180x0_resize_box_3.png","tags":[{"title":"AWS","url":"/tags/aws/"},{"title":"Amazon EMR","url":"/tags/amazon-emr/"},{"title":"Amazon MSK","url":"/tags/amazon-msk/"},{"title":"Amazon MSK Connect","url":"/tags/amazon-msk-connect/"},{"title":"Apache Spark","url":"/tags/apache-spark/"},{"title":"Apache Hudi","url":"/tags/apache-hudi/"},{"title":"Apache Kafka","url":"/tags/apache-kafka/"},{"title":"Kafka Connect","url":"/tags/kafka-connect/"},{"title":"Change Data Capture","url":"/tags/change-data-capture/"},{"title":"Data Lake","url":"/tags/data-lake/"},{"title":"Docker","url":"/tags/docker/"},{"title":"Terraform","url":"/tags/terraform/"}],"timestamp":1639267200,"title":"Data Lake Demo Using Change Data Capture (CDC) on AWS – Part 2 Implement CDC"},{"categories":[{"title":"Data Engineering","url":"/categories/data-engineering/"}],"content":"Change data capture (CDC) is a proven data integration pattern that has a wide range of applications. Among those, data replication to data lakes is a good use case in data engineering. Coupled with best-in-breed data lake formats such as Apache Hudi, we can build an efficient data replication solution. This is the first post of the data lake demo series. Over time, we\u0026rsquo;ll build a data lake that uses CDC. As a starting point, we\u0026rsquo;ll discuss the source database and CDC streaming infrastructure in the local environment.\nPart 1 Local Development (this post) Part 2 Implement CDC Part 3 Implement Data Lake Architecture As described in a Red Hat IT topics article, change data capture (CDC) is a proven data integration pattern to track when and what changes occur in data then alert other systems and services that must respond to those changes. Change data capture helps maintain consistency and functionality across all systems that rely on data.\nThe primary use of CDC is to enable applications to respond almost immediately whenever data in databases change. Specifically its use cases cover microservices integration, data replication with up-to-date data, building time-sensitive analytics dashboards, auditing and compliance, cache invalidation, full-text search and so on. There are a number of approaches for CDC - polling, dual writes and log-based CDC. Among those, log-based CDC has advantages to other approaches.\nBoth Amazon DMS and Debezium implement log-based CDC. While the former is a managed service, the latter can be deployed to a Kafka cluster as a (source) connector. It uses Apache Kafka as a messaging service to deliver database change notifications to the applicable systems and applications. Note that Kafka Connect is a tool for streaming data between Apache Kafka and other data systems by connectors in a scalable and reliable way. In AWS, we can use Amazon MSK and MSK Connect for building a Debezium based CDC solution.\nData replication to data lakes using CDC can be much more effective if data is stored to a format that supports atomic transactions and consistent updates. Popular choices are Apache Hudi, Apache Iceberg and Delta Lake. Among those, Apache Hudi can be a good option as it is well-integrated with AWS services.\nBelow shows the architecture of the data lake solution that we will be building in this series of posts.\nEmploying the transactional outbox pattern, the source database publishes change event records to the CDC event table. The event records are generated by triggers that listen to insert and update events on source tables. CDC is implemented in a streaming environment and Amazon MSK is used to build the streaming infrastructure. In order to process the real-time CDC event records, a source and sink connectors are set up in Amazon MSK Connect. The Debezium connector for PostgreSQL is used as the source connector and the Lenses S3 connector is used as the sink connector. The sink connector pushes messages to a S3 bucket. Hudi DeltaStreamer is run on Amazon EMR. As a spark application, it reads files from the S3 bucket and upserts Hudi records to another S3 bucket. The Hudi table is created in the AWS Glue Data Catalog. The Hudi table is queried in Amazon Athena while the table is registered in the AWS Glue Data Catalog. Dashboards are created in Amazon Quicksight where the dataset is created using Amazon Athena. As a starting point, we\u0026rsquo;ll discuss the source database and streaming infrastructure in the local environment.\nSource Database Data Model We will use the Northwind database as the source database. It was originally created by Microsoft and used by various tutorials of their database products. It contains the sales data for a fictitious company called Northwind Traders that deals with specialty foods from around the world. As shown in the following entity relationship diagram, it includes a schema for a small business ERP with customers, products, orders, employees and so on. The version that is ported to PostgreSQL is obtained from YugabyteDB sample datasets and the SQL scripts can be found in the project GitHub repository. For local development, a service is created using docker compose, and it\u0026rsquo;ll be illustrated in the next section.\nOutbox Table and Event Generation It is straightforward to capture changes from multiple tables in a database using Kafka connectors where a separate topic is created for each table. Data ingestion to Hudi, however, can be complicated if messages are stored in multiple Kafka topics. Note that we will use the DeltaStreamer utility, and it maps to a single topic. In order to simplify the data ingestion process, we can employ the transactional outbox pattern. Using this pattern, we can create an outbox table (cdc_events) and upsert a record to it when a new transaction is made. In this way, all database changes can be pushed to a single topic, resulting in one DeltaStreamer process to listen to the change events.\nBelow shows the table creation statement of the outbox table. It aims to store all details of an order entry in a row. The columns that have the JSONB data type store attributes of other entities. For example, the order_items column includes ordered product information. As multiple products can be purchased, it keeps an array of product ID, unit price, quantity and discount.\n1-- ./data/sql/03_cdc_events.sql 2CREATE TABLE cdc_events( 3 order_id SMALLINT NOT NULL PRIMARY KEY, 4 customer_id BPCHAR NOT NULL, 5 order_date DATE, 6 required_date DATE, 7 shipped_date DATE, 8 order_items JSONB, 9 products JSONB, 10 customer JSONB, 11 employee JSONB, 12 shipper JSONB, 13 shipment JSONB, 14 updated_at TIMESTAMPTZ 15); In order to create event records, triggers are added to the orders and order_details tables. They execute the fn_insert_order_event function after an INSERT or UPDATE action occurs to the respective tables. Note the DELETE action is not considered in the event generation process for simplicity. The trigger function basically collects details of an order entry and attempts to insert a new record to the outbox table. If a record with the same order ID exists, it updates the record instead.\n1-- ./data/sql/03_cdc_events.sql 2CREATE TRIGGER orders_triggered 3 AFTER INSERT OR UPDATE 4 ON orders 5 FOR EACH ROW 6 EXECUTE PROCEDURE fn_insert_order_event(); 7 8CREATE TRIGGER order_details_triggered 9 AFTER INSERT OR UPDATE 10 ON order_details 11 FOR EACH ROW 12 EXECUTE PROCEDURE fn_insert_order_event(); 13 14CREATE OR REPLACE FUNCTION fn_insert_order_event() 15 RETURNS TRIGGER 16 LANGUAGE PLPGSQL 17 AS 18$$ 19BEGIN 20 IF (TG_OP IN (\u0026#39;INSERT\u0026#39;, \u0026#39;UPDATE\u0026#39;)) THEN 21 WITH product_details AS ( 22 SELECT p.product_id, 23 row_to_json(p.*)::jsonb AS product_details 24 FROM ( 25 SELECT * 26 FROM products p 27 JOIN suppliers s ON p.supplier_id = s.supplier_id 28 JOIN categories c ON p.category_id = c.category_id 29 ) AS p 30 ), order_items AS ( 31 SELECT od.order_id, 32 jsonb_agg(row_to_json(od.*)::jsonb - \u0026#39;order_id\u0026#39;) AS order_items, 33 jsonb_agg(pd.product_details) AS products 34 FROM order_details od 35 JOIN product_details pd ON od.product_id = pd.product_id 36 WHERE od.order_id = NEW.order_id 37 GROUP BY od.order_id 38 ), emps AS ( 39 SELECT employee_id, 40 row_to_json(e.*)::jsonb AS details 41 FROM employees e 42 ), emp_territories AS ( 43 SELECT et.employee_id, 44 jsonb_agg( 45 row_to_json(t.*) 46 ) AS territories 47 FROM employee_territories et 48 JOIN ( 49 SELECT t.territory_id, t.territory_description, t.region_id, r.region_description 50 FROM territories t 51 JOIN region r ON t.region_id = r.region_id 52 ) AS t ON et.territory_id = t.territory_id 53 GROUP BY et.employee_id 54 ), emp_details AS ( 55 SELECT e.employee_id, 56 e.details || jsonb_build_object(\u0026#39;territories\u0026#39;, et.territories) AS details 57 FROM emps AS e 58 JOIN emp_territories AS et ON e.employee_id = et.employee_id 59 ) 60 INSERT INTO cdc_events 61 SELECT o.order_id, 62 o.customer_id, 63 o.order_date, 64 o.required_date, 65 o.shipped_date, 66 oi.order_items, 67 oi.products, 68 row_to_json(c.*)::jsonb AS customer, 69 ed.details::jsonb AS employee, 70 row_to_json(s.*)::jsonb AS shipper, 71 jsonb_build_object( 72 \u0026#39;freight\u0026#39;, o.freight, 73 \u0026#39;ship_name\u0026#39;, o.ship_name, 74 \u0026#39;ship_address\u0026#39;, o.ship_address, 75 \u0026#39;ship_city\u0026#39;, o.ship_city, 76 \u0026#39;ship_region\u0026#39;, o.ship_region, 77 \u0026#39;ship_postal_code\u0026#39;, o.ship_postal_code, 78 \u0026#39;ship_country\u0026#39;, o.ship_country 79 ) AS shipment, 80 now() 81 FROM orders o 82 LEFT JOIN order_items oi ON o.order_id = oi.order_id 83 JOIN customers c ON o.customer_id = c.customer_id 84 JOIN emp_details ed ON o.employee_id = ed.employee_id 85 JOIN shippers s ON o.ship_via = s.shipper_id 86 WHERE o.order_id = NEW.order_id 87 ON CONFLICT (order_id) 88 DO UPDATE 89 SET order_id = excluded.order_id, 90 customer_id = excluded.customer_id, 91 order_date = excluded.order_date, 92 required_date = excluded.required_date, 93 shipped_date = excluded.shipped_date, 94 order_items = excluded.order_items, 95 products = excluded.products, 96 customer = excluded.customer, 97 shipper = excluded.shipper, 98 shipment = excluded.shipment, 99 updated_at = excluded.updated_at; 100 END IF; 101 RETURN NULL; 102END 103$$; Create Initial Event Records In order to create event records for existing order entries, a stored procedure is created - usp_init_order_events. It is quite similar to the trigger function and can be checked in the project GitHub repository. The procedure is called at database initialization and a total of 829 event records are created by that.\n1-- ./data/sql/03_cdc_events.sql 2CALL usp_init_order_events(); Below shows a simplified event record, converted into JSON. For the order with ID 10248, 3 products are ordered by a customer whose ID is VINET.\n1{ 2 \u0026#34;order_id\u0026#34;: 10248, 3 \u0026#34;customer_id\u0026#34;: \u0026#34;VINET\u0026#34;, 4 \u0026#34;order_date\u0026#34;: \u0026#34;1996-07-04\u0026#34;, 5 \u0026#34;required_date\u0026#34;: \u0026#34;1996-08-01\u0026#34;, 6 \u0026#34;shipped_date\u0026#34;: \u0026#34;1996-07-16\u0026#34;, 7 \u0026#34;order_items\u0026#34;: [ 8 { \u0026#34;discount\u0026#34;: 0, \u0026#34;quantity\u0026#34;: 12, \u0026#34;product_id\u0026#34;: 11, \u0026#34;unit_price\u0026#34;: 14 }, 9 { \u0026#34;discount\u0026#34;: 0, \u0026#34;quantity\u0026#34;: 10, \u0026#34;product_id\u0026#34;: 42, \u0026#34;unit_price\u0026#34;: 9.8 }, 10 { \u0026#34;discount\u0026#34;: 0, \u0026#34;quantity\u0026#34;: 5, \u0026#34;product_id\u0026#34;: 72, \u0026#34;unit_price\u0026#34;: 34.8 } 11 ], 12 \u0026#34;products\u0026#34;: [ 13 { \u0026#34;product_id\u0026#34;: 11, \u0026#34;product_name\u0026#34;: \u0026#34;Queso Cabrales\u0026#34; }, 14 { \u0026#34;product_id\u0026#34;: 42, \u0026#34;product_name\u0026#34;: \u0026#34;Singaporean Hokkien Fried Mee\u0026#34; }, 15 { \u0026#34;product_id\u0026#34;: 72, \u0026#34;product_name\u0026#34;: \u0026#34;Mozzarella di Giovanni\u0026#34; } 16 ], 17 \u0026#34;customer\u0026#34;: { 18 \u0026#34;customer_id\u0026#34;: \u0026#34;VINET\u0026#34;, 19 \u0026#34;company_name\u0026#34;: \u0026#34;Vins et alcools Chevalier\u0026#34; 20 }, 21 \u0026#34;employee\u0026#34;: { 22 \u0026#34;title\u0026#34;: \u0026#34;Sales Manager\u0026#34;, 23 \u0026#34;last_name\u0026#34;: \u0026#34;Buchanan\u0026#34;, 24 \u0026#34;employee_id\u0026#34;: 5 25 }, 26 \u0026#34;shipper\u0026#34;: { 27 \u0026#34;company_name\u0026#34;: \u0026#34;Federal Shipping\u0026#34; 28 }, 29 \u0026#34;shipment\u0026#34;: { 30 \u0026#34;freight\u0026#34;: 32.38, 31 \u0026#34;ship_name\u0026#34;: \u0026#34;Vins et alcools Chevalier\u0026#34; 32 }, 33 \u0026#34;updated_at\u0026#34;: \u0026#34;2021-11-27T20:30:13.644579+11:00\u0026#34; 34} Create Publication As discussed further in the next section, we\u0026rsquo;ll be using the native pgoutput logical replication stream support. Debezium, Kafka source connector, automatically creates a publication that contains all tables if it doesn\u0026rsquo;t exist. It can cause trouble to update a record to a table that doesn\u0026rsquo;t have the primary key or replica identity. In order to handle such an issue, a publication that contains only the outbox table is created. This publication will be used when configuring the source connector.\n1-- ./data/sql/03_cdc_events.sql 2CREATE PUBLICATION cdc_publication 3 FOR TABLE cdc_events; CDC Development Docker Compose The Confluent platform can be handy for local development, although we\u0026rsquo;ll be deploying the solution using Amazon MSK and MSK Connect. The quick start guide provides a docker compose file that includes its various components in separate services. It also contains the control center, a graphical user interface, which helps check brokers, topics, messages and connectors easily.\nAdditionally, we need a PostgreSQL instance for the Northwind database and a service named postgres is added. The database is initialised by a set of SQL scripts. They are executed by volume-mapping to the initialisation folder - the scripts can be found in the project GitHub repository. Also, the Kafka Connect instance, running in the connect service, needs an update to include the source and sink connectors. It\u0026rsquo;ll be illustrated further below.\nBelow shows a cut-down version of the docker compose file that we use for local development. The complete file can be found in the project GitHub repository.\n1# ./docker-compose.yml 2version: \u0026#34;2\u0026#34; 3services: 4 postgres: 5 image: debezium/postgres:13 6 ... 7 ports: 8 - 5432:5432 9 volumes: 10 - ./data/sql:/docker-entrypoint-initdb.d 11 environment: 12 - POSTGRES_DB=devdb 13 - POSTGRES_USER=devuser 14 - POSTGRES_PASSWORD=password 15 zookeeper: 16 image: confluentinc/cp-zookeeper:6.2.1 17 ... 18 ports: 19 - \u0026#34;2181:2181\u0026#34; 20 environment: 21 ... 22 broker: 23 image: confluentinc/cp-server:6.2.1 24 ... 25 depends_on: 26 - zookeeper 27 ports: 28 - \u0026#34;9092:9092\u0026#34; 29 - \u0026#34;9101:9101\u0026#34; 30 environment: 31 ... 32 KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:29092,PLAINTEXT_HOST://localhost:9092 33 ... 34 schema-registry: 35 image: confluentinc/cp-schema-registry:6.2.1 36 ... 37 depends_on: 38 ... 39 ports: 40 - \u0026#34;8081:8081\u0026#34; 41 environment: 42 ... 43 connect: 44 build: .connector/local/cp-server-connect-datagen 45 ... 46 depends_on: 47 ... 48 ports: 49 - \u0026#34;8083:8083\u0026#34; 50 volumes: 51 - ${HOME}/.aws:/home/appuser/.aws 52 environment: 53 CONNECT_BOOTSTRAP_SERVERS: \u0026#34;broker:29092\u0026#34; 54 CONNECT_REST_ADVERTISED_HOST_NAME: connect 55 CONNECT_REST_PORT: 8083 56 ... 57 # include /usr/local/share/kafka/plugins for community connectors 58 CONNECT_PLUGIN_PATH: \u0026#34;/usr/share/java,/usr/share/confluent-hub-components,/usr/local/share/kafka/plugins\u0026#34; 59 ... 60 control-center: 61 image: confluentinc/cp-enterprise-control-center:6.2.1 62 ... 63 depends_on: 64 ... 65 ports: 66 - \u0026#34;9021:9021\u0026#34; 67 environment: 68 CONTROL_CENTER_BOOTSTRAP_SERVERS: \u0026#34;broker:29092\u0026#34; 69 CONTROL_CENTER_CONNECT_CONNECT-DEFAULT_CLUSTER: \u0026#34;connect:8083\u0026#34; 70 ... 71 PORT: 9021 72 rest-proxy: 73 image: confluentinc/cp-kafka-rest:6.2.1 74 ... 75 depends_on: 76 ... 77 ports: 78 - 8082:8082 79 environment: 80 ... Install Connectors We use the Debezium connector for PostgreSQL as the source connector and Lenses S3 Connector as the sink connector. The source connector is installed via the confluent hub client while the sink connector is added as a community connector. Note that the environment variable of CONNECT_PLUGIN_PATH is updated to include the kafka plugin folder (/usr/local/share/kafka/plugins).\n1# .connector/local/cp-server-connect-datagen/Dockerfile 2FROM cnfldemos/cp-server-connect-datagen:0.5.0-6.2.1 3 4# install debezium postgresql connector from confluent hub 5RUN confluent-hub install --no-prompt debezium/debezium-connector-postgresql:1.7.1 6 7# install lenses S3 connector as a community connector - https://docs.confluent.io/home/connect/community.html 8USER root 9RUN mkdir -p /usr/local/share/kafka/plugins/kafka-connect-aws-s3 \u0026amp;\u0026amp; \\ 10 curl -SsL https://github.com/lensesio/stream-reactor/releases/download/3.0.0/kafka-connect-aws-s3-3.0.0-2.5.0-all.tar.gz \\ 11 | tar -C /usr/local/share/kafka/plugins/kafka-connect-aws-s3 --warning=no-unknown-keyword -xzf - 12 13# update connect plugin path 14ENV CONNECT_PLUGIN_PATH=$CONNECT_PLUGIN_PATH,/usr/local/share/kafka/plugins 15USER appuser Start Services After starting the docker compose services, we can check a local Kafka cluster in the control center via http://localhost:9021. We can go to the cluster overview page by clicking the cluster card item.\nWe can check an overview of the cluster in the page. For example, it shows the cluster has 1 broker and there is no running connector. On the left side, there are menus for individual components. Among those, Topics and Connect will be our main interest in this post.\nCreate Connectors Source Connector Debezium\u0026rsquo;s PostgreSQL connector captures row-level changes in the schemas of a PostgreSQL database. PostgreSQL versions 9.6, 10, 11, 12 and 13 are supported. The first time it connects to a PostgreSQL server or cluster, the connector takes a consistent snapshot of all schemas. After that snapshot is complete, the connector continuously captures row-level changes that insert, update, and delete database content and that were committed to a PostgreSQL database. The connector generates data change event records and streams them to Kafka topics. For each table, the default behavior is that the connector streams all generated events to a separate Kafka topic for that table. Applications and services consume data change event records from that topic.\nThe connector has a number of connector properties including name, connector class, database connection details, key/value converter and so on - the full list of properties can be found in this page. The properties that need explanation are listed below.\nplugin.name - Using the logical decoding feature, an output plug-in enables clients to consume changes to the transaction log in a user-friendly manner. Debezium supports decoderbufs, wal2json and pgoutput plug-ins. Both wal2json and pgoutput are available in Amazon RDS for PostgreSQL and Amazon Aurora PostgreSQL. decoderbufs requires a separate installation, and it is excluded from the option. Among the 2 supported plug-ins, pgoutput is selected because it is the standard logical decoding output plug-in in PostgreSQL 10+ and has better performance for large transactions. publication.name - With the pgoutput plug-in, the Debezium connector creates a publication (if not exists) and sets publication.autocreate.mode to all_tables. It can cause an issue to update a record to a table that doesn\u0026rsquo;t have the primary key or replica identity. We can set the value to filtered where the connector adjusts the applicable tables by other property values. Alternatively we can create a publication on our own and add the name to publication.name property. I find creating a publication explicitly is easier to maintain. Note a publication alone is not sufficient to handle the issue. All affected tables by the publication should have the primary key or replica identity. In our example, the _orders _and _order_details _tables should meet the condition. In short, creating an explicit publication can prevent the event generation process from interrupting other processes by limiting the scope of CDC event generation. key.converter/value.converter - Although Avro serialization is recommended, JSON is a format that can be generated without schema registry and can be read by DeltaStreamer. transforms - A Debezium event data has a complex structure that provides a wealth of information. It can be quite difficult to process such a structure using DeltaStreamer. Debezium\u0026rsquo;s event flattening single message transformation (SMT) is configured to flatten the output payload. Note once the connector is deployed, the CDC event records will be published to demo.datalake.cdc_events topic.\n1// ./connector/local/source-debezium.json 2{ 3 \u0026#34;name\u0026#34;: \u0026#34;orders-source\u0026#34;, 4 \u0026#34;config\u0026#34;: { 5 \u0026#34;connector.class\u0026#34;: \u0026#34;io.debezium.connector.postgresql.PostgresConnector\u0026#34;, 6 \u0026#34;tasks.max\u0026#34;: \u0026#34;1\u0026#34;, 7 \u0026#34;plugin.name\u0026#34;: \u0026#34;pgoutput\u0026#34;, 8 \u0026#34;publication.name\u0026#34;: \u0026#34;cdc_publication\u0026#34;, 9 \u0026#34;database.hostname\u0026#34;: \u0026#34;postgres\u0026#34;, 10 \u0026#34;database.port\u0026#34;: \u0026#34;5432\u0026#34;, 11 \u0026#34;database.user\u0026#34;: \u0026#34;devuser\u0026#34;, 12 \u0026#34;database.password\u0026#34;: \u0026#34;password\u0026#34;, 13 \u0026#34;database.dbname\u0026#34;: \u0026#34;devdb\u0026#34;, 14 \u0026#34;database.server.name\u0026#34;: \u0026#34;demo\u0026#34;, 15 \u0026#34;schema.include\u0026#34;: \u0026#34;datalake\u0026#34;, 16 \u0026#34;table.include.list\u0026#34;: \u0026#34;datalake.cdc_events\u0026#34;, 17 \u0026#34;key.converter\u0026#34;: \u0026#34;org.apache.kafka.connect.json.JsonConverter\u0026#34;, 18 \u0026#34;value.converter\u0026#34;: \u0026#34;org.apache.kafka.connect.json.JsonConverter\u0026#34;, 19 \u0026#34;transforms\u0026#34;: \u0026#34;unwrap\u0026#34;, 20 \u0026#34;transforms.unwrap.type\u0026#34;: \u0026#34;io.debezium.transforms.ExtractNewRecordState\u0026#34;, 21 \u0026#34;transforms.unwrap.drop.tombstones\u0026#34;: \u0026#34;false\u0026#34;, 22 \u0026#34;transforms.unwrap.delete.handling.mode\u0026#34;: \u0026#34;rewrite\u0026#34;, 23 \u0026#34;transforms.unwrap.add.fields\u0026#34;: \u0026#34;op,db,table,schema,lsn,source.ts_ms\u0026#34; 24 } 25} The source connector is created using the API and its status can be checked as shown below.\n1## create debezium source connector 2curl -i -X POST -H \u0026#34;Accept:application/json\u0026#34; -H \u0026#34;Content-Type:application/json\u0026#34; \\ 3 http://localhost:8083/connectors/ -d @connector/local/source-debezium.json 4 5## check connector status 6curl http://localhost:8083/connectors/orders-source/status 7 8#{ 9# \u0026#34;name\u0026#34;: \u0026#34;orders-source\u0026#34;, 10# \u0026#34;connector\u0026#34;: { \u0026#34;state\u0026#34;: \u0026#34;RUNNING\u0026#34;, \u0026#34;worker_id\u0026#34;: \u0026#34;connect:8083\u0026#34; }, 11# \u0026#34;tasks\u0026#34;: [{ \u0026#34;id\u0026#34;: 0, \u0026#34;state\u0026#34;: \u0026#34;RUNNING\u0026#34;, \u0026#34;worker_id\u0026#34;: \u0026#34;connect:8083\u0026#34; }], 12# \u0026#34;type\u0026#34;: \u0026#34;source\u0026#34; 13#} Sink Connector Lenses S3 Connector is a Kafka Connect sink connector for writing records from Kafka to AWS S3 Buckets. It extends the standard connect config adding a parameter for a SQL command (Lenses Kafka Connect Query Language or \u0026ldquo;KCQL\u0026rdquo;). This defines how to map data from the source (in this case Kafka) to the target (S3). Importantly, it also includes how data should be partitioned into S3, the bucket names and the serialization format (support includes JSON, Avro, Parquet, Text, CSV and binary).\nI find the Lenses S3 connector is more straightforward to configure than the Confluent S3 sink connector for its SQL-like syntax. The KCQL configuration indicates that object files are set to be\nmoved from a Kafka topic (demo.datalake.cdc_events) to an S3 bucket (data-lake-demo-cevo) with object prefix of _cdc-events-local, partitioned by customer_id and order_id e.g. customer_id=\u0026lt;customer-id\u0026gt;/order_id=\u0026lt;order-id\u0026gt;, stored as the JSON format and, flushed every 60 seconds or when there are 50 records. 1// ./connector/local/sink-s3.json 2{ 3 \u0026#34;name\u0026#34;: \u0026#34;orders-sink\u0026#34;, 4 \u0026#34;config\u0026#34;: { 5 \u0026#34;connector.class\u0026#34;: \u0026#34;io.lenses.streamreactor.connect.aws.s3.sink.S3SinkConnector\u0026#34;, 6 \u0026#34;tasks.max\u0026#34;: \u0026#34;1\u0026#34;, 7 \u0026#34;connect.s3.kcql\u0026#34;: \u0026#34;INSERT INTO data-lake-demo-cevo:cdc-events-local SELECT * FROM demo.datalake.cdc_events PARTITIONBY customer_id,order_id STOREAS `json` WITH_FLUSH_INTERVAL = 60 WITH_FLUSH_COUNT = 50\u0026#34;, 8 \u0026#34;aws.region\u0026#34;: \u0026#34;ap-southeast-2\u0026#34;, 9 \u0026#34;aws.custom.endpoint\u0026#34;: \u0026#34;https://s3.ap-southeast-2.amazonaws.com/\u0026#34;, 10 \u0026#34;topics\u0026#34;: \u0026#34;demo.datalake.cdc_events\u0026#34;, 11 \u0026#34;key.converter.schemas.enable\u0026#34;: \u0026#34;false\u0026#34;, 12 \u0026#34;schema.enable\u0026#34;: \u0026#34;false\u0026#34;, 13 \u0026#34;errors.log.enable\u0026#34;: \u0026#34;true\u0026#34;, 14 \u0026#34;key.converter\u0026#34;: \u0026#34;org.apache.kafka.connect.json.JsonConverter\u0026#34;, 15 \u0026#34;value.converter\u0026#34;: \u0026#34;org.apache.kafka.connect.json.JsonConverter\u0026#34; 16 } 17} The sink connector is created using the API and its status can be checked as shown below.\n1### create s3 sink connector 2curl -i -X POST -H \u0026#34;Accept:application/json\u0026#34; -H \u0026#34;Content-Type:application/json\u0026#34; \\ 3 http://localhost:8083/connectors/ -d @connector/local/sink-s3.json 4 5## check connector status 6curl http://localhost:8083/connectors/orders-sink/status 7 8#{ 9# \u0026#34;name\u0026#34;: \u0026#34;orders-sink\u0026#34;, 10# \u0026#34;connector\u0026#34;: { \u0026#34;state\u0026#34;: \u0026#34;RUNNING\u0026#34;, \u0026#34;worker_id\u0026#34;: \u0026#34;connect:8083\u0026#34; }, 11# \u0026#34;tasks\u0026#34;: [{ \u0026#34;id\u0026#34;: 0, \u0026#34;state\u0026#34;: \u0026#34;RUNNING\u0026#34;, \u0026#34;worker_id\u0026#34;: \u0026#34;connect:8083\u0026#34; }], 12# \u0026#34;type\u0026#34;: \u0026#34;sink\u0026#34; 13#} We can also check the details of the connectors from the control center.\nIn the Topics menu, we are able to see that the demo.datalake.cdc_events topic is created by the Debezium connector.\nWe can check messages of a topic by clicking the topic name. After adding an offset value (e.g. 0) to the input element, we are able to see messages of the topic. We can check message fields on the left-hand side or download a message in the JSON or CSV format.\nCheck Event Output Files We can check the output files that are processed by the sink connector in S3. Below shows an example record where _customer_id _is RATTC and _order_id _is 11077. As configured, the objects are prefixed by cdc-events-local and further partitioned by _customer_id _and order_id. The naming convention of output files is \u0026lt;topic-name\u0026gt;(partition_offset).ext.\nUpdate Event Example The above order record has a NULL shipped_date value. When we update it using the following SQL statement, we should be able to see a new output file with the updated value.\n1BEGIN TRANSACTION; 2 UPDATE orders 3 SET shipped_date = \u0026#39;1998-06-15\u0026#39;::date 4 WHERE order_id = 11077; 5COMMIT TRANSACTION; 6END; In S3, we are able to see that a new output file is stored. In the new output file, the _shipped_date _value is updated to 10392. Note that the Debezium connector converts the DATE type to the INT32 type, which represents the number of days since the epoch.\nInsert Event Example When a new order is created, it\u0026rsquo;ll insert a record to the _orders _table as well as one or more order items to the _order_details _table. Therefore, we expect multiple event records will be created when a new order is created. We can check it by inserting an order and related order details items.\n1BEGIN TRANSACTION; 2 INSERT INTO orders VALUES (11075, \u0026#39;RICSU\u0026#39;, 8, \u0026#39;1998-05-06\u0026#39;, \u0026#39;1998-06-03\u0026#39;, NULL, 2, 6.19000006, \u0026#39;Richter Supermarkt\u0026#39;, \u0026#39;Starenweg 5\u0026#39;, \u0026#39;Genève\u0026#39;, NULL, \u0026#39;1204\u0026#39;, \u0026#39;Switzerland\u0026#39;); 3 INSERT INTO order_details VALUES (11075, 2, 19, 10, 0.150000006); 4 INSERT INTO order_details VALUES (11075, 46, 12, 30, 0.150000006); 5 INSERT INTO order_details VALUES (11075, 76, 18, 2, 0.150000006); 6COMMIT TRANSACTION; 7END; We can see the output file includes 4 JSON objects where the first object has NULL _order_items _and _products _value. We can also see that those values are expanded gradually in subsequent event records.\nConclusion We discussed a data lake solution where data ingestion is performed using change data capture (CDC) and the output files are upserted to a Hudi table. Being registered to Glue Data Catalog, it can be used for ad-hoc queries and report/dashboard creation. For the solution, the Northwind database is used as the source database and, following the transactional outbox pattern, order-related changes are upserted to an outbox table by triggers. The data ingestion is developed in the local Confluent platform where the Debezium for PostgreSQL is used as the source connector and the Lenses S3 sink connector is used as the sink connector. We confirmed order creation and update events are captured as expected. In the next post, we\u0026rsquo;ll build the data ingestion part of the solution with Amazon MSK and MSK Connect.\n","date":"December 5, 2021","img":"/blog/2021-12-05-datalake-demo-part1/featured.png","lang":"en","langName":"English","largeImg":"/blog/2021-12-05-datalake-demo-part1/featured_hua25eccd3824300d3b1ed87f56797248c_164526_500x0_resize_box_3.png","permalink":"/blog/2021-12-05-datalake-demo-part1/","series":[{"title":"Data Lake Demo Using Change Data Capture","url":"/series/data-lake-demo-using-change-data-capture/"}],"smallImg":"/blog/2021-12-05-datalake-demo-part1/featured_hua25eccd3824300d3b1ed87f56797248c_164526_180x0_resize_box_3.png","tags":[{"title":"AWS","url":"/tags/aws/"},{"title":"Amazon EMR","url":"/tags/amazon-emr/"},{"title":"Amazon MSK","url":"/tags/amazon-msk/"},{"title":"Amazon MSK Connect","url":"/tags/amazon-msk-connect/"},{"title":"Apache Spark","url":"/tags/apache-spark/"},{"title":"Apache Hudi","url":"/tags/apache-hudi/"},{"title":"Apache Kafka","url":"/tags/apache-kafka/"},{"title":"Kafka Connect","url":"/tags/kafka-connect/"},{"title":"Change Data Capture","url":"/tags/change-data-capture/"},{"title":"Data Lake","url":"/tags/data-lake/"},{"title":"Docker","url":"/tags/docker/"},{"title":"Docker Compose","url":"/tags/docker-compose/"}],"timestamp":1638662400,"title":"Data Lake Demo Using Change Data Capture (CDC) on AWS – Part 1 Local Development"},{"categories":[{"title":"Data Engineering","url":"/categories/data-engineering/"}],"content":"In an earlier post, I demonstrated how to set up a local development environment for AWS Glue 1.0 and 2.0 using a docker image that is published by the AWS Glue team and the Visual Studio Code Remote – Containers extension. Recently AWS Glue 3.0 was released, but a docker image for this version is not published. In this post, I\u0026rsquo;ll illustrate how to create a development environment for AWS Glue 3.0 (and later versions) by building a custom docker image.\nGlue Base Docker Image The Glue base images are built while referring to the official AWS Glue Python local development documentation. For example, the latest image that targets Glue 3.0 is built on top of the official Python image on the latest stable Debian version (python:3.7.12-bullseye). After installing utilities (zip and AWS CLI V2), Open JDK 8 is installed. Then Maven, Spark and Glue Python libraries (aws-glue-libs) are added to the /opt directory and Glue dependencies are downloaded by sourcing glue-setup.sh. It ends up downloading default Python packages and updating the _GLUE_HOME _and PYTHONPATH environment variables. The Dockerfile can be shown below, and it can also be found in the project GitHub repository.\n1## glue-base/3.0/Dockerfile 2FROM python:3.7.12-bullseye 3 4## Install utils 5RUN apt-get update \u0026amp;\u0026amp; apt-get install -y zip 6 7RUN curl \u0026#34;https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\u0026#34; -o \u0026#34;awscliv2.zip\u0026#34; \\ 8 \u0026amp;\u0026amp; unzip awscliv2.zip \u0026amp;\u0026amp; ./aws/install 9 10## Install Open JDK 8 11RUN apt-get update \\ 12 \u0026amp;\u0026amp; apt-get install -y software-properties-common \\ 13 \u0026amp;\u0026amp; apt-add-repository \u0026#39;deb http://security.debian.org/debian-security stretch/updates main\u0026#39; \\ 14 \u0026amp;\u0026amp; apt-get update \\ 15 \u0026amp;\u0026amp; apt-get install -y openjdk-8-jdk 16 17## Create environment variables 18ENV M2_HOME=/opt/apache-maven-3.6.0 19ENV SPARK_HOME=/opt/spark-3.1.1-amzn-0-bin-3.2.1-amzn-3 20ENV PATH=\u0026#34;${PATH}:${M2_HOME}/bin\u0026#34; 21 22## Add Maven, Spark and AWS Glue Libs to /opt 23RUN curl -SsL https://aws-glue-etl-artifacts.s3.amazonaws.com/glue-common/apache-maven-3.6.0-bin.tar.gz \\ 24 | tar -C /opt --warning=no-unknown-keyword -xzf - 25RUN curl -SsL https://aws-glue-etl-artifacts.s3.amazonaws.com/glue-3.0/spark-3.1.1-amzn-0-bin-3.2.1-amzn-3.tgz \\ 26 | tar -C /opt --warning=no-unknown-keyword -xf - 27RUN curl -SsL https://github.com/awslabs/aws-glue-libs/archive/refs/tags/v3.0.tar.gz \\ 28 | tar -C /opt --warning=no-unknown-keyword -xzf - 29 30# Install Glue dependencies 31RUN cd /opt/evoaustraliaaws-glue-libs-3.0/bin/ \\ 32 \u0026amp;\u0026amp; bash -c \u0026#34;source glue-setup.sh\u0026#34; 33 34## Add default Python packages 35COPY ./requirements.txt /tmp/requirements.txt 36RUN pip install -r /tmp/requirements.txt 37 38## Update Python path 39ENV GLUE_HOME=/opt/aws-glue-libs-3.0 40ENV PYTHONPATH=$GLUE_HOME:$SPARK_HOME/python/lib/pyspark.zip:$SPARK_HOME/python/lib/py4j-0.10.9-src.zip:$SPARK_HOME/python 41 42EXPOSE 4040 43 44CMD [\u0026#34;bash\u0026#34;] It is published to the glue-base repository of Cevo Australia\u0026rsquo;s public ECR registry with the following tags. Later versions of Glue base images will be published with relevant tags.\npublic.ecr.aws/cevoaustralia/glue-base:latest public.ecr.aws/cevoaustralia/glue-base:3.0 Usage The Glue base image can be used for running a Pyspark shell or submitting a spark application as shown below. For the spark application, I assume the project repository is mapped to the container\u0026rsquo;s /tmp folder. The Glue Python libraries also support Pytest, and it\u0026rsquo;ll be discussed later in the post.\n1docker run --rm -it \\ 2 -v $HOME/.aws:/root/.aws \\ 3 public.ecr.aws/cevoaustralia/glue-base bash -c \u0026#34;/opt/aws-glue-libs-3.0/bin/gluepyspark\u0026#34; 4 5docker run --rm -it \\ 6 -v $HOME/.aws:/root/.aws \\ 7 -v $PWD:/tmp/glue-vscode \\ 8 public.ecr.aws/cevoaustralia/glue-base bash -c \u0026#34;/opt/aws-glue-libs-3.0/bin/gluesparksubmit /tmp/glue-vscode/example.py\u0026#34; Extend Glue Base Image We can extend the Glue base image using the Visual Studio Code Dev Containers extension. The configuration for the extension can be found in the .devcontainer folder. The folder includes the Dockerfile for the development docker image and remote container configuration file (devcontainer.json). The other contents include the source for the Glue base image and materials for Pyspark, spark-submit and Pytest demonstrations. These will be illustrated below.\n1. 2├── .devcontainer 3│ ├── pkgs 4│ │ └── dev.txt 5│ ├── Dockerfile 6│ └── devcontainer.json 7├── .gitignore 8├── README.md 9├── example.py 10├── execute.sh 11├── glue-base 12│ └── 3.0 13│ ├── Dockerfile 14│ └── requirements.txt 15├── src 16│ └── utils.py 17└── tests 18 ├── __init__.py 19 ├── conftest.py 20 └── test_utils.py Development Docker Image The Glue base Docker image runs as the root user, and it is not convenient to write code with it. Therefore, a non-root user is created whose username corresponds to the logged-in user\u0026rsquo;s username - the _USERNAME _argument will be set accordingly in devcontainer.json. Next the sudo program is installed and the non-root user is added to the Sudo group. Note the Python Glue library\u0026rsquo;s executables are configured to run with the root user so that the sudo program is necessary to run those executables. Finally, it installs additional development Python packages.\n1## .devcontainer/Dockerfile 2FROM public.ecr.aws/i0m5p1b5/glue-base:3.0 3 4ARG USERNAME 5ARG USER_UID 6ARG USER_GID 7 8## Create non-root user 9RUN groupadd --gid $USER_GID $USERNAME \\ 10 \u0026amp;\u0026amp; useradd --uid $USER_UID --gid $USER_GID -m $USERNAME 11 12## Add sudo support in case we need to install software after connecting 13RUN apt-get update \\ 14 \u0026amp;\u0026amp; apt-get install -y sudo nano \\ 15 \u0026amp;\u0026amp; echo $USERNAME ALL=\\(root\\) NOPASSWD:ALL \u0026gt; /etc/sudoers.d/$USERNAME \\ 16 \u0026amp;\u0026amp; chmod 0440 /etc/sudoers.d/$USERNAME 17 18## Install Python packages 19COPY ./pkgs /tmp/pkgs 20RUN pip install -r /tmp/pkgs/dev.txt Container Configuration The development container will be created by building an image from the Dockerfile illustrated above. The logged-in user\u0026rsquo;s username is provided to create a non-root user and the container is set to run as the user as well. And 2 Visual Studio Code extensions are installed - Python and Prettier. Also, the current folder is mounted to the container\u0026rsquo;s workspace folder and 2 additional folders are mounted - they are to share AWS credentials and SSH keys. Note that AWS credentials are mounted to /root/.aws because the Python Glue library\u0026rsquo;s executables will be run as the root user. Then the port 4040 is set to be forwarded, which is used for the Spark UI. Finally, additional editor settings are added at the end.\n1// .devcontainer/devcontainer.json 2{ 3 \u0026#34;name\u0026#34;: \u0026#34;glue\u0026#34;, 4 \u0026#34;build\u0026#34;: { 5 \u0026#34;dockerfile\u0026#34;: \u0026#34;Dockerfile\u0026#34;, 6 \u0026#34;args\u0026#34;: { 7 \u0026#34;USERNAME\u0026#34;: \u0026#34;${localEnv:USER}\u0026#34;, 8 \u0026#34;USER_UID\u0026#34;: \u0026#34;1000\u0026#34;, 9 \u0026#34;USER_GID\u0026#34;: \u0026#34;1000\u0026#34; 10 } 11 }, 12 \u0026#34;containerUser\u0026#34;: \u0026#34;${localEnv:USER}\u0026#34;, 13 \u0026#34;extensions\u0026#34;: [ 14 \u0026#34;ms-python.python\u0026#34;, 15 \u0026#34;esbenp.prettier-vscode\u0026#34; 16 ], 17 \u0026#34;workspaceMount\u0026#34;: \u0026#34;source=${localWorkspaceFolder},target=${localEnv:HOME}/glue-vscode,type=bind,consistency=cached\u0026#34;, 18 \u0026#34;workspaceFolder\u0026#34;: \u0026#34;${localEnv:HOME}/glue-vscode\u0026#34;, 19 \u0026#34;forwardPorts\u0026#34;: [4040], 20 \u0026#34;mounts\u0026#34;: [ 21 \u0026#34;source=${localEnv:HOME}/.aws,target=/root/.aws,type=bind,consistency=cached\u0026#34;, 22 \u0026#34;source=${localEnv:HOME}/.ssh,target=${localEnv:HOME}/.ssh,type=bind,consistency=cached\u0026#34; 23 ], 24 \u0026#34;settings\u0026#34;: { 25 \u0026#34;terminal.integrated.profiles.linux\u0026#34;: { 26 \u0026#34;bash\u0026#34;: { 27 \u0026#34;path\u0026#34;: \u0026#34;/bin/bash\u0026#34; 28 } 29 }, 30 \u0026#34;terminal.integrated.defaultProfile.linux\u0026#34;: \u0026#34;bash\u0026#34;, 31 \u0026#34;editor.formatOnSave\u0026#34;: true, 32 \u0026#34;editor.defaultFormatter\u0026#34;: \u0026#34;esbenp.prettier-vscode\u0026#34;, 33 \u0026#34;editor.tabSize\u0026#34;: 2, 34 \u0026#34;python.testing.pytestEnabled\u0026#34;: true, 35 \u0026#34;python.linting.enabled\u0026#34;: true, 36 \u0026#34;python.linting.pylintEnabled\u0026#34;: false, 37 \u0026#34;python.linting.flake8Enabled\u0026#34;: false, 38 \u0026#34;python.formatting.provider\u0026#34;: \u0026#34;black\u0026#34;, 39 \u0026#34;python.formatting.blackPath\u0026#34;: \u0026#34;black\u0026#34;, 40 \u0026#34;python.formatting.blackArgs\u0026#34;: [\u0026#34;--line-length\u0026#34;, \u0026#34;100\u0026#34;], 41 \u0026#34;[python]\u0026#34;: { 42 \u0026#34;editor.tabSize\u0026#34;: 4, 43 \u0026#34;editor.defaultFormatter\u0026#34;: \u0026#34;ms-python.python\u0026#34; 44 } 45 } 46} Launch Container The development container can be run by executing the following command in the command palette.\nRemote-Containers: Open Folder in Container\u0026hellip; Once the development container is ready, the workspace folder will be open within the container.\nExamples I\u0026rsquo;ve created a script (execute.sh) to run the executables easily. The main command indicates which executable to run and possible values are pyspark, spark-submit and pytest. Below shows some example commands.\n1./execute.sh pyspark # pyspark 2./execute.sh spark-submit example.py # spark submit 3./execute.sh pytest -svv # pytest 1## execute.sh 2#!/usr/bin/env bash 3 4## remove first argument 5execution=$1 6echo \u0026#34;execution type - $execution\u0026#34; 7 8shift 1 9echo $@ 10 11## set up command 12if [ $execution == \u0026#39;pyspark\u0026#39; ]; then 13 sudo su -c \u0026#34;$GLUE_HOME/bin/gluepyspark\u0026#34; 14elif [ $execution == \u0026#39;spark-submit\u0026#39; ]; then 15 sudo su -c \u0026#34;$GLUE_HOME/bin/gluesparksubmit $@\u0026#34; 16elif [ $execution == \u0026#39;pytest\u0026#39; ]; then 17 sudo su -c \u0026#34;$GLUE_HOME/bin/gluepytest $@\u0026#34; 18else 19 echo \u0026#34;unsupported execution type - $execution\u0026#34; 20 exit 1 21fi Pyspark Using the script above, we can launch PySpark. A screenshot of the PySpark shell can be found below.\n1./execute.sh pyspark Spark Submit Below shows one of the Python samples in the Glue documentation. It pulls 3 data sets from a database called legislators. Then they are joined to create a history data set (l_history) and saved into S3.\n1./execute.sh spark-submit example.py 1## example.py 2from awsglue.dynamicframe import DynamicFrame 3from awsglue.transforms import Join 4from awsglue.utils import getResolvedOptions 5from pyspark.context import SparkContext 6from awsglue.context import GlueContext 7 8glueContext = GlueContext(SparkContext.getOrCreate()) 9 10DATABASE = \u0026#34;legislators\u0026#34; 11OUTPUT_PATH = \u0026#34;s3://glue-python-samples-fbe445ee/output_dir\u0026#34; 12 13## create dynamic frames from data catalog 14persons: DynamicFrame = glueContext.create_dynamic_frame.from_catalog( 15 database=DATABASE, table_name=\u0026#34;persons_json\u0026#34; 16) 17 18memberships: DynamicFrame = glueContext.create_dynamic_frame.from_catalog( 19 database=DATABASE, table_name=\u0026#34;memberships_json\u0026#34; 20) 21 22orgs: DynamicFrame = glueContext.create_dynamic_frame.from_catalog( 23 database=DATABASE, table_name=\u0026#34;organizations_json\u0026#34; 24) 25 26## manipulate data 27orgs = ( 28 orgs.drop_fields([\u0026#34;other_names\u0026#34;, \u0026#34;identifiers\u0026#34;]) 29 .rename_field(\u0026#34;id\u0026#34;, \u0026#34;org_id\u0026#34;) 30 .rename_field(\u0026#34;name\u0026#34;, \u0026#34;org_name\u0026#34;) 31) 32 33l_history: DynamicFrame = Join.apply( 34 orgs, Join.apply(persons, memberships, \u0026#34;id\u0026#34;, \u0026#34;person_id\u0026#34;), \u0026#34;org_id\u0026#34;, \u0026#34;organization_id\u0026#34; 35) 36l_history = l_history.drop_fields([\u0026#34;person_id\u0026#34;, \u0026#34;org_id\u0026#34;]) 37 38l_history.printSchema() 39 40## write to s3 41glueContext.write_dynamic_frame.from_options( 42 frame=l_history, 43 connection_type=\u0026#34;s3\u0026#34;, 44 connection_options={\u0026#34;path\u0026#34;: f\u0026#34;{OUTPUT_PATH}/legislator_history\u0026#34;}, 45 format=\u0026#34;parquet\u0026#34;, 46) When the execution completes, we can see the joined data set is stored as a parquet file in the output S3 bucket.\nNote that we can monitor and inspect Spark job executions in the Spark UI on port 4040.\nPytest We can test a function that deals with a DynamicFrame. Below shows a test case for a simple function that filters a DynamicFrame based on a column value.\n1./execute.sh pytest -svv 1## src/utils.py 2from awsglue.dynamicframe import DynamicFrame 3 4def filter_dynamic_frame(dyf: DynamicFrame, column_name: str, value: int): 5 return dyf.filter(f=lambda x: x[column_name] \u0026gt; value) 6 7## tests/conftest.py 8from pyspark.context import SparkContext 9from awsglue.context import GlueContext 10import pytest 11 12@pytest.fixture(scope=\u0026#34;session\u0026#34;) 13def glueContext(): 14 sparkContext = SparkContext() 15 glueContext = GlueContext(sparkContext) 16 yield glueContext 17 sparkContext.stop() 18 19 20## tests/test_utils.py 21from typing import List 22from awsglue.dynamicframe import DynamicFrame 23import pandas as pd 24from src.utils import filter_dynamic_frame 25 26def _get_sorted_data_frame(pdf: pd.DataFrame, columns_list: List[str] = None): 27 if columns_list is None: 28 columns_list = list(pdf.columns.values) 29 return pdf.sort_values(columns_list).reset_index(drop=True) 30 31 32def test_filter_dynamic_frame_by_value(glueContext): 33 spark = glueContext.spark_session 34 35 input = spark.createDataFrame( 36 [(\u0026#34;charly\u0026#34;, 15), (\u0026#34;fabien\u0026#34;, 18), (\u0026#34;sam\u0026#34;, 21), (\u0026#34;sam\u0026#34;, 25), (\u0026#34;nick\u0026#34;, 19), (\u0026#34;nick\u0026#34;, 40)], 37 [\u0026#34;name\u0026#34;, \u0026#34;age\u0026#34;], 38 ) 39 40 expected_output = spark.createDataFrame( 41 [(\u0026#34;sam\u0026#34;, 25), (\u0026#34;sam\u0026#34;, 21), (\u0026#34;nick\u0026#34;, 40)], 42 [\u0026#34;name\u0026#34;, \u0026#34;age\u0026#34;], 43 ) 44 45 real_output = filter_dynamic_frame(DynamicFrame.fromDF(input, glueContext, \u0026#34;output\u0026#34;), \u0026#34;age\u0026#34;, 20) 46 47 pd.testing.assert_frame_equal( 48 _get_sorted_data_frame(real_output.toDF().toPandas(), [\u0026#34;name\u0026#34;, \u0026#34;age\u0026#34;]), 49 _get_sorted_data_frame(expected_output.toPandas(), [\u0026#34;name\u0026#34;, \u0026#34;age\u0026#34;]), 50 check_like=True, 51 ) Conclusion In this post, I demonstrated how to build local development environments for AWS Glue 3.0 and later using a custom docker image and the Visual Studio Code Remote - Containers extension. Then examples of launching Pyspark shells, submitting an application and running a test are shown. I hope this post is useful to develop and test Glue ETL scripts locally.\n","date":"November 14, 2021","img":"/blog/2021-11-14-glue-3-local-development/featured.png","lang":"en","langName":"English","largeImg":"/blog/2021-11-14-glue-3-local-development/featured_hu689859cb443a98b1c98384fedaa00395_30923_500x0_resize_box_3.png","permalink":"/blog/2021-11-14-glue-3-local-development/","series":[],"smallImg":"/blog/2021-11-14-glue-3-local-development/featured_hu689859cb443a98b1c98384fedaa00395_30923_180x0_resize_box_3.png","tags":[{"title":"AWS","url":"/tags/aws/"},{"title":"AWS Glue","url":"/tags/aws-glue/"},{"title":"Docker","url":"/tags/docker/"},{"title":"Apache Spark","url":"/tags/apache-spark/"},{"title":"PySpark","url":"/tags/pyspark/"},{"title":"Python","url":"/tags/python/"},{"title":"Visual Studio Code","url":"/tags/visual-studio-code/"}],"timestamp":1636848000,"title":"Local Development of AWS Glue 3.0 and Later"},{"categories":[{"title":"Development","url":"/categories/development/"}],"content":"Triggering a Lambda function by an EventBridge Events rule can be used as a _serverless _replacement of cron job. The highest frequency of it is one invocation per minute so that it cannot be used directly if you need to schedule a Lambda function more frequently. For example, it may be refreshing an application with real time metrics from an Amazon Connect instance where some metrics are updated every 15 seconds. There is a post in the AWS Architecture Blog, and it suggests using AWS Step Functions. Or a usual recommendation is using Amazon EC2. Albeit being serverless, the former gets a bit complicated especially in order to handle the hard quota of 25,000 entries in the execution history. And the latter is not an option if you look for a serverless solution. In this post, I’ll demonstrate another serverless solution of scheduling a Lambda function at a sub-minute frequency using Amazon SQS.\nArchitecture The solution contains 2 Lambda functions and each of them has its own event source: EventBridge Events rule and SQS.\nA Lambda function (sender) is invoked every minute by an EventBridge Events rule. The function sends messages to a queue with different delay seconds values. For example, if we want to invoke the consumer Lambda function every 10 seconds, we can send 6 messages with delay seconds values of 0, 10, 20, 30, 40 and 50. The consumer is invoked after the delay seconds as the messages are visible. I find this architecture is simpler than other options.\nLambda Functions The sender Lambda function sends messages with different delay second values to a queue. An array of those values are generated by generateDelaySeconds(), given an interval value. Note that this function works well if the interval value is less than or equal to 30. If we want to set up a higher interval value, we should update the function together with the EventBridge Event rule. The source can be found in the GitHub repository.\n1// src/sender.js 2const AWS = require(\u0026#34;aws-sdk\u0026#34;); 3 4const sqs = new AWS.SQS({ 5 apiVersion: \u0026#34;2012-11-05\u0026#34;, 6 region: process.env.AWS_REGION || \u0026#34;us-east-1\u0026#34;, 7}); 8 9/** 10 * Generate delay seconds by an interval value. 11 * 12 * @example 13 * // returns [ 0, 30 ] 14 * generateDelaySeconds(30) 15 * // returns [ 0, 20, 40 ] 16 * generateDelaySeconds(20) 17 * // returns [ 0, 15, 30, 45 ] 18 * generateDelaySeconds(15) 19 * // returns [ 0, 10, 20, 30, 40, 50 ] 20 * generateDelaySeconds(10) 21 */ 22const generateDelaySeconds = (interval) =\u0026gt; { 23 const numElem = Math.round(60 / interval); 24 const array = Array.apply(0, Array(numElem + 1)).map((_, index) =\u0026gt; { 25 return index; 26 }); 27 const min = Math.min(...array); 28 const max = Math.max(...array); 29 return array 30 .map((a) =\u0026gt; Math.round(((a - min) / (max - min)) * 60)) 31 .filter((a) =\u0026gt; a \u0026lt; 60); 32}; 33 34const handler = async () =\u0026gt; { 35 const interval = process.env.SCHEDULE_INTERVAL || 30; 36 const delaySeconds = generateDelaySeconds(interval); 37 for (const ds of delaySeconds) { 38 const params = { 39 MessageBody: JSON.stringify({ delaySecond: ds }), 40 QueueUrl: process.env.QUEUE_URL, 41 DelaySeconds: ds, 42 }; 43 await sqs.sendMessage(params).promise(); 44 } 45 console.log(`send messages, delay seconds - ${delaySeconds.join(\u0026#34;, \u0026#34;)}`); 46}; 47 48module.exports = { handler }; The consumer Lambda function simply polls the messages. It is set to finish after 1 second followed by logging the delay second value.\n1// src/consumer.js 2const sleep = (ms) =\u0026gt; { 3 return new Promise((resolve) =\u0026gt; { 4 setTimeout(resolve, ms); 5 }); 6}; 7 8const handler = async (event) =\u0026gt; { 9 for (const rec of event.Records) { 10 const body = JSON.parse(rec.body); 11 console.log(`delay second - ${body.delaySecond}`); 12 await sleep(1000); 13 } 14}; 15 16module.exports = { handler }; Serverless Service Two Lambda functions (sender and consumer) and a queue are created by Serverless Framework. As discussed earlier the sender function has an EventBridge Event rule trigger, and it invokes the function at the rate of 1 minute. The schedule interval is set to 10, which is used to create delay seconds values. The consumer is set to be triggered by the queue.\n1# serverless.yml 2service: ${self:custom.serviceName} 3 4plugins: 5 - serverless-iam-roles-per-function 6 7custom: 8 serviceName: lambda-schedule 9 scheduleInterval: 10 10 queue: 11 name: ${self:custom.serviceName}-queue-${self:provider.stage} 12 url: !Ref Queue 13 arn: !GetAtt Queue.Arn 14 15… 16 17provider: 18 name: aws 19 runtime: nodejs12.x 20 stage: ${opt:stage, \u0026#39;dev\u0026#39;} 21 region: ${opt:region, \u0026#39;us-east-1\u0026#39;} 22 lambdaHashingVersion: 20201221 23 memorySize: 128 24 logRetentionInDays: 7 25 deploymentBucket: 26 tags: 27 OWNER: ${env:owner} 28 stackTags: 29 OWNER: ${env:owner} 30 31functions: 32 sender: 33 handler: src/sender.handler 34 name: ${self:custom.serviceName}-sender-${self:provider.stage} 35 events: 36 - eventBridge: 37 schedule: rate(1 minute) 38 enabled: true 39 environment: 40 SCHEDULE_INTERVAL: ${self:custom.scheduleInterval} 41 QUEUE_URL: ${self:custom.queue.url} 42 iamRoleStatements: 43 - Effect: Allow 44 Action: 45 - sqs:SendMessage 46 Resource: 47 - ${self:custom.queue.arn} 48 consumer: 49 handler: src/consumer.handler 50 name: ${self:custom.serviceName}-consumer-${self:provider.stage} 51 events: 52 - sqs: 53 arn: ${self:custom.queue.arn} 54 55resources: 56 Resources: 57 Queue: 58 Type: AWS::SQS::Queue 59 Properties: 60 QueueName: ${self:custom.queue.name} Performance We can filter the log of the consumer function in the CloudWatch page. The function is invoked as expected, but I see the interval gets shortened periodically especially when the delay second value is 0. We’ll have a closer look at that below.\nI created a chart that shows delay (milliseconds) by invocation. It shows periodic downward spikes, and they correspond to the invocations where the delay seconds value is 0. For some early invocations, the delay values are more than 1000 milliseconds, which means that the consumer function’s intervals are less than 9 seconds. The delays get stable at or after the 200th invocation. The table in the right-hand side shows the summary statistics of delays after that invocation. It shows the consumer invocation delays spread in a range of 300 milliseconds in general.\nCaveats An EventBridge Events rule can be triggered more than once and a message in an Amazon SQS queue can be delivered more than once as well. Therefore, it is important to design the consumer Lambda function to be idempotent.\nConclusion In this post, I demonstrated a serverless solution for scheduling a Lambda function at a sub-minute frequency with Amazon SQS. The architecture of the serverless solution is simpler than other options and its performance is acceptable in spite of some negative delays. Due to the at-least-once delivery feature of EventBridge Events and Amazon SQS, it is important to design the application to be idempotent. I hope this post is useful to build a Lambda scheduling solution.\n","date":"October 13, 2021","img":"/blog/2021-10-13-lambda-schedule/featured.png","lang":"en","langName":"English","largeImg":"/blog/2021-10-13-lambda-schedule/featured_huea874edbd388125a335a7aabc342a4f8_46921_500x0_resize_box_3.png","permalink":"/blog/2021-10-13-lambda-schedule/","series":[],"smallImg":"/blog/2021-10-13-lambda-schedule/featured_huea874edbd388125a335a7aabc342a4f8_46921_180x0_resize_box_3.png","tags":[{"title":"AWS Lambda","url":"/tags/aws-lambda/"},{"title":"Amazon SQS","url":"/tags/amazon-sqs/"},{"title":"Node.js","url":"/tags/node.js/"}],"timestamp":1634083200,"title":"Yet Another Serverless Solution for Invoking AWS Lambda at a Sub-Minute Frequency"},{"categories":[{"title":"Data Engineering","url":"/categories/data-engineering/"}],"content":"As described in the product page, AWS Glue is a serverless data integration service that makes it easy to discover, prepare, and combine data for analytics, machine learning, and application development. For development, a development endpoint is recommended, but it can be costly, inconvenient or unavailable (for Glue 2.0). The AWS Glue team published a Docker image that includes the AWS Glue binaries and all the dependencies packaged together. After inspecting it, I find some modifications are necessary in order to build a development environment on it. In this post, I\u0026rsquo;ll demonstrate how to build development environments for AWS Glue 1.0 and 2.0 using the Docker image and the Visual Studio Code Remote - Containers extension.\nConfiguration Although AWS Glue 1.0 and 2.0 have different dependencies and versions, the Python library (aws-glue-libs) shares the same branch (glue-1.0) and Spark version. On the other hand, AWS Glue 2.0 supports Python 3.7 and has different default python packages. Therefore, in order to set up an AWS Glue 2.0 development environment, it would be necessary to install Python 3.7 and the default packages while sharing the same Spark-related dependencies.\nThe Visual Studio Code Remote - Containers extension lets you use a Docker container as a full-featured development environment. It allows you to open any folder or repository inside a container and take advantage of Visual Studio Code\u0026rsquo;s full feature set. The development container configuration (devcontainer.json) and associating files can be found in the .devcontainer folder of the GitHub repository for this post. Apart from the configuration file, the folder includes a Dockerfile, files to keep Python packages to install and a custom Pytest executable for AWS Glue 2.0 (gluepytest2) - this executable will be explained later.\n1. 2├── .devcontainer 3│ ├── 3.6 4│ │ └── dev.txt 5│ ├── 3.7 6│ │ ├── default.txt 7│ │ └── dev.txt 8│ ├── Dockerfile 9│ ├── bin 10│ │ └── gluepytest2 11│ └── devcontainer.json 12├── .gitignore 13├── README.md 14├── example.py 15├── execute.sh 16├── src 17│ └── utils.py 18└── tests 19 ├── __init__.py 20 ├── conftest.py 21 └── test_utils.py Dockerfile The Docker image (amazon/aws-glue-libs:glue_libs_1.0.0_image_01) runs as the root user, and it is not convenient to write code with it. Therefore, a non-root user is created whose username corresponds to the logged-in user\u0026rsquo;s username - the USERNAME argument will be set accordingly in devcontainer.json. Next the sudo program is added in order to install other programs if necessary. More importantly, the Python Glue library\u0026rsquo;s executables are configured to run with the root user so that the sudo program is necessary to run those executables. Then the 3rd-party Python packages are installed for the Glue 1.0 and 2.0 development environments. Note that a virtual environment is created for the latter and the default Python packages and additional development packages are installed in it. Finally, a Pytest executable is copied to the Python Glue library\u0026rsquo;s executable path. It is because the Pytest path is hard-coded in the existing executable (gluepytest) and I just wanted to run test cases in the Glue 2.0 environment without touching existing ones - the Pytest path is set to /root/venv/bin/pytest_ in _gluepytest2.\n1## .devcontainer/Dockerfile 2FROM amazon/aws-glue-libs:glue_libs_1.0.0_image_01 3 4ARG USERNAME 5ARG USER_UID 6ARG USER_GID 7 8## Create non-root user 9RUN groupadd --gid $USER_GID $USERNAME \\ 10 \u0026amp;\u0026amp; useradd --uid $USER_UID --gid $USER_GID -m $USERNAME 11 12## Add sudo support in case we need to install software after connecting 13## Jessie is not the latest stable Debian release - jessie-backports is not available 14RUN rm -rf /etc/apt/sources.list.d/jessie-backports.list 15 16RUN apt-get update \\ 17 \u0026amp;\u0026amp; apt-get install -y sudo \\ 18 \u0026amp;\u0026amp; echo $USERNAME ALL=\\(root\\) NOPASSWD:ALL \u0026gt; /etc/sudoers.d/$USERNAME \\ 19 \u0026amp;\u0026amp; chmod 0440 /etc/sudoers.d/$USERNAME 20 21## Install extra packages for python 3.6 22COPY ./3.6 /tmp/3.6 23RUN pip install -r /tmp/3.6/dev.txt 24 25## Setup python 3.7 and install default and development packages to a virtual env 26RUN apt-get update \\ 27 \u0026amp;\u0026amp; apt-get install -y python3.7 python3.7-venv 28 29RUN python3.7 -m venv /root/venv 30 31COPY ./3.7 /tmp/3.7 32RUN /root/venv/bin/pip install -r /tmp/3.7/dev.txt 33 34## Copy pytest execution script to /aws-glue-libs/bin 35## in order to run pytest from the virtual env 36COPY ./bin/gluepytest2 /home/aws-glue-libs/bin/gluepytest2 Container Configuration The development container will be created by building an image from the Dockerfile illustrated above. The logged-in user\u0026rsquo;s username is provided to create a non-root user and the container is set to run as the user as well. And 2 Visual Studio Code extensions are installed - Python and Prettier. Also, the current folder is mounted to the container\u0026rsquo;s workspace folder and 2 additional folders are mounted - they are to share AWS credentials and SSH keys. Note that AWS credentials are mounted to /roo/.aws because the Python Glue library\u0026rsquo;s executables will be run as the root user. Then the port 4040 is set to be forwarded, which is used for the Spark UI. Finally, additional editor settings are added at the end.\n1// .devcontainer/devcontainer.json 2{ 3 \u0026#34;name\u0026#34;: \u0026#34;glue\u0026#34;, 4 \u0026#34;build\u0026#34;: { 5 \u0026#34;dockerfile\u0026#34;: \u0026#34;Dockerfile\u0026#34;, 6 \u0026#34;args\u0026#34;: { 7 \u0026#34;USERNAME\u0026#34;: \u0026#34;${localEnv:USER}\u0026#34;, 8 \u0026#34;USER_UID\u0026#34;: \u0026#34;1000\u0026#34;, 9 \u0026#34;USER_GID\u0026#34;: \u0026#34;1000\u0026#34; 10 } 11 }, 12 \u0026#34;containerUser\u0026#34;: \u0026#34;${localEnv:USER}\u0026#34;, 13 \u0026#34;extensions\u0026#34;: [ 14 \u0026#34;ms-python.python\u0026#34;, 15 \u0026#34;esbenp.prettier-vscode\u0026#34; 16 ], 17 \u0026#34;workspaceMount\u0026#34;: \u0026#34;source=${localWorkspaceFolder},target=${localEnv:HOME}/glue-vscode,type=bind,consistency=cached\u0026#34;, 18 \u0026#34;workspaceFolder\u0026#34;: \u0026#34;${localEnv:HOME}/glue-vscode\u0026#34;, 19 \u0026#34;forwardPorts\u0026#34;: [4040], 20 \u0026#34;mounts\u0026#34;: [ 21 \u0026#34;source=${localEnv:HOME}/.aws,target=/root/.aws,type=bind,consistency=cached\u0026#34;, 22 \u0026#34;source=${localEnv:HOME}/.ssh,target=${localEnv:HOME}/.ssh,type=bind,consistency=cached\u0026#34; 23 ], 24 \u0026#34;settings\u0026#34;: { 25 \u0026#34;terminal.integrated.profiles.linux\u0026#34;: { 26 \u0026#34;bash\u0026#34;: { 27 \u0026#34;path\u0026#34;: \u0026#34;/bin/bash\u0026#34; 28 } 29 }, 30 \u0026#34;terminal.integrated.defaultProfile.linux\u0026#34;: \u0026#34;bash\u0026#34;, 31 \u0026#34;editor.formatOnSave\u0026#34;: true, 32 \u0026#34;editor.defaultFormatter\u0026#34;: \u0026#34;esbenp.prettier-vscode\u0026#34;, 33 \u0026#34;editor.tabSize\u0026#34;: 2, 34 \u0026#34;python.testing.pytestEnabled\u0026#34;: true, 35 \u0026#34;python.linting.enabled\u0026#34;: true, 36 \u0026#34;python.linting.pylintEnabled\u0026#34;: false, 37 \u0026#34;python.linting.flake8Enabled\u0026#34;: false, 38 \u0026#34;python.formatting.provider\u0026#34;: \u0026#34;black\u0026#34;, 39 \u0026#34;python.formatting.blackPath\u0026#34;: \u0026#34;black\u0026#34;, 40 \u0026#34;python.formatting.blackArgs\u0026#34;: [\u0026#34;--line-length\u0026#34;, \u0026#34;100\u0026#34;], 41 \u0026#34;[python]\u0026#34;: { 42 \u0026#34;editor.tabSize\u0026#34;: 4, 43 \u0026#34;editor.defaultFormatter\u0026#34;: \u0026#34;ms-python.python\u0026#34; 44 } 45 } 46} Launch Container The development container can be run by executing the following command in the command palette.\nRemote-Containers: Open Folder in Container\u0026hellip; Once the development container is ready, the workspace folder will be open within the container. You will see 2 new images are created from the base Glue image and a container is run from the latest image.\nExamples I\u0026rsquo;ve created a simple script (execute.sh) to run the executables easily. The main command indicates which executable to run and possible values are pyspark, spark-submit and pytest. Note that the IPython notebook is available, but it is not added because I don\u0026rsquo;t think a notebook is good for development. However, you may try by just adding it. Below shows some example commands.\n1# pyspark 2version=1 ./execute.sh pyspark OR version=2 ./execute.sh pyspark 3# spark submit 4version=1 ./execute.sh spark-submit example.py OR version=2 ./execute.sh spark-submit example.py 5# pytest 6version=1 ./execute.sh pytest -svv OR version=2 ./execute.sh pytest -svv 1# ./execute.sh 2#!/usr/bin/env bash 3 4## configure python runtime 5if [ \u0026#34;$version\u0026#34; == \u0026#34;1\u0026#34; ]; then 6 pyspark_python=python 7elif [ \u0026#34;$version\u0026#34; == \u0026#34;2\u0026#34; ]; then 8 pyspark_python=/root/venv/bin/python 9else 10 echo \u0026#34;unsupported version - $version, only 1 or 2 is accepted\u0026#34; 11 exit 1 12fi 13echo \u0026#34;pyspark python - $pyspark_python\u0026#34; 14 15execution=$1 16echo \u0026#34;execution type - $execution\u0026#34; 17 18## remove first argument 19shift 1 20echo $@ 21 22## set up command 23if [ $execution == \u0026#39;pyspark\u0026#39; ]; then 24 sudo su -c \u0026#34;PYSPARK_PYTHON=$pyspark_python /home/aws-glue-libs/bin/gluepyspark\u0026#34; 25elif [ $execution == \u0026#39;spark-submit\u0026#39; ]; then 26 sudo su -c \u0026#34;PYSPARK_PYTHON=$pyspark_python /home/aws-glue-libs/bin/gluesparksubmit $@\u0026#34; 27elif [ $execution == \u0026#39;pytest\u0026#39; ]; then 28 if [ $version == \u0026#34;1\u0026#34; ]; then 29 sudo su -c \u0026#34;PYSPARK_PYTHON=$pyspark_python /home/aws-glue-libs/bin/gluepytest $@\u0026#34; 30 else 31 sudo su -c \u0026#34;PYSPARK_PYTHON=$pyspark_python /home/aws-glue-libs/bin/gluepytest2 $@\u0026#34; 32 fi 33else 34 echo \u0026#34;unsupported execution type - $execution\u0026#34; 35 exit 1 36fi Pyspark Using the script above, we can launch the PySpark shells for each of the environments. Python 3.6.10 is associated with the AWS Glue 1.0 while Python 3.7.3 in a virtual environment is with the AWS Glue 2.0.\nSpark Submit Below shows one of the Python samples in the Glue documentation. It pulls 3 data sets from a database called legislators. Then they are joined to create a history data set (l_history) and saved into S3.\n1# ./example.py 2from awsglue.dynamicframe import DynamicFrame 3from awsglue.transforms import Join 4from awsglue.utils import getResolvedOptions 5from pyspark.context import SparkContext 6from awsglue.context import GlueContext 7 8glueContext = GlueContext(SparkContext.getOrCreate()) 9 10DATABASE = \u0026#34;legislators\u0026#34; 11OUTPUT_PATH = \u0026#34;s3://glue-python-samples-fbe445ee/output_dir\u0026#34; 12 13## create dynamic frames from data catalog 14persons: DynamicFrame = glueContext.create_dynamic_frame.from_catalog( 15 database=DATABASE, table_name=\u0026#34;persons_json\u0026#34; 16) 17 18memberships: DynamicFrame = glueContext.create_dynamic_frame.from_catalog( 19 database=DATABASE, table_name=\u0026#34;memberships_json\u0026#34; 20) 21 22orgs: DynamicFrame = glueContext.create_dynamic_frame.from_catalog( 23 database=DATABASE, table_name=\u0026#34;organizations_json\u0026#34; 24) 25 26## manipulate data 27orgs = ( 28 orgs.drop_fields([\u0026#34;other_names\u0026#34;, \u0026#34;identifiers\u0026#34;]) 29 .rename_field(\u0026#34;id\u0026#34;, \u0026#34;org_id\u0026#34;) 30 .rename_field(\u0026#34;name\u0026#34;, \u0026#34;org_name\u0026#34;) 31) 32 33l_history: DynamicFrame = Join.apply( 34 orgs, Join.apply(persons, memberships, \u0026#34;id\u0026#34;, \u0026#34;person_id\u0026#34;), \u0026#34;org_id\u0026#34;, \u0026#34;organization_id\u0026#34; 35) 36l_history = l_history.drop_fields([\u0026#34;person_id\u0026#34;, \u0026#34;org_id\u0026#34;]) 37 38l_history.printSchema() 39 40## write to s3 41glueContext.write_dynamic_frame.from_options( 42 frame=l_history, 43 connection_type=\u0026#34;s3\u0026#34;, 44 connection_options={\u0026#34;path\u0026#34;: f\u0026#34;{OUTPUT_PATH}/legislator_history\u0026#34;}, 45 format=\u0026#34;parquet\u0026#34;, 46) When the execution completes, we can see the joined data set is stored as a parquet file in the output S3 bucket.\nNote that we can monitor and inspect Spark job executions in the Spark UI on port 4040.\nPytest We can test a function that deals with a DynamicFrame. Below shows a test case for a simple function that filters a DynamicFrame based on a column value.\n1# ./src/utils.py 2from awsglue.dynamicframe import DynamicFrame 3 4def filter_dynamic_frame(dyf: DynamicFrame, column_name: str, value: int): 5 return dyf.filter(f=lambda x: x[column_name] \u0026gt; value) 6 7# ./tests/conftest.py 8from pyspark.context import SparkContext 9from awsglue.context import GlueContext 10import pytest 11 12@pytest.fixture(scope=\u0026#34;session\u0026#34;) 13def glueContext(): 14 sparkContext = SparkContext() 15 glueContext = GlueContext(sparkContext) 16 yield glueContext 17 sparkContext.stop() 18 19 20# ./tests/test_utils.py 21from typing import List 22from awsglue.dynamicframe import DynamicFrame 23import pandas as pd 24from src.utils import filter_dynamic_frame 25 26def _get_sorted_data_frame(pdf: pd.DataFrame, columns_list: List[str] = None): 27 if columns_list is None: 28 columns_list = list(pdf.columns.values) 29 return pdf.sort_values(columns_list).reset_index(drop=True) 30 31 32def test_filter_dynamic_frame_by_value(glueContext): 33 spark = glueContext.spark_session 34 35 input = spark.createDataFrame( 36 [(\u0026#34;charly\u0026#34;, 15), (\u0026#34;fabien\u0026#34;, 18), (\u0026#34;sam\u0026#34;, 21), (\u0026#34;sam\u0026#34;, 25), (\u0026#34;nick\u0026#34;, 19), (\u0026#34;nick\u0026#34;, 40)], 37 [\u0026#34;name\u0026#34;, \u0026#34;age\u0026#34;], 38 ) 39 40 expected_output = spark.createDataFrame( 41 [(\u0026#34;sam\u0026#34;, 25), (\u0026#34;sam\u0026#34;, 21), (\u0026#34;nick\u0026#34;, 40)], 42 [\u0026#34;name\u0026#34;, \u0026#34;age\u0026#34;], 43 ) 44 45 real_output = filter_dynamic_frame(DynamicFrame.fromDF(input, glueContext, \u0026#34;output\u0026#34;), \u0026#34;age\u0026#34;, 20) 46 47 pd.testing.assert_frame_equal( 48 _get_sorted_data_frame(real_output.toDF().toPandas(), [\u0026#34;name\u0026#34;, \u0026#34;age\u0026#34;]), 49 _get_sorted_data_frame(expected_output.toPandas(), [\u0026#34;name\u0026#34;, \u0026#34;age\u0026#34;]), 50 check_like=True, 51 ) Conclusion In this post, I demonstrated how to build local development environments for AWS Glue 1.0 and 2.0 using Docker and the Visual Studio Code Remote - Containers extension. Then examples of launching Pyspark shells, submitting an application and running a test are shown. I hope this post is useful to develop and test Glue ETL scripts locally.\n","date":"August 20, 2021","img":"/blog/2021-08-20-glue-local-development/featured.png","lang":"en","langName":"English","largeImg":"/blog/2021-08-20-glue-local-development/featured_hu248b4052e45f408d4fe80445a9d59f15_19535_500x0_resize_box_3.png","permalink":"/blog/2021-08-20-glue-local-development/","series":[],"smallImg":"/blog/2021-08-20-glue-local-development/featured_hu248b4052e45f408d4fe80445a9d59f15_19535_180x0_resize_box_3.png","tags":[{"title":"AWS","url":"/tags/aws/"},{"title":"AWS Glue","url":"/tags/aws-glue/"},{"title":"Docker","url":"/tags/docker/"},{"title":"Apache Spark","url":"/tags/apache-spark/"},{"title":"PySpark","url":"/tags/pyspark/"},{"title":"Python","url":"/tags/python/"},{"title":"Visual Studio Code","url":"/tags/visual-studio-code/"}],"timestamp":1629417600,"title":"AWS Glue Local Development With Docker and Visual Studio Code"},{"categories":[{"title":"Development","url":"/categories/development/"}],"content":"Authorization is the mechanism that controls who can do what on which resource in an application. Although it is a critical part of an application, there are limited resources available on how to build authorization into an app effectively. In this post, I\u0026rsquo;ll be illustrating how to set up authorization in a GraphQL API using a custom directive and Oso, an open-source authorization library. This tutorial covers the NodeJS variant of Oso, but it also supports Python and other languages.\nRequirements There are a number of users and each of them belongs to one or more user groups. The groups are guest, member and admin. Also, a user can be given escalated permission on one or more projects if he/she belongs to a certain project user group (e.g. contributor).\nDepending on the membership, users have varying levels of permission on user, project and indicator resources. Specifically\nUser\nAll users can be fetched if a user belongs to the admin user group. Project\nA project or all permitted projects can be queried if a user belongs to the _admin or member _user group or to the _contributor _user project group. For a project record, the contract_sum field can be queried only if a user belongs to the _admin _user group or contributor user project group. The project status can be updated if a user belongs to the admin user group or contributor user project group. Indicator\nAll permitted project indicators can be fetched if a user belongs to the admin user group or contributor user project group. Building Blocks Permission Specification on Directive A directive decorates part of a GraphQL schema or operation with additional configuration. Tools like Apollo Server (and Apollo Client) can read a GraphQL document\u0026rsquo;s directives and perform custom logic as appropriate.\nA directive can be useful to define permission. Below shows the type definitions used to meet the authorization requirements listed above. For example, the auth directive (@auth) is applied to the project query where admin and member are required for the user groups and contributor for the project user group.\n1// src/schema.js 2const typeDefs = gql` 3 directive @auth( 4 userGroups: [UserGroup] 5 projGroups: [ProjectGroup] 6 ) on OBJECT | FIELD_DEFINITION 7 8 ... 9 10 type User { 11 id: ID! 12 name: String 13 groups: [String] 14 } 15 16 type Project { 17 id: ID! 18 name: String 19 status: String 20 contract_sum: Int @auth(userGroups: [admin], projGroups: [contributor]) 21 } 22 23 type Indicator { 24 id: ID! 25 project_id: Int 26 risk: Int 27 quality: Int 28 } 29 30 type Query { 31 users: [User] @auth(userGroups: [admin]) 32 project(projectId: ID!): Project 33 @auth(userGroups: [admin, member], projGroups: [contributor]) 34 projects: [Project] 35 @auth(userGroups: [admin, member], projGroups: [contributor]) 36 indicators: [Indicator] 37 @auth(userGroups: [admin], projGroups: [contributor]) 38 } 39 40 type Mutation { 41 updateProjectStatus(projectId: ID!, status: String!): Project 42 @auth(userGroups: [admin], projGroups: [contributor]) 43 } 44`; Policy Building Using Oso Oso is a batteries-included library for building authorization in your application. Oso gives you a mental model and an authorization system – a set of APIs built on top of a declarative policy language called Polar, plus a debugger and REPL – to define who can do what in your application. You can express common concepts from “users can see their own data” and role-based access control, to others like multi-tenancy, organizations and teams, hierarchies and relationships.\nAn authorization policy is a set of logical rules for who is allowed to access what resources in an application. For example, the policy that describes the get:project action allows the actor (user) to perform it on the project resource if he/she belongs to required user or project groups. The actor and resource can be either a custom class or one of the built-in classes (Dictionary, List, String …). Note methods of a custom class can be used instead of built-in operations as well.\n1# src/polars/policy.polar 2allow(user: User, \u0026#34;list:users\u0026#34;, _: String) if 3 user.isRequiredUserGroup(); 4 5allow(user: User, \u0026#34;get:project\u0026#34;, project: Dictionary) if 6 user.isRequiredUserGroup() 7 or 8 user.isRequiredProjectGroup(project); 9 10allow(user: User, \u0026#34;update:project\u0026#34;, projectId: Integer) if 11 user.isRequiredUserGroup() 12 or 13 projectId in user.filterAllowedProjectIds(); 14 15allow(user: User, \u0026#34;list:indicators\u0026#34;, _: String) if 16 user.isRequiredUserGroup() 17 or 18 user.filterAllowedProjectIds().length \u0026gt; 0; Policy Enforcement Within Directive The auth directive collects the user and project group configuration on an object or field definition. Then it updates the user object in the context and passes it to the resolver. In this way, policy enforcement for queries and mutations can be performed within the resolver, and it is more manageable while the number of queries and mutations increases.\nOn the other hand, the policy of an object field (e.g. contract_sum) is enforced within the directive. It is because, once a query (e.g. project) or mutation is resolved, and its parent object is returned, the directive is executed for the field with different configuration values.\n1// src/utils/directive.js 2class AuthDirective extends SchemaDirectiveVisitor { 3 ... 4 5 ensureFieldsWrapped(objectType) { 6 if (objectType._authFieldsWrapped) return; 7 objectType._authFieldsWrapped = true; 8 9 const fields = objectType.getFields(); 10 11 Object.keys(fields).forEach((fieldName) =\u0026gt; { 12 const field = fields[fieldName]; 13 const { resolve = defaultFieldResolver } = field; 14 field.resolve = async function (...args) { 15 const userGroups = field._userGroups || objectType._userGroups; 16 const projGroups = field._projGroups || objectType._projGroups; 17 if (!userGroups \u0026amp;\u0026amp; !projGroups) { 18 return resolve.apply(this, args); 19 } 20 21 const context = args[2]; 22 context.user.requires = { userGroups, projGroups }; 23 24 // check permission of fields that have a specific parent type 25 if (args[3].parentType.name == \u0026#34;Project\u0026#34;) { 26 const user = User.clone(context.user); 27 if (!(await context.oso.isAllowed(user, \u0026#34;get:project\u0026#34;, args[0]))) { 28 throw new ForbiddenError( 29 JSON.stringify({ requires: user.requires, groups: user.groups }) 30 ); 31 } 32 } 33 34 return resolve.apply(this, args); 35 }; 36 }); 37 } 38} Within Resolver The Oso object is instantiated and stored in the context. Then a policy can be enforced with the corresponding actor, action and _resource _triples. For list endpoints, different strategies can be employed. For example, the projects query fetches all records, but returns only authorized records. On the other hand, the indicators query is set to fetch only permitted records, which is more effective when dealing with sensitive data or a large amount of data.\n1// src/resolvers.js 2const resolvers = { 3 Query: { 4 users: async (_, __, context) =\u0026gt; { 5 const user = User.clone(context.user); 6 if (await context.oso.isAllowed(user, \u0026#34;list:users\u0026#34;, \u0026#34;_\u0026#34;)) { 7 return await User.fetchUsers(); 8 } else { 9 throw new ForbiddenError( 10 JSON.stringify({ requires: user.requires, groups: user.groups }) 11 ); 12 } 13 }, 14 project: async (_, args, context) =\u0026gt; { 15 const user = User.clone(context.user); 16 const result = await Project.fetchProjects([args.projectId]); 17 if (await context.oso.isAllowed(user, \u0026#34;get:project\u0026#34;, result[0])) { 18 return result[0]; 19 } else { 20 throw new ForbiddenError(...); 21 } 22 }, 23 projects: async (_, __, context) =\u0026gt; { 24 const user = User.clone(context.user); 25 const results = await Project.fetchProjects(); 26 const authorizedResults = []; 27 for (const result of results) { 28 if (await context.oso.isAllowed(user, \u0026#34;get:project\u0026#34;, result)) { 29 authorizedResults.push(result); 30 } 31 } 32 return authorizedResults; 33 }, 34 indicators: async (_, __, context) =\u0026gt; { 35 const user = User.clone(context.user); 36 if (await context.oso.isAllowed(user, \u0026#34;list:indicators\u0026#34;, \u0026#34;_\u0026#34;)) { 37 let projectIds; 38 if (user.isRequiredUserGroup()) { 39 projectIds = []; 40 } else { 41 projectIds = user.filterAllowedProjectIds(); 42 if (projectIds.length == 0) { 43 throw new Error(\u0026#34;fails to populate project ids\u0026#34;); 44 } 45 } 46 return await Project.fetchProjectIndicators(projectIds); 47 } else { 48 throw new ForbiddenError(...); 49 } 50 }, 51 }, 52 Mutation: { 53 updateProjectStatus: async (_, args, context) =\u0026gt; { 54 const user = User.clone(context.user); 55 if ( 56 await context.oso.isAllowed( 57 User, \u0026#34;update:project\u0026#34;, parseInt(args.projectId) 58 ) 59 ) { 60 return Project.updateProjectStatus(args.projectId, args.status); 61 } else { 62 throw new ForbiddenError(...); 63 } 64 }, 65 }, 66}; Examples The application source can be found in this GitHub repository, and it can be started as follows.\n1docker-compose up 2# if first time 3docker-compose up --build Apollo Studio can be used to query the example API. Note the server is running on port 5000, and it is expected to have one of the following values in the name request header.\nguest-user user group: guest member-user user group: member user project group: contributor of project 1 and 3 admin-user user group: admin contributor-user user group: guest user project group: contributor of project 1, 3, 5, 8 and 12 The member user can query the project thanks to her user group membership. Also, as the user is a contributor of project 1 and 3, she has access to contract_sum.\nThe query returns an error if a project that she is not a contributor is requested. The project query is resolved because of her user group membership while contract_sum turns to null.\nThe contributor user can query all permitted projects without an error as shown below.\nConclusion In this post, it is illustrated how to build authorization in a GraphQL API using a custom directive and an open source authorization library, Oso. A custom directive is effective to define permission on a schema, to pass configuration to the resolver and even to enforce policies directly. The Oso library helps build policies in a declarative way while expressing common concepts. Although it’s not covered in this post, the library supports building common authorization models such as role-based access control, multi-tenancy, hierarchies and relationships. It has a huge potential! I hope you find this post useful when building authorization in an application.\n","date":"July 20, 2021","img":"/blog/2021-07-20-graphql-api-authorization/featured.png","lang":"en","langName":"English","largeImg":"/blog/2021-07-20-graphql-api-authorization/featured_hueddcdd7752048c40aa557a0cee455633_52143_500x0_resize_box_3.png","permalink":"/blog/2021-07-20-graphql-api-authorization/","series":[],"smallImg":"/blog/2021-07-20-graphql-api-authorization/featured_hueddcdd7752048c40aa557a0cee455633_52143_180x0_resize_box_3.png","tags":[{"title":"Apollo","url":"/tags/apollo/"},{"title":"Authorization","url":"/tags/authorization/"},{"title":"GraphQL","url":"/tags/graphql/"},{"title":"Node.js","url":"/tags/node.js/"}],"timestamp":1626739200,"title":"Adding Authorization to a Graphql API"},{"categories":[{"title":"Data Engineering","url":"/categories/data-engineering/"}],"content":"Apache Airflow is a popular open-source workflow management platform. Typically tasks run remotely by Celery workers for scalability. In AWS, however, scalability can also be achieved using serverless computing services in a simpler way. For example, the ECS Operator allows to run dockerized tasks and, with the Fargate launch type, they can run in a serverless environment.\nThe ECS Operator alone is not sufficient because it can take up to several minutes to pull a Docker image and to set up network interface (for the case of Fargate launch type). Due to its latency, it is not suitable for frequently-running tasks. On the other hand, the latency of a Lambda function is negligible so that it\u0026rsquo;s more suitable for managing such tasks.\nIn this post, it is demonstrated how AWS Lambda can be integrated with Apache Airflow using a custom operator inspired by the ECS Operator.\nHow it works The following shows steps when an Airflow task is executed by the ECS Operator.\nRunning the associating ECS task Waiting for the task ended Checking the task status The status of a task is checked by searching a stopped reason and raises AirflowException if the reason is considered to be failure. While checking the status, the associating CloudWatch log events are pulled and printed so that the ECS task\u0026rsquo;s container logs can be found in Airflow web server.\nThe key difference between ECS and Lambda is that the former sends log events to a dedicated CloudWatch Log Stream while the latter may reuse an existing Log Stream due to container reuse. Therefore it is not straightforward to pull execution logs for a specific Lambda invocation. It can be handled by creating a custom CloudWatch Log Group and sending log events to a CloudWatch Log Stream within the custom Log Group. For example, let say there is a Lambda function named as airflow-test. In order to pull log events for a specific Lambda invocation, a custom Log Group (eg /airflow/lambda/airflow-test) can be created and, inside the Lambda function, log events can be sent to a Log Stream within the custom Log Group. Note that the CloudWatch Log Stream name can be determined by the operator and sent to the Lambda function in the Lambda payload. In this way, the Lambda function can send log events to a Log Stream that Airflow knows. Then the steps of a custom Lambda Operator can be as following.\nInvoking the Lambda function Wating for function ended Checking the invocation status Lambda Operator Below shows a simplified version of the custom Lambda Operator - the full version can be found here. Note that the associating CloudWatch Log Group name is a required argument (awslogs_group) while the Log Stream name is determined by a combination of execution date, qualifier and UUID. These are sent to the Lambda function in the payload. Note also that, in _check_success_invocation(), whether a function invocation is failed or succeeded is identified by searching ERROR within message of log events. I find this gives a more stable outcome than Lambda invocation response.\n1import re, time, json, math, uuid 2from datetime import datetime 3from botocore import exceptions 4from airflow.exceptions import AirflowException 5from airflow.models import BaseOperator 6from airflow.utils import apply_defaults 7from airflow.contrib.hooks.aws_hook import AwsHook 8from airflow.contrib.hooks.aws_logs_hook import AwsLogsHook 9 10class LambdaOperator(BaseOperator): 11 @apply_defaults 12 def __init__( 13 self, function_name, awslogs_group, qualifier=\u0026#34;$LATEST\u0026#34;, 14 payload={}, aws_conn_id=None, region_name=None, *args, **kwargs 15 ): 16 super(LambdaOperator, self).__init__(**kwargs) 17 self.function_name = function_name 18 self.qualifier = qualifier 19 self.payload = payload 20 # log stream is created and added to payload 21 self.awslogs_group = awslogs_group 22 self.awslogs_stream = \u0026#34;{0}/[{1}]{2}\u0026#34;.format( 23 datetime.utcnow().strftime(\u0026#34;%Y/%m/%d\u0026#34;), 24 self.qualifier, 25 re.sub(\u0026#34;-\u0026#34;, \u0026#34;\u0026#34;, str(uuid.uuid4())), 26 ) 27 # lambda client and cloudwatch logs hook 28 self.client = AwsHook(aws_conn_id=aws_conn_id).get_client_type(\u0026#34;lambda\u0026#34;) 29 self.awslogs_hook = AwsLogsHook(aws_conn_id=aws_conn_id, region_name=region_name) 30 31 def execute(self, context): 32 # invoke - wait - check 33 payload = json.dumps( 34 { 35 **{\u0026#34;group_name\u0026#34;: self.awslogs_group, \u0026#34;stream_name\u0026#34;: self.awslogs_stream}, 36 **self.payload, 37 } 38 ) 39 invoke_opts = { 40 \u0026#34;FunctionName\u0026#34;: self.function_name, 41 \u0026#34;Qualifier\u0026#34;: self.qualifier, 42 \u0026#34;InvocationType\u0026#34;: \u0026#34;RequestResponse\u0026#34;, 43 \u0026#34;Payload\u0026#34;: bytes(payload, encoding=\u0026#34;utf8\u0026#34;), 44 } 45 try: 46 resp = self.client.invoke(**invoke_opts) 47 self.log.info(\u0026#34;Lambda function invoked - StatusCode {0}\u0026#34;.format(resp[\u0026#34;StatusCode\u0026#34;])) 48 except exceptions.ClientError as e: 49 raise AirflowException(e.response[\u0026#34;Error\u0026#34;]) 50 51 self._wait_for_function_ended() 52 53 self._check_success_invocation() 54 self.log.info(\u0026#34;Lambda Function has been successfully invoked\u0026#34;) 55 56 def _wait_for_function_ended(self): 57 waiter = self.client.get_waiter(\u0026#34;function_active\u0026#34;) 58 waiter.config.max_attempts = math.ceil( 59 self._get_function_timeout() / 5 60 ) # poll interval - 5 seconds 61 waiter.wait(FunctionName=self.function_name, Qualifier=self.qualifier) 62 63 def _check_success_invocation(self): 64 self.log.info(\u0026#34;Lambda Function logs output\u0026#34;) 65 has_message, invocation_failed = False, False 66 messages, max_trial, current_trial = [], 5, 0 67 # sometimes events are not retrieved, run for 5 times if so 68 while True: 69 current_trial += 1 70 for event in self.awslogs_hook.get_log_events(self.awslogs_group, self.awslogs_stream): 71 dt = datetime.fromtimestamp(event[\u0026#34;timestamp\u0026#34;] / 1000.0) 72 self.log.info(\u0026#34;[{}] {}\u0026#34;.format(dt.isoformat(), event[\u0026#34;message\u0026#34;])) 73 messages.append(event[\u0026#34;message\u0026#34;]) 74 if len(messages) \u0026gt; 0 or current_trial \u0026gt; max_trial: 75 break 76 time.sleep(2) 77 if len(messages) == 0: 78 raise AirflowException(\u0026#34;Fails to get log events\u0026#34;) 79 for m in reversed(messages): 80 if re.search(\u0026#34;ERROR\u0026#34;, m) != None: 81 raise AirflowException(\u0026#34;Lambda Function invocation is not successful\u0026#34;) 82 83 def _get_function_timeout(self): 84 resp = self.client.get_function(FunctionName=self.function_name, Qualifier=self.qualifier) 85 return resp[\u0026#34;Configuration\u0026#34;][\u0026#34;Timeout\u0026#34;] Lambda Function Below shows a simplified version of the Lambda function - the full version can be found here. CustomLogManager includes methods to create CloudWatch Log Stream and to put log events. LambdaDecorator manages actions before/after the function invocation as well as when an exception occurs - it\u0026rsquo;s used as a decorator and modified from the lambda_decorators package. Before an invocation, it initializes a custom Log Stream. Log events are put to the Log Stream after an invocation or there is an exception. Note that traceback is also sent to the Log Stream when there\u0026rsquo;s an exception. The Lambda function simply exits after a loop or raises an exception at random.\n1import time, re, random, logging, traceback, boto3 2from datetime import datetime 3from botocore import exceptions 4from io import StringIO 5from functools import update_wrapper 6 7# save logs to stream 8stream = StringIO() 9logger = logging.getLogger() 10log_handler = logging.StreamHandler(stream) 11formatter = logging.Formatter(\u0026#34;%(levelname)-8s %(asctime)-s %(name)-12s %(message)s\u0026#34;) 12log_handler.setFormatter(formatter) 13logger.addHandler(log_handler) 14logger.setLevel(logging.INFO) 15 16cwlogs = boto3.client(\u0026#34;logs\u0026#34;) 17 18class CustomLogManager(object): 19 # create log stream and send logs to it 20 def __init__(self, event): 21 self.group_name = event[\u0026#34;group_name\u0026#34;] 22 self.stream_name = event[\u0026#34;stream_name\u0026#34;] 23 24 def has_log_group(self): 25 group_exists = True 26 try: 27 resp = cwlogs.describe_log_groups(logGroupNamePrefix=self.group_name) 28 group_exists = len(resp[\u0026#34;logGroups\u0026#34;]) \u0026gt; 0 29 except exceptions.ClientError as e: 30 logger.error(e.response[\u0026#34;Error\u0026#34;][\u0026#34;Code\u0026#34;]) 31 group_exists = False 32 return group_exists 33 34 def create_log_stream(self): 35 is_created = True 36 try: 37 cwlogs.create_log_stream(logGroupName=self.group_name, logStreamName=self.stream_name) 38 except exceptions.ClientError as e: 39 logger.error(e.response[\u0026#34;Error\u0026#34;][\u0026#34;Code\u0026#34;]) 40 is_created = False 41 return is_created 42 43 def delete_log_stream(self): 44 is_deleted = True 45 try: 46 cwlogs.delete_log_stream(logGroupName=self.group_name, logStreamName=self.stream_name) 47 except exceptions.ClientError as e: 48 # ResourceNotFoundException is ok 49 codes = [ 50 \u0026#34;InvalidParameterException\u0026#34;, 51 \u0026#34;OperationAbortedException\u0026#34;, 52 \u0026#34;ServiceUnavailableException\u0026#34;, 53 ] 54 if e.response[\u0026#34;Error\u0026#34;][\u0026#34;Code\u0026#34;] in codes: 55 logger.error(e.response[\u0026#34;Error\u0026#34;][\u0026#34;Code\u0026#34;]) 56 is_deleted = False 57 return is_deleted 58 59 def init_log_stream(self): 60 if not all([self.has_log_group(), self.delete_log_stream(), self.create_log_stream()]): 61 raise Exception(\u0026#34;fails to create log stream\u0026#34;) 62 logger.info(\u0026#34;log stream created\u0026#34;) 63 64 def create_log_events(self, stream): 65 fmt = \u0026#34;%Y-%m-%d %H:%M:%S,%f\u0026#34; 66 log_events = [] 67 for m in [s for s in stream.getvalue().split(\u0026#34;\\n\u0026#34;) if s]: 68 match = re.search(r\u0026#34;\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2},\\d{3}\u0026#34;, m) 69 dt_str = match.group() if match else datetime.utcnow().strftime(fmt) 70 log_events.append( 71 {\u0026#34;timestamp\u0026#34;: int(datetime.strptime(dt_str, fmt).timestamp()) * 1000, \u0026#34;message\u0026#34;: m} 72 ) 73 return log_events 74 75 def put_log_events(self, stream): 76 try: 77 resp = cwlogs.put_log_events( 78 logGroupName=self.group_name, 79 logStreamName=self.stream_name, 80 logEvents=self.create_log_events(stream), 81 ) 82 logger.info(resp) 83 except exceptions.ClientError as e: 84 logger.error(e) 85 raise Exception(\u0026#34;fails to put log events\u0026#34;) 86 87class LambdaDecorator(object): 88 # keep functions to run before, after and on exception 89 # modified from lambda_decorators (https://lambda-decorators.readthedocs.io/en/latest/) 90 def __init__(self, handler): 91 update_wrapper(self, handler) 92 self.handler = handler 93 94 def __call__(self, event, context): 95 try: 96 self.event = event 97 self.log_manager = CustomLogManager(event) 98 return self.after(self.handler(*self.before(event, context))) 99 except Exception as exception: 100 return self.on_exception(exception) 101 102 def before(self, event, context): 103 # remove existing logs 104 stream.seek(0) 105 stream.truncate(0) 106 # create log stream 107 self.log_manager.init_log_stream() 108 logger.info(\u0026#34;Start Request\u0026#34;) 109 return event, context 110 111 def after(self, retval): 112 logger.info(\u0026#34;End Request\u0026#34;) 113 # send logs to stream 114 self.log_manager.put_log_events(stream) 115 return retval 116 117 def on_exception(self, exception): 118 logger.error(str(exception)) 119 # log traceback 120 logger.error(traceback.format_exc()) 121 # send logs to stream 122 self.log_manager.put_log_events(stream) 123 return str(exception) 124 125@LambdaDecorator 126def lambda_handler(event, context): 127 max_len = event.get(\u0026#34;max_len\u0026#34;, 6) 128 fails_at = random.randint(0, max_len * 2) 129 for i in range(max_len): 130 if i != fails_at: 131 logger.info(\u0026#34;current run {0}\u0026#34;.format(i)) 132 else: 133 raise Exception(\u0026#34;fails at {0}\u0026#34;.format(i)) 134 time.sleep(1) Run Lambda Task A simple demo task is created as following. It just runs the Lambda function every 30 seconds.\n1import airflow 2from airflow import DAG 3from airflow.utils.dates import days_ago 4from datetime import timedelta 5from dags.operators import LambdaOperator 6 7function_name = \u0026#34;airflow-test\u0026#34; 8 9demo_dag = DAG( 10 dag_id=\u0026#34;demo-dag\u0026#34;, 11 start_date=days_ago(1), 12 catchup=False, 13 max_active_runs=1, 14 concurrency=1, 15 schedule_interval=timedelta(seconds=30), 16) 17 18demo_task = LambdaOperator( 19 task_id=\u0026#34;demo-task\u0026#34;, 20 function_name=function_name, 21 awslogs_group=\u0026#34;/airflow/lambda/{0}\u0026#34;.format(function_name), 22 payload={\u0026#34;max_len\u0026#34;: 6}, 23 dag=demo_dag, 24) The task can be tested by the following docker compose services. Note that the web server and scheduler are split into separate services although it doesn\u0026rsquo;t seem to be recommended for Local Executor - I had an issue to launch Airflow when those are combined in ECS.\n1version: \u0026#34;3.7\u0026#34; 2services: 3 postgres: 4 image: postgres:11 5 container_name: airflow-postgres 6 networks: 7 - airflow-net 8 ports: 9 - 5432:5432 10 environment: 11 - POSTGRES_USER=airflow 12 - POSTGRES_PASSWORD=airflow 13 - POSTGRES_DB=airflow 14 webserver: 15 image: puckel/docker-airflow:1.10.6 16 container_name: webserver 17 command: webserver 18 networks: 19 - airflow-net 20 user: root # for DockerOperator 21 volumes: 22 - ${HOME}/.aws:/root/.aws # run as root user 23 - ./requirements.txt:/requirements.txt 24 - ./dags:/usr/local/airflow/dags 25 - ./config/airflow.cfg:/usr/local/airflow/config/airflow.cfg 26 - ./entrypoint.sh:/entrypoint.sh # override entrypoint 27 - /var/run/docker.sock:/var/run/docker.sock # for DockerOperator 28 - ./custom:/usr/local/airflow/custom 29 ports: 30 - 8080:8080 31 environment: 32 - AIRFLOW__CORE__EXECUTOR=LocalExecutor 33 - AIRFLOW__CORE__LOAD_EXAMPLES=False 34 - AIRFLOW__CORE__LOGGING_LEVEL=INFO 35 - AIRFLOW__CORE__FERNET_KEY=Gg3ELN1gITETZAbBQpLDBI1y2P0d7gHLe_7FwcDjmKc= 36 - AIRFLOW__CORE__REMOTE_LOGGING=True 37 - AIRFLOW__CORE__REMOTE_BASE_LOG_FOLDER=s3://airflow-lambda-logs 38 - AIRFLOW__CORE__ENCRYPT_S3_LOGS=True 39 - POSTGRES_HOST=postgres 40 - POSTGRES_USER=airflow 41 - POSTGRES_PASSWORD=airflow 42 - POSTGRES_DB=airflow 43 - AWS_DEFAULT_REGION=ap-southeast-2 44 restart: always 45 healthcheck: 46 test: [\u0026#34;CMD-SHELL\u0026#34;, \u0026#34;[ -f /usr/local/airflow/config/airflow-webserver.pid ]\u0026#34;] 47 interval: 30s 48 timeout: 30s 49 retries: 3 50 scheduler: 51 image: puckel/docker-airflow:1.10.6 52 container_name: scheduler 53 command: scheduler 54 networks: 55 - airflow-net 56 user: root # for DockerOperator 57 volumes: 58 - ${HOME}/.aws:/root/.aws # run as root user 59 - ./requirements.txt:/requirements.txt 60 - ./logs:/usr/local/airflow/logs 61 - ./dags:/usr/local/airflow/dags 62 - ./config/airflow.cfg:/usr/local/airflow/config/airflow.cfg 63 - ./entrypoint.sh:/entrypoint.sh # override entrypoint 64 - /var/run/docker.sock:/var/run/docker.sock # for DockerOperator 65 - ./custom:/usr/local/airflow/custom 66 environment: 67 - AIRFLOW__CORE__EXECUTOR=LocalExecutor 68 - AIRFLOW__CORE__LOAD_EXAMPLES=False 69 - AIRFLOW__CORE__LOGGING_LEVEL=INFO 70 - AIRFLOW__CORE__FERNET_KEY=Gg3ELN1gITETZAbBQpLDBI1y2P0d7gHLe_7FwcDjmKc= 71 - AIRFLOW__CORE__REMOTE_LOGGING=True 72 - AIRFLOW__CORE__REMOTE_BASE_LOG_FOLDER=s3://airflow-lambda-logs 73 - AIRFLOW__CORE__ENCRYPT_S3_LOGS=True 74 - POSTGRES_HOST=postgres 75 - POSTGRES_USER=airflow 76 - POSTGRES_PASSWORD=airflow 77 - POSTGRES_DB=airflow 78 - AWS_DEFAULT_REGION=ap-southeast-2 79 restart: always 80 81networks: 82 airflow-net: 83 name: airflow-network Below shows the demo DAG after running for a while.\nLambda logs (and traceback) are found for both succeeded and failed tasks.\n","date":"April 13, 2020","img":"/blog/2020-04-13-airflow-lambda-operator/featured.png","lang":"en","langName":"English","largeImg":"/blog/2020-04-13-airflow-lambda-operator/featured_hu1fb366ed8468cd95c289e8b53adf2b4c_44994_500x0_resize_box_3.png","permalink":"/blog/2020-04-13-airflow-lambda-operator/","series":[],"smallImg":"/blog/2020-04-13-airflow-lambda-operator/featured_hu1fb366ed8468cd95c289e8b53adf2b4c_44994_180x0_resize_box_3.png","tags":[{"title":"AWS","url":"/tags/aws/"},{"title":"AWS Lambda","url":"/tags/aws-lambda/"},{"title":"Apache Airflow","url":"/tags/apache-airflow/"},{"title":"Python","url":"/tags/python/"},{"title":"Docker","url":"/tags/docker/"},{"title":"Docker Compose","url":"/tags/docker-compose/"}],"timestamp":1586736000,"title":"Thoughts on Apache Airflow AWS Lambda Operator"},{"categories":[{"title":"Development","url":"/categories/development/"}],"content":"Ingress in Kubernetes exposes HTTP and HTTPS routes from outside the cluster to services within the cluster. By setting rules, it routes requests to appropriate services (precisely requests are sent to individual Pods by Ingress Controller). Rules can be set up dynamically and I find it\u0026rsquo;s more efficient compared to traditional reverse proxy.\nTraefik is a modern HTTP reverse proxy and load balancer and it can be used as a Kubernetes Ingress Controller. Moreover it supports other providers, which are existing infrastructure components such as orchestrators, container engines, cloud providers, or key-value stores. To name a few, Docker, Kubernetes, AWS ECS, AWS DynamoDB and Consul are supported providers. With Traefik, it is possible to configure routing dynamically. Another interesting feature is Forward Authentication where authentication can be handled by an external service. In this post, it\u0026rsquo;ll be demonstrated how path-based routing can be set up by Traefik with Docker. Also a centralized authentication will be illustrated with the Forward Authentication feature of Traefik.\nHow Traefik works Below shows an illustration of internal architecture of Traefik.\nThe Traefik website explains workflow of requests as following.\nIncoming requests end on entrypoints, as the name suggests, they are the network entry points into Traefik (listening port, SSL, traffic redirection\u0026hellip;). Traffic is then forwarded to a matching frontend. A frontend defines routes from entrypoints to backends. Routes are created using requests fields (Host, Path, Headers\u0026hellip;) and can match or not a request. The frontend will then send the request to a backend. A backend can be composed by one or more servers, and by a load-balancing strategy. Finally, the server will forward the request to the corresponding microservice in the private network. In this example, a HTTP entrypoint is setup on port 80. Requests through it are forwarded to 2 web services by the following frontend rules.\nHost is k8s-traefik.info and path is /pybackend Host is k8s-traefik.info and path is /rbackend As the paths of the rules suggest, requests to /pybackend are sent to a backend service, created with FastAPI. If the other rule is met, requests are sent to the Rserve backend service. Note that only requests from authenticated users are fowarded to relevant backends and it is configured in frontend rules as well. Below shows how authentication is handled.\nTraefik setup Here is the traefik service defined in the compose file of this example - the full version can be found here.\n1version: \u0026#34;3.7\u0026#34; 2services: 3 traefik: 4 image: \u0026#34;traefik:v1.7.19\u0026#34; 5 networks: 6 - traefik-net 7 command: \u0026gt; 8 --docker 9 --docker.domain=k8s-traefik.info 10 --docker.exposedByDefault=false 11 --docker.network=traefik-net 12 --defaultentrypoints=http 13 --entrypoints=\u0026#34;Name:http Address::80\u0026#34; 14 --api.dashboard 15 ports: 16 - 80:80 17 - 8080:8080 18 labels: 19 - \u0026#34;traefik.frontend.rule=Host:k8s-traefik.info\u0026#34; 20 - \u0026#34;traefik.port=8080\u0026#34; 21 volumes: 22 - /var/run/docker.sock:/var/run/docker.sock 23... 24networks: 25 traefik-net: 26 name: traefik-network In commands, the Docker provider is enabled (--docker) with a custom domain name (k8s-traefik.info). A dedicated network is created and it is used for this and the other services (trafic-net). A single HTTP entrypoint is enabled as the default entrypoint. Finally monitoring dashboard is enabled (--api.dashboard). In lables, it is set to be served via the custom domain (hostname) - port 80 is for individual services while 8080 is for the monitoring UI.\nIt is necessary to have a custom hostname when setting up rules that include multiple hosts or enabling a HTTPS entrypoint. Although neither is discussed in this post, a custom domain (k8s-traefik.info), which is accessible only in local environment, is added - another post may come later. The location of hosts file is\nWindows - %WINDIR%\\System32\\drivers\\etc\\hosts or C:\\Windows\\System32\\drivers\\etc\\hosts Linux - /etc/hosts And the following entry is added.\n1# using a virtual machine 2\u0026lt;VM-IP-ADDRESS\u0026gt; k8s-traefik.info 3# or in the same machine 40.0.0.0 k8s-traefik.info In order to show how routes are configured dynamically, only the Traefik service is started as following.\n1docker-compose up -d traefik When visiting the monitoring UI via http://k8s-traefik.info:8080/dashboard, it\u0026rsquo;s shown that no frontend and backend exists in the docker provider tab.\nServices The authentication service is just checking if there\u0026rsquo;s an authorization header and the JWT value is foobar. If so, it returns 200 response so that requests can be forward to relevant backends. The source is shown below.\n1import os 2from typing import Dict, List 3from fastapi import FastAPI, Depends, HTTPException 4from pydantic import BaseModel 5from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials 6from starlette.requests import Request 7from starlette.status import HTTP_401_UNAUTHORIZED 8 9app = FastAPI(title=\u0026#34;Forward Auth API\u0026#34;, docs_url=None, redoc_url=None) 10 11## authentication 12class JWTBearer(HTTPBearer): 13 def __init__(self, auto_error: bool = True): 14 super().__init__(scheme_name=\u0026#34;Novice JWT Bearer\u0026#34;, auto_error=auto_error) 15 16 async def __call__(self, request: Request) -\u0026gt; None: 17 credentials: HTTPAuthorizationCredentials = await super().__call__(request) 18 19 if credentials.credentials != \u0026#34;foobar\u0026#34;: 20 raise HTTPException(HTTP_401_UNAUTHORIZED, detail=\u0026#34;Invalid Token\u0026#34;) 21 22 23## response models 24class StatusResp(BaseModel): 25 status: str 26 27 28## service methods 29@app.get(\u0026#34;/auth\u0026#34;, response_model=StatusResp, dependencies=[Depends(JWTBearer())]) 30async def forward_auth(): 31 return {\u0026#34;status\u0026#34;: \u0026#34;ok\u0026#34;} The service is defined in the compose file as following.\n1... 2 forward-auth: 3 image: kapps/trafik-demo:pybackend 4 networks: 5 - traefik-net 6 depends_on: 7 - traefik 8 command: \u0026gt; 9 forward_auth:app 10 --host=0.0.0.0 11 --port=8000 12 --reload 13... The Python service has 3 endpoints. The app\u0026rsquo;s title and path value are returned when requests are made to / and /{p} - a variable path value. Those to /admission calls the Rserve service and relays results from it - see the Rseve service section for the request payload. Note that an authorization header is not necessary between services.\n1import os 2import httpx 3from fastapi import FastAPI 4from pydantic import BaseModel, Schema 5from typing import Optional 6 7APP_PREFIX = os.environ[\u0026#34;APP_PREFIX\u0026#34;] 8 9app = FastAPI(title=\u0026#34;{0} API\u0026#34;.format(APP_PREFIX), docs_url=None, redoc_url=None) 10 11## response models 12class NameResp(BaseModel): 13 title: str 14 15 16class PathResp(BaseModel): 17 title: str 18 path: str 19 20 21class AdmissionReq(BaseModel): 22 gre: int = Schema(None, ge=0, le=800) 23 gpa: float = Schema(None, ge=0.0, le=4.0) 24 rank: str = Schema(None) 25 26 27class AdmissionResp(BaseModel): 28 result: bool 29 30 31## service methods 32@app.get(\u0026#34;/\u0026#34;, response_model=NameResp) 33async def whoami(): 34 return {\u0026#34;title\u0026#34;: app.title} 35 36 37@app.post(\u0026#34;/admission\u0026#34;) 38async def admission(*, req: Optional[AdmissionReq]): 39 host = os.getenv(\u0026#34;RSERVE_HOST\u0026#34;, \u0026#34;localhost\u0026#34;) 40 port = os.getenv(\u0026#34;RSERVE_PORT\u0026#34;, \u0026#34;8000\u0026#34;) 41 async with httpx.AsyncClient() as client: 42 dat = req.json() if req else None 43 r = await client.post(\u0026#34;http://{0}:{1}/{2}\u0026#34;.format(host, port, \u0026#34;admission\u0026#34;), data=dat) 44 return r.json() 45 46 47@app.get(\u0026#34;/{p}\u0026#34;, response_model=PathResp) 48async def whichpath(p: str): 49 print(p) 50 return {\u0026#34;title\u0026#34;: app.title, \u0026#34;path\u0026#34;: p} The Python service is configured with lables. Traefik is enabled and the same docker network is used. In frontend rules,\nrequests are set to be forwarded if host is k8s-traefik.info and path is /pybackend - PathPrefixStrip is to allow the path and its subpaths. authentication service is called and its address is http://forward-auth:8080/auth. authorization header is set to be copied to request - it\u0026rsquo;s for adding a custom header to a request and this label is mistakenly added. If the frontend rules pass, requests are sent to pybackend backend on port 8000.\n1... 2 pybackend: 3 image: kapps/trafik-demo:pybackend 4 networks: 5 - traefik-net 6 depends_on: 7 - traefik 8 - forward-auth 9 - rbackend 10 command: \u0026gt; 11 main:app 12 --host=0.0.0.0 13 --port=8000 14 --reload 15 expose: 16 - 8000 17 labels: 18 - \u0026#34;traefik.enable=true\u0026#34; 19 - \u0026#34;traefik.docker.network=traefik-net\u0026#34; 20 - \u0026#34;traefik.frontend.rule=Host:k8s-traefik.info;PathPrefixStrip:/pybackend\u0026#34; 21 - \u0026#34;traefik.frontend.auth.forward.address=http://forward-auth:8000/auth\u0026#34; 22 - \u0026#34;traefik.frontend.auth.forward.authResponseHeaders=Authorization\u0026#34; 23 - \u0026#34;traefik.backend=pybackend\u0026#34; 24 - \u0026#34;traefik.port=8000\u0026#34; 25 environment: 26 APP_PREFIX: \u0026#34;Python Backend\u0026#34; 27 RSERVE_HOST: \u0026#34;rbackend\u0026#34; 28 RSERVE_PORT: \u0026#34;8000\u0026#34; 29... R Service whoami() that returns the service name is executed when a request is made to the base path (/) - see here for details. To /admission, an admission result of a graduate school is returned by fitting a simple logistic regression. The result is based on 3 fields - GRE (Graduate Record Exam scores), GPA (grade point average) and prestige of the undergraduate institution. It\u0026rsquo;s from UCLA Institute for Digital Research \u0026amp; Education. If a field is missing, the mean or majority level is selected.\n1DAT \u0026lt;- read.csv(\u0026#39;./binary.csv\u0026#39;) 2DAT$rank \u0026lt;- factor(DAT$rank) 3 4value_if_null \u0026lt;- function(v, DAT) { 5 if (class(DAT[[v]]) == \u0026#39;factor\u0026#39;) { 6 tt \u0026lt;- table(DAT[[v]]) 7 names(tt[tt==max(tt)]) 8 } else { 9 mean(DAT[[v]]) 10 } 11} 12 13set_newdata \u0026lt;- function(args_called) { 14 args_init \u0026lt;- list(gre=NULL, gpa=NULL, rank=NULL) 15 newdata \u0026lt;- lapply(names(args_init), function(n) { 16 if (is.null(args_called[[n]])) { 17 args_init[[n]] \u0026lt;- value_if_null(n, DAT) 18 } else { 19 args_init[[n]] \u0026lt;- args_called[[n]] 20 } 21 }) 22 names(newdata) \u0026lt;- names(args_init) 23 lapply(names(newdata), function(n) { 24 flog.info(sprintf(\u0026#34;%s - %s\u0026#34;, n, newdata[[n]])) 25 }) 26 newdata \u0026lt;- as.data.frame(newdata) 27 newdata$rank \u0026lt;- factor(newdata$rank, levels = levels(DAT$rank)) 28 newdata 29} 30 31admission \u0026lt;- function(gre=NULL, gpa=NULL, rank=NULL, ...) { 32 newdata \u0026lt;- set_newdata(args_called = as.list(sys.call())) 33 logit \u0026lt;- glm(admit ~ gre + gpa + rank, data = DAT, family = \u0026#34;binomial\u0026#34;) 34 resp \u0026lt;- predict(logit, newdata=newdata, type=\u0026#34;response\u0026#34;) 35 flog.info(sprintf(\u0026#34;resp - %s\u0026#34;, resp)) 36 list(result = resp \u0026gt; 0.5) 37} 38 39whoami \u0026lt;- function() { 40 list(title=sprintf(\u0026#34;%s API\u0026#34;, Sys.getenv(\u0026#34;APP_PREFIX\u0026#34;, \u0026#34;RSERVE\u0026#34;))) 41} The Rserve service is configured with lables as well.\n1... 2 rbackend: 3 image: kapps/trafik-demo:rbackend 4 networks: 5 - traefik-net 6 depends_on: 7 - traefik 8 - forward-auth 9 command: \u0026gt; 10 --slave 11 --RS-conf /home/app/rserve.conf 12 --RS-source /home/app/rserve-src.R 13 expose: 14 - 8000 15 labels: 16 - \u0026#34;traefik.enable=true\u0026#34; 17 - \u0026#34;traefik.docker.network=traefik-net\u0026#34; 18 - \u0026#34;traefik.frontend.rule=Host:k8s-traefik.info;PathPrefixStrip:/rbackend\u0026#34; 19 - \u0026#34;traefik.frontend.auth.forward.address=http://forward-auth:8000/auth\u0026#34; 20 - \u0026#34;traefik.frontend.auth.forward.authResponseHeaders=Authorization\u0026#34; 21 - \u0026#34;traefik.backend=rbackend\u0026#34; 22 - \u0026#34;traefik.port=8000\u0026#34; 23 environment: 24 APP_PREFIX: \u0026#34;R Backend\u0026#34; 25... In order to check dynamic routes configuration, the Python service is started as following. Note that, as it depends on the authentication and Rserve service, these are started as well.\n1docker-compose up -d pybackend Once those services are started, the frontends/backends of the Python and Rserve services appear in the monitoring UI.\nBelow shows some request examples.\n1#### Authentication failure responses from authentication server 2http http://k8s-traefik.info/pybackend 3# HTTP/1.1 403 Forbidden 4# ... 5# { 6# \u0026#34;detail\u0026#34;: \u0026#34;Not authenticated\u0026#34; 7# } 8 9http http://k8s-traefik.info/pybackend \u0026#34;Authorization: Bearer foo\u0026#34; 10# HTTP/1.1 401 Unauthorized 11# ... 12# { 13# \u0026#34;detail\u0026#34;: \u0026#34;Invalid Token\u0026#34; 14# } 15 16#### Successful responses from Python service 17http http://k8s-traefik.info/pybackend \u0026#34;Authorization: Bearer foobar\u0026#34; 18# { 19# \u0026#34;title\u0026#34;: \u0026#34;Python Backend API\u0026#34; 20# } 21 22http http://k8s-traefik.info/pybackend/foobar \u0026#34;Authorization: Bearer foobar\u0026#34; 23# { 24# \u0026#34;path\u0026#34;: \u0026#34;foobar\u0026#34;, 25# \u0026#34;title\u0026#34;: \u0026#34;Python Backend API\u0026#34; 26# } 27 28#### Succesesful responses from Rserve service 29http http://k8s-traefik.info/rbackend \u0026#34;Authorization: Bearer foobar\u0026#34; 30# { 31# \u0026#34;title\u0026#34;: \u0026#34;R Backend API\u0026#34; 32# } 33 34#### Successful responses from requests to /admission 35echo \u0026#39;{\u0026#34;gre\u0026#34;: 600, \u0026#34;rank\u0026#34;: \u0026#34;1\u0026#34;}\u0026#39; \\ 36 | http POST http://k8s-traefik.info/rbackend/admission \u0026#34;Authorization: Bearer foobar\u0026#34; 37# { 38# \u0026#34;result\u0026#34;: true 39# } 40 41echo \u0026#39;{\u0026#34;gre\u0026#34;: 600, \u0026#34;rank\u0026#34;: \u0026#34;1\u0026#34;}\u0026#39; \\ 42 | http POST http://k8s-traefik.info/pybackend/admission \u0026#34;Authorization: Bearer foobar\u0026#34; 43# { 44# \u0026#34;result\u0026#34;: true 45# } The HEALTH tab of the monitoring UI shows some request metrics. After running the following for a while, the page is updated as shown below.\n1while true; do echo \u0026#39;{\u0026#34;gre\u0026#34;: 600, \u0026#34;rank\u0026#34;: \u0026#34;1\u0026#34;}\u0026#39; \\ 2 | http POST http://k8s-traefik.info/pybackend/admission \u0026#34;Authorization: Bearer foobar\u0026#34;; sleep 1; done ","date":"November 29, 2019","img":"/blog/2019-11-29-traefik-example/featured.png","lang":"en","langName":"English","largeImg":"/blog/2019-11-29-traefik-example/featured_huf02131f0aec4656166a63d0e0c90fae1_139790_500x0_resize_box_3.png","permalink":"/blog/2019-11-29-traefik-example/","series":[],"smallImg":"/blog/2019-11-29-traefik-example/featured_huf02131f0aec4656166a63d0e0c90fae1_139790_180x0_resize_box_3.png","tags":[{"title":"Traefik","url":"/tags/traefik/"},{"title":"FastAPI","url":"/tags/fastapi/"},{"title":"Rserve","url":"/tags/rserve/"},{"title":"R","url":"/tags/r/"},{"title":"Python","url":"/tags/python/"},{"title":"Docker","url":"/tags/docker/"},{"title":"Docker Compose","url":"/tags/docker-compose/"}],"timestamp":1574985600,"title":"Dynamic Routing and Centralized Auth With Traefik, Python and R Example"},{"categories":[{"title":"Development","url":"/categories/development/"}],"content":"While I\u0026rsquo;m looking into Apache Airflow, a workflow management tool, I thought it would be beneficial to get some understanding of how Celery works. To do so, I built a simple web service that sends tasks to Celery workers and collects the results from them. FastAPI is used for developing the web service and Redis is used for the message broker and result backend. During the development, I thought it would be possible to implement similar functionality in R with Rserve. Therefore a Rserve worker is added as an example as well. Coupling a web service with distributed task queue is beneficial on its own as it helps the service be more responsive by offloading heavyweight and long running processes to task workers.\nIn this post, it\u0026rsquo;ll be illustrated how a web service is created using FastAPI framework where tasks are sent to multiple workers. The workers are built with Celery and Rserve. Redis is used as a message broker/result backend for Celery and a key-value store for Rserve. Demos can be run in both Docker Compose and Kubernetes.\nThe following diagram shows how the apps work together and the source can be found in this GitHub repository.\nCelery Worker The source of the Celery app and task is shown below - /queue_celery/tasks.py. The same Redis DB is used as the message broker and result backend. The task is nothing but iterating to total - the value is from a request. In each iteration, it updates its state (bind=True) followed by sleeping for 1 second and it is set that a task can be sent by referring to its name (name=\u0026quot;long_task\u0026quot;).\n1import os 2import math 3import time 4from celery import Celery 5 6redis_url = \u0026#34;redis://{0}:{1}/{2}\u0026#34;.format( 7 os.environ[\u0026#34;REDIS_HOST\u0026#34;], os.environ[\u0026#34;REDIS_PORT\u0026#34;], os.environ[\u0026#34;REDIS_DB\u0026#34;] 8) 9 10app = Celery(\u0026#34;tasks\u0026#34;, backend=redis_url, broker=redis_url) 11app.conf.update(broker_transport_options={\u0026#34;visibility_timeout\u0026#34;: 3600}) 12 13 14@app.task(bind=True, name=\u0026#34;long_task\u0026#34;) 15def long_task(self, total): 16 message = \u0026#34;\u0026#34; 17 for i in range(total): 18 message = \u0026#34;Percentage completion {0} ...\u0026#34;.format(math.ceil(i / total * 100)) 19 self.update_state(state=\u0026#34;PROGRESS\u0026#34;, meta={\u0026#34;current\u0026#34;: i, \u0026#34;total\u0026#34;: total, \u0026#34;status\u0026#34;: message}) 20 time.sleep(1) 21 return {\u0026#34;current\u0026#34;: total, \u0026#34;total\u0026#34;: total, \u0026#34;status\u0026#34;: \u0026#34;Task completed!\u0026#34;, \u0026#34;result\u0026#34;: total} Rserve Worker redux package, Redis client for R, and RSclient package, R-based client for Rserve, are used to set up the Rserve worker. The function RR() checks if a Redis DB is available and returns a hiredis object, which is an interface to Redis. The task (long_task()) is constructed to be similar to the Celery task. In order for the task to be executed asynchronously, a handler function (handle_long_task()) is used to receive a request from the main web service. Once called, the task function is sent to be evaluated by a Rserve client (RS.eval()) - note wait=FALSE and lazy=TRUE. Its evaluation is asynchronous as the task function is run by a separate forked process. Finally the status of a task can be obtained by get_task() and it pulls the status output from the Redis DB - note a R list is converted as binary. The source of the Rserve worker can be found in /queue_rserve/tasks.R.\n1RR \u0026lt;- function(check_conn_only = FALSE) { 2 redis_host \u0026lt;- Sys.getenv(\u0026#34;REDIS_HOST\u0026#34;, \u0026#34;localhost\u0026#34;) 3 redis_port \u0026lt;- Sys.getenv(\u0026#34;REDIS_PORT\u0026#34;, \u0026#34;6379\u0026#34;) 4 redis_db \u0026lt;- Sys.getenv(\u0026#34;REDIS_DB\u0026#34;, \u0026#34;1\u0026#34;) 5 info \u0026lt;- sprintf(\u0026#34;host %s, port %s, db %s\u0026#34;, redis_host, redis_port, redis_db) 6 ## check if redis is available 7 is_available = redis_available(host=redis_host, port=redis_port, db=redis_db) 8 if (is_available) { 9 flog.info(sprintf(\u0026#34;Redis is available - %s\u0026#34;, info)) 10 } else { 11 flog.error(sprintf(\u0026#34;Redis is not available - %s\u0026#34;, info)) 12 } 13 ## create an interface to redis 14 if (!check_conn_only) { 15 return(hiredis(host=redis_host, port=redis_port, db=redis_db)) 16 } 17} 18 19 20long_task \u0026lt;- function(task_id, total) { 21 rr \u0026lt;- RR() 22 for (i in seq.int(total)) { 23 is_total \u0026lt;- i == max(seq.int(total)) 24 state \u0026lt;- if (is_total) \u0026#34;SUCESS\u0026#34; else \u0026#34;PROGRESS\u0026#34; 25 msg \u0026lt;- sprintf(\u0026#34;Percent completion %s ...\u0026#34;, ceiling(i / total * 100)) 26 val \u0026lt;- list(state = state, current = i, total = total, status = msg) 27 if (is_total) { 28 val \u0026lt;- append(val, list(result = total)) 29 } 30 flog.info(sprintf(\u0026#34;task id: %s, message: %s\u0026#34;, task_id, msg)) 31 rr$SET(task_id, object_to_bin(val)) 32 Sys.sleep(1) 33 } 34} 35 36 37handle_long_task \u0026lt;- function(task_id, total) { 38 flog.info(sprintf(\u0026#34;task started, task_id - %s, total - %s\u0026#34;, task_id, total)) 39 conn \u0026lt;- RS.connect() 40 RS.eval(conn, library(redux)) 41 RS.eval(conn, library(futile.logger)) 42 RS.eval(conn, setwd(\u0026#34;/home/app\u0026#34;)) 43 RS.eval(conn, source(\u0026#34;./tasks.R\u0026#34;)) 44 RS.assign(conn, task_id) 45 RS.assign(conn, total) 46 RS.eval(conn, long_task(task_id, total), wait=FALSE, lazy=TRUE) 47 RS.close(conn) 48 flog.info(sprintf(\u0026#34;task executed, task_id - %s, total - %s\u0026#34;, task_id, total)) 49 list(task_id = task_id, status = \u0026#34;created\u0026#34;) 50} 51 52get_task \u0026lt;- function(task_id) { 53 rr \u0026lt;- RR() 54 val \u0026lt;- bin_to_object(rr$GET(task_id)) 55 flog.info(sprintf(\u0026#34;task id - %s\u0026#34;, task_id)) 56 flog.info(val) 57 val 58} Main Web Service The main service has 2 methods for each of the workers - POST for executing a task and GET for collecting its status. To execute a task, a value named total is required in request body. As soon as a task is sent or requested, it returns the task ID and status value - ExecuteResp. A task\u0026rsquo;s status can be obtained by calling the associating collect method with its ID in query string. The response is defined by ResultResp. The source of the Rserve worker can be found in /main.py.\n1import os 2import json 3import httpx 4from uuid import uuid4 5from fastapi import FastAPI, Body, HTTPException 6from pydantic import BaseModel, Schema 7 8from queue_celery.tasks import app as celery_app, long_task 9 10 11def set_rserve_url(fname): 12 return \u0026#34;http://{0}:{1}/{2}\u0026#34;.format( 13 os.getenv(\u0026#34;RSERVE_HOST\u0026#34;, \u0026#34;localhost\u0026#34;), os.getenv(\u0026#34;RSERVE_PORT\u0026#34;, \u0026#34;8000\u0026#34;), fname 14 ) 15 16 17app = FastAPI(title=\u0026#34;FastAPI Job Queue Example\u0026#34;, version=\u0026#34;0.0.1\u0026#34;) 18 19 20class ExecuteResp(BaseModel): 21 task_id: str 22 status: str 23 24 25class ResultResp(BaseModel): 26 current: int 27 total: int 28 status: str 29 result: int = None 30 31 32class ErrorResp(BaseModel): 33 detail: str 34 35 36@app.post(\u0026#34;/celery/execute\u0026#34;, response_model=ExecuteResp, status_code=202, tags=[\u0026#34;celery\u0026#34;]) 37async def execute_celery_task(total: int = Body(..., min=1, max=50, embed=True)): 38 task = celery_app.send_task(\u0026#34;long_task\u0026#34;, args=[total]) 39 return {\u0026#34;task_id\u0026#34;: task.id, \u0026#34;status\u0026#34;: \u0026#34;created\u0026#34;} 40 41 42@app.get( 43 \u0026#34;/celery/collect\u0026#34;, 44 response_model=ResultResp, 45 responses={500: {\u0026#34;model\u0026#34;: ErrorResp}}, 46 tags=[\u0026#34;celery\u0026#34;], 47) 48async def collect_celery_result(task_id: str): 49 resp = long_task.AsyncResult(task_id) 50 if resp.status == \u0026#34;FAILURE\u0026#34;: 51 raise HTTPException(status_code=500, detail=\u0026#34;Fails to collect result\u0026#34;) 52 return resp.result 53 54 55@app.post(\u0026#34;/rserve/execute\u0026#34;, response_model=ExecuteResp, status_code=202, tags=[\u0026#34;rserve\u0026#34;]) 56async def execute_rserve_task(total: int = Body(..., min=1, max=50, embed=True)): 57 jsn = json.dumps({\u0026#34;task_id\u0026#34;: str(uuid4()), \u0026#34;total\u0026#34;: total}) 58 async with httpx.AsyncClient() as client: 59 r = await client.post(set_rserve_url(\u0026#34;handle_long_task\u0026#34;), json=jsn) 60 return r.json() 61 62 63@app.get( 64 \u0026#34;/rserve/collect\u0026#34;, 65 response_model=ResultResp, 66 responses={500: {\u0026#34;model\u0026#34;: ErrorResp}}, 67 tags=[\u0026#34;rserve\u0026#34;], 68) 69async def collect_rserve_task(task_id: str): 70 jsn = json.dumps({\u0026#34;task_id\u0026#34;: task_id}) 71 async with httpx.AsyncClient() as client: 72 try: 73 r = await client.post(set_rserve_url(\u0026#34;get_task\u0026#34;), json=jsn) 74 return {k: v for k, v in r.json().items() if k != \u0026#34;state\u0026#34;} 75 except Exception: 76 raise HTTPException(status_code=500, detail=\u0026#34;Fails to collect result\u0026#34;) Docker Compose The apps can be started with Docker Compose as following - the compose file can be found here.\n1git clone https://github.com/jaehyeon-kim/k8s-job-queue.git 2cd k8s-job-queue 3docker-compose up -d The swagger document of the main web service can be visited via http://localhost:9000/docs or http://\u0026lt;vm-ip-address\u0026gt;:9000 if it\u0026rsquo;s started in a VM.\nA task can be started by clicking the Try it out button, followed by clicking the Execute button. Any value between 1 and 50 can be set as the value total.\nThe status of a task can be checked by adding its ID to query string.\nKubernetes 4 groups of resources are necessary to run the apps in Kubernetes and they can be found in /manifests.\nwebservice.yaml - main web service Deployment and Service queue_celery - Celery worker Deployment queue_rserve - Rserve worker Deployment and Service redis.yaml - Redis Deployment and Service In Kubernetes, Pod is one or more containers that work together. Deployment handles a replica of Pod (ReplicaSet), update strategy and so on. And Service allows to connect to a set of Pods from within and outside a Kubernetes cluster. Note that the Celery worker doesn\u0026rsquo;t have a Service resource as it is accessed by the Redis message broker/result backend.\nWith kubectl apply, the following resources are created as shown below. Note only the main web service is accessible by a client outside the cluster. The service is mapped to a specific node port (30000). In Minikube, it can be accessed by http://\u0026lt;node-ip-address\u0026gt;:3000. The node IP address can be found by minikube ip command.\n1## create resources 2kubectl apply -f manifests 3 4## get resources 5kubectl get po,rs,deploy,svc 6 7NAME READY STATUS RESTARTS AGE 8pod/celery-deployment-674d8fb968-2x97k 1/1 Running 0 25s 9pod/celery-deployment-674d8fb968-44lw4 1/1 Running 0 25s 10pod/main-deployment-79cf8fc5df-45w4p 1/1 Running 0 25s 11pod/main-deployment-79cf8fc5df-hkz6r 1/1 Running 0 25s 12pod/redis-deployment-5ff8646968-hcsbk 1/1 Running 0 25s 13pod/rserve-deployment-59dfbd955-db4v9 1/1 Running 0 25s 14pod/rserve-deployment-59dfbd955-fxfxn 1/1 Running 0 25s 15 16NAME DESIRED CURRENT READY AGE 17replicaset.apps/celery-deployment-674d8fb968 2 2 2 25s 18replicaset.apps/main-deployment-79cf8fc5df 2 2 2 25s 19replicaset.apps/redis-deployment-5ff8646968 1 1 1 25s 20replicaset.apps/rserve-deployment-59dfbd955 2 2 2 25s 21 22NAME READY UP-TO-DATE AVAILABLE AGE 23deployment.apps/celery-deployment 2/2 2 2 25s 24deployment.apps/main-deployment 2/2 2 2 25s 25deployment.apps/redis-deployment 1/1 1 1 25s 26deployment.apps/rserve-deployment 2/2 2 2 25s 27 28NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE 29service/main-service NodePort 10.98.60.194 \u0026lt;none\u0026gt; 80:30000/TCP 25s 30service/redis-service ClusterIP 10.99.52.18 \u0026lt;none\u0026gt; 6379/TCP 25s 31service/rserve-service ClusterIP 10.105.249.199 \u0026lt;none\u0026gt; 8000/TCP 25s The execute/collect pair of requests to the Celery worker are shown below. HttPie is used to make HTTP requests.\n1echo \u0026#39;{\u0026#34;total\u0026#34;: 30}\u0026#39; | http POST http://172.28.175.23:30000/celery/execute 2{ 3 \u0026#34;status\u0026#34;: \u0026#34;created\u0026#34;, 4 \u0026#34;task_id\u0026#34;: \u0026#34;87ae7a42-1ec0-4848-bf30-2f68175b38db\u0026#34; 5} 6 7export TASK_ID=87ae7a42-1ec0-4848-bf30-2f68175b38db 8http http://172.28.175.23:30000/celery/collect?task_id=$TASK_ID 9{ 10 \u0026#34;current\u0026#34;: 18, 11 \u0026#34;result\u0026#34;: null, 12 \u0026#34;status\u0026#34;: \u0026#34;Percentage completion 60 ...\u0026#34;, 13 \u0026#34;total\u0026#34;: 30 14} 15 16# after a while 17 18http http://172.28.175.23:30000/celery/collect?task_id=$TASK_ID 19{ 20 \u0026#34;current\u0026#34;: 30, 21 \u0026#34;result\u0026#34;: 30, 22 \u0026#34;status\u0026#34;: \u0026#34;Task completed!\u0026#34;, 23 \u0026#34;total\u0026#34;: 30 24} The following shows the execute/collect pair of the Rserve worker.\n1echo \u0026#39;{\u0026#34;total\u0026#34;: 30}\u0026#39; | http POST http://172.28.175.23:30000/rserve/execute 2{ 3 \u0026#34;status\u0026#34;: \u0026#34;created\u0026#34;, 4 \u0026#34;task_id\u0026#34;: \u0026#34;f5d46986-1e89-4322-9d4e-7c1da6454534\u0026#34; 5} 6 7export TASK_ID=f5d46986-1e89-4322-9d4e-7c1da6454534 8http http://172.28.175.23:30000/rserve/collect?task_id=$TASK_ID 9{ 10 \u0026#34;current\u0026#34;: 16, 11 \u0026#34;result\u0026#34;: null, 12 \u0026#34;status\u0026#34;: \u0026#34;Percent completion 54 ...\u0026#34;, 13 \u0026#34;total\u0026#34;: 30 14} 15 16# after a while 17 18http http://172.28.175.23:30000/rserve/collect?task_id=$TASK_ID 19{ 20 \u0026#34;current\u0026#34;: 30, 21 \u0026#34;result\u0026#34;: 30, 22 \u0026#34;status\u0026#34;: \u0026#34;Percent completion 100 ...\u0026#34;, 23 \u0026#34;total\u0026#34;: 30 24} ","date":"November 15, 2019","img":"/blog/2019-11-15-task-queue/featured.png","lang":"en","langName":"English","largeImg":"/blog/2019-11-15-task-queue/featured_hu7f23bb50744730950217f66f197f569b_51615_500x0_resize_box_3.png","permalink":"/blog/2019-11-15-task-queue/","series":[],"smallImg":"/blog/2019-11-15-task-queue/featured_hu7f23bb50744730950217f66f197f569b_51615_180x0_resize_box_3.png","tags":[{"title":"FastAPI","url":"/tags/fastapi/"},{"title":"Celery","url":"/tags/celery/"},{"title":"Redis","url":"/tags/redis/"},{"title":"Rserve","url":"/tags/rserve/"},{"title":"R","url":"/tags/r/"},{"title":"Python","url":"/tags/python/"},{"title":"Docker","url":"/tags/docker/"},{"title":"Docker Compose","url":"/tags/docker-compose/"},{"title":"Kubernetes","url":"/tags/kubernetes/"}],"timestamp":1573776000,"title":"Distributed Task Queue With Python and R Example"},{"categories":[{"title":"Development","url":"/categories/development/"}],"content":"I use Linux containers a lot for development. Having Windows computers at home and work, I used to use Linux VMs on VirtualBox or VMWare Workstation. It\u0026rsquo;s not a bad option but it requires a lot of resources. Recently, after my home computer was updated, I was not able to start my hypervisor anymore. Also I didn\u0026rsquo;t like huge resource consumption of it so that I began to look for a different development environment. A while ago, I played with Windows Subsystem for Linux (WSL) and it was alright. Also Visual Studio Code (VSCode), my favourite editor, now supports remote development. Initially I thought I would be able to create a new development environment with WSL and Docker for Windows. However it was until I tried a bigger app with Docker Compose that Docker for Windows has a number of issues especially when containers are started by Docker Compose in WSL. I didn\u0026rsquo;t like to spend too much time on fixing those issues as I concerned those might not be the only ones. Then I decided to install a Linux VM on Hyper-V. Luckly VSCode also supports a remote VM via SSH.\nWhat I want was a Linux VM where Docker is installed and it should be possible to access a remote folder from the host for development. Also, as I\u0026rsquo;m getting interested in Kubernetes more and more, another VM where Minikube is installed was necessary. In this post, it\u0026rsquo;ll be illustrated how the new development environment is created. Also an example app (Rserve web service with a sidecar container) will be demonstrated. The app\u0026rsquo;s main functionality is served by Rserve while the sidecar container handles authentication and relays requests from authenticated users. The sidecar container is built by FastAPI, which is a modern, performant and developer-friendly Python web framework.\nWindows Subsystem for Linux (WSL) In order to use WSL, it is necessary to enable Windows Subsystem for Linux in Windows features as following.\nThen a Linux distribution need to be installed from Windows Store. I chose Ubuntu 18.04.\nOnce installed, you can hit the Launch button and a new terminal will pop up as shown below. If it\u0026rsquo;s the first launch, you\u0026rsquo;d need to create a default user account. I set the username to be jaehyeon. The default terminal is not the only way to access to WSL. For example you can enter bash to access to it on PowerShell.\nRemote WSL Having WSL alone wouldn\u0026rsquo;t be of much help for development. What\u0026rsquo;s really important is, for example, editing or debugging code seamlessly from the host. The new VSCode extension, Remote WSL makes it possible.\nOn VSCode \u0026gt; Extensions, search and install Remote WSL. Then Remote Explorer tab will appear in the left sidebar. There you can open a folder in WSL.\nAs can be seen in the Extensions tab, some extensions are installed in the host (LOCAL) and some are in WSL. For example, in order for Python syntax highlighting or autocompletion to work, the Python extension should be installed in WSL where source code exists. One of the great features of VSCode is installation of extensions and configuration is automatic and easy. For some standard extensions, it\u0026rsquo;s sufficient to open a file of a certain file extension.\nYou may wonder why I set up Remote WSL although I\u0026rsquo;m going to install a VM on Hyper-V. It is because not all development can be done in a separate VM easily. For example, a single node Kubernetes cluster will be created by Minikube in a VM but I\u0026rsquo;m not sure how to connect to it from another VM. On the other hand, connection from WSL can be made without a problem.\nLinux Virtual Machine on Hyper-V A Linux VM creation can be started by Quick Create\u0026hellip; button in the right panel of Hyper-V Manager.\nI used a Ubuntu 18.04 server ISO image rather than using the desktop version in the default list. Clicking Local installation source will allow to select an ISO or virtual hard disk file. I named it as ubuntu and left Default Switch selected for Network - it allows to connect from host to guest.\nOnce installed, it\u0026rsquo;s possible to connect by clicking Connect\u0026hellip;. A separate window will pop up.\nIt\u0026rsquo;s possible to log in by the default username and password, which are set during installation. Once logged in, it\u0026rsquo;ll go into the user\u0026rsquo;s home directory. Keep the value of IP address for eth0 as it\u0026rsquo;ll be used for setting-up Remote SSH.\nFor public key authentication for SSH, check /etc/ssh/sshd_config if RSAAuthentication is enabled. You may need to add/update the following entries.\n1RSAAuthentication yes 2PubkeyAuthentication yes 3AuthorizedKeysFile %h/.ssh/authorized_keys Password authentication is enabled by default. If you don\u0026rsquo;t like it, it can be disabled as following.\n1PasswordAuthentication no Finally you need to add your RSA public key to ~/.ssh/authorized_keys followed by setting up necessary permissions to the file as well as its parent folder - see this page for further details.\nRemote SSH For Remote SSH, I installed VSCode Insiders as the guest OS is Ubuntu 18.04 - see System requirements. I\u0026rsquo;ve decided to keep both stable and insiders versions of VSCode - stable for WSL and insiders for SSH in order not to be confused. In the Remote Explorer, you can see Add New and Configure buttons.\nTo add a new SSH target, you can click the Add New button and enter the following SSH command.\n1# vm-ip-address - \u0026#39;IP address for eth0\u0026#39;. 2ssh \u0026lt;username\u0026gt;@\u0026lt;vm-ip-address\u0026gt; -A I find the VM IP address changes from time to time and a new IP address can be updated by clicking the Configure button followed by changing the IP address as shown below.\nCurrently R doesn\u0026rsquo;t seem to be supported well by VSCode and it may not be necessary thanks to RStudio IDE. I haven\u0026rsquo;t tried installing RStudio Server in WSL but it\u0026rsquo;ll definitely be possible to install it in a VM. Another way of accessing RStudio Server is via Docker. The Docker extension of VSCode can make it easier to run an existing image or to customize your own.\nConEmu ConEmu is a handy Windows terminal tool. By default it has multiple terminals pre-configured - PowerShell, Git Bash, Putty, Chocolatey and more. Also it\u0026rsquo;s possible to set up a custom terminal eg) for SSH.\nIn Setup tasks\u0026hellip;, I created 3 terminals and moved them to top for easy access.\nAlso it supports split windows. Below shows an example of 3 terminals in a single window. They are created by New console dialog\u0026hellip;.\nMinikube Minikube is a tool that makes it easy to run Kubernetes locally. Minikube runs a single-node Kubernetes cluster inside a Virtual Machine (VM) on your laptop for users looking to try out Kubernetes or develop with it day-to-day.\nAlthough Docer for Windows supports Kubernetes, I decided not to rely on it and Minikube supports Hyper-V as a VM driver. Using Chocolatey, a package manager for Windows, Minikube can be installed simply as following.\n1PS \u0026gt; choco install minikube It\u0026rsquo;ll also install Kubectl, a CLI tool for Kubenetes. ConEmu includes a Chocolatey terminal. If it\u0026rsquo;s the first time, it\u0026rsquo;ll inform what to do before executing the install command.\nOnce installed, a single node Kubernetes cluster can be created as following.\n1PS \u0026gt; minikube start --vm-driver hyperv --hyperv-virtual-switch \u0026#34;Default Switch\u0026#34; --v=7 --alsologtostderr Hyper-V is selected as the VM driver and a detailed logging option is added (--v=7 --alsologtostderr) because it takes quite some time at first for installation and configuration. For networking, I selected Default Switch. Note that many tutorials instruct to create an external Virtual Switch but it didn\u0026rsquo;t work on my computer at work. Also note that it\u0026rsquo;s not possible to create a Minikube cluster in WSL due to insufficient permission.\nIf it\u0026rsquo;s created successfully, you can check the status of the cluster.\n1PS \u0026gt; minikube status 2host: Running 3kubelet: Running 4apiserver: Running 5kubectl: Correctly Configured: pointing to minikube-vm at 172.28.175.24 During installation, Kubectl is configured in C:\\Users\\\u0026lt;username\u0026gt;\\.kube\\config as shown below.\n1apiVersion: v1 2clusters: 3- cluster: 4 certificate-authority: C:\\Users\\jakim\\.minikube\\ca.crt 5 server: https://172.28.175.24:8443 6 name: minikube 7contexts: 8- context: 9 cluster: minikube 10 user: minikube 11 name: minikube 12current-context: minikube 13kind: Config 14preferences: {} 15users: 16- name: minikube 17 user: 18 client-certificate: C:\\Users\\jakim\\.minikube\\client.crt 19 client-key: C:\\Users\\jakim\\.minikube\\client.key Although it\u0026rsquo;s possible to access the cluster with Kubectl on PowerShell, I\u0026rsquo;d like to do it on a linux terminal. I find a slight modification is enough on WSL. After installing Kubectl in WSL, I created another folder in C:\\Users\\\u0026lt;username\u0026gt;\\.kubewsl and added a WSL version of Kubectl config file.\n1apiVersion: v1 2clusters: 3- cluster: 4 certificate-authority: /c/Users/jakim/.minikube/ca.crt 5 server: https://172.28.175.24:8443 6 name: minikube 7contexts: 8- context: 9 cluster: minikube 10 user: minikube 11 name: minikube 12current-context: minikube 13kind: Config 14preferences: {} 15users: 16- name: minikube 17 user: 18 client-certificate: /c/Users/jakim/.minikube/client.crt 19 client-key: /c/Users/jakim/.minikube/client.key The only difference is path notation. For example, C:\\Users\\\u0026lt;username\u0026gt; to /c/Users/\u0026lt;username\u0026gt;. Note that your mount point is likely to be different because usually a host drive is mounted to WSL in /mnt as the root directory. In this case, it should be /mnt/c/Users/\u0026lt;username\u0026gt;. Or you may change the default mount point. After creating a /c folder in WSL, update WSL config to /etc/wsl.conf as shown below. You\u0026rsquo;d have to restart WSL afterwards.\n1[automount] 2root = / Then it\u0026rsquo;s necessary to update KUBECONFIG environment variable as following - you may add it to ~/.bashrc.\n1export KUBECONFIG=$KUBECONFIG:/c/Users/jakim/.kubewsl/config Once the setup is complete, cluser information can be checked on WSL as following.\n1$ kubectl cluster-info 2Kubernetes master is running at https://172.28.175.24:8443 3KubeDNS is running at https://172.28.175.24:8443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy 4 5To further debug and diagnose cluster problems, use \u0026#39;kubectl cluster-info dump\u0026#39;. Rserve Sidecar Example As demonstrated in my earlier post, Rserve is an effective tool to build a web service in R. It\u0026rsquo;ll be useful to serve an analytics model without modifying the core if it\u0026rsquo;s built in R. However equipping comprehensive features to other web frameworks may not be an easy job. Another way of utilising Rserve may be deploying it with one or more helper containers. For example, if the service should be secured by authentication but it\u0026rsquo;s not easy to implement it in R, a sidecar container may be used to handle authentication. In this setup, the sidecar container can sit before Rserve and relay requests from authenticated users. A simple example will be illustrated in this post. Another example may be using a helper container as an ambassador. For example, let say it\u0026rsquo;s required to communicate with an external system but there\u0026rsquo;s no reliable client library in R. In this case, it\u0026rsquo;s possible to set up so that the ambassador does the communication and provides outputs to R.\nThe example app is created by 2 containers. The main functionality is from the Rserve container while the sidecar is to handle authentication and relays requests from authentication users. The sidecar container is build with the FastAPI framework, which is a modern, performant and developer-friendly Python web framework. The source can be found in the sidecar branch of this GitHub repository.\nWith Docker Compose, it can be started as following. The compose file can be found here.\n1git clone https://github.com/jaehyeon-kim/k8s-rserve.git 2cd k8s-rserve 3git fetch \u0026amp;\u0026amp; git checkout sidecar 4docker-compose up -d The swagger document created by the sidecar web service can be visited via http://localhost:9000/docs or http://\u0026lt;vm-ip-address\u0026gt;:9000 if it\u0026rsquo;s started in a VM. It basically has a main POST method in /rserve/test.\nThe web service is secured by Bearer Authentication so that a JWT token needs to be added to requests. A token can be obtained in the /auth/debug/{username} endpoint and it can also be tried out in the swagger document as shown below.\nThe token can be added to the Authrization section.\nAfter that, a request to /rserve/test can be made as an authenticated user. It returns a JSON object that has 3 properties: n, wait and hostname. The first 2 is just returning values from the request while the last shows the hostname where Rserve container is hosted.\nContainerization makes development easier. When it comes to deployment and management of potentially a large number of containers, a container orchestration engin is quite important. Kubernetes, among other engins, does such jobs well. Developers and administrator can define what it should be and then Kubernetes achieves it. This declarative nature can make complicated jobs to be tractable.\nIn Kubernetes, Pod is one or more containers that work together - eg) Rserve with sidecar. Deployment handles a replica of Pod, update strategy and so on. And Service allows to connect to a replica of Pod from within and outside a Kubernetes Cluster. The following Kubernetes manifest creates 2 Kubernetes resources where the Rserve with sidecar web service can be managed (by Deployment) and accessed (by Service). The manifest file can be found here.\n1apiVersion: v1 2kind: Service 3metadata: 4 name: rserve-sidecar-service 5spec: 6 selector: # Pods with this label will be served 7 app: rserve-sidecar 8 type: NodePort 9 ports: 10 - protocol: TCP 11 port: 80 12 targetPort: 9000 13 nodePort: 30000 # Service will exposed at this port 14--- 15apiVersion: apps/v1 16kind: Deployment 17metadata: 18 name: rserve-sidecar-deployment 19 labels: 20 app: rserve 21spec: 22 selector: 23 matchLabels: # Pods with this label will be managed by Deployment 24 app: rserve-sidecar 25 replicas: 2 26 template: 27 metadata: 28 labels: 29 app: rserve-sidecar 30 spec: 31 containers: 32 - name: rserve 33 image: kapps/sidecar:rserve 34 ports: 35 - containerPort: 8000 36 - name: pyapi 37 image: kapps/sidecar:pyapi 38 env: 39 - name: RSERVE_HOST # containers share network interface 40 value: localhost 41 - name: RSERVE_PORT 42 value: \u0026#34;8000\u0026#34; 43 - name: JWT_SECRET # it\u0026#39;s not secure!! 44 value: chickenAndSons 45 ports: 46 - containerPort: 9000 With kubectl, it can be created as following. The --record flag is for keeping revision history, which can be particulary useful for updating the app.\n1$ kubectl apply -f manifest.yml --record 2# service/rserve-sidecar-service created 3# deployment.apps/rserve-sidecar-deployment created The resources can be listed as shown below.\n1$ kubectl get deployment,svc 2NAME READY UP-TO-DATE AVAILABLE AGE 3deployment.apps/rserve-sidecar-deployment 2/2 2 2 21s 4 5NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE 6service/rserve-sidecar-service NodePort 10.106.51.149 \u0026lt;none\u0026gt; 80:30000/TCP 21s The service is mapped to a specific node port (30000) and, as Minikube creates a single node cluster, the web service can be accessed by http://\u0026lt;node-ip-address\u0026gt;:3000. The node IP address can be found by minikube ip on PowerShell.\nFor authentication, the debug method can be called and it\u0026rsquo;ll return a token.\n1$ http http://172.28.175.24:30000/auth/debug/JAKIM 2 3# { 4# \u0026#34;token\u0026#34;: \u0026#34;eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJKQUtJTSIsImRlYnVnIjp0cnVlfQ.ddWXLcsB4IzQ5743vq-WVC2n-D9Z5yFIkSqjkpOAcs4\u0026#34; 5# } All requests to the main method need the token in the Authorization header. If authenticated, the sidecar will make a request to Rserve and the response from it will be returned back to the client. I made 2 requests using HttPie and the responses show different hostname values. It\u0026rsquo;s because requests are load-balanced by the Service.\n1export token=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJKQUtJTSIsImRlYnVnIjp0cnVlfQ.ddWXLcsB4IzQ5743vq-WVC2n-D9Z5yFIkSqjkpOAcs4 2 3$ echo \u0026#39;{\u0026#34;n\u0026#34;: 100, \u0026#34;wait\u0026#34;: 0.1}\u0026#39; \\ 4 | http POST \u0026#34;http://172.28.175.24:30000/rserve/test\u0026#34; \u0026#34;Authorization:Bearer $token\u0026#34; 5 6# { 7# \u0026#34;hostname\u0026#34;: \u0026#34;rserve-sidecar-deployment-5cbd6569f-ndjnb\u0026#34;, 8# \u0026#34;n\u0026#34;: 100.0, 9# \u0026#34;wait\u0026#34;: 0.1 10# } 11 12$ echo \u0026#39;{\u0026#34;n\u0026#34;: 100, \u0026#34;wait\u0026#34;: 0.1}\u0026#39; \\ 13 | http POST \u0026#34;http://172.28.175.24:30000/rserve/test\u0026#34; \u0026#34;Authorization:Bearer $token\u0026#34; 14 15# { 16# \u0026#34;hostname\u0026#34;: \u0026#34;rserve-sidecar-deployment-5cbd6569f-kv599\u0026#34;, 17# \u0026#34;n\u0026#34;: 100.0, 18# \u0026#34;wait\u0026#34;: 0.1 19# } ","date":"November 1, 2019","img":"/blog/2019-11-01-linux-on-windows/featured.png","lang":"en","langName":"English","largeImg":"/blog/2019-11-01-linux-on-windows/featured_huc10c59749850e2c6e75df8112745c889_187978_500x0_resize_box_3.png","permalink":"/blog/2019-11-01-linux-on-windows/","series":[],"smallImg":"/blog/2019-11-01-linux-on-windows/featured_huc10c59749850e2c6e75df8112745c889_187978_180x0_resize_box_3.png","tags":[{"title":"VSCode","url":"/tags/vscode/"},{"title":"WSL","url":"/tags/wsl/"},{"title":"Docker","url":"/tags/docker/"},{"title":"Docker Compose","url":"/tags/docker-compose/"},{"title":"Kubernetes","url":"/tags/kubernetes/"},{"title":"Minikube","url":"/tags/minikube/"},{"title":"Rserve","url":"/tags/rserve/"},{"title":"FastAPI","url":"/tags/fastapi/"},{"title":"R","url":"/tags/r/"},{"title":"Python","url":"/tags/python/"}],"timestamp":1572566400,"title":"Linux Dev Environment on Windows"},{"categories":[{"title":"Development","url":"/categories/development/"}],"content":"LocalStack provides an easy-to-use test/mocking framework for developing AWS applications. In this post, I\u0026rsquo;ll demonstrate how to utilize LocalStack for development using a web service.\nSpecifically a simple web service built with Flask-RestPlus is used. It supports simple CRUD operations against a database table. It is set that SQS and Lambda are used for creating and updating a record. When a POST or PUT request is made, the service sends a message to a SQS queue and directly returns 204 reponse. Once a message is received, a Lambda function is invoked and a relevant database operation is performed.\nThe source of this post can be found here.\nWeb Service As usual, the GET requests returns all records or a single record when an ID is provided as a path parameter. When an ID is not specified, it\u0026rsquo;ll create a new record (POST). Otherwise it\u0026rsquo;ll update an existing record (PUT). Note that both the POST and PUT method send a message and directly returns 204 response - the source can be found here.\n1ns = Namespace(\u0026#34;records\u0026#34;) 2 3@ns.route(\u0026#34;/\u0026#34;) 4class Records(Resource): 5 parser = ns.parser() 6 parser.add_argument(\u0026#34;message\u0026#34;, type=str, required=True) 7 8 def get(self): 9 \u0026#34;\u0026#34;\u0026#34; 10 Get all records 11 \u0026#34;\u0026#34;\u0026#34; 12 conn = conn_db() 13 cur = conn.cursor(real_dict_cursor=True) 14 cur.execute( 15 \u0026#34;\u0026#34;\u0026#34; 16 SELECT * FROM records ORDER BY created_on DESC 17 \u0026#34;\u0026#34;\u0026#34;) 18 19 records = cur.fetchall() 20 return jsonify(records) 21 22 @ns.expect(parser) 23 def post(self): 24 \u0026#34;\u0026#34;\u0026#34; 25 Create a record via queue 26 \u0026#34;\u0026#34;\u0026#34; 27 try: 28 body = { 29 \u0026#34;id\u0026#34;: None, 30 \u0026#34;message\u0026#34;: self.parser.parse_args()[\u0026#34;message\u0026#34;] 31 } 32 send_message(flask.current_app.config[\u0026#34;QUEUE_NAME\u0026#34;], json.dumps(body)) 33 return \u0026#34;\u0026#34;, 204 34 except Exception as e: 35 return \u0026#34;\u0026#34;, 500 36 37 38@ns.route(\u0026#34;/\u0026lt;string:id\u0026gt;\u0026#34;) 39class Record(Resource): 40 parser = ns.parser() 41 parser.add_argument(\u0026#34;message\u0026#34;, type=str, required=True) 42 43 def get(self, id): 44 \u0026#34;\u0026#34;\u0026#34; 45 Get a record given id 46 \u0026#34;\u0026#34;\u0026#34; 47 record = Record.get_record(id) 48 if record is None: 49 return {\u0026#34;message\u0026#34;: \u0026#34;No record\u0026#34;}, 404 50 return jsonify(record) 51 52 @ns.expect(parser) 53 def put(self, id): 54 \u0026#34;\u0026#34;\u0026#34; 55 Update a record via queue 56 \u0026#34;\u0026#34;\u0026#34; 57 record = Record.get_record(id) 58 if record is None: 59 return {\u0026#34;message\u0026#34;: \u0026#34;No record\u0026#34;}, 404 60 61 try: 62 message = { 63 \u0026#34;id\u0026#34;: record[\u0026#34;id\u0026#34;], 64 \u0026#34;message\u0026#34;: self.parser.parse_args()[\u0026#34;message\u0026#34;] 65 } 66 send_message(flask.current_app.config[\u0026#34;QUEUE_NAME\u0026#34;], json.dumps(message)) 67 return \u0026#34;\u0026#34;, 204 68 except Exception as e: 69 return \u0026#34;\u0026#34;, 500 70 71 @staticmethod 72 def get_record(id): 73 conn = conn_db() 74 cur = conn.cursor(real_dict_cursor=True) 75 cur.execute( 76 \u0026#34;\u0026#34;\u0026#34; 77 SELECT * FROM records WHERE id = %(id)s 78 \u0026#34;\u0026#34;\u0026#34;, {\u0026#34;id\u0026#34;: id}) 79 80 return cur.fetchone() Lambda The SQS queue that messages are sent by the web service is an event source of the following lambda function. It polls the queue and processes messages as shown below.\n1import os 2import logging 3import json 4import psycopg2 5 6logger = logging.getLogger() 7logger.setLevel(logging.INFO) 8 9try: 10 conn = psycopg2.connect(os.environ[\u0026#34;DB_CONNECT\u0026#34;], connect_timeout=5) 11except psycopg2.Error as e: 12 logger.error(e) 13 sys.exit() 14 15logger.info(\u0026#34;SUCCESS: Connection to DB\u0026#34;) 16 17def lambda_handler(event, context): 18 for r in event[\u0026#34;Records\u0026#34;]: 19 body = json.loads(r[\u0026#34;body\u0026#34;]) 20 logger.info(\u0026#34;Body: {0}\u0026#34;.format(body)) 21 with conn.cursor() as cur: 22 if body[\u0026#34;id\u0026#34;] is None: 23 cur.execute( 24 \u0026#34;\u0026#34;\u0026#34; 25 INSERT INTO records (message) VALUES (%(message)s) 26 \u0026#34;\u0026#34;\u0026#34;, {k:v for k,v in body.items() if v is not None}) 27 else: 28 cur.execute( 29 \u0026#34;\u0026#34;\u0026#34; 30 UPDATE records 31 SET message = %(message)s 32 WHERE id = %(id)s 33 \u0026#34;\u0026#34;\u0026#34;, body) 34 conn.commit() 35 36 logger.info(\u0026#34;SUCCESS: Processing {0} records\u0026#34;.format(len(event[\u0026#34;Records\u0026#34;]))) Database As RDS is not yet supported by LocalStack, a postgres db is created with Docker. The web service will do CRUD operations against the table named as records. The initialization SQL script is shown below.\n1CREATE DATABASE testdb; 2\\connect testdb; 3 4CREATE SCHEMA testschema; 5GRANT ALL ON SCHEMA testschema TO testuser; 6 7-- change search_path on a connection-level 8SET search_path TO testschema; 9 10-- change search_path on a database-level 11ALTER database \u0026#34;testdb\u0026#34; SET search_path TO testschema; 12 13CREATE TABLE testschema.records ( 14\tid serial NOT NULL, 15\tmessage varchar(30) NOT NULL, 16\tcreated_on timestamptz NOT NULL DEFAULT now(), 17\tCONSTRAINT records_pkey PRIMARY KEY (id) 18); 19 20INSERT INTO testschema.records (message) 21VALUES (\u0026#39;foo\u0026#39;), (\u0026#39;bar\u0026#39;), (\u0026#39;baz\u0026#39;); Launch Services Below shows the docker-compose file that creates local AWS services and postgres database.\n1version: \u0026#39;3.7\u0026#39; 2services: 3 localstack: 4 image: localstack/localstack 5 ports: 6 - \u0026#39;4563-4584:4563-4584\u0026#39; 7 - \u0026#39;8080:8080\u0026#39; 8 privileged: true 9 environment: 10 - SERVICES=s3,sqs,lambda 11 - DEBUG=1 12 - DATA_DIR=/tmp/localstack/data 13 - DEFAULT_REGION=ap-southeast-2 14 - LAMBDA_EXECUTOR=docker-reuse 15 - LAMBDA_REMOTE_DOCKER=false 16 - LAMBDA_DOCKER_NETWORK=play-localstack_default 17 - AWS_ACCESS_KEY_ID=foobar 18 - AWS_SECRET_ACCESS_KEY=foobar 19 - AWS_DEFAULT_REGION=ap-southeast-2 20 - DB_CONNECT=\u0026#39;postgresql://testuser:testpass@postgres:5432/testdb\u0026#39; 21 - TEST_QUEUE=test-queue 22 - TEST_LAMBDA=test-lambda 23 volumes: 24 - ./init/create-resources.sh:/docker-entrypoint-initaws.d/create-resources.sh 25 - ./init/lambda_package:/tmp/lambda_package 26 # - \u0026#39;./.localstack:/tmp/localstack\u0026#39; 27 - \u0026#39;/var/run/docker.sock:/var/run/docker.sock\u0026#39; 28 postgres: 29 image: postgres 30 ports: 31 - 5432:5432 32 volumes: 33 - ./init/db:/docker-entrypoint-initdb.d 34 depends_on: 35 - localstack 36 environment: 37 - POSTGRES_USER=testuser 38 - POSTGRES_PASSWORD=testpass For LocalStack, it\u0026rsquo;s easier to illustrate by the environment variables.\nSERVICES - S3, SQS and Lambda services are selected DEFAULT_REGION - Local AWS resources will be created in ap-southeast-2 by default LAMBDA_EXECUTOR - By selecting docker-reuse, Lambda function will be invoked by another container (based on lambci/lambda image). Once a container is created, it\u0026rsquo;ll be reused. Note that, in order to invoke a Lambda function in a separate Docker container, it should run in privileged mode (privileged: true) LAMBDA_REMOTE_DOCKER - It is set to false so that a Lambda function package can be added from a local path instead of a zip file. LAMBDA_DOCKER_NETWORK - Although the Lambda function is invoked in a separate container, it should be able to discover the database service (postgres). By default, Docker Compose creates a network (\u0026lt;parent-folder\u0026gt;_default) and, specifying the network name, the Lambda function can connect to the database with the DNS set by DB_CONNECT Actual AWS resources is created by create-resources.sh, which will be executed at startup. A SQS queue and Lambda function are created and the queue is mapped to be an event source of the Lambda function.\n1#!/bin/bash 2 3echo \u0026#34;Creating $TEST_QUEUE and $TEST_LAMBDA\u0026#34; 4 5aws --endpoint-url=http://localhost:4576 sqs create-queue \\ 6 --queue-name $TEST_QUEUE 7 8aws --endpoint-url=http://localhost:4574 lambda create-function \\ 9 --function-name $TEST_LAMBDA \\ 10 --code S3Bucket=\u0026#34;__local__\u0026#34;,S3Key=\u0026#34;/tmp/lambda_package\u0026#34; \\ 11 --runtime python3.6 \\ 12 --environment Variables=\u0026#34;{DB_CONNECT=$DB_CONNECT}\u0026#34; \\ 13 --role arn:aws:lambda:ap-southeast-2:000000000000:function:$TEST_LAMBDA \\ 14 --handler lambda_function.lambda_handler \\ 15 16aws --endpoint-url=http://localhost:4574 lambda create-event-source-mapping \\ 17 --function-name $TEST_LAMBDA \\ 18 --event-source-arn arn:aws:sqs:elasticmq:000000000000:$TEST_QUEUE The services can be launched as following.\n1docker-compose up -d Test Web Service Before testing the web service, it can be shown how the SQS and Lambda work by sending a message as following.\n1aws --endpoint-url http://localhost:4576 sqs send-message \\ 2 --queue-url http://localhost:4576/queue/test-queue \\ 3 --message-body \u0026#39;{\u0026#34;id\u0026#34;: null, \u0026#34;message\u0026#34;: \u0026#34;test\u0026#34;}\u0026#39; As shown in the image below, LocalStack invokes the Lambda function in a separate Docker container.\nThe web service can be started as following.\n1FLASK_APP=api FLASK_ENV=development flask run Using HttPie, the record created just before can be checked as following.\n1http http://localhost:5000/api/records/4 1{ 2 \u0026#34;created_on\u0026#34;: \u0026#34;2019-07-20T04:26:33.048841+00:00\u0026#34;, 3 \u0026#34;id\u0026#34;: 4, 4 \u0026#34;message\u0026#34;: \u0026#34;test\u0026#34; 5} For updating it,\n1echo \u0026#39;{\u0026#34;message\u0026#34;: \u0026#34;test put\u0026#34;}\u0026#39; | \\ 2 http PUT http://localhost:5000/api/records/4 3 4http http://localhost:5000/api/records/4 1{ 2 \u0026#34;created_on\u0026#34;: \u0026#34;2019-07-20T04:26:33.048841+00:00\u0026#34;, 3 \u0026#34;id\u0026#34;: 4, 4 \u0026#34;message\u0026#34;: \u0026#34;test put\u0026#34; 5} ","date":"July 20, 2019","img":"/blog/2019-07-20-aws-localstack/featured.png","lang":"en","langName":"English","largeImg":"/blog/2019-07-20-aws-localstack/featured_hu76363859e2b475a85e538ad8f869426e_164886_500x0_resize_box_3.png","permalink":"/blog/2019-07-20-aws-localstack/","series":[],"smallImg":"/blog/2019-07-20-aws-localstack/featured_hu76363859e2b475a85e538ad8f869426e_164886_180x0_resize_box_3.png","tags":[{"title":"AWS","url":"/tags/aws/"},{"title":"Docker","url":"/tags/docker/"},{"title":"Docker Compose","url":"/tags/docker-compose/"},{"title":"AWS Lambda","url":"/tags/aws-lambda/"},{"title":"LocalStack","url":"/tags/localstack/"},{"title":"S3","url":"/tags/s3/"},{"title":"SQS","url":"/tags/sqs/"},{"title":"Python","url":"/tags/python/"},{"title":"Flask","url":"/tags/flask/"},{"title":"Flask-RestPlus","url":"/tags/flask-restplus/"}],"timestamp":1563580800,"title":"AWS Local Development With LocalStack"},{"categories":[{"title":"Development","url":"/categories/development/"}],"content":"Accroding to the project GitHub repository,\nCronicle is a multi-server task scheduler and runner, with a web based front-end UI. It handles both scheduled, repeating and on-demand jobs, targeting any number of slave servers, with real-time stats and live log viewer.\nBy default, Cronicle is configured to launch a single master server - task scheduling is controlled by the master server. For high availability, it is important that another server takes the role of master when the existing master server fails.\nIn this post, multi-server configuration of Cronicle will be demonstrated with Docker and Nginx as load balancer. Specifically a single master and backup server will be set up and they will be served behind a load balancer - backup server is a slave server that can take the role of master when the master is not avaialble.\nThe source of this post can be found here.\nCronicle Docker Image There isn\u0026rsquo;t an official Docker image for Cronicle. I just installed it from python:3.6 image. The docker file can be found as following.\n1FROM python:3.6 2 3ARG CRONICLE_VERSION=v0.8.28 4ENV CRONICLE_VERSION=${CRONICLE_VERSION} 5 6# Node 7RUN curl -sL https://deb.nodesource.com/setup_8.x | bash - \\ 8 \u0026amp;\u0026amp; apt-get install -y nodejs \\ 9 \u0026amp;\u0026amp; curl -sL https://dl.yarnpkg.com/debian/pubkey.gpg | apt-key add - \\ 10 \u0026amp;\u0026amp; echo \u0026#34;deb https://dl.yarnpkg.com/debian/ stable main\u0026#34; | tee /etc/apt/sources.list.d/yarn.list \\ 11 \u0026amp;\u0026amp; apt-get update \u0026amp;\u0026amp; apt-get install -y yarn 12 13# Cronicle 14RUN curl -s \u0026#34;https://raw.githubusercontent.com/jhuckaby/Cronicle/${CRONICLE_VERSION}/bin/install.js\u0026#34; | node \\ 15 \u0026amp;\u0026amp; cd /opt/cronicle \\ 16 \u0026amp;\u0026amp; npm install aws-sdk 17 18EXPOSE 3012 19EXPOSE 3014 20 21ENTRYPOINT [\u0026#34;/docker-entrypoint.sh\u0026#34;] As Cronicle is written in Node.js, it should be installed as well. aws-sdk is not required strictly but it is added to test S3 integration later. Port 3012 is the default web port of Cronicle and 3014 is used for server auto-discovery via UDP broadcast - it may not be required.\ndocker-entrypoint.sh is used to start a Cronicle server. For master, one more step is necessary, which is initializing the storage system. An environment variable (IS_MASTER) will be used to control storage initialization.\n1#!/bin/bash 2 3set -e 4 5if [ \u0026#34;$IS_MASTER\u0026#34; = \u0026#34;0\u0026#34; ] 6then 7 echo \u0026#34;Running SLAVE server\u0026#34; 8else 9 echo \u0026#34;Running MASTER server\u0026#34; 10 /opt/cronicle/bin/control.sh setup 11fi 12 13/opt/cronicle/bin/control.sh start 14 15while true; 16do 17 sleep 30; 18 /opt/cronicle/bin/control.sh status 19done A custom docker image, cronicle-base, is built using as following.\n1docker build -t=cronicle-base . Load Balancer Nginx is used as a load balancer. The config file can be found as following. It listens port 8080 and passes a request to cronicle1:3012 or cronicle2:3012.\n1events { worker_connections 1024; } 2 3http { 4 upstream cronicles { 5 server cronicle1:3012; 6 server cronicle2:3012; 7 } 8 9 server { 10 listen 8080; 11 12 location / { 13 proxy_pass http://cronicles; 14 proxy_set_header Host $host; 15 } 16 } 17} In order for Cronicle servers to be served behind the load balancer, the following changes are made (complete config file can be found here).\n1{ 2\t\u0026#34;base_app_url\u0026#34;: \u0026#34;http://loadbalancer:8080\u0026#34;, 3 4 ... 5\t6\t\u0026#34;web_direct_connect\u0026#34;: true, 7 8 ...\t9} First, base_app_url should be changed to the load balancer URL instead of an individual server\u0026rsquo;s URL. Secondly web_direct_connect should be changed to true. According to the project repository,\nIf you set this parameter (web_direct_connect) to true, then the Cronicle web application will connect directly to your individual Cronicle servers. This is more for multi-server configurations, especially when running behind a load balancer with multiple backup servers. The Web UI must always connect to the master server, so if you have multiple backup servers, it needs a direct connection.\nLaunch Servers Docke Compose is used to launch 2 Cronicle servers (master and backup) and a load balancer. The service cronicle1 is for the master while cronicle2 is for the backup server. Note that both servers should have the same configuration file (config.json). Also, as the backup server will take the role of master, it should have access to the same data - ./backend/cronicle/data is mapped to both the servers. (Cronicle supports S3 or Couchbase as well.)\n1version: \u0026#39;3.2\u0026#39; 2 3services: 4 loadbalancer: 5 container_name: loadbalancer 6 hostname: loadbalancer 7 image: nginx 8 volumes: 9 - ./loadbalancer/nginx.conf:/etc/nginx/nginx.conf 10 tty: true 11 links: 12 - cronicle1 13 ports: 14 - 8080:8080 15 cronicle1: 16 container_name: cronicle1 17 hostname: cronicle1 18 image: cronicle-base 19 #restart: always 20 volumes: 21 - ./sample_conf/config.json:/opt/cronicle/conf/config.json 22 - ./sample_conf/emails:/opt/cronicle/conf/emails 23 - ./docker-entrypoint.sh:/docker-entrypoint.sh 24 - ./backend/cronicle/data:/opt/cronicle/data 25 entrypoint: /docker-entrypoint.sh 26 environment: 27 IS_MASTER: \u0026#34;1\u0026#34; 28 cronicle2: 29 container_name: cronicle2 30 hostname: cronicle2 31 image: cronicle-base 32 #restart: always 33 volumes: 34 - ./sample_conf/config.json:/opt/cronicle/conf/config.json 35 - ./sample_conf/emails:/opt/cronicle/conf/emails 36 - ./docker-entrypoint.sh:/docker-entrypoint.sh 37 - ./backend/cronicle/data:/opt/cronicle/data 38 entrypoint: /docker-entrypoint.sh 39 environment: 40 IS_MASTER: \u0026#34;0\u0026#34; It can be started as following.\n1docker-compose up -d Add Backup Server Once started, Cronicle web app will be accessible at http://localhost:8080 and it\u0026rsquo;s possible to log in as the admin user - username and password are all admin.\nIn Admin \u0026gt; Servers, it\u0026rsquo;s possible to see that the 2 Cronicle servers are shown. The master server is recognized as expected but the backup server (cronicle2) is not yet added.\nBy default, 2 server groups (All Servers and Master Group) are created and the backup server should be added to the Master Group. To do so, the Hostname Match regular expression is modified as following: ^(cronicle[1-2])$.\nThen it can be shown that the backup server is recognized correctly.\nCreate Event A test event is created in order to show that an event that\u0026rsquo;s created in the original master can be available in the backup server when it takes the role of master.\nCronicle has a web UI so that it is easy to manage/monitor scheduled events. It also has management API that many jobs can be performed programmatically. Here an event that runs a simple shell script is created.\nOnce created, it is listed in Schedule tab.\nBackup Becomes Master As mentioned earlier, the backup server will take the role of master when the master becomes unavailable. In order to see this, I removed the master server as following.\n1docker-compose rm -f cronicle1 After a while, it\u0026rsquo;s possible to see that the backup server becomes master.\nIt can also be checked in Admin \u0026gt; Activity Log.\nIn Schedule, the test event can be found.\n","date":"July 19, 2019","img":"/blog/2019-07-19-cronicle-multi-server-setup/featured.png","lang":"en","langName":"English","largeImg":"/blog/2019-07-19-cronicle-multi-server-setup/featured_huf31defd43c31d7741cd0651b807bf58f_64396_500x0_resize_box_3.png","permalink":"/blog/2019-07-19-cronicle-multi-server-setup/","series":[],"smallImg":"/blog/2019-07-19-cronicle-multi-server-setup/featured_huf31defd43c31d7741cd0651b807bf58f_64396_180x0_resize_box_3.png","tags":[{"title":"Cronicle","url":"/tags/cronicle/"},{"title":"Docker","url":"/tags/docker/"},{"title":"Nginx","url":"/tags/nginx/"}],"timestamp":1563494400,"title":"Cronicle Multi Server Setup"},{"categories":[{"title":"Development","url":"/categories/development/"}],"content":"In the last post, the async feature of Shiny was discussed. Although it is a remarkable step forward to web development in R, it is not to the full extent that a Javascript application can bring. In fact, (long running) requests of a user (or session) are not impacted by those of other users (or sessions) but, for a given user, all requests are handled sequentially. On the other hand, it is not the case for a Javascript-backed app where all requests are processed asynchronously.\nAlthough Javascript helps develop a more performant web app, for a Shiny developer, the downside is that key features that Shiny provides are no longer available. Some of them are built-in data binding, event handling and state management. For example, think about what reactive*() and observe*() do in a Shiny app. Although it is possible to implement those with plain Javascript or JQuery, it can be problemsome due to the aysnc nature of Javascript (eg Callback Hell) or it may be ending up with a slow app (eg Why do developers think the DOM is slow?).\nJavascript frameworks (Angular, React and Vue) support such key features effectively. Also they help avoid those development issues, together with the recent Javascript standard. In this post, it\u0026rsquo;ll be demonstrated how to render htmlwidgets in a Vue application as well as replacing htmlwidgets with native JavaScript libraries.\nWhat is Vue? According to the project website,\nVue (pronounced /vjuː/, like view) is a progressive framework for building user interfaces. Unlike other monolithic frameworks, Vue is designed from the ground up to be incrementally adoptable. The core library is focused on the view layer only, and is easy to pick up and integrate with other libraries or existing projects. On the other hand, Vue is also perfectly capable of powering sophisticated Single-Page Applications when used in combination with modern tooling and supporting libraries.\nSome of the key features mentioned earlier are supported in the core library.\ndata binding event handling And the other by an official library.\nstate management And even more\nrouting Vue is taken here among the popular Javascript frameworks because it is simpler to get jobs done and easier to learn. See this article for a quick comparison.\nBuilding Vue Apps Vue Setup Prerequisites of building a vue app are\nNode.js Javascript runtime built on Chrome\u0026rsquo;s V8 JavaScript engine version \u0026gt;=6.x or 8.x (preferred) npm package manager for Javascript and software registry version 3+ installed with Node.js Git vue-cli a simple CLI for scaffolding Vue.js projects provides templates and webpack-simple is used for the apps of this post install globally - npm install -g vue-cli Yarn fast, reliable and secure dependency management tool used instead of npm install globally - npm install -g yarn The apps can be found in vue-htmlwidgets and vue-native folders of this GitHub repository. They are built with webpack and can be started as following.\n1cd path-to-folder 2yarn install 3npm run dev Libraries for the Apps Common libraries User Interface Vuetify - Although Bootstrap is popular for user interface, I find most UI libraries that rely on Bootstrap also depend on JQuery. And it is possible the JQuery for htmlwidgets is incompatible with those for the UI libraries. Therefore Vuetify is used instead, which is inspired by Material Design. HTTP request axios - Promise based HTTP client for the browser and node.js For vue-native state management Vuex - Vuex is a state management pattern + library for Vue.js applications. It serves as a centralized store for all the components in an application, with rules ensuring that the state can only be mutated in a predictable fashion. data table Vuetify - The built-in data table component of Vuetify is used. plotly plotly - official library @statnett/vue-plotly - plotly as a vue component ify-loader and transform-loader - for building with webpack highcharts highcharts - official library highcharts-vue - highcharts as vue components Render Htmlwidgets index.html The entry point of the app is index.html and htmlwidgets dependencies need to be included in head followed by material fonts and icons. 3 components are created in src/components - DataTable.vue, Highchart.vue and Plotly.vue. These components are bundled into build.js and sourced by the app.\n1\u0026lt;!DOCTYPE html\u0026gt; 2\u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; 3 \u0026lt;head\u0026gt; 4 \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt; 5 \u0026lt;title\u0026gt;Vue - htmlwidgets\u0026lt;/title\u0026gt; 6 \u0026lt;meta http-equiv=\u0026#34;X-UA-Compatible\u0026#34; content=\u0026#34;IE=edge\u0026#34;\u0026gt; 7 \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width, initial-scale=1.0\u0026#34;\u0026gt; 8 \u0026lt;!-- necessary to control htmlwidgets --\u0026gt; 9 \u0026lt;script src=\u0026#34;/public/htmlwidgets/htmlwidgets.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; 10 \u0026lt;!-- need a shared JQuery lib --\u0026gt; 11 \u0026lt;script src=\u0026#34;/public/shared/jquery.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; 12 \u0026lt;!-- DT --\u0026gt; 13 \u0026lt;script src=\u0026#34;/public/datatables/datatables.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; 14 ... more DT dependencies 15 \u0026lt;!-- highchater --\u0026gt; 16 \u0026lt;script src=\u0026#34;/public/highcharter/lib/proj4js/proj4.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; 17 ... more highcharts dependencies 18 \u0026lt;script src=\u0026#34;/public/highcharter/highchart.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; 19 \u0026lt;!-- plotly --\u0026gt; 20 \u0026lt;script src=\u0026#34;/public/plotly/plotly.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; 21 ... more plotly dependencies 22 \u0026lt;!-- crosstalk --\u0026gt; 23 \u0026lt;script src=\u0026#34;/public/crosstalk/js/crosstalk.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; 24 ... more crosstalk depencencies 25 \u0026lt;!-- bootstrap, etc --\u0026gt; 26 \u0026lt;script src=\u0026#34;/public/shared/bootstrap/js/bootstrap.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; 27 ... more bootstrap, etc depencencies 28 \u0026lt;!-- vuetify --\u0026gt; 29 \u0026lt;link href=\u0026#39;https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Material+Icons\u0026#39; rel=\u0026#34;stylesheet\u0026#34; type=\u0026#34;text/css\u0026#34;\u0026gt; 30 \u0026lt;/head\u0026gt; 31 \u0026lt;body\u0026gt; 32 \u0026lt;div id=\u0026#34;app\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; 33 \u0026lt;script src=\u0026#34;./dist/build.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; 34 \u0026lt;/body\u0026gt; 35\u0026lt;/html\u0026gt; Components The widgets are constructed as single file components. The button (update table) listens on a button click event and it\u0026rsquo;ll trigger update() defined in the script of the component. Note v-html directive. This directive allows to render raw HTML.\nRecall that the body of a htmlwidgets object is\ndiv - widget container and element script - application/json for widget data script - application/htmlwidget-sizing for widget size And /widget resource of the API renders all of those if html is specified as type.\nAll the HTML elements is updated in this Vue application (type = html) while only the application/json script is appended/updated in the Javascript application (type = src).\n1\u0026lt;template\u0026gt; 2 \u0026lt;v-layout\u0026gt; 3 \u0026lt;v-flex xs12 sm6 offset-sm2\u0026gt; 4 \u0026lt;v-card\u0026gt; 5 \u0026lt;v-card-media height=\u0026#34;350px\u0026#34;\u0026gt; 6 \u0026lt;v-container\u0026gt; 7 \u0026lt;v-layout row wrap justify-center\u0026gt; 8 \u0026lt;div v-if=\u0026#34;!isLoading\u0026#34; v-html=\u0026#34;dat\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; 9 \u0026lt;/v-layout\u0026gt; 10 \u0026lt;/v-container\u0026gt; 11 \u0026lt;/v-card-media\u0026gt; 12 \u0026lt;v-card-actions\u0026gt; 13 \u0026lt;v-container\u0026gt; 14 \u0026lt;v-layout row wrap justify-center\u0026gt; 15 \u0026lt;v-btn 16 @click=\u0026#34;update\u0026#34; 17 color=\u0026#34;primary\u0026#34; 18 \u0026gt;update table\u0026lt;/v-btn\u0026gt; 19 \u0026lt;div v-if=\u0026#34;isLoading\u0026#34;\u0026gt; 20 \u0026lt;v-progress-circular indeterminate color=\u0026#34;info\u0026#34;\u0026gt;\u0026lt;/v-progress-circular\u0026gt; 21 \u0026lt;/div\u0026gt; 22 \u0026lt;/v-layout\u0026gt; 23 \u0026lt;/v-container\u0026gt; 24 \u0026lt;/v-card-actions\u0026gt; 25 \u0026lt;/v-card\u0026gt; 26 \u0026lt;/v-flex\u0026gt; 27 \u0026lt;/v-layout\u0026gt; 28\u0026lt;/template\u0026gt; Here the HTTP request is made with axios. The element is set to null at the beginning and updated by the output of the API followed by executing window.HTMLWidgets.staticRender().\n1\u0026lt;script\u0026gt; 2import axios from \u0026#39;axios\u0026#39; 3 4export default { 5 data: () =\u0026gt; ({ 6 dat: null, 7 isLoading: false 8 }), 9 methods: { 10 update() { 11 this.dat = null 12 this.isLoading = true 13 14 let params = { element_id: \u0026#39;dt_out\u0026#39;, type: \u0026#39;html\u0026#39; } 15 axios.post(\u0026#39;http://[hostname]:8000/widget\u0026#39;, params) 16 .then(res =\u0026gt; { 17 this.dat = res.data.replace(\u0026#39;width:960px;\u0026#39;, \u0026#39;width:100%\u0026#39;) 18 console.log(this.dat) 19 setTimeout(function() { 20 window.HTMLWidgets.staticRender() 21 }, 500) 22 this.isLoading = false 23 }) 24 .catch(err =\u0026gt; { 25 this.isLoading = false 26 console.log(err) 27 }) 28 } 29 } 30} 31\u0026lt;/script\u0026gt; Layout The application layout is setup in ./src/App.vue where the individual components are imported into content.\n1\u0026lt;template\u0026gt; 2 \u0026lt;v-app\u0026gt; 3 \u0026lt;v-toolbar dense color=\u0026#34;light-blue\u0026#34; dark fixed app\u0026gt; 4 \u0026lt;v-toolbar-title\u0026gt; 5 Vue - htmlwidgets 6 \u0026lt;/v-toolbar-title\u0026gt; 7 \u0026lt;/v-toolbar\u0026gt; 8 \u0026lt;v-content\u0026gt; 9 \u0026lt;app-data-table\u0026gt;\u0026lt;/app-data-table\u0026gt; 10 \u0026lt;app-high-chart\u0026gt;\u0026lt;/app-high-chart\u0026gt; 11 \u0026lt;app-plotly\u0026gt;\u0026lt;/app-plotly\u0026gt; 12 \u0026lt;/v-content\u0026gt; 13 \u0026lt;/v-app\u0026gt; 14\u0026lt;/template\u0026gt; 15 16\u0026lt;script\u0026gt; 17import DataTable from \u0026#39;./components/DataTable.vue\u0026#39; 18import HighChart from \u0026#39;./components/HighChart.vue\u0026#39; 19import Plotly from \u0026#39;./components/Plotly.vue\u0026#39; 20 21export default { 22 components: { 23 appDataTable: DataTable, 24 appHighChart: HighChart, 25 appPlotly: Plotly 26 } 27} 28\u0026lt;/script\u0026gt; The screen shot of the app is shown below.\nNative Libraries instead of Htmlwidgets If an app doesn\u0026rsquo;t rely on htmlwidgets, it only requires data to create charts and tables. The API has /hdata resource to return the iris data. Here the scenario is the iris data will be pulled at the beginning and 10 records are selected randomly when a user clicks a button, resulting in updating components. Note one of the key benefits of this structure is that components can communicate with each other - see what crosstalk is aimed for.\nindex.html The entry point of the app becomes quite simple without htmlwidgets dependency.\n1\u0026lt;!DOCTYPE html\u0026gt; 2\u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; 3 \u0026lt;head\u0026gt; 4 \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt; 5 \u0026lt;title\u0026gt;Vue - native\u0026lt;/title\u0026gt; 6 \u0026lt;meta http-equiv=\u0026#34;X-UA-Compatible\u0026#34; content=\u0026#34;IE=edge\u0026#34;\u0026gt; 7 \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width, initial-scale=1.0\u0026#34;\u0026gt; 8 \u0026lt;link href=\u0026#39;https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Material+Icons\u0026#39; rel=\u0026#34;stylesheet\u0026#34; type=\u0026#34;text/css\u0026#34;\u0026gt; 9 \u0026lt;/head\u0026gt; 10 \u0026lt;body\u0026gt; 11 \u0026lt;div id=\u0026#34;app\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; 12 \u0026lt;script src=\u0026#34;./dist/build.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; 13 \u0026lt;/body\u0026gt; 14\u0026lt;/html\u0026gt; State Management Normally a Vue app has multiple components so that it is important to keep changes in sync across components. Although Vue supports custom events and event bus, I find state management with Vuex is more straightforward (and better for larger apps).\nIn the store (./src/store.js), there are 3 state properties.\nrawData - iris data vizData - randomly selected records isLoading - indicator if data request is completed These state properties can be accessed by getters and modified by mutations. While mutations are synchronous, actions can be asynchronous. Therefore dispatching actions is better for something that requires some time. In this example, the HTTP request that returns the iris data is performed by dispatching getRawData() and, on success, the following mutations are commtted.\ngetRawData updateVisData toggleIsLoading 1import Vue from \u0026#39;vue\u0026#39; 2import Vuex from \u0026#39;vuex\u0026#39; 3 4import axios from \u0026#39;axios\u0026#39; 5axios.defaults.baseURL = \u0026#39;http://[hostname]:8000\u0026#39; 6 7Vue.use(Vuex) 8 9export default new Vuex.Store({ 10 state: { 11 rawData: [], 12 visData: [], 13 isLoading: false 14 }, 15 getters: { 16 rawData (state) { 17 return state.rawData 18 }, 19 visData (state) { 20 return state.visData 21 }, 22 isLoading (state) { 23 return state.isLoading 24 } 25 }, 26 mutations: { 27 getRawData (state, payload) { 28 state.rawData = payload 29 }, 30 updateVisData (state) { 31 state.visData = state.rawData.sort(() =\u0026gt; .5 - Math.random()).slice(0, 10) 32 }, 33 toggleIsLoading (state) { 34 state.isLoading = !state.isLoading 35 } 36 }, 37 actions: { 38 getRawData ({ commit }) { 39 commit(\u0026#39;toggleIsLoading\u0026#39;) 40 41 axios.post(\u0026#39;/hdata\u0026#39;) 42 .then(res =\u0026gt; { 43 commit(\u0026#39;getRawData\u0026#39;, res.data) 44 commit(\u0026#39;updateVisData\u0026#39;) 45 commit(\u0026#39;toggleIsLoading\u0026#39;) 46 }) 47 .catch(err =\u0026gt; { 48 console.log(\u0026#39;error\u0026#39;) 49 commit(\u0026#39;toggleIsLoading\u0026#39;) 50 console.log(err) 51 }) 52 } 53 } 54}) Components Here the source looks quite different from the DT object because it\u0026rsquo;s created by the built-in data table component of Vuetify - the other 2 compoents look rather similar. The headers of the table is predefined as data property while the records (visData) are obtained from the store - it keeps in sync as a computed property.\n1\u0026lt;template\u0026gt; 2 \u0026lt;v-data-table 3 :headers=\u0026#34;headers\u0026#34; 4 :items=\u0026#34;visData\u0026#34; 5 class=\u0026#34;elevation-1\u0026#34; 6 \u0026gt; 7 \u0026lt;template slot=\u0026#34;items\u0026#34; slot-scope=\u0026#34;props\u0026#34;\u0026gt; 8 \u0026lt;td class=\u0026#34;text-xs-right\u0026#34;\u0026gt;{{ props.item.SepalLength }}\u0026lt;/td\u0026gt; 9 \u0026lt;td class=\u0026#34;text-xs-right\u0026#34;\u0026gt;{{ props.item.SepalWidth }}\u0026lt;/td\u0026gt; 10 \u0026lt;td class=\u0026#34;text-xs-right\u0026#34;\u0026gt;{{ props.item.PetalLength }}\u0026lt;/td\u0026gt; 11 \u0026lt;td class=\u0026#34;text-xs-right\u0026#34;\u0026gt;{{ props.item.PetalWidth }}\u0026lt;/td\u0026gt; 12 \u0026lt;td class=\u0026#34;text-xs-right\u0026#34;\u0026gt;{{ props.item.Species }}\u0026lt;/td\u0026gt; 13 \u0026lt;/template\u0026gt; 14 \u0026lt;template slot=\u0026#34;pageText\u0026#34; slot-scope=\u0026#34;props\u0026#34;\u0026gt; 15 Lignes {{ props.pageStart }} - {{ props.pageStop }} of {{ props.itemsLength }} 16 \u0026lt;/template\u0026gt; 17 \u0026lt;/v-data-table\u0026gt; 18\u0026lt;/template\u0026gt; 19 20\u0026lt;script\u0026gt; 21export default { 22 data () { 23 return { 24 headers: [ 25 { text: \u0026#39;Sepal Length\u0026#39;, value: \u0026#39;SepalLength\u0026#39;}, 26 { text: \u0026#39;Sepal Width\u0026#39;, value: \u0026#39;SepalWidth\u0026#39;}, 27 { text: \u0026#39;Petal Length\u0026#39;, value: \u0026#39;PetalLength\u0026#39;}, 28 { text: \u0026#39;Petal Width\u0026#39;, value: \u0026#39;PetalWidth\u0026#39;}, 29 { text: \u0026#39;Species\u0026#39;, value: \u0026#39;Species\u0026#39;} 30 ] 31 } 32 }, 33 computed: { 34 visData() { 35 return this.$store.getters[\u0026#39;visData\u0026#39;] 36 } 37 } 38} 39\u0026lt;/script\u0026gt; Layout Instead of requesting individual htmlwidgets objects, charts/table are created by individual components. Also the components are updated by clicking the button. The conditional directives (v-if and v-else) controls which to render depending on the value of isLoading.\n1\u0026lt;template\u0026gt; 2 \u0026lt;v-app\u0026gt; 3 \u0026lt;v-toolbar dense color=\u0026#34;light-blue\u0026#34; dark fixed app\u0026gt; 4 \u0026lt;v-toolbar-title\u0026gt; 5 Vue - native 6 \u0026lt;/v-toolbar-title\u0026gt; 7 \u0026lt;/v-toolbar\u0026gt; 8 \u0026lt;v-content\u0026gt; 9 \u0026lt;div v-if=\u0026#34;isLoading\u0026#34; class=\u0026#34;centered\u0026#34;\u0026gt; 10 \u0026lt;v-progress-circular 11 indeterminate color=\u0026#34;info\u0026#34; 12 :size=\u0026#34;100\u0026#34; 13 :width=\u0026#34;10\u0026#34; 14 \u0026gt;\u0026lt;/v-progress-circular\u0026gt; 15 \u0026lt;/div\u0026gt; 16 \u0026lt;div v-else\u0026gt; 17 \u0026lt;v-btn @click=\u0026#34;update\u0026#34;\u0026gt;update data\u0026lt;/v-btn\u0026gt; 18 \u0026lt;v-container fluid\u0026gt; 19 \u0026lt;v-layout row wrap\u0026gt; 20 \u0026lt;v-flex xs12 sm12 md6\u0026gt; 21 \u0026lt;div style=\u0026#34;display: inline-block;\u0026#34;\u0026gt; 22 \u0026lt;app-data-table\u0026gt;\u0026lt;/app-data-table\u0026gt; 23 \u0026lt;/div\u0026gt; 24 \u0026lt;/v-flex\u0026gt; 25 \u0026lt;v-flex xs12 sm12 md6\u0026gt; 26 \u0026lt;div style=\u0026#34;display: inline-block;\u0026#34;\u0026gt; 27 \u0026lt;app-highchart\u0026gt;\u0026lt;/app-highchart\u0026gt; 28 \u0026lt;/div\u0026gt; 29 \u0026lt;/v-flex\u0026gt; 30 \u0026lt;v-flex xs12 sm12 md6\u0026gt; 31 \u0026lt;div style=\u0026#34;display: inline-block;\u0026#34;\u0026gt; 32 \u0026lt;app-plotly\u0026gt;\u0026lt;/app-plotly\u0026gt; 33 \u0026lt;/div\u0026gt; 34 \u0026lt;/v-flex\u0026gt; 35 \u0026lt;/v-layout\u0026gt; 36 \u0026lt;/v-container\u0026gt; 37 \u0026lt;/div\u0026gt; 38 \u0026lt;/v-content\u0026gt; 39 \u0026lt;/v-app\u0026gt; 40\u0026lt;/template\u0026gt; Upon creation of the component (created()), getRawData() is dispatched. While the request is being processed, the computed property of isLoading remains as true, resulting in rendering the loader. Once succeeded, the compoents are updated with the initial random records. If a user click the button, it\u0026rsquo;ll commit updateVisData(), resulting in compoent updates.\n1\u0026lt;script\u0026gt; 2import DataTable from \u0026#39;./components/DataTable.vue\u0026#39; 3import Highchart from \u0026#39;./components/HighChart.vue\u0026#39; 4import Plotly from \u0026#39;./components/Plotly.vue\u0026#39; 5 6export default { 7 components: { 8 appDataTable: DataTable, 9 appHighchart: Highchart, 10 appPlotly: Plotly 11 }, 12 computed: { 13 visData() { 14 return this.$store.getters[\u0026#39;visData\u0026#39;] 15 }, 16 isLoading() { 17 return this.$store.getters[\u0026#39;isLoading\u0026#39;] 18 } 19 }, 20 methods: { 21 update() { 22 this.$store.commit(\u0026#39;updateVisData\u0026#39;) 23 } 24 }, 25 created () { 26 this.$store.dispatch(\u0026#39;getRawData\u0026#39;) 27 } 28} 29\u0026lt;/script\u0026gt; 30 31\u0026lt;style scoped\u0026gt; 32.centered { 33 position: fixed; /* or absolute */ 34 top: 50%; 35 left: 50%; 36} 37\u0026lt;/style\u0026gt; The screen shot of the app is shown below.\n","date":"May 26, 2018","img":"/blog/2018-05-26-shiny-to-vue.js/featured.png","lang":"en","langName":"English","largeImg":"/blog/2018-05-26-shiny-to-vue.js/featured_hu0f69ef413c1c0e126895b33362437da9_247205_500x0_resize_box_3.png","permalink":"/blog/2018-05-26-shiny-to-vue.js/","series":[],"smallImg":"/blog/2018-05-26-shiny-to-vue.js/featured_hu0f69ef413c1c0e126895b33362437da9_247205_180x0_resize_box_3.png","tags":[{"title":"R","url":"/tags/r/"},{"title":"Shiny","url":"/tags/shiny/"},{"title":"JavaScript","url":"/tags/javascript/"},{"title":"Vue.js","url":"/tags/vue.js/"}],"timestamp":1527292800,"title":"Shiny to Vue.js"},{"categories":[{"title":"Development","url":"/categories/development/"}],"content":"A Shiny app is served by one (single-threaded blocking) process by Open Source Shiny Server. This causes a scalability issue because all requests are handled one by one in a queue. Recently the creator of Shiny introduced the promises package, which brings asynchronous programming capabilities to R. This is a remarkable step forward to web development in R.\nIn this post, it\u0026rsquo;ll be demonstrated how to implement the async feature of Shiny. Then its limitation will be discussed with an alternative app, which is built by JavaScript for the frontend and RServe for the backend.\nAsync Shiny and Its Limitation Brief Intro to Promises A basic idea of how the promises package works is that a (long running) process is passed to a forked process while it immediately returns a promise object. Then the result can be obtained once it\u0026rsquo;s finished (or failed) by handlers eg) onFulfilled and onRejected. The package also provides pipe operators (eg %...\u0026gt;%) for ease of use. Typically a promise object can be created with the future package. See this page for further details.\nShiny App A simple Shiny app is created for demonstration that renders 3 htmlwidgets: DT, highcharter and plotly. For this, the following async-compatible packages are necessary.\nShiny v1.1+ DT from rstudio/DT@async htmlwidgets from ramnathv/htmlwidgets@async plotly from jcheng5/plotly@joe/feature/async highcharter - supported by htmlwidgets At startup, it is necessary to allocate the number of workers (forked processes). Note it doesn\u0026rsquo;t necessarily be the same to the number of cores. Rather it may be better to set it higher if the machine has enough resource. This is because, if there are n workers and n+m requests, the m requests tend to be queued.\n1library(magrittr) 2library(DT) 3library(highcharter) 4library(plotly) 5 6library(shiny) 7 8library(promises) 9library(future) 10plan(multiprocess, workers = 100) Then a simple NavBar page is created as the UI. A widget will be rendered by clicking a button.\n1tab \u0026lt;- tabPanel( 2 title = \u0026#39;Demo\u0026#39;, 3 fluidPage( 4 fluidRow( 5 div( 6 style=\u0026#39;height: 400px;\u0026#39;, 7 column(2, actionButton(\u0026#39;dt\u0026#39;, \u0026#39;update data table\u0026#39;)), 8 column(10, dataTableOutput(\u0026#39;dt_out\u0026#39;)) 9 ) 10 ), 11 fluidRow( 12 div( 13 style=\u0026#39;height: 400px;\u0026#39;, 14 column(2, actionButton(\u0026#39;highchart\u0026#39;, \u0026#39;update highchart\u0026#39;)), 15 column(10, highchartOutput(\u0026#39;highchart_out\u0026#39;)) 16 ) 17 ), 18 fluidRow( 19 div( 20 style=\u0026#39;height: 400px;\u0026#39;, 21 column(2, actionButton(\u0026#39;plotly\u0026#39;, \u0026#39;update plotly\u0026#39;)), 22 column(10, plotlyOutput(\u0026#39;plotly_out\u0026#39;)) 23 ) 24 25 ) 26 ) 27) 28 29ui \u0026lt;- navbarPage(\u0026#34;Async Shiny\u0026#34;, tab) A future object is created by get_iris(), which returns 10 records randomly from the iris data after 2 seconds. evantReactive()s generate htmlwidget objects and they are passed to the relevant render functions.\n1server \u0026lt;- function(input, output, session) { 2 3 get_iris \u0026lt;- function() { 4 future({ Sys.sleep(2); iris[sample(1:nrow(iris), 10),] }) 5 } 6 7 dt_df \u0026lt;- eventReactive(input$dt, { 8 get_iris() %...\u0026gt;% 9 datatable(options = list( 10 pageLength = 5, 11 lengthMenu = c(5, 10) 12 )) 13 }) 14 15 highchart_df \u0026lt;- eventReactive(input$highchart, { 16 get_iris() 17 }) 18 19 plotly_df \u0026lt;- eventReactive(input$plotly, { 20 get_iris() 21 }) 22 23 output$dt_out \u0026lt;- renderDataTable(dt_df()) 24 25 output$highchart_out \u0026lt;- renderHighchart({ 26 highchart_df() %...\u0026gt;% 27 hchart(\u0026#39;scatter\u0026#39;, hcaes(x = \u0026#39;Sepal.Length\u0026#39;, y = \u0026#39;Sepal.Width\u0026#39;, group = \u0026#39;Species\u0026#39;)) %...\u0026gt;% 28 hc_title(text = \u0026#39;Iris Scatter\u0026#39;) 29 }) 30 31 output$plotly_out \u0026lt;- renderPlotly({ 32 plotly_df() %...\u0026gt;% 33 plot_ly(x = ~Sepal.Length, y = ~Sepal.Width, z = ~Petal.Length, color = ~Species) 34 }) 35} The app code can also be found in this GitHub repository.\nFor deployment, a Docker image is created that includes the async-compatible packages, Open Source Shiny Server and RServe. It is available as rockerextra/shiny-async-dev:3.4 and its Dockerfile can be found in this repo. The app can be deployed with Docker Compose as can be seen here.\nA screen shot of the Shiny app is seen below. It is possible to check the async feature by opening the app in 2 different browsers and hit buttons multiple times across.\nLimitation You may notice the htmlwidgets are rendered without delay across browsers but it\u0026rsquo;s not the case in the same browser. This is due to the way how Shiny\u0026rsquo;s flush cycle is implemented. Simply put, a user (or session) is not affected by other users (or sessions) for their async requests. However the async feature of Shiny is of little help for multiple async requests by a single user because all requests are processed one by one as its sync version.\nThis limitation can have a significant impact on developing a web application. In general, almost all events/actions are handled through the server in a Shiny app. However the async feature of the server is not the full extent that a typical JavaScript app can bring.\nAlternative Implementation In order to compare the async Shiny app to a typical web app, an app is created with JavaScript for the frontend and RServe for the backend. In the UI, JQuery will be used for AJAX requests by clicking buttons. Then the same htmlwidget elements will be rendered to the app. With this setup, it\u0026rsquo;s possible to make multiple requests concurrently in a session and they are all handled asynchronously by a JavaScript-backed app.\nRServe Backend So as to render htmlwidgets to UI, it is necessary to have a backend API. As discussed in API Development with R series (Part I, Part II), RServe can be a performant option for building an API.\nI don\u0026rsquo;t plan to use native JavaScript libraries for creating individual widgets. Rather I\u0026rsquo;m going to render widgets that are created by R. Therefore it is necessary to understand the structure of a widget. saveWidget() of the htmlwidgets package helps save a widget into a HTML file and it executes save_html() of the htmltools package.\nKey parts of a widget is\nhead - dependent JavaScript and CSS body div - widget container and element script - application/json for widget data script - application/htmlwidget-sizing for widget size For example,\n1\u0026lt;!DOCTYPE html\u0026gt; 2\u0026lt;html\u0026gt; 3 \u0026lt;head\u0026gt; 4 \u0026lt;meta charset=\u0026#39;utf-8\u0026#39;/\u0026gt; 5 \u0026lt;script src=\u0026#34;src-to/htmlwidgets.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; 6 \u0026lt;script src=\u0026#34;src-to/jquery.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; 7 \u0026lt;script src=\u0026#34;src-to/datatables.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; 8 ... more DT dependencies 9 \u0026lt;/head\u0026gt; 10 \u0026lt;body\u0026gt; 11 \u0026lt;div id=\u0026#34;htmlwidget_container\u0026#34;\u0026gt; 12 \u0026lt;div id=\u0026#34;htmlwidget\u0026#34; 13 style=\u0026#34;width:960px;height:500px;\u0026#34; 14 class=\u0026#34;datatables html-widget\u0026#34;\u0026gt; 15 \u0026lt;/div\u0026gt; 16 \u0026lt;/div\u0026gt; 17 \u0026lt;script type=\u0026#34;application/json\u0026#34; 18 data-for=\u0026#34;htmlwidget\u0026#34;\u0026gt;JSON DATA\u0026lt;/script\u0026gt; 19 \u0026lt;script type=\u0026#34;application/htmlwidget-sizing\u0026#34; 20 data-for=\u0026#34;htmlwidget\u0026#34;\u0026gt;SIGING INFO\u0026lt;/script\u0026gt; 21 \u0026lt;/body\u0026gt; 22\u0026lt;/html\u0026gt; write_widget() is a slight modification of saveWidget() and save_html(). Given the following arguments, it returns the necessary widget string (HTML or JSON) and it can be passed to the UI. In this post, src type will be used exclusively.\nw htmlwidget object see widget() illustrated below element_id DOM element id (eg dt_out) type json - JSON DATA only src - application/json script html - body elements all - entire html page 1library(htmlwidgets) 2 3write_widget \u0026lt;- function(w, element_id, type = NULL, 4 cdn = NULL, output_path = NULL) { 5 w$elementId \u0026lt;- sprintf(\u0026#39;htmlwidget_%s\u0026#39;, element_id) 6 toHTML \u0026lt;- utils::getFromNamespace(x = \u0026#39;toHTML\u0026#39;, ns = \u0026#39;htmlwidgets\u0026#39;) 7 html \u0026lt;- toHTML(w, standalone = TRUE, knitrOptions = list()) 8 9 type \u0026lt;- match.arg(type, c(\u0026#39;src\u0026#39;, \u0026#39;json\u0026#39;, \u0026#39;html\u0026#39;, \u0026#39;all\u0026#39;)) 10 if (type == \u0026#39;src\u0026#39;) { 11 out \u0026lt;- html[[2]] 12 } else if (type == \u0026#39;json\u0026#39;) { 13 bptn \u0026lt;- paste0(\u0026#39;\u0026lt;script type=\u0026#34;application/json\u0026#34; data-for=\u0026#34;htmlwidget_\u0026#39;, 14 element_id, \u0026#39;\u0026#34;\u0026gt;\u0026#39;) 15 eptn \u0026lt;- \u0026#39;\u0026lt;/script\u0026gt;\u0026#39; 16 out \u0026lt;- sub(eptn, \u0026#39;\u0026#39;, sub(bptn, \u0026#39;\u0026#39;, html[[2]])) 17 } else { 18 html_tags \u0026lt;- htmltools::renderTags(html) 19 html_tags$html \u0026lt;- sub(\u0026#39;htmlwidget_container\u0026#39;, 20 sprintf(\u0026#39;htmlwidget_container_%s\u0026#39;, element_id) , 21 html_tags$html) 22 if (type == \u0026#39;html\u0026#39;) { 23 out \u0026lt;- html_tags$html 24 } else { # all 25 libdir \u0026lt;- gsub(\u0026#39;\\\\\\\\\u0026#39;, \u0026#39;/\u0026#39;, tempdir()) 26 libdir \u0026lt;- gsub(\u0026#39;[[:space:]]|[A-Z]:\u0026#39;, \u0026#39;\u0026#39;, libdir) 27 28 deps \u0026lt;- lapply(html_tags$dependencies, update_dep_path, libdir = libdir) 29 deps \u0026lt;- htmltools::renderDependencies(dependencies = deps, 30 srcType = c(\u0026#39;hred\u0026#39;, \u0026#39;file\u0026#39;)) 31 deps \u0026lt;- ifelse(!is.null(cdn), gsub(libdir, cdn, deps), deps) 32 33 out \u0026lt;- c( 34 \u0026#34;\u0026lt;!DOCTYPE html\u0026gt;\u0026#34;, 35 \u0026#34;\u0026lt;html\u0026gt;\u0026#34;, 36 \u0026#34;\u0026lt;head\u0026gt;\u0026#34;, 37 \u0026#34;\u0026lt;meta charset=\u0026#39;utf-8\u0026#39;/\u0026gt;\u0026#34;, 38 deps, 39 html_tags$head, 40 \u0026#34;\u0026lt;/head\u0026gt;\u0026#34;, 41 \u0026#34;\u0026lt;body\u0026gt;\u0026#34;, 42 html_tags$html, 43 \u0026#34;\u0026lt;/body\u0026gt;\u0026#34;, 44 \u0026#34;\u0026lt;/html\u0026gt;\u0026#34;) 45 } 46 } 47 48 if (!is.null(output_path)) { 49 writeLines(out, output_path, useBytes = TRUE) 50 } else { 51 paste(out, collapse = \u0026#39;\u0026#39;) 52 } 53} 54 55update_dep_path \u0026lt;- function(dep, libdir = \u0026#39;lib\u0026#39;) { 56 dir \u0026lt;- dep$src$file 57 if (!is.null(dep$package)) 58 dir \u0026lt;- system.file(dir, package = dep$package) 59 60 if (length(libdir) != 1 || libdir %in% c(\u0026#34;\u0026#34;, \u0026#34;/\u0026#34;)) 61 stop(\u0026#34;libdir must be of length 1 and cannot be \u0026#39;\u0026#39; or \u0026#39;/\u0026#39;\u0026#34;) 62 63 target \u0026lt;- if (getOption(\u0026#39;htmltools.dir.version\u0026#39;, TRUE)) { 64 paste(dep$name, dep$version, sep = \u0026#39;-\u0026#39;) 65 } else { 66 dep$name 67 } 68 dep$src$file \u0026lt;- file.path(libdir, target) 69 dep 70} Essentially the API has 2 endpoints.\nwidget returns output from write_widget() given element_id and type hdata returns iris data as JSON 1widget \u0026lt;- function(element_id, type, get_all = FALSE, cdn = \u0026#39;public\u0026#39;, ...) { 2 dat \u0026lt;- get_iris(get_all) 3 if (grepl(\u0026#39;dt\u0026#39;, element_id)) { 4 w \u0026lt;- dat %\u0026gt;% 5 datatable(options = list( 6 pageLength = 5, 7 lengthMenu = c(5, 10) 8 )) 9 } else if (grepl(\u0026#39;highchart\u0026#39;, element_id)) { 10 w \u0026lt;- dat %\u0026gt;% 11 hchart(\u0026#39;scatter\u0026#39;, hcaes(x = \u0026#39;Sepal.Length\u0026#39;, y = \u0026#39;Sepal.Width\u0026#39;, group = \u0026#39;Species\u0026#39;)) %\u0026gt;% 12 hc_title(text = \u0026#39;Iris Scatter\u0026#39;) 13 } else if (grepl(\u0026#39;plotly\u0026#39;, element_id)) { 14 w \u0026lt;- dat %\u0026gt;% 15 plot_ly(x = ~Sepal.Length, y = ~Sepal.Width, z = ~Petal.Length, color = ~Species) 16 } else { 17 stop(\u0026#39;Unexpected element\u0026#39;) 18 } 19 write_widget(w, element_id, type, cdn) 20} 21 22hdata \u0026lt;- function() { 23 dat \u0026lt;- get_iris(TRUE) 24 names(dat) \u0026lt;- sub(\u0026#39;\\\\.\u0026#39;, \u0026#39;\u0026#39;, names(dat)) 25 dat %\u0026gt;% toJSON() 26 27} 28 29get_iris \u0026lt;- function(get_all = FALSE) { 30 Sys.sleep(2) 31 if (!get_all) { 32 iris[sample(1:nrow(iris), 10),] 33 } else { 34 iris 35 } 36} process_request() remains largely the same but needs some modification so that it can be used as a backend of a web app.\nCross-Origin Resource Sharing (CORS) requests from browser will fail without necessary headers and handling OPTIONS method Response content type depending on type, response content type will be either application/json or text/html See API Development with R series (Part I, Part II) for further details of process_request() and how RServe\u0026rsquo;s built-in HTTP server works.\n1process_request \u0026lt;- function(url, query, body, headers) { 2 #### building request object 3 request \u0026lt;- list(uri = url, method = \u0026#39;POST\u0026#39;, query = query, body = body) 4 5 ## parse headers 6 request$headers \u0026lt;- parse_headers(headers) 7 if (\u0026#34;request-method\u0026#34; %in% names(request$headers)) 8 request$method \u0026lt;- c(request$headers[\u0026#34;request-method\u0026#34;]) 9 10 set_headers \u0026lt;- function(...) { 11 paste(list(...), collapse = \u0026#39;\\r\\n\u0026#39;) 12 } 13 14 h1 \u0026lt;- \u0026#39;Access-Control-Allow-Headers: Content-Type\u0026#39; 15 h2 \u0026lt;- \u0026#39;Access-Control-Allow-Methods: POST,GET,OPTIONS\u0026#39; 16 h3 \u0026lt;- \u0026#39;Access-Control-Allow-Origin: *\u0026#39; 17 18 cors_headers \u0026lt;- set_headers(h1, h2, h3) 19 20 if (request$method == \u0026#39;OPTIONS\u0026#39;) { 21 return (list(\u0026#39;\u0026#39;, \u0026#39;text/plain\u0026#39;, cors_headers)) 22 } 23 24 request$pars \u0026lt;- list() 25 if (request$method == \u0026#39;POST\u0026#39;) { 26 if (!is.null(body)) { 27 if (is.raw(body)) 28 body \u0026lt;- rawToChar(body) 29 if (any(grepl(\u0026#39;application/json\u0026#39;, request$headers))) 30 body \u0026lt;- jsonlite::fromJSON(body) 31 request$pars \u0026lt;- as.list(body) 32 } 33 } else { 34 if (!is.null(query)) { 35 request$pars \u0026lt;- as.list(query) 36 } 37 } 38 39 if (\u0026#39;type\u0026#39; %in% names(request$pars)) { 40 if (request$pars$type == \u0026#39;json\u0026#39;) { 41 content_type \u0026lt;- \u0026#39;application/json; charset=utf-8\u0026#39; 42 } else { 43 content_type \u0026lt;- \u0026#39;text/html; charset=utf-8\u0026#39; 44 } 45 } else { 46 content_type \u0026lt;- \u0026#39;text/plain; charset=utf-8\u0026#39; 47 } 48 49 message(sprintf(\u0026#39;Header:\\n%s\u0026#39;, cors_headers)) 50 message(sprintf(\u0026#39;Content Type: %s\u0026#39;, content_type)) 51 message(\u0026#39;Params:\u0026#39;) 52 print(do.call(c, request$pars)) 53 54 #### building output object 55 matched_fun \u0026lt;- gsub(\u0026#39;^/\u0026#39;, \u0026#39;\u0026#39;, request$uri) 56 57 payload \u0026lt;- tryCatch({ 58 do.call(matched_fun, request$pars) 59 }, error = function(err) { 60 \u0026#39;Internal Server Error\u0026#39; 61 }) 62 63 return (list(payload, content_type, cors_headers)) 64} The source can be found in here and the API deployment is included in the docker compose.\nJavaScript Frontend The app will be kept in index.html and will be served by a simple python web server. Basically the same Bootstrap page is created.\nIt is important to keep all the widgets\u0026rsquo; dependent JavaScript and CSS in head. We have 3 htmlwidgets and they are wrapped by the htmlwidgets package. Therefore it depends on\nhtmlwidgets DataTables for DT Highcharts for highcharter Plotly for plotly CrossTalk for DT and plotly Bootstrap for layout JQuery for all Note that a htmlwidget package tends to rely on a specific JQuery library. For example, the DT package uses 1.12.4 while the highcharter uses 1.11.1. Therefore there is a chance to encounter version incompatibility if multiple htmlwidget packages rendered at the same time. The HTML source of Shiny can be helpful because it holds a JQuery lib that can be shared across all widget packages.\n1\u0026lt;head\u0026gt; 2 \u0026lt;meta charset=\u0026#39;utf-8\u0026#39;/\u0026gt; 3 \u0026lt;!-- necessary to control htmlwidgets --\u0026gt; 4 \u0026lt;script src=\u0026#34;/public/htmlwidgets/htmlwidgets.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; 5 \u0026lt;!-- need a shared JQuery lib --\u0026gt; 6 \u0026lt;script src=\u0026#34;/public/shared/jquery.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; 7 \u0026lt;!-- DT --\u0026gt; 8 \u0026lt;script src=\u0026#34;/public/datatables/datatables.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; 9 ... more DT dependencies 10 \u0026lt;!-- highchater --\u0026gt; 11 \u0026lt;script src=\u0026#34;/public/highcharter/lib/proj4js/proj4.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; 12 ... more highcharts dependencies 13 \u0026lt;script src=\u0026#34;/public/highcharter/highchart.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; 14 \u0026lt;!-- plotly --\u0026gt; 15 \u0026lt;script src=\u0026#34;/public/plotly/plotly.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; 16 ... more plotly dependencies 17 \u0026lt;!-- crosstalk --\u0026gt; 18 \u0026lt;script src=\u0026#34;/public/crosstalk/js/crosstalk.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; 19 ... more crosstalk depencencies 20 \u0026lt;!-- bootstrap, etc --\u0026gt; 21 \u0026lt;script src=\u0026#34;/public/shared/bootstrap/js/bootstrap.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; 22 ... more bootstrap, etc depencencies 23\u0026lt;/head\u0026gt; The widget containers/elements as well as sizing script are added in body. The naming rules for the container and element are\ncontainer - htmlwidget_container_[element_id] element - htmlwidget_[element_id] In this structure, widgets can be updated if their data (application/json script) is added/updated to the page.\n1\u0026lt;body\u0026gt; 2 ... NAV 3 \u0026lt;div class=\u0026#34;container-fluid\u0026#34;\u0026gt; 4 ... TAB 5 \u0026lt;div class=\u0026#34;container-fluid\u0026#34;\u0026gt; 6 \u0026lt;div class=\u0026#34;row\u0026#34;\u0026gt; 7 \u0026lt;div style=\u0026#34;height: 400px;\u0026#34;\u0026gt; 8 \u0026lt;div class=\u0026#34;col-sm-2\u0026#34;\u0026gt; 9 \u0026lt;button id=\u0026#34;dt\u0026#34; type=\u0026#34;button\u0026#34; 10 class=\u0026#34;btn btn-default action-button\u0026#34;\u0026gt; 11 update data table 12 \u0026lt;/button\u0026gt; 13 \u0026lt;/div\u0026gt; 14 \u0026lt;div class=\u0026#34;col-sm-10\u0026#34;\u0026gt; 15 \u0026lt;div id=\u0026#34;htmlwidget_container_dt_out\u0026#34;\u0026gt; 16 \u0026lt;div id=\u0026#34;htmlwidget_dt_out\u0026#34; 17 style=\u0026#34;width:100%;height:100%;\u0026#34; 18 class=\u0026#34;datatables html-widget\u0026#34;\u0026gt; 19 \u0026lt;/div\u0026gt; 20 \u0026lt;/div\u0026gt; 21 \u0026lt;/div\u0026gt; 22 \u0026lt;/div\u0026gt; 23 \u0026lt;/div\u0026gt; 24 ... highcharter wrapper 25 ... plotly wrapper 26 \u0026lt;/div\u0026gt; 27 28 \u0026lt;/div\u0026gt; 29 30\u0026lt;script type=\u0026#34;application/htmlwidget-sizing\u0026#34; 31 data-for=\u0026#34;htmlwidget_dt_out\u0026#34;\u0026gt; 32 {\u0026#34;browser\u0026#34;:{\u0026#34;width\u0026#34;:\u0026#34;100%\u0026#34;,\u0026#34;height\u0026#34;:400,\u0026#34;padding\u0026#34;:40,\u0026#34;fill\u0026#34;:true}} 33\u0026lt;/script\u0026gt; 34... hicharter sizing 35... plotly sizing As mentioned earlier, AJAX requests are made by clicking buttons and it\u0026rsquo;s implemented in req(). Key steps are\nremove html-widget-static-bound class from a widget makes a call with element_id and type=src note to change hostname append or replace application/json script for plotly, purge() chart. Otherwise traces added continuously execute window.HTMLWidgets.staticRender() 1\u0026lt;script type = \u0026#34;text/javascript\u0026#34; language = \u0026#34;javascript\u0026#34;\u0026gt; 2 function req(elem, tpe) { 3 var btn_id = \u0026#34;#\u0026#34; + elem; 4 var widget_id = \u0026#34;#htmlwidget_\u0026#34; + elem + \u0026#34;_out\u0026#34;; 5 var elem_id = elem + \u0026#34;_out\u0026#34;; 6 var data_for = \u0026#34;htmlwidget_\u0026#34; + elem + \u0026#34;_out\u0026#34;; 7 var scr_selector = \u0026#39;script[type=\u0026#34;application/json\u0026#34;][data-for=\u0026#34;\u0026#39; + 8 data_for + \u0026#39;\u0026#34;]\u0026#39;; 9 $.support.cors = true; 10 $(btn_id).prop(\u0026#34;disabled\u0026#34;, true); 11 $(widget_id).removeClass(\u0026#34;html-widget-static-bound\u0026#34;); 12 $.ajax({ 13 url: \u0026#34;http://[hostname]:8000/widget\u0026#34;, 14 data: { element_id: elem_id, type: tpe }, 15 error: function(err) { 16 $(btn_id).removeAttr(\u0026#39;disabled\u0026#39;); 17 }, 18 success: function(data) { 19 //console.log(data) 20 if($(scr_selector).length == 0) { 21 $(\u0026#39;body\u0026#39;).append(data) 22 } else { 23 if (elem.includes(\u0026#39;plotly\u0026#39;)) { 24 try { 25 //Plotly.deleteTraces(htmlwidget_plotly_out, [0]) 26 Plotly.purge(data_for); 27 } 28 catch(err) { 29 console.log(err); 30 } 31 } 32 $(scr_selector).replaceWith(data); 33 } 34 setTimeout(function(){ 35 window.HTMLWidgets.staticRender(); 36 }, 500); 37 $(btn_id).removeAttr(\u0026#39;disabled\u0026#39;); 38 } 39 }); 40 } 41 42 $(document).ready(function() { 43 $(\u0026#34;#dt\u0026#34;).click(function() { 44 req(\u0026#39;dt\u0026#39;, \u0026#39;src\u0026#39;); 45 }); 46 }); 47 48 ... button clicks for highcharter and plotly 49\u0026lt;/script\u0026gt; For comparison, the async Shiny app and the JavaScript frontend/backend are included in the docker compose. The JavaScript app can be accessed in port 7000. Once started, it\u0026rsquo;s possible to see widgets are rendered without delay when buttons are clicked multiple times.\nCompared to the async Shiny app, the JavaScript app is more effective in handling multiple requests. The downside of it is the benefits that Shiny provides are no longer available. Some of them are built-in data binding, event handling and state management. For example, think about what reactive*() and observe*() do for a Shiny app. Although it is possible to setup those with plain JavaScript or JQuery, life will be a lot easier if an app is built with one of the popular JavaScript frameworks: Angular, React and Vue. In the next post, it\u0026rsquo;ll be shown how to render htmlwidgets in a Vue application as well as building those with native JavaScript libraries.\n","date":"May 19, 2018","img":"/blog/2018-05-19-asyn-shiny-and-its-limitation/featured.png","lang":"en","langName":"English","largeImg":"/blog/2018-05-19-asyn-shiny-and-its-limitation/featured_hu0f69ef413c1c0e126895b33362437da9_247205_500x0_resize_box_3.png","permalink":"/blog/2018-05-19-asyn-shiny-and-its-limitation/","series":[],"smallImg":"/blog/2018-05-19-asyn-shiny-and-its-limitation/featured_hu0f69ef413c1c0e126895b33362437da9_247205_180x0_resize_box_3.png","tags":[{"title":"R","url":"/tags/r/"},{"title":"Shiny","url":"/tags/shiny/"},{"title":"JavaScript","url":"/tags/javascript/"}],"timestamp":1526688000,"title":"Async Shiny and Its Limitation"},{"categories":[{"title":"Development","url":"/categories/development/"}],"content":"In Part I, it is discussed how to serve an R function with plumber, Rserve and rApache. In this post, the APIs are deployed in a Docker container and, after showing example requests, their performance is compared. The rocker/r-ver:3.4 is used as the base image and each of the APIs is added to it. For simplicity, the APIs are served by Supervisor. For performance testing, Locust is used. The source of this post can be found in this GitHub repository.\nDeployment As can be seen in the Dockerfile below, plumber, Rserve and rApache are installed in order. Plumber is an R package so that it can be installed by install.packages(). The latest versions of Rserve and rApache are built after installing their dependencies. Note, for rApache, the Rook package is installed as well because the test function is served as a Rook application.\nThen the source files are copied to /home/docker/\u0026lt;api-name\u0026gt;. rApache requires extra configuration. First the Rook app (rapache-app.R) and site configuration file (rapache-site.conf) are symlinked to the necessary paths and the site is enabled.\nFinaly Suervisor is started with the config file that monitors/manages the APIs.\n1FROM rocker/r-ver:3.4 2MAINTAINER Jaehyeon Kim \u0026lt;dottami@gmail.com\u0026gt; 3 4RUN apt-get update \u0026amp;\u0026amp; apt-get install -y wget supervisor 5 6## Plumber 7RUN R -e \u0026#39;install.packages(c(\u0026#34;plumber\u0026#34;, \u0026#34;jsonlite\u0026#34;))\u0026#39; 8 9## Rserve 10RUN apt-get install -y libssl-dev 11RUN wget http://www.rforge.net/Rserve/snapshot/Rserve_1.8-5.tar.gz \\ 12 \u0026amp;\u0026amp; R CMD INSTALL Rserve_1.8-5.tar.gz 13 14## rApache 15RUN apt-get install -y \\ 16 libpcre3-dev liblzma-dev libbz2-dev libzip-dev libicu-dev 17RUN apt-get install -y apache2 apache2-dev 18RUN wget https://github.com/jeffreyhorner/rapache/archive/v1.2.8.tar.gz \\ 19 \u0026amp;\u0026amp; tar xvf v1.2.8.tar.gz \\ 20 \u0026amp;\u0026amp; cd rapache-1.2.8 \u0026amp;\u0026amp; ./configure \u0026amp;\u0026amp; make \u0026amp;\u0026amp; make install 21 22RUN R -e \u0026#39;install.packages(c(\u0026#34;Rook\u0026#34;, \u0026#34;rjson\u0026#34;))\u0026#39; 23 24RUN echo \u0026#39;/usr/local/lib/R/lib/\u0026#39; \u0026gt;\u0026gt; /etc/ld.so.conf.d/libR.conf \\ 25 \u0026amp;\u0026amp; ldconfig 26 27## copy sources to /home/docker 28RUN useradd docker \u0026amp;\u0026amp; mkdir /home/docker \\ 29\t\u0026amp;\u0026amp; chown docker:docker /home/docker 30 31RUN mkdir /home/docker/plumber /home/docker/rserve /home/docker/rapache 32COPY ./src/plumber /home/docker/plumber/ 33COPY ./src/rserve /home/docker/rserve/ 34COPY ./src/rapache /home/docker/rapache/ 35COPY ./src/api-supervisor.conf /home/docker/api-supervisor.conf 36RUN chmod -R 755 /home/docker 37 38RUN ln -s /home/docker/rapache/rapache-site.conf \\ 39 /etc/apache2/sites-available/rapache-site.conf \\ 40 \u0026amp;\u0026amp; ln -s /home/docker/rapache/rapache-app.R /var/www/rapache-app.R 41 42## config rApache 43RUN echo \u0026#39;ServerName localhost\u0026#39; \u0026gt;\u0026gt; /etc/apache2/apache2.conf \\ 44 \u0026amp;\u0026amp; /bin/bash -c \u0026#34;source /etc/apache2/envvars\u0026#34; \u0026amp;\u0026amp; mkdir -p /var/run/apache2 \\ 45 \u0026amp;\u0026amp; a2ensite rapache-site 46 47CMD [\u0026#34;/usr/bin/supervisord\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;/home/docker/api-supervisor.conf\u0026#34;] Plumber As can be seen in api-supervisor.conf, the plumber API can be started at port 9000 as following. (plumber-src.R and plumber-serve.R are discussed in Part I)\n1/usr/local/bin/Rscript /home/docker/plumber/plumber-serve.R Rserve In order to utilize the built-in HTTP server of Rserve, http.port should be specified in rserve.conf. Also it is necessary to set daemon disable to manage Rserve by Supervisor.\n1http.port 8000 2remote disable 3auth disable 4daemon disable 5control disable Then it is possible to start the Rserve API at port 8000 as shown below. (rserve-src.R is discussed in Part I.)\n1/usr/local/bin/R CMD Rserve --slave --RS-conf /home/docker/rserve/rserve.conf \\ 2 --RS-source /home/docker/rserve/rserve-src.R rApache The site config file of the rApache API is shown below.\n1LoadModule R_module /usr/lib/apache2/modules/mod_R.so 2\u0026lt;Location /test\u0026gt; 3 SetHandler r-handler 4 RFileEval /var/www/rapache-app.R:Rook::Server$call(test) 5\u0026lt;/Location\u0026gt; It is possible to start the rApache API at port 80 as following. (rapache-app.R is discussed in Part I.)\n1apache2ctl -DFOREGROUND This Docker container can be built and run as following. Note the container\u0026rsquo;s port 80 is mapped to the host\u0026rsquo;s port 7000 to prevent a possible conflict.\n1## build 2docker build -t=api ./api/. 3 4## run 5# rApache - 7000, Rserve - 8000, plumber - 9000 6# all APIs managed by supervisor 7docker run -d -p 7000:80 -p 8000:8000 -p 9000:9000 --name api api:latest Example Request Example requests to the APIs and their responses are shown below. When a request includes both n and wait parameters, the APIs return 200 response as expected. Only the Rserve API properly shows 400 response and the others need some modification.\n1library(httr) 2plumber200 \u0026lt;- POST(url = \u0026#39;http://localhost:9000/test\u0026#39;, encode = \u0026#39;json\u0026#39;, 3 body = list(n = 10, wait = 0.5)) 4unlist(c(api = \u0026#39;plumber\u0026#39;, status = status_code(plumber200), 5 content = content(plumber200))) 1## api status content.value 2## \u0026#34;plumber\u0026#34; \u0026#34;200\u0026#34; \u0026#34;10\u0026#34; 1rapache200 \u0026lt;- POST(url = \u0026#39;http://localhost:7000/test\u0026#39;, encode = \u0026#39;json\u0026#39;, 2 body = list(n = 10, wait = 0.5)) 3unlist(c(api = \u0026#39;rapache\u0026#39;, status = status_code(rapache200), 4 content = content(rapache200))) 1## api status content.value 2## \u0026#34;rapache\u0026#34; \u0026#34;200\u0026#34; \u0026#34;10\u0026#34; 1rserve200 \u0026lt;- POST(url = \u0026#39;http://localhost:8000/test\u0026#39;, encode = \u0026#39;json\u0026#39;, 2 body = list(n = 10, wait = 0.5)) 3unlist(c(api = \u0026#39;rserve\u0026#39;, status = status_code(rserve200), 4 content = content(rserve200))) 1## api status content.value 2## \u0026#34;rserve\u0026#34; \u0026#34;200\u0026#34; \u0026#34;10\u0026#34; 1rserve400 \u0026lt;- POST(url = \u0026#39;http://localhost:8000/test\u0026#39;, encode = \u0026#39;json\u0026#39;, 2 body = list(wait = 0.5)) 3unlist(c(api = \u0026#39;rserve\u0026#39;, status = status_code(rserve400), 4 content = content(rserve400))) 1## api status content.message 2## \u0026#34;rserve\u0026#34; \u0026#34;400\u0026#34; \u0026#34;Missing parameter - n\u0026#34; Performance Test A way to examine performance of an API is to look into how effectively it can serve multiple concurrent requests. For this, Locust, a Python based load testing tool, is used to simulate 1, 3 and 6 concurrent requests successively.\nThe test locust file is shown below.\n1import json 2from locust import HttpLocust, TaskSet, task 3 4class TestTaskSet(TaskSet): 5 6 @task 7 def test(self): 8 payload = {\u0026#39;n\u0026#39;:10, \u0026#39;wait\u0026#39;: 0.5} 9 headers = {\u0026#39;content-type\u0026#39;: \u0026#39;application/json\u0026#39;} 10 self.client.post(\u0026#39;/test\u0026#39;, data=json.dumps(payload), headers=headers) 11 12class MyLocust(HttpLocust): 13 min_wait = 0 14 max_wait = 0 15 task_set = TestTaskSet With this file, testing can be made as following (eg for 3 concurrent requests).\n1locust -f ./locustfile.py --host http://localhost:8000 --no-web -c 3 -r 3 When only 1 request is made successively, the average response time of the APIs is around 500ms. When there are multiple concurrent requests, however, the average response time of the plumber API increases significantly. This is because R is single threaded and requests are queued by httpuv. On the other hand, the average response time of the Rserve API stays the same and this is because Rserve handles concurrent requests by forked processes. The performance of the rApache API is in the middle. In practice, it is possible to boost the performance of the rApache API by enabling Prefork Multi-Processing Module although it will consume more memory.\nAs expected, the Rserve API handles considerably many requests per second.\nNote that the test function in this post is a bit unrealistic as it just waits before returning a value. In practice, R functions will consume more CPU and the average response time will tend to increase when multiple requests are made concurrently. Even in this case, the benefit of forking will persist.\nThis series investigate exposing R functions via an API. I hope you enjoy reading this series.\n","date":"November 19, 2017","img":"/blog/2017-11-19-api-development-with-r-2/featured.png","lang":"en","langName":"English","largeImg":"/blog/2017-11-19-api-development-with-r-2/featured_hu14becfc4256de54ed32f12a303a50027_367256_500x0_resize_box_3.png","permalink":"/blog/2017-11-19-api-development-with-r-2/","series":[{"title":"API Development With R","url":"/series/api-development-with-r/"}],"smallImg":"/blog/2017-11-19-api-development-with-r-2/featured_hu14becfc4256de54ed32f12a303a50027_367256_180x0_resize_box_3.png","tags":[{"title":"R","url":"/tags/r/"},{"title":"Rserve","url":"/tags/rserve/"},{"title":"RApache","url":"/tags/rapache/"},{"title":"Plumber","url":"/tags/plumber/"}],"timestamp":1511049600,"title":"API Development With R Part II"},{"categories":[{"title":"Development","url":"/categories/development/"}],"content":"API is an effective way of distributing analysis outputs to external clients. When it comes to API development with R, however, there are not many choices. Probably development would be made with plumber, Rserve, rApache or OpenCPU if a client or bridge layer to R is not considered.\nThis is 2 part series in relation to API development with R. In this post, serving an R function with plumber, Rserve and rApache is discussed. OpenCPU is not discussed partly because it could be overkill for API. Also its performance may be similar to rApache with Prefork Multi-Processing Module enabled. Then deploying the APIs in a Docker container, making example HTTP requests and their performance will be discussed in Part II.\nPlumber The plumber package is the easiest way of exposing an R function via API and it is built on top of the httpuv package.\nHere a simple function named test is defined in plumber-src.R. test() returns a number after waiting the amount of seconds specified by wait. Note the details of HTTP methods and resource paths can be specified as the way how a function is documented. By default, the response is converted into a json string and it is set to be unboxed.\n1#\u0026#39; Test function 2#\u0026#39; @serializer unboxedJSON 3#\u0026#39; @get /test 4#\u0026#39; @post /test 5test \u0026lt;- function(n, wait = 0.5, ...) { 6 Sys.sleep(wait) 7 list(value = n) 8} The function can be served as shown below. Port 9000 is set for the plumber API.\n1library(plumber) 2r \u0026lt;- plumb(\u0026#34;path-to-plumber-src.R\u0026#34;) 3r$run(port=9000, host=\u0026#34;0.0.0.0\u0026#34;) Rserve According to its project site,\nRserve is a TCP/IP server which allows other programs to use facilities of R from various languages without the need to initialize R or link against R library.\nThere are a number of Rserve client libraries and a HTTP API can be developed with one of them. For example, it is possible to set up a client layer to invoke an R function using the pyRserve library while a Python web freamwork serves HTTP requests.\nSince Rserve 1.7-0, however, a client layer is not mandatory because it includes the built-in R HTTP server. Using the built-in server has a couple of benefits. First development can be simpler without a client or bridge layer. Also performance of the API can be improved. For example, pyRserve waits for 200ms upon connecting to Rserve and this kind of overhead can be reduced significantly if HTTP requests are handled directly.\nThe FastRWeb package relies on Rserve\u0026rsquo;s built-in HTTP server and basically it serves HTTP requests by sourcing an R script and executing a function named as run - all source scripts must have run() as can be checked in the source.\nI find the FastRWeb package is not convenient for API development for several reasons. First, as mentioned earlier, it sources an R script and executes run(). However, after looking into the source code, it doesn\u0026rsquo;t need to be that way. Rather a more flexible way can be executing a function that\u0026rsquo;s already loaded. Secondly application/json is a popular content type but it is not understood by the built-in server. Finally, while it mainly aims to serve R graphics objects and HTML pages, json string can be effective for HTTP responses. In this regards, some modifications are maded as discussed below.\ntest() is the same to the plumber API.\n1#### HTTP RESOURCES 2test \u0026lt;- function(n, wait = 0.5, ...) { 3 Sys.sleep(wait) 4 list(value = n) 5} In order to use the built-in server, a function named .http.request should be found. Here another function named process_request is created and it is used instead of .http.request() defined in the FastRWeb package. process_request() is basically divided into 2 parts: builing request object and building output object.\nbuilding request object - the request obeject is built so that the headers are parsed so as to identify the request method. Then the request parameters are parsed according to the request method and content type. building output object - the output object is a list of payload, content-type, headers and status-code. A function can be found by the request URL and it is checked if all function arguments are found in the request parameters. Then payload is obtained by executing the matching function if all arguments are found. Otherwise the 400 (Bad Request) error will be returned. 1#### PROCESS REQUEST 2process_request \u0026lt;- function(url, query, body, headers) { 3 #### building request object 4 ## not strictly necessary as in FastRWeb, 5 ## just to make clear of request related variables 6 request \u0026lt;- list(uri = url, method = \u0026#39;POST\u0026#39;, 7 query = query, body = body) 8 9 ## parse headers 10 request$headers \u0026lt;- parse_headers(headers) 11 if (\u0026#34;request-method\u0026#34; %in% names(request$headers)) 12 request$method \u0026lt;- c(request$headers[\u0026#34;request-method\u0026#34;]) 13 14 ## parse parameters (function arguments) 15 ## POST accept only 2 content types 16 ## - application/x-www-form-urlencoded by built-in server 17 ## - application/json 18 ## used below as do.call(function_name, request$pars) 19 request$pars \u0026lt;- list() 20 if (request$method == \u0026#39;POST\u0026#39;) { 21 if (!is.null(body)) { 22 if (is.raw(body)) 23 body \u0026lt;- rawToChar(body) 24 if (any(grepl(\u0026#39;application/json\u0026#39;, request$headers))) 25 body \u0026lt;- jsonlite::fromJSON(body) 26 request$pars \u0026lt;- as.list(body) 27 } 28 } else { 29 if (!is.null(query)) { 30 request$pars \u0026lt;- as.list(query) 31 } 32 } 33 34 #### building output object 35 ## list(payload, content-type, headers, status_code) 36 ## https://github.com/s-u/Rserve/blob/master/src/http.c#L358 37 payload \u0026lt;- NULL 38 content_type \u0026lt;- \u0026#39;application/json; charset=utf-8\u0026#39; 39 headers \u0026lt;- character(0) 40 status_code \u0026lt;- 200 41 42 ## generate payload (function output) 43 ## function name must match to resource path for now 44 matched_fun \u0026lt;- gsub(\u0026#39;^/\u0026#39;, \u0026#39;\u0026#39;, request$uri) 45 46 ## no resource path means no matching function 47 if (matched_fun == \u0026#39;\u0026#39;) { 48 payload \u0026lt;- list(api_version = \u0026#39;1.0\u0026#39;) 49 if (grepl(\u0026#39;application/json\u0026#39;, content_type)) 50 payload \u0026lt;- jsonlite::toJSON(payload, auto_unbox = TRUE) 51 return (list(payload, content_type, headers)) # default status 200 52 } 53 54 ## check if all defined arguments are supplied 55 defined_args \u0026lt;- formalArgs(matched_fun)[formalArgs(matched_fun) != \u0026#39;...\u0026#39;] 56 args_exist \u0026lt;- defined_args %in% names(request$pars) 57 if (!all(args_exist)) { 58 missing_args \u0026lt;- defined_args[!args_exist] 59 payload \u0026lt;- list(message = paste(\u0026#39;Missing parameter -\u0026#39;, 60 paste(missing_args, collapse = \u0026#39;, \u0026#39;))) 61 status_code \u0026lt;- 400 62 } 63 64 if (is.null(payload)) { 65 payload \u0026lt;- tryCatch({ 66 do.call(matched_fun, request$pars) 67 }, error = function(err) { 68 list(message = \u0026#39;Internal Server Error\u0026#39;) 69 }) 70 71 if (\u0026#39;message\u0026#39; %in% names(payload)) 72 status_code \u0026lt;- 500 73 } 74 75 if (grepl(\u0026#39;application/json\u0026#39;, content_type)) 76 payload \u0026lt;- jsonlite::toJSON(payload, auto_unbox = TRUE) 77 78 return (list(payload, content_type, headers, status_code)) 79} 80 81# parse headers in process_request() 82# https://github.com/s-u/FastRWeb/blob/master/R/run.R#L65 83parse_headers \u0026lt;- function(headers) { 84 ## process headers to pull out request method (if supplied) and cookies 85 if (is.raw(headers)) headers \u0026lt;- rawToChar(headers) 86 if (is.character(headers)) { 87 ## parse the headers into key/value pairs, collapsing multi-line values 88 h.lines \u0026lt;- unlist(strsplit(gsub(\u0026#34;[\\r\\n]+[ \\t]+\u0026#34;,\u0026#34; \u0026#34;, headers), \u0026#34;[\\r\\n]+\u0026#34;)) 89 h.keys \u0026lt;- tolower(gsub(\u0026#34;:.*\u0026#34;, \u0026#34;\u0026#34;, h.lines)) 90 h.vals \u0026lt;- gsub(\u0026#34;^[^:]*:[[:space:]]*\u0026#34;, \u0026#34;\u0026#34;, h.lines) 91 names(h.vals) \u0026lt;- h.keys 92 h.vals \u0026lt;- h.vals[grep(\u0026#34;^[^:]+:\u0026#34;, h.lines)] 93 return (h.vals) 94 } else { 95 return (NULL) 96 } 97} process_request() replaces .http.request() in the source script of Rserve - it\u0026rsquo;ll be explained futher in Part II.\n1## Rserve requires .http.request function for handling HTTP request 2.http.request \u0026lt;- process_request rApache rApache is a project supporting web application development using the R statistical language and environment and the Apache web server.\nrApache provides multiple ways to specify an R function that handles incoming HTTP requests - see the manual for details. Among the multiple RHandlers, I find using a Rook application can be quite effective.\nHere is the test function as a Rook application. As process_request(), it parses function arguments according to the request method and content type. Then a value is returned after wating the specified seconds. The response of a Rook application is a list of status, headers and body.\n1test \u0026lt;- function(env) { 2 req \u0026lt;- Request$new(env) 3 res \u0026lt;- Response$new() 4 5 request_method \u0026lt;- env[[\u0026#39;REQUEST_METHOD\u0026#39;]] 6 rook_input \u0026lt;- env[[\u0026#39;rook.input\u0026#39;]]$read() 7 content_type \u0026lt;- env[[\u0026#39;CONTENT_TYPE\u0026#39;]] 8 9 req_args \u0026lt;- if (request_method == \u0026#39;GET\u0026#39;) { 10 req$GET() 11 } else { 12 # only accept application/json 13 if (!grepl(\u0026#39;application/json\u0026#39;, content_type, ignore.case = TRUE)) { 14 NULL 15 } else if (length(rook_input) == 0) { 16 NULL 17 } else { 18 if (is.raw(rook_input)) 19 rook_input \u0026lt;- rawToChar(rook_input) 20 rjson::fromJSON(rook_input) 21 } 22 } 23 24 if (!is.null(req_args)) { 25 wait \u0026lt;- if (\u0026#39;wait\u0026#39; %in% names(req_args)) req_args$wait else 1 26 n \u0026lt;- if (\u0026#39;n\u0026#39; %in% names(req_args)) req_args$n else 10 27 Sys.sleep(wait) 28 list( 29 status = 200, 30 headers = list(\u0026#39;Content-Type\u0026#39; = \u0026#39;application/json\u0026#39;), 31 body = rjson::toJSON(list(value=n)) 32 ) 33 } else { 34 list( 35 status = 400, 36 headers = list(\u0026#39;Content-Type\u0026#39; = \u0026#39;application/json\u0026#39;), 37 body = rjson::toJSON(list(message=\u0026#39;No parameters specified\u0026#39;)) 38 ) 39 } 40} This is all for Part I. In Part II, it\u0026rsquo;ll be discussed how to deploy the APIs via a Docker container, how to make example requests and their performance. I hope this article is interesting.\n","date":"November 18, 2017","img":"/blog/2017-11-18-api-development-with-r-1/featured.png","lang":"en","langName":"English","largeImg":"/blog/2017-11-18-api-development-with-r-1/featured_hu14becfc4256de54ed32f12a303a50027_367256_500x0_resize_box_3.png","permalink":"/blog/2017-11-18-api-development-with-r-1/","series":[{"title":"API Development With R","url":"/series/api-development-with-r/"}],"smallImg":"/blog/2017-11-18-api-development-with-r-1/featured_hu14becfc4256de54ed32f12a303a50027_367256_180x0_resize_box_3.png","tags":[{"title":"R","url":"/tags/r/"},{"title":"Rserve","url":"/tags/rserve/"},{"title":"RApache","url":"/tags/rapache/"},{"title":"Plumber","url":"/tags/plumber/"}],"timestamp":1510963200,"title":"API Development With R Part I"},{"categories":[{"title":"Development","url":"/categories/development/"}],"content":"In the previous posts, it is discussed how to package/deploy an R model with AWS Lambda and to expose the Lambda function via Amazon API Gateway. Main benefits of serverless architecture is cost-effectiveness and being hassle-free from provisioning/managing servers. While the API returns a predicted admission status value given GRE, GPA and Rank, there is an issue if it is served within a web application: Cross-Origin Resource Sharing (CORS). This post discusses how to resolve this issue by updating API configuration and the Lambda function handler with a simple web application. Also it is illustrated how to host the application in a serverless environment.\nBackend Part I - Packaging R ML Model for Lambda Part II - Deploying R ML Model via Lambda Part III - Exposing R ML Model via APIG Frontend Part IV - Serving R ML Model via S3 - this post Frontend A simple single page application is created using React. By clicking the Check! button after entering the GRE, GPA and Rank values, information of the expected admimission status pops up in a modal. The status value is fetched from the API of the POC application that is discussed in Part III. The code of this application can be found here.\nUpdate Lambda function hander CORS According to Wikipedia,\nCross-origin resource sharing (CORS) is a mechanism that allows restricted resources (e.g. fonts) on a web page to be requested from another domain outside the domain from which the first resource was served. A web page may freely embed cross-origin images, stylesheets, scripts, iframes, and videos. Certain \u0026ldquo;cross-domain\u0026rdquo; requests, notably Ajax requests, however are forbidden by default by the same-origin security policy.\nHere is an example from a Stack Overflow answer why it can be important to prevent CORS.\nYou go to website X and the author of website X has written an evil script which gets sent to your browser. That script running on your browser logs onto your bank website and does evil stuff and because it\u0026rsquo;s running as you in your browser it has permission to do so. Therefore your bank\u0026rsquo;s website needs some way to tell your browser if scripts on website X should be trusted to access pages at your bank. The domain name of the API is api.jaehyeon.me so that requests fail within the application. An example of the error is shown below.\n1Fetch API cannot load ... 2No \u0026#39;Access-Control-Allow-Origin\u0026#39; header is present on the requested resource. 3Origin \u0026#39;http://localhost:9090\u0026#39; is therefore not allowed access. The response had HTTP status code 403. Cofigure API API Gateway allows to enable CORS and it can be either on a resource or a method within a resource. The GET method is selected and Enable CORS is clicked after pulling down actions.\nSimply put, another method of OPTIONS is created and the following response headers are added to the GET and OPTIONS methods.\nAccess-Control-Allow-Methods: Added to OPTIONS only Access-Control-Allow-Headers: Added to OPTIONS only, X-API-Key is allowed Access-Control-Allow-Origin: Added to both GET and OPTIONS Here is all the steps that enables CORS in API Gateway. Note that the necessary headers are added to 200 response only, not to 400 response so that the above error can\u0026rsquo;t be eliminated for 400 response unless the headers are set separately.\nAfter that, the API needs to be deployed again and, as can be seen in the deployment history, the latest deployment is selected as the current stage.\nUpdate handler Despite enabling CORS, it was not possilbe to resolve the issue. After some search, a way is found in a Stack Overflow answer. It requires to update the Lambda function handler (handler.py) that extends the 200 response with headers elements - one for CORS support to work and the other for cookies, authorization headers with HTTPS. The original response is added to the body element of the new response. Note it\u0026rsquo;d be necessary to modify the case of 400 response in order to reduce the risk of encountering the error although it is not covered here.\n1def lambda_handler(event, context): 2 try: 3 gre = event[\u0026#34;gre\u0026#34;] 4 gpa = event[\u0026#34;gpa\u0026#34;] 5 rnk = event[\u0026#34;rank\u0026#34;] 6 can_be_admitted = pred_admit(gre, gpa, rnk) 7 res = { 8 \u0026#34;httpStatus\u0026#34;: 200, 9 \u0026#34;headers\u0026#34;: { 10 # Required for CORS support to work 11 \u0026#34;Access-Control-Allow-Origin\u0026#34; : \u0026#34;*\u0026#34;, 12 # Required for cookies, authorization headers with HTTPS 13 \u0026#34;Access-Control-Allow-Credentials\u0026#34; : True 14 }, 15 \u0026#34;body\u0026#34;: {\u0026#34;result\u0026#34;: can_be_admitted} 16 } 17 return res 18 except Exception as e: 19 logging.error(\u0026#39;Payload: {0}\u0026#39;.format(event)) 20 logging.error(\u0026#39;Error: {0}\u0026#39;.format(e.message)) 21 err = { 22 \u0026#39;errorType\u0026#39;: type(e).__name__, 23 \u0026#39;httpStatus\u0026#39;: 400, 24 \u0026#39;request_id\u0026#39;: context.aws_request_id, 25 \u0026#39;message\u0026#39;: e.message.replace(\u0026#39;\\n\u0026#39;, \u0026#39; \u0026#39;) 26 } 27 raise Exception(json.dumps(err)) The updated handler is packaged again and copied to S3.\n1# pull git repo 2cd serverless-poc/ 3git pull origin master 4 5# copy handler.py and create admission.zip 6cd .. 7export HANDLER=handler 8 9cp -v serverless-poc/poc-logit-handler/*.py $HOME/$HANDLER 10cd $HOME/$HANDLER 11zip -r9 $HOME/admission.zip * 12 13# copy to S3 14aws s3 cp $HOME/admission.zip s3://serverless-poc-handlers The AWS web console doesn\u0026rsquo;t have an option to update a Lambda function where the deployment package is in S3 so that aws cli is used instead.\n1#http://docs.aws.amazon.com/cli/latest/reference/lambda/update-function-code.html 2aws lambda update-function-code --function-name ServerlessPOCAdmission \\ 3\t--s3-bucket serverless-poc-handlers --s3-key admission.zip The test result shown below indicates the updated handler is executed correctly.\nIt can also be checked by the API and now the API can be served in a web application.\n1# updated response 2#curl -H \u0026#39;x-api-key:xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\u0026#39; \\ 3# \u0026#39;https://api.jaehyeon.me/poc/admit?gre=800\u0026amp;gpa=4\u0026amp;rank=1\u0026#39; 4r \u0026lt;- GET(\u0026#34;https://api.jaehyeon.me/poc/admit\u0026#34;, 5 add_headers(`x-api-key` = \u0026#39;xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\u0026#39;), 6 query = list(gre = 800, gpa = 4, rank = 1)) 7status_code(r) 8[1] 200 9 10content(r) 11$body 12$body$result 13[1] TRUE 14 15$headers 16$headers$`Access-Control-Allow-Origin` 17[1] \u0026#34;*\u0026#34; 18 19$headers$`Access-Control-Allow-Credentials` 20[1] TRUE 21 22$httpStatus 23[1] 200 Hosting Amazon S3 is one of the popular ways to store static web contents and it can be used as a way to host a static web site. The React application can be hosted on S3 as the backend logic of calling the API is bundled and accessible. 2 ways are illustrated in this section. The former is via the Static website hosting property of Amazon S3 Buckets while the latter is through Amazon CloudFront, which is a content delivery network (CDN) service. Note that only HTTP is avaialble if the application is hosted without CloudFront.\nSeparate S3 buckets are created to store the application as shown below.\npoc.jaehyeon.me - For hosting Static website hosting property web.jaehyeon.me - For hosting through CloudFront In AWS console, two folders are created: app and css. Then the application files are saved to each of the buckets as following.\n1app 2 bundle.js 3css 4 bootstrap.css 5 style.css 6index.html Static website hosting First read-access is given to all objects in the bucket (poc.jaehyeon.me). It is set in Bucket Policy of the permissions tab - Policy is discussed in Part II.\nThen, in the properties tab, static website hosting is enabled where index.html is set to be rendered for both the default and error document. Now it is possible to have access to the application by the endpoint.\nIn order to replace the endpoint with a custom domain name, a Canonical name (CNAME) record is created in Amazon Route 53. Note that the CNAME record (poc.jaehyeon.me) has to be the same to the bucket name. s3-website-us-east-1.amazonaws.com. is entered in Value, which is used to define the host name as an alias for the Amazon S3 bucket. Note the period at the end is necessary as it signifies the DNS root and, if it is not specified, a DNS resolver could append it\u0026rsquo;s default domain to the domain you provided. (See Customizing Amazon S3 URLs with CNAMEs for further details.) Now the application can be accessed using http://poc.jaehyeon.me.\nCloudFront It is possible to host the application using Amazon CloudFront which is a global content delivery network (CDN) service that accelerates delivery of websites, APIs, video content or other web assets.\nWeb is taken as the delivery method.\nThe S3 bucket (web.jaehyeon.me) is selected as the origin domain name. Note, unlike relying on the static website hosting property where all objects in the bucket are given read-access, in this way, access to the bucket is restricted only to CloudFront with a newly created identity. The updated bucket policy is shown below. (See Using an Origin Access Identity to Restrict Access to Your Amazon S3 Content for further details.)\n1{ 2 \u0026#34;Version\u0026#34;: \u0026#34;2008-10-17\u0026#34;, 3 \u0026#34;Id\u0026#34;: \u0026#34;PolicyForCloudFrontPrivateContent\u0026#34;, 4 \u0026#34;Statement\u0026#34;: [ 5 { 6 \u0026#34;Sid\u0026#34;: \u0026#34;1\u0026#34;, 7 \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, 8 \u0026#34;Principal\u0026#34;: { 9 \u0026#34;AWS\u0026#34;: \u0026#34;arn:aws:iam::cloudfront:user/CloudFront Origin Access Identity xxxxxxxxxxxxxx\u0026#34; 10 }, 11 \u0026#34;Action\u0026#34;: \u0026#34;s3:GetObject\u0026#34;, 12 \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::web.jaehyeon.me/*\u0026#34; 13 } 14 ] 15} In default cache behavior settings, Redirect HTTP to HTTPS is selected for the viewer protocol policy. All other options are left untouched - they are not shown.\nIn distribution settings, a CNAME record (web.jaehyeon.me) is created to be the same to the bucket name. The custom SSL certificate that is obtained from AWS Certificate Manager is chosen rather than the default CloudFront certificate - see Part III. Finally it is selected to support only clients that support server name indication (SNI). Note all the other options are left untouched - they are not shown.\nOnce the distribution is created, the distribution\u0026rsquo;s CloudFront domain name is created and it is possible to use it to create a custom domain.\nIn Route 53, a new record set is created and web.jaehyeon.me is entered in the name field, followed by selecting A - IPv4 address as the type. Alias is set to be yes and the distribution domain name is entered as the alias target.\nOnce it is ready, the application can be accessed using either http://web.jaehyeon.me or https://web.jaehyeon.me where HTTP is redirected to HTTPS.\nFinal thoughts This is the end of the Serverless Data Product POC series. I consider a good amount of information is shared in relation to serverless data product development and I hope you find the posts useful. For demonstration, I used the AWS web console but it wouldn\u0026rsquo;t be suitable in a production environment as it involves a lot of manual jobs as well as those jobs are not reproducible. There are a number of notable frameworks that help develop applications in serverless environment: Serverless Framework, Apex, Chalice and Zappa. I hope there will be another series that cover one of these frameworks.\n","date":"April 17, 2017","img":"/blog/2017-04-17-serverless-data-product-4/featured.png","lang":"en","langName":"English","largeImg":"/blog/2017-04-17-serverless-data-product-4/featured_huf7a77a5394e53f84146299d65bd38b83_225463_500x0_resize_box_3.png","permalink":"/blog/2017-04-17-serverless-data-product-4/","series":[{"title":"Serverless Data Product","url":"/series/serverless-data-product/"}],"smallImg":"/blog/2017-04-17-serverless-data-product-4/featured_huf7a77a5394e53f84146299d65bd38b83_225463_180x0_resize_box_3.png","tags":[{"title":"AWS","url":"/tags/aws/"},{"title":"AWS Lambda","url":"/tags/aws-lambda/"},{"title":"Amazon S3","url":"/tags/amazon-s3/"},{"title":"Amazon API Gateway","url":"/tags/amazon-api-gateway/"},{"title":"R","url":"/tags/r/"},{"title":"Python","url":"/tags/python/"},{"title":"CloudFront","url":"/tags/cloudfront/"},{"title":"Route53","url":"/tags/route53/"},{"title":"React","url":"/tags/react/"}],"timestamp":1492387200,"title":"Serverless Data Product POC Backend Part IV - Serving R ML Model via S3"},{"categories":[{"title":"Development","url":"/categories/development/"}],"content":"In Part I of this series, R and necessary libraries/packages together with a Lambda function handler are packaged and saved to Amazon S3. Then, in Part II, the package is deployed at AWS Lambda after creating and assigning a role to the Lambda function. Although the Lambda function can be called via the Invoke API, it\u0026rsquo;ll be much more useful if the function can be called as a web service (or API). In this post, it is discussed how to expose the Lambda function via Amazon API Gateway. After creating an API by integrating the Lambda function, it is protected with an API key. Finally a custom domain name is used as an alternative URL of the API.\nBackend Part I - Packaging R ML Model for Lambda Part II - Deploying R ML Model via Lambda Part III - Exposing R ML Model via APIG - this post Frontend Part IV - Serving R ML Model via S3 [EDIT 2017-04-17] The Lambda function hander (handler.py) has been modified to resolve an issue of Cross-Origin Resource Sharing (CORS). See Part IV for further details.\nCreate API It can be started by clicking the Get Started button if there\u0026rsquo;s no existing API or the Create API button if there is an existing one.\nAmazon API Gageway provides several options to create an API. New API is selected for the API of the POC application and the name of the API (ServerlessPOC) and description are entered.\nCreate resource and method According to Thoughts on RESTful API Design,\nIn any RESTful API, a resource is an object with a type, associated data, relationships to other resources, and a set of methods that operate on it.\nA resource is represented in the URL and, if the resource is named as admit, the resource URL becomes /admit (eg http://example.com/admit) and a client application can make a request to the URL.\nAs can be seen below, the Lambda function hander requires that the event object has 3 elements: gre, gpa and rank.\n1def lambda_handler(event, context): 2 try: 3 gre = event[\u0026#34;gre\u0026#34;] 4 gpa = event[\u0026#34;gpa\u0026#34;] 5 rnk = event[\u0026#34;rank\u0026#34;] 6 can_be_admitted = pred_admit(gre, gpa, rnk) 7 res = {\u0026#34;result\u0026#34;: can_be_admitted} 8 return res 9 except Exception as e: 10 logging.error(\u0026#39;Payload: {0}\u0026#39;.format(event)) 11 logging.error(\u0026#39;Error: {0}\u0026#39;.format(e.message)) 12 err = { 13 \u0026#39;errorType\u0026#39;: type(e).__name__, 14 \u0026#39;httpStatus\u0026#39;: 400, 15 \u0026#39;request_id\u0026#39;: context.aws_request_id, 16 \u0026#39;message\u0026#39;: e.message.replace(\u0026#39;\\n\u0026#39;, \u0026#39; \u0026#39;) 17 } 18 raise Exception(json.dumps(err)) In Amazon API Gateway, there are two ways to create the resource for the Lambda function of the POC application.\nQuery string\nIt is possible to create only a resource and the 3 elements can be added in query string. Then a request with the 3 elements can be made to /admit?gre=800\u0026amp;gpa=4\u0026amp;rank=1. 1/ 2 /admit Proxy resource\nProxy resources can be created by covering path parameters by brackets. Then the equivalent request can be made to /800/4/1/admit. 1/ 2 /{gre} 3 /{gpa} 4 /{rank} 5 /admit For the API of the POC application, the way with query string is used. First it is necessary to create a resource.\nThen the resource is named as Admit.\nAfter creating the resource, it is necessary to create one or more HTTP methods on it.\nOnly the GET method is created for this API.\nNow it is time to integrate the method with the Lambda function. Lambda Function is selected as the interation type and ServerlessPOCAdmission is selected - note that the region where the Lambda function is deployed should be selected first.\nConfigure method execution The lifecycle of a Lambda function is shown below. A Lambda function is called after Method Request and Integration Request. Also there are two steps until the result is returned back to the client: Method Response and Integration Response.\nMethod request As discussed earlier, only a single resource is created so that a request is made with query string. Therefore the 3 event elements (gre, gpa and rank) should be created in URL Query String Parameters. Note that API Key Required is set to be false and it is necessary to change it to be true if the API needs to be protected with an API key - it\u0026rsquo;ll be discussed further below. The other sections (HTTP Request Header, Request Body, \u0026hellip;) are not touched for this API.\nIntegration request It is possible to update the target backend or to modify data from the incoming request. It is not necessary to change the target backend as it is already set appropriately.\nAmong the 3 event elements (gre, gpa and rank), rank is a factor or, at least, it should be a string while the others can be either numbers or numeric strings. Therefore the Lambda function will complain if a numeric rank value is included in a query string (eg rank=1). Although it is possible to modify the Lambda function handler, an easier way is to modify data from the incoming request.\nIn Body Mapping Templates, the recommended option of When there are no templates defined (recommended) is selected in request body passthrough and application/json is added to Content-Type. Data from incoming request can be updated in the template that is shown by clicking the added content type (application/json). As shown below, rank is changed into a string before the Lambda function is called. Note Velocity Template Engine is used in Amazon API Gateway.\n1{ 2 \u0026#34;gre\u0026#34;: $input.params(\u0026#39;gre\u0026#39;), 3 \u0026#34;gpa\u0026#34;: $input.params(\u0026#39;gpa\u0026#39;), 4 \u0026#34;rank\u0026#34;: \u0026#34;$input.params(\u0026#39;rank\u0026#39;)\u0026#34; 5} Method response If a request is successful, the HTTP status code of 200 is returned. As can be seen in the code of the Lambda function handler above, the status code of 400 is planned to be returned if there is an error. Therefore it is necessary to add 400 response so that it is mapped in Integration Response.\nIntegration response The output of a response can be mapped in Body Mapping Templates. The body of the default 200 response doesn\u0026rsquo;t need modification as the Lambda function already returns a JSON string - {\u0026quot;result\u0026quot;: true} or {\u0026quot;result\u0026quot;: false}. If the function returns only True or False, however, the response can be modified as shown below. (Note that this is only for illustration and nothing is added to the content type.)\n1{ 2 \u0026#34;result\u0026#34;: $input.path(\u0026#39;$\u0026#39;) 3} For 400 response, the HTTP status is identified by .*\u0026quot;httpStatus\u0026quot;:400.* and the body is mapped as following.\n1#set ($errorMessageObj = $util.parseJson($input.path(\u0026#39;$.errorMessage\u0026#39;))) 2{ 3 \u0026#34;code\u0026#34; : $errorMessageObj.httpStatus, 4 \u0026#34;message\u0026#34; : \u0026#34;$errorMessageObj.message\u0026#34;, 5 \u0026#34;request-id\u0026#34; : \u0026#34;$errorMessageObj.request_id\u0026#34; 6} Test API The API can be tested by adding the 3 elements in query string. As expected, the response returns {\u0026quot;result\u0026quot;: true} with the HTTP status code of 200.\nIn order to test 400 response, the value of gre is set to be a string (gre). The status code of 400 is returned as expected but it fails to parse the message of the error into JSON. It is necessary to modify the message, referring to Error Handling Patterns in Amazon API Gateway and AWS Lambda.\n1 ... 2 3 err = { 4 \u0026#39;errorType\u0026#39;: type(e).__name__, 5 \u0026#39;httpStatus\u0026#39;: 400, 6 \u0026#39;request_id\u0026#39;: context.aws_request_id, 7 \u0026#39;message\u0026#39;: e.message.replace(\u0026#39;\\n\u0026#39;, \u0026#39; \u0026#39;) 8 } 9 ... Deploy API Once testing is done, it is ready to deploy the API.\nIt is possible to create a new stage by selecting [New Stage] or to update an existing one by selecting its name in deployment stage. Although it is recommended to create at least 2 stages (eg development and production stage), only a singe production stage is created for the POC application.\nOnce created, the invoke URL can be found when the relevant method (GET) is clicked. The default root URL is of the following format.\n1https://api-id.execute-api.region.amazonaws.com/stage The API has been deployed successfully and it is possible to make a request using curl and R\u0026rsquo;s httr package as following - note the API ID is hidden.\n1## no API Key 2#curl \u0026#39;https://xxxxxxxxxx.execute-api.us-east-1.amazonaws.com/prod/admit?gre=800\u0026amp;gpa=4\u0026amp;rank=1\u0026#39; 3r \u0026lt;- GET(\u0026#34;https://xxxxxxxxxx.execute-api.us-east-1.amazonaws.com/prod/admit\u0026#34;, 4 query = list(gre = 800, gpa = 4, rank = 1)) 5 6status_code(r) 7[1] 200 8 9content(r) 10$result 11[1] TRUE Protecting by API key Enable API key It is on individual methods whether to enable an API key or not. In order to enable an API key, select the GET method in the resources section and change API Key Required to true in Method Request. Note that the API has to be deployed again in order to have the change in effect.\nCreate usage plan A usage plan enforces Throttling (Rate and Burst) and Quota of an API and it associates API stages and keys. Since its launch on August 11, 2016, it is enabled in a region where API Gateway is used for the first time. The meaning of the throttling and quota values are as following.\nRate is the rate at which tokens are added to the Token Bucket and this value indicates the average number of requests per second over an extended period of time. Burst is the capacity of the Token Bucket. Quota is the total number of requests in a given time period. For further details, see Manage API Request Throttling and Token Bucket vs Leaky Bucket.\nA usage plan named ServerlessPOC is created where the rate, burst and quote are 10 requests per second, 20 requests and 500 requests per day respectively.\nThen the production stage (prod) of ServerlessPOC API is added to the plan.\nCreate API key An API key can be created in API Keys section of the Console. The key is named as ServerlessPOC and it is set to be auto-generated.\nThe usage plan created earlier is added to the API key.\nNow the API has been protected with an API key and it is possible to make a request using curl and R\u0026rsquo;s httr package as following. Note that the API key should be added with the key named x-api-key. Without the API key in the header, the request returns 403 Forbidden error. (Note also tick marks rather than single quotations in GET())\n1## API Key 2# 403 Forbidden without api key 3#curl -H \u0026#39;x-api-key:xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\u0026#39; \\ 4# \u0026#39;https://xxxxxxxxxx.execute-api.us-east-1.amazonaws.com/prod/admit?gre=800\u0026amp;gpa=4\u0026amp;rank=1\u0026#39; 5r \u0026lt;- GET(\u0026#34;https://xxxxxxxxxx.execute-api.us-east-1.amazonaws.com/prod/admit\u0026#34;, 6 add_headers(`x-api-key` = \u0026#39;xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\u0026#39;), 7 query = list(gre = 800, gpa = 4, rank = 1)) 8 9status_code(r) 10[1] 200 11 12content(r) 13$result 14[1] TRUE Using custom domain name The invoke URL generated by API Gateway can be difficult to recall and not user-friendly. In order to have a more inituitive URL for the API, it is possible to set up a custom domain name as the API\u0026rsquo;s host name and choose a base path to present an alternative URL of the API. For example, instead of using xxxxxxxxxx.execute-api.us-east-1.amazonaws.com, it is possible to use api.jaehyeon.me.\nThe prerequisites for using a custom dome name for an API are\nDomain name ACM Certificate (us-east-1 only) I registered a domain name (jaehyeon.me) in Amazon Route 53 and requested ACM Certificate through AWS Certificate Manager. It was quite quick to me and it took less than 1 day. See the following articles for how-to.\nRegistering Domain Names Using Amazon Route 53 Requesting and Managing ACM Certificates The domain name of the API is set to be api.jaehyeon.me and the approved ACM Certificate is selected. In Base Path Mappings, poc is added to the path and the production stage of the ServerlessPOC API is selected as the destination. In this way, it is possible to change the resource URL as following.\n1# default resource URL 2https://xxxxxxxxxx.execute-api.us-east-1.amazonaws.com/prod/admit 3 4# custom resource URL 5https://api.jaehyeon.me/poc/admit When clicking the save button above, a distribution domain name is assigned by Amazon CloudFront. This step takes up to 40 minutes to complete and, in the meantime, A-record alias for the API domain name is set up so that it can be mapped to the associated distribution domain name.\nIn Route 53, a new record set is created and api.jaehyeon.me is entered in the name field, followed by selecting A - IPv4 address as the type. Alias is set to be yes and the distribution domain name is entered as the alias target.\nOnce it is ready, the custom domain name can be used as an alternative domain name of the API and it is possible to make a request using curl and R\u0026rsquo;s httr package as following.\n1## custom domain name 2#curl -H \u0026#39;x-api-key:xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\u0026#39; \\ 3# \u0026#39;https://api.jaehyeon.me/poc/admit?gre=800\u0026amp;gpa=4\u0026amp;rank=1\u0026#39; 4r \u0026lt;- GET(\u0026#34;https://api.jaehyeon.me/poc/admit\u0026#34;, 5 add_headers(`x-api-key` = \u0026#39;xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\u0026#39;), 6 query = list(gre = 800, gpa = 4, rank = 1)) 7 8status_code(r) 9[1] 200 10 11content(r) 12$result 13[1] TRUE That\u0026rsquo;s it! This is all that I was planning to discuss with regard to exposing a Lambda function backed by a prediction model in R via an API. I hope this series of posts are useful to productionize your analysis.\n","date":"April 13, 2017","img":"/blog/2017-04-13-serverless-data-product-3/featured.png","lang":"en","langName":"English","largeImg":"/blog/2017-04-13-serverless-data-product-3/featured_hue5e1a794303feb35ce63690649b07766_173293_500x0_resize_box_3.png","permalink":"/blog/2017-04-13-serverless-data-product-3/","series":[{"title":"Serverless Data Product","url":"/series/serverless-data-product/"}],"smallImg":"/blog/2017-04-13-serverless-data-product-3/featured_hue5e1a794303feb35ce63690649b07766_173293_180x0_resize_box_3.png","tags":[{"title":"AWS","url":"/tags/aws/"},{"title":"AWS Lambda","url":"/tags/aws-lambda/"},{"title":"Amazon API Gateway","url":"/tags/amazon-api-gateway/"},{"title":"R","url":"/tags/r/"},{"title":"Python","url":"/tags/python/"}],"timestamp":1492041600,"title":"Serverless Data Product POC Backend Part III - Exposing R ML Model via APIG"},{"categories":[{"title":"Development","url":"/categories/development/"}],"content":"In the previous post, serverless event-driven application development is introduced. Also how to package R, necessary libraries/packages and a Lambda function handler is discussed. No need of provisioning/managing servers is one of the key benefits of the architecture. It is also a cost-effective way of delivering a data product as functions are executed on-demand rather than in servers that run 24/7. Furthermore AWS Lambda free tier includes 1M free requests per month and 400,000 GB-seconds of compute time per month, which is available to both existing and new AWS customers indefinitely. (GB-seconds is applicable when execution is made with 1 GB of memory.) Lowering the size of memory increases the execution time and thus 3.2M seconds or about 37 days are free with 128 MB of memory (1 GB divided by 8) - note that CPU power is proportional to allocated memory.\nInitially I was planning to discuss how to deploy a package at AWS Lambda and to expose it via Amazon API Gateway in this post. However it\u0026rsquo;d be too long with so many screenshots and I split them in Part II and III. Here is an updated series plan.\nBackend Part I - Packaging R ML Model for Lambda Part II - Deploying R ML Model via Lambda - this post Part III - Exposing R ML Model via APIG Frontend Part IV - Serving R ML Model via S3 [EDIT 2017-04-17] The Lambda function handler (handler.py) has been modified to resolve an issue of Cross-Origin Resource Sharing (CORS). See Part IV for further details.\nManaging security Before deploying a Lambda package, it is necessary to understand the security framework provided by AWS mostly based on AWS Identity and Access Management (IAM). To use AWS services (e.g. downloading a file from S3), AWS needs to identify the user or service (eg AWS Lambda) that makes the API call - this is the authentication. Also it needs to be checked whether the user or service has the permission - this is the authorization. For example, for this POC product development, I created a user and authentication is made by permanent credentials (Access Key ID and Secret Access Key) given to the user. Authorization is managed by policy and the following 3 AWS managed policies are attached to the user.\nAWSLambdaFullAccess AmazonAPIGatewayAdministrator AmazonCognitoPowerUser These policies define permissions to relevant AWS services such as Amazon S3, Amazon CloudWatch, AWS Lambda and Amazon API Gateway - note that they are user-based policies. The attached policies can be found in the Users section of IAM.\nUnlike users (and groups that can have one or more users with relevant permissions), AWS services (eg AWS Lambda) can assume a role, inheriting the permissions given to the role. Roles don\u0026rsquo;t have permanent credentials assigned to them but, when a service assumes a role, temporary credentials are assigned and they are used to authorize the service to do what\u0026rsquo;s defined in the (user-defined) policies attached to the role. Note that temporary credentials are made up of Access Key ID, Secret Access Key and Security Token and are generated by AWS Security Token Service (STS). In order to assume a role, a service needs another type of policy called trust policy, which defines who can assume a role. Therefore we need both types of policy so as to deploy the POC application at AWS Lambda.\nUser-based policy - to give permissions to AWS services Trust policy - to assume a role Note that this section is based on the Ch4 of AWS Lambda in Action and I find this book is quite a good material to learn AWS Lambda.\nCreating role and policy Before creating a role, it is necessary to create (user-based) policies so that the Lambda function can be given permissions to relevant AWS services. For the POC application, the function needs to download the model object (admission.rds) from S3 and to log messages to Amazon CloudWatch. For the former, a tailored policy (ServerlessPOC) is created while an AWS managed policy (AWSLambdaBasicExecutionRole) is used for the latter. In a policy, permissions are defined in statements and the main elements of statments are shown below.\nEffect - Allow or deny Action - What actions are in effect? Resource - On which resources? Principal - Who is allowed or denied access to a resource? (relevant to trust policies) ServerlessPOC is read-only permission to all keys in the serverless-poc-models bucket while AWSLambdaBasicExecutionRole is write permission to all resources in Amazon CloudWatch.\n1//ServerlessPOC 2{ 3 \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, 4 \u0026#34;Statement\u0026#34;: [ 5 { 6 \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, 7 \u0026#34;Action\u0026#34;: [ 8 \u0026#34;s3:GetObject\u0026#34; 9 ], 10 \u0026#34;Resource\u0026#34;: [ 11 \u0026#34;arn:aws:s3:::serverless-poc-models/*\u0026#34; 12 ] 13 } 14 ] 15} 16 17//AWSLambdaBasicExecutionRole 18{ 19 \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, 20 \u0026#34;Statement\u0026#34;: [ 21 { 22 \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, 23 \u0026#34;Action\u0026#34;: [ 24 \u0026#34;logs:CreateLogGroup\u0026#34;, 25 \u0026#34;logs:CreateLogStream\u0026#34;, 26 \u0026#34;logs:PutLogEvents\u0026#34; 27 ], 28 \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; 29 } 30 ] 31} Only ServerlessPOC needs to be created and it is created in AWS Console. In the Policies section of IAM, the last option (Create Your Own Policy) is selected as it is quite a simple policy.\nThen the name, description and policy document is completed - it is possible to validate the policy document by clicking the Validate Policy button.\nNow it is ready to create a role for the Lambda function while attaching the policies described above. A role can be created in the Roles section of IAM.\nStep 1 : Set Role Name Step 2 : Select Role Type Step 3 : Establish Trust Step 4 : Attach Policy Step 5 : Review Step 1 : Set Role Name This is simply setting the name of the role.\nStep 2 : Select Role Type AWS Lambda is selected in the AWS Service Roles group.\nStep 3 : Establish Trust This step passes automatically while the following trust policy is created, which allows to assume a role for AWS Lambda.\n1// Trust Relationship 2{ 3 \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, 4 \u0026#34;Statement\u0026#34;: [ 5 { 6 \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, 7 \u0026#34;Principal\u0026#34;: { 8 \u0026#34;Service\u0026#34;: \u0026#34;lambda.amazonaws.com\u0026#34; 9 }, 10 \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRole\u0026#34; 11 } 12 ] 13} Step 4 : Attach Policy Here ServerlessPOC and AWSLambdaBasicExecutionRole are attached to the role - it is possible to select multiple policies by checking tick boxes.\nStep 5 : Review After reviewing, it is possible to create the role.\nNow the role (ServerlessPOC) can be seen in the Roles section of IAM. The user-based and trust policies are found in the Permissions and Trust Relationships tabs respectively.\nDeployment Select blueprint Configure triggers Configure function Review Select blueprint Blueprints are sample configurations of event sources and Lambda functions. Blank Function is selected for the Lambda function of the POC application.\nConfigure triggers A Lambda function can be triggered by another AWS service and clicking the dashed box populates available services. No trigger is selected for the Lambda function of the POC application. Note that triggering a Lambda function by changes in another service is useful because an application can be structured in an event-driven way and there is no need to control everything in a big program/script.\nConfigure function Now it is time to configure the function. The function\u0026rsquo;s name (ServerlessPOCAdmission) and description are filled in followed by selecting the runtime - Python 2.7 is the only supported version of Python at the moment. Then code for the function needs to be provided and the following 3 options are avaialble.\nEdit code inline Upload a .ZIP file Upload a file from Amazon S3 As the deployment package (admission.zip) is copied to S3 already, the last option is selected and its S3 link URL is filled in.\nThe value of handler is set as handler.lambda_handler as the Lambda function handler is in handler.py and named as lambda_handler. Then the role named ServerlessPOC is chosen after selecting the value of role to be Choose an existing role among the options listed below.\nChoose an existing role Create new role from template(s) Create a custom role For memory and timeout, 128 MB and 6 seconds are chosen respectively. Note that they can be modified and the timeout is set back to the default 3 seconds while testing.\nThe remaining settings are left as they are.\nAfter clicking the Next button, it is possible to review and create the function.\nTesting and invoke API Testing Testing a Lambda function can be done on the Console. When clicking the Test button, an editor pops up and it is possible to enter an event. For the Lambda function, gre, gpa and rank are set to be \u0026ldquo;800\u0026rdquo;, \u0026ldquo;4\u0026rdquo; and \u0026ldquo;1\u0026rdquo; respectively. Testing can be done by clicking the Save and Test button at the bottom - not shown in the screen shot.\nThe testing output includes Execution result, Summary and Log output. With the event values specified earlier, it returns true. Note that the Python dictionary output of the hander function is coverted to JSON. Also an event specified as JSON is automatically converted to Python dictionary.\nAs shown in the summary, it took 658.93 ms to complete. If testing is made multiple times without delay, the duration decreases quite significantly to less than 200 ms thanks to container reuse in AWS Lambda - recall that the model object is not downloaded if it exists in /tmp. If latency is an issue, it is possible to call the function in every, let say, 5 minutes. As mentioned eariler, a Lambda function can be triggered by another AWS service and, if an Amazon CloudWatch event is set, the function call can be scheduled.\nInvoke API AWS Lambda provides the invoke API so that a Lambda function can be called programmatically as shown below.\n1$ aws lambda invoke --function-name ServerlessPOCAdmission \\ 2 --payload \u0026#39;{\u0026#34;gre\u0026#34;:\u0026#34;800\u0026#34;, \u0026#34;gpa\u0026#34;:\u0026#34;4\u0026#34;, \u0026#34;rank\u0026#34;:\u0026#34;1\u0026#34;}\u0026#39; output.txt 3{ 4 \u0026#34;StatusCode\u0026#34;: 200 5} 6 7$ cat output.txt | grep result 8{\u0026#34;result\u0026#34;: true} This is all for the part II. The POC application is successfully deployed at AWS Lambda and now it is ready to expose it via Amazon API Gateway. This will be discussed in the next post. I hope you enjoy reading this post.\n","date":"April 11, 2017","img":"/blog/2017-04-11-serverless-data-product-2/featured.png","lang":"en","langName":"English","largeImg":"/blog/2017-04-11-serverless-data-product-2/featured_huf27bf8f64e529c91b4fbc544b4ca4599_139725_500x0_resize_box_3.png","permalink":"/blog/2017-04-11-serverless-data-product-2/","series":[{"title":"Serverless Data Product","url":"/series/serverless-data-product/"}],"smallImg":"/blog/2017-04-11-serverless-data-product-2/featured_huf27bf8f64e529c91b4fbc544b4ca4599_139725_180x0_resize_box_3.png","tags":[{"title":"AWS","url":"/tags/aws/"},{"title":"AWS Lambda","url":"/tags/aws-lambda/"},{"title":"Amazon API Gateway","url":"/tags/amazon-api-gateway/"},{"title":"R","url":"/tags/r/"},{"title":"Python","url":"/tags/python/"}],"timestamp":1491868800,"title":"Serverless Data Product POC Backend Part II - Deploying R ML Model via Lambda"},{"categories":[{"title":"Development","url":"/categories/development/"}],"content":"Let say you\u0026rsquo;ve got a prediction model built in R and you\u0026rsquo;d like to productionize it, for example, by serving it in a web application. One way is exposing the model through an API that returns the predicted result as a web service. However there are many issues. Firstly R is not a language for API development although there may be some ways - eg the plumber package. More importantly developing an API is not the end of the story as the API can\u0026rsquo;t be served in a production system if it is not deployed/managed/upgraded/patched/\u0026hellip; appropriately in a server or if it is not scalable, protected via authentication/authorization and so on. Therefore it requires quite a vast range of skill sets that cover both development and DevOps (engineering).\nA developer can be relieved from the overwhelming DevOps stuff if his/her model is deployed in a serverless environment that is provided by cloud computing companies - Amazon Web Service, Microsoft Azure, Google Cloud Platform and IBM OpenWhisk. They provide FaaS (Function as a Service) and, simply put, it allows to run code on demand without provisioning or managing servers. Furthermore an application can be developed/managed in a more efficient way if the workflow is streamlined by events. Let say the model has to be updated periodically. It requires to save new raw data into a place, to export it to a database, to manipulate and save it back to another place for modelling\u0026hellip; This kind of workflow can be efficiently managed by events where a function is configured to subscribe a specific event and its code is run accordingly. In this regards, I find there is a huge potential for serverless event-driven architecture in data product development.\nThis is the first post of Serverless Data Product POC series and I\u0026rsquo;m planning to introduce a data product in a serverless environment. For the backend, a simple logistic regression model is packaged and tested for AWS Lambda - R is not included in Lambda runtime so that it is packaged and run via the Python rpy2 package. Then the model is deployed at AWS Lambda and the Lambda function is exposed via Amazon API Gateway. For the frontend, a simple single page application is served from Amazon S3.\nBackend Part I - Packaging R ML Model for Lambda - this post Part II - Deploying R ML Model via Lambda Part III - Exposing R ML Model via APIG Frontend Part IV - Serving R ML Model via S3 [EDIT 2017-04-11] Deploying at AWS Lambda and exposing via API Gateway are split into 2 posts (Part II and III).\n[EDIT 2017-04-17] The Lambda function hander (handler.py) has been modified to resolve an issue of Cross-Origin Resource Sharing (CORS). See Part IV for further details.\nModel The data is from the LOGIT REGRESSION - R DATA ANALYSIS EXAMPLES of UCLA: Statistical Consulting Group. It is hypothetical data about graduate school admission and has 3 featues (gre, gpa, rank) and 1 binary response (admit).\n1data \u0026lt;- read.csv(\u0026#34;http://www.ats.ucla.edu/stat/data/binary.csv\u0026#34;) 2data$rank \u0026lt;- as.factor(data$rank) 3summary(data) 1## admit gre gpa rank 2## Min. :0.0000 Min. :220.0 Min. :2.260 1: 61 3## 1st Qu.:0.0000 1st Qu.:520.0 1st Qu.:3.130 2:151 4## Median :0.0000 Median :580.0 Median :3.395 3:121 5## Mean :0.3175 Mean :587.7 Mean :3.390 4: 67 6## 3rd Qu.:1.0000 3rd Qu.:660.0 3rd Qu.:3.670 7## Max. :1.0000 Max. :800.0 Max. :4.000 GLM is fit to the data and the fitted object is saved as admission.rds. The choice of logistic regression is because it is included in the stats package, which is one of the default packages, and I\u0026rsquo;d like to have R as small as possible for this POC application. Note that AWS Lambda has limits in deployment package size (50MB compressed) so that it is important to keep a deployment package small - see AWS Lambda Limits for further details. Then the saved file is uploaded to S3 to a bucket named serverless-poc-models - the Lambda function handler will use this object for prediction as described in the next section.\n1fit \u0026lt;- glm(admit ~ ., data = data, family = \u0026#34;binomial\u0026#34;) 2saveRDS(fit, \u0026#34;admission.rds\u0026#34;) Note that, if data is transformed for better performance, a model object alone may not be sufficient as transformed records are necessary as well. A way to handle this situation is using the caret package. The package has preProcess() and associating predict() so that a separate object can be created to transform records for prediction - see this page for further details.\nLambda function handler Lambda function handler is a function that AWS Lambda can invoke when the service executes the code. In this example, it downloads the model objects from S3, predicts admission status and returns the result - handler.py and test_handler.py can be found in the GitHub repository.\nThis and the next sections are based on the following posts with necessary modifications.\nAnalyzing Genomics Data at Scale using R, AWS Lambda, and Amazon API Gateway Run ML predictions with R on AWS Lambda handler.py begins with importing packages and setting-up environment variables. The above posts indicate C shared libraries of R must be loaded. When I tested the handler while uncommenting the for-loop of loading those libraries, however, I encountered the following error - OSError: lib/libRrefblas.so: undefined symbol: xerbla_. It is only when the for-loop is commented out that the script runs through to the handler. I guess the necessary C shared libraries are loaded via Lambda environment variables although I\u0026rsquo;m not sure why manual loading creates such an error. According to Lambda Execution Environment and Available Libraries, the following environment variables are available.\nLAMBDA_TASK_ROOT - Contains the path to your Lambda function code. LD_LIBRARY_PATH - Contains /lib64, /usr/lib64, LAMBDA_TASK_ROOT, LAMBDA_TASK_ROOT/lib. Used to store helper libraries and function code. As can be seen in the next section, the shared libraries are saved in LAMBDA_TASK_ROOT/lib so that they are loaded appropriately.\n1import ctypes 2import json 3import os 4import boto3 5import logging 6 7# use python logging module to log to CloudWatch 8# http://docs.aws.amazon.com/lambda/latest/dg/python-logging.html 9logging.getLogger().setLevel(logging.DEBUG) 10 11################### load R 12# must load all shared libraries and set the 13# R environment variables before you can import rpy2 14# load R shared libraries from lib dir 15 16# for file in os.listdir(\u0026#39;lib\u0026#39;): 17# if os.path.isfile(os.path.join(\u0026#39;lib\u0026#39;, file)): 18# ctypes.cdll.LoadLibrary(os.path.join(\u0026#39;lib\u0026#39;, file)) 19# 20# # set R environment variables 21os.environ[\u0026#34;R_HOME\u0026#34;] = os.getcwd() 22os.environ[\u0026#34;R_LIBS\u0026#34;] = os.path.join(os.getcwd(), \u0026#39;site-library\u0026#39;) 23 24# windows only 25# os.environ[\u0026#34;R_USER\u0026#34;] = r\u0026#39;C:\\Users\\jaehyeon\u0026#39; 26 27import rpy2 28from rpy2 import robjects 29from rpy2.robjects import r 30################## end of loading R Then 3 functions are defined as following.\nget_file_path - Given a S3 object key, it returns a file name or file path. Note that only /tmp has write-access so that a file should be downloaded to this folder download_file - Given bucket and key names, it downloads the S3 object having the key (eg admission.rds). Note that it does nothing if the object file already exists pred_admit - Given gre, gpa and rank, it returns True or False depending on the predicted probablity 1BUCKET = \u0026#39;serverless-poc-models\u0026#39; 2KEY = \u0026#39;admission.rds\u0026#39; 3s3 = boto3.client(\u0026#39;s3\u0026#39;) 4 5def get_file_path(key, name_only=True): 6 file_name = key.split(\u0026#39;/\u0026#39;)[len(key.split(\u0026#39;/\u0026#39;))-1] 7 if name_only: 8 return file_name 9 else: 10 return \u0026#39;/tmp/\u0026#39; + file_name 11 12def download_file(bucket, key): 13 # caching strategies used to avoid the download of the model file every time from S3 14 file_name = get_file_path(key, name_only=True) 15 file_path = get_file_path(key, name_only=False) 16 if os.path.isfile(file_path): 17 logging.debug(\u0026#39;{} already downloaded\u0026#39;.format(file_name)) 18 return 19 else: 20 logging.debug(\u0026#39;attempt to download model object to {}\u0026#39;.format(file_path)) 21 try: 22 s3.download_file(bucket, key, file_path) 23 except Exception as e: 24 logging.error(\u0026#39;error downloading key {} from bucket {}\u0026#39;.format(key, bucket)) 25 logging.error(e) 26 raise e 27 28def pred_admit(gre, gpa, rnk, bucket=BUCKET, key=KEY): 29 download_file(bucket, key) 30 r.assign(\u0026#39;gre\u0026#39;, gre) 31 r.assign(\u0026#39;gpa\u0026#39;, gpa) 32 r.assign(\u0026#39;rank\u0026#39;, rnk) 33 mod_path = get_file_path(key, name_only=False) 34 r(\u0026#39;fit \u0026lt;- readRDS(\u0026#34;{}\u0026#34;)\u0026#39;.format(mod_path)) 35 r(\u0026#39;newdata \u0026lt;- data.frame(gre=as.numeric(gre),gpa=as.numeric(gpa),rank=rank)\u0026#39;) 36 r(\u0026#39;pred \u0026lt;- predict(fit, newdata=newdata, type=\u0026#34;response\u0026#34;)\u0026#39;) 37 return robjects.r(\u0026#39;pred\u0026#39;)[0] \u0026gt; 0.5 This is the Lambda function handler for this POC application. It returns a prediction result if there is no error. 400 HTTP error will be returned if there is an error.\n1def lambda_handler(event, context): 2 try: 3 gre = event[\u0026#34;gre\u0026#34;] 4 gpa = event[\u0026#34;gpa\u0026#34;] 5 rnk = event[\u0026#34;rank\u0026#34;] 6 can_be_admitted = pred_admit(gre, gpa, rnk) 7 res = {\u0026#34;result\u0026#34;: can_be_admitted} 8 return res 9 except Exception as e: 10 logging.error(\u0026#39;Payload: {0}\u0026#39;.format(event)) 11 logging.error(\u0026#39;Error: {0}\u0026#39;.format(e.message)) 12 err = { 13 \u0026#39;errorType\u0026#39;: type(e).__name__, 14 \u0026#39;httpStatus\u0026#39;: 400, 15 \u0026#39;request_id\u0026#39;: context.aws_request_id, 16 \u0026#39;message\u0026#39;: e.message.replace(\u0026#39;\\n\u0026#39;, \u0026#39; \u0026#39;) 17 } 18 raise Exception(json.dumps(err)) Optionally code of the test handler is shown below.\n1import unittest 2import handler 3 4class AdmitHandlerTest(unittest.TestCase): 5 def test_admit(self): 6 gre = \u0026#39;800\u0026#39; 7 gpa = \u0026#39;4\u0026#39; 8 ranks = {\u0026#39;1\u0026#39;: True, \u0026#39;4\u0026#39;: False} 9 for rnk in ranks.keys(): 10 self.assertEqual(handler.pred_admit(gre, gpa, rnk), ranks.get(rnk)) 11 12if __name__ == \u0026#34;__main__\u0026#34;: 13 unittest.main() Packaging According to Lambda Execution Environment and Available Libraries, Lambda functions run in AMI name: amzn-ami-hvm-2016.03.3.x86_64-gp2. A t2.medium EC2 instance is used from this AMI to create the Lambda deployment package. In order to use R in AWS Lambda, R, some of its C shared libraries, the Lambda function handler (handler.py) and the handler\u0026rsquo;s dependent packages should be included in a zip deployment package file.\nPreparation In this step, R and necessary libraries are installed followed by cloning the project repository. The package folder is created as $HOME/$HANDLER (i.e. /home/ec2-user/handler). The subfolder $HOME/$HANDLER/library is to copy necessary R default packages separately - remind that I\u0026rsquo;d like to have R as small as possible.\n1sudo yum -y update 2sudo yum -y upgrade 3 4# readline for rpy2 and fortran for R 5sudo yum install -y python27-devel python27-pip gcc gcc-c++ readline-devel libgfortran.x86_64 R.x86_64 6 7# install Git and clone repository 8sudo yum install -y git 9git clone https://github.com/jaehyeon-kim/serverless-poc.git 10 11# create folder to R and lambda handler 12# note R packages will be copied to $HOME/$HANDLER/library separately 13export HANDLER=handler 14mkdir -p $HOME/$HANDLER/library Copy R and shared libraries Firstly all files and folders in /usr/lib64/R except for library are copyed to the Lambda package folder. By default 29 packages are installed as can be seen in /usr/lib64/R/library but not all them are necessary. Actually only the 7 packages listed below are loaded at startup and 1 package is required additionally by the rpy2 package. Therefore only the 8 default R packages are copyed to $HOME/$HANDLER/library/.\nLoaded at startup - stats, graphics, grDevices, utils, datasets, methods, base Required by rpy2 - tools In relation to C shared libraries, the default installation includes 4 libraries as can be checked in /usr/lib64/R/lib and 1 library is required additionally by the rpy2 package - this additional library is for regex processing.\nDefault C shared libraries - libRblas.so, libRlapack.so, libRrefblas.so, libR.so Required by rpy2 - libtre.so.5 Together with the above 5 shared libraries, the following 3 libraries are added: libgomp.so.1, libgfortran.so.3 and libquadmath.so.0. Further investigation is necessary to what extent these are used. Note that the above posts inclue 2 libraries for linear algebra (libblas.so.3 and liblapack.so.3) but they are not added as equivalent libraries seem to exist - I guess they are necessary if R is build from source with the following options: \u0026ndash;with-blas and \u0026ndash;with-lapack. A total of 8 C shared libraries are added to the deployment package.\n1# copy R except for packages - minimum R packages are copied separately 2ls /usr/lib64/R | grep -v library | xargs -I \u0026#39;{}\u0026#39; cp -vr /usr/lib64/R/\u0026#39;{}\u0026#39; $HOME/$HANDLER/ 3 4# copy minimal default libraries 5# loaded at R startup - stats, graphics, grDevices, utils, datasets, methods and base 6# needed for Rpy2 - tools 7ls /usr/lib64/R/library | grep \u0026#39;stats$\\|graphics\\|grDevices\\|utils\\|datasets\\|methods\\|base\\|^tools\u0026#39; | \\ 8\txargs -I \u0026#39;{}\u0026#39; cp -vr /usr/lib64/R/library/\u0026#39;{}\u0026#39; $HOME/$HANDLER/library/ 9 10# copy shared libraries 11ldd /usr/lib64/R/bin/exec/R | grep \u0026#34;=\u0026gt; /\u0026#34; | awk \u0026#39;{print $3}\u0026#39; | \\ 12\tgrep \u0026#39;libgomp.so.1\\|libgfortran.so.3\\|libquadmath.so.0\\|libtre.so.5\u0026#39; | \\ 13\txargs -I \u0026#39;{}\u0026#39; cp -v \u0026#39;{}\u0026#39; $HOME/$HANDLER/lib/ Install rpy2 and copy to Lamdba package folder Python virtualenv is used to install the rpy2 package. The idea is straightforward but actually it was a bit tricky as the rpy2 and its dependent packages can be found in either site-packages or dist-packages folder even in a single EC2 instance - the AWS Doc doesn\u0026rsquo;t explain clearly. pip install rpy2 -t folder-path was tricky as well because the rpy2 package was not installed sometimes while its dependent packages were installed. One way to check is executing pip list in the virtualenv and, if the rpy2 package is not shown, it is in dist-packages.\n1virtualenv ~/env \u0026amp;\u0026amp; source ~/env/bin/activate 2pip install rpy2 3# either in site-packages or dist-packages 4# http://docs.aws.amazon.com/lambda/latest/dg/lambda-python-how-to-create-deployment-package.html 5export PY_PACK=dist-packages 6cp -vr $VIRTUAL_ENV/lib64/python2.7/$PY_PACK/rpy2* $HOME/$HANDLER 7cp -vr $VIRTUAL_ENV/lib/python2.7/$PY_PACK/singledispatch* $HOME/$HANDLER 8cp -vr $VIRTUAL_ENV/lib/python2.7/$PY_PACK/six* $HOME/$HANDLER 9deactivate Copy handler.py/test_handler.py, compress and copy to S3 bucket handler.py and test_handler.py are copied to the Lambda package folder and all contents in the folder are compressed. Note that handler.py should exist in the root of the compressed file so that it is necessary to run zip in the deployment package folder. The size of admission.zip is about 27MB so that it is good to deploy. Finally the package file is copied to a S3 bucket called serverless-poc-handlers - note that the aws cli should be configured to copy the file to S3.\n1cp -v serverless-poc/poc-logit-handler/*.py $HOME/$HANDLER 2 3cd $HOME/$HANDLER 4zip -r9 $HOME/admission.zip * 5# du -sh ~/admission.zip # check file size 6# 27M /home/ec2-user/admission.zip 7 8# aws s3 mb s3://serverless-poc-handlers # create bucket 9aws s3 cp $HOME/admission.zip s3://serverless-poc-handlers Testing For testing, an EC2 instance without R is necessary so that testing is made in a separate t2.micro instance from the same AMI. Configure the aws-cli and install the boto3 package - the boto3 package is avaialble in Lambda execution environment so that it doesn\u0026rsquo;t need to be added to the deployment package. LD_LIBRARY_PATH is an environment variable that points to the C shared libraries of the Lambda package. After downloading the package and decompressing it, testing can be made by running test_handler.py.\n1# configure aws-cli if necessary 2 3sudo pip install boto3 4export R_HOME=$HOME 5export LD_LIBRARY_PATH=$HOME/lib 6 7aws s3 cp s3://serverless-poc-handlers/admission.zip . 8unzip admission.zip 9 10python ./test_handler.py This is an example testing output where the model object has been downloaded already.\n1[ec2-user@ip-172-31-71-13 ~]$ python ./test_handler.py 2DEBUG:root:admission.rds already downloaded 3DEBUG:root:admission.rds already downloaded 4. 5---------------------------------------------------------------------- 6Ran 1 test in 0.024s 7 8OK This is all that I\u0026rsquo;ve prepared for this post and I hope you don\u0026rsquo;t feel bored. The next posts will be much more interesting as this package will be exposed via an API.\n","date":"April 8, 2017","img":"/blog/2017-04-08-serverless-data-product-1/featured.png","lang":"en","langName":"English","largeImg":"/blog/2017-04-08-serverless-data-product-1/featured_huf27bf8f64e529c91b4fbc544b4ca4599_139725_500x0_resize_box_3.png","permalink":"/blog/2017-04-08-serverless-data-product-1/","series":[{"title":"Serverless Data Product","url":"/series/serverless-data-product/"}],"smallImg":"/blog/2017-04-08-serverless-data-product-1/featured_huf27bf8f64e529c91b4fbc544b4ca4599_139725_180x0_resize_box_3.png","tags":[{"title":"AWS","url":"/tags/aws/"},{"title":"AWS Lambda","url":"/tags/aws-lambda/"},{"title":"Amazon API Gateway","url":"/tags/amazon-api-gateway/"},{"title":"R","url":"/tags/r/"},{"title":"Python","url":"/tags/python/"}],"timestamp":1491609600,"title":"Serverless Data Product POC Backend Part I - Packaging R ML Model for Lambda"},{"categories":[{"title":"Development","url":"/categories/development/"}],"content":"R Shiny applications are served as a single page application and it is not built to render multiple pages. There are benefits of rendering multiple pages such as code management and implement authentication. In this page, we discuss how to implement multi-page rendering in a Shiny app.\nAs indicated above, Shiny is not designed to render multiple pages and, in general, the UI is rendered on the fly as defined in ui.R or app.R. However this is not the only way as the UI can be rendered as a html output using htmlOutput() in ui.R and renderUI() in server.R. In this post, rendering multiple pages will be illustrated using an example application.\nExample application structure A total of 6 pages exist in the application as shown below.\nAt the beginning, the login page is rendered. A user can enter credentials for authentication or move to the register page. User credentials are kept in a SQLite db and the following user information is initialized at each start-up - passwords are encrypted using the bcrypt package.\n1library(bcrypt) 2app_name \u0026lt;- \u0026#34;multipage demo\u0026#34; 3added_ts \u0026lt;- format(Sys.time(), \u0026#34;%Y-%m-%d %H:%M:%S\u0026#34;) 4users \u0026lt;- data.frame(name = c(\u0026#34;admin\u0026#34;, \u0026#34;john.doe\u0026#34;, \u0026#34;jane.doe\u0026#34;), 5 password = unlist(lapply(c(\u0026#34;admin\u0026#34;, \u0026#34;john.doe\u0026#34;, \u0026#34;jane.doe\u0026#34;), hashpw)), 6 app_name = c(\u0026#34;all\u0026#34;, rep(app_name, 2)), 7 added_ts = rep(added_ts, 3), 8 stringsAsFactors = FALSE) 9users 1## name password 2## 1 admin $2a$12$RhUwtbJnr3Uo75npOeE96u1eRGpyQD2tJ38S2lCJ7wtBa.THxMGf2 3## 2 john.doe $2a$12$Svzr/4/Ti5u6YVgx04Cy7OXhar71NgjD.gPpoX3hUJ4Pgd.gN1V.u 4## 3 jane.doe $2a$12$CGAfSfYWP9eOuZxM1njwtOfGR2MlqDbcCeUE.CkXlZvBGHPlSORDW 5## app_name added_ts 6## 1 all 2016-06-10 17:59:39 7## 2 multipage demo 2016-06-10 17:59:39 8## 3 multipage demo 2016-06-10 17:59:39 Note that the authentication plan of this application is for demonstration only. In practice, for instance, LDAP or Active Directory authentication may be considered if it is possible to contact to a directory server - for Active Directory authentication, the radhelper package might be useful.\nIt is assumed that an application key (application-key) should be specified for registration together with user name and password. The register page is shown below.\nOnce logged on, two extra buttons appear: Profile and App. The screenshots of before and after login are shown below.\nThe main purpose of the profile page is to change the password.\nThe application page keeps the main contents of the application. The default Shiny application is used.\nUI elements Each UI elements are constructed in a function and it is set to be rendered using htmlOutput() in ui.R. Actual rendering is made by renderUI() in server.R.\nBelow shows the main login page after removing CSS and Javascript tags.\n1ui_login \u0026lt;- function(...) { 2 args \u0026lt;- list(...) 3 fluidRow( 4 column(3, offset = 4, 5 wellPanel( 6 div(id = \u0026#34;login_link\u0026#34;, 7 actionButton(\u0026#34;login_leave\u0026#34;, \u0026#34;Leave\u0026#34;, icon = icon(\u0026#34;close\u0026#34;), width = \u0026#34;100px\u0026#34;) 8 ), 9 br(), 10 br(), 11 h4(\u0026#34;LOGIN\u0026#34;), 12 textInput(\u0026#34;login_username\u0026#34;, \u0026#34;User name\u0026#34;), 13 div(class = \u0026#34;input_msg\u0026#34;, textOutput(\u0026#34;login_username_msg\u0026#34;)), 14 passwordInput(\u0026#34;login_password\u0026#34;, \u0026#34;Password\u0026#34;), 15 div(class = \u0026#34;input_msg\u0026#34;, textOutput(\u0026#34;login_password_msg\u0026#34;)), 16 actionButton(\u0026#34;login_login\u0026#34;, \u0026#34;Log in\u0026#34;, icon = icon(\u0026#34;sign-in\u0026#34;), width = \u0026#34;100px\u0026#34;), 17 actionButton(\u0026#34;login_register\u0026#34;, \u0026#34;Register\u0026#34;, icon = icon(\u0026#34;user-plus\u0026#34;), width = \u0026#34;100px\u0026#34;), 18 br(), 19 div(class = \u0026#34;input_fail\u0026#34;, textOutput(\u0026#34;login_fail\u0026#34;)), 20 uiOutput(\u0026#34;login_more\u0026#34;) 21 ) 22 ) 23 ) 24} Each UI function has unspecified argument (...) so that some values can be passed from server.R. For example, the logout and application pages include message and username from the server.\nAt the end, UI is set to be rendered as a html output.\n1ui \u0026lt;- (htmlOutput(\u0026#34;page\u0026#34;)) Application logic A page is rendered using render_page() in server.R. This function accepts a UI element function in ui.R and renders a fluid page with some extra values. I didn\u0026rsquo;t have much luck with Shiny Dashboard that the flud page layout is chosen instead.\n1render_page \u0026lt;- function(..., f, title = app_name, theme = shinytheme(\u0026#34;cerulean\u0026#34;)) { 2 page \u0026lt;- f(...) 3 renderUI({ 4 fluidPage(page, title = title, theme = theme) 5 }) 6} 7 8server \u0026lt;- function(input, output, session) { 9 ... 10 11 ## render default login page 12 output$page \u0026lt;- render_page(f = ui_login) 13 14 ... 15} The authentication process shows a tricky part of implementing this setup. Depending on which page is currently rendered, only a part of inputs exist in the current page. In this circumstance, if an input is captured in reactive context such as observe() and reactive() but it doesn\u0026rsquo;t exist in the current page, an error will be thrown. Therefore whether an input exists or not should be checked as seen in the observer below. On the other hand, observeEvent() is free from this error as it works only if the input exists.\n1 user_info \u0026lt;- reactiveValues(is_logged = is_logged) 2 3 # whether an input element exists should be checked 4 observe({ 5 if(!is.null(input$login_login)) { 6 username \u0026lt;- input$login_username 7 password \u0026lt;- input$login_password 8 9 if(username != \u0026#34;\u0026#34;) output$login_username_msg \u0026lt;- renderText(\u0026#34;\u0026#34;) 10 if(password != \u0026#34;\u0026#34;) output$login_password_msg \u0026lt;- renderText(\u0026#34;\u0026#34;) 11 } 12 }) 13 14 observeEvent(input$login_login, { 15 username \u0026lt;- isolate(input$login_username) 16 password \u0026lt;- isolate(input$login_password) 17 18 if(username == \u0026#34;\u0026#34;) output$login_username_msg \u0026lt;- renderText(\u0026#34;Please enter user name\u0026#34;) 19 if(password == \u0026#34;\u0026#34;) output$login_password_msg \u0026lt;- renderText(\u0026#34;Please enter password\u0026#34;) 20 21 if(!any(username == \u0026#34;\u0026#34;, password == \u0026#34;\u0026#34;)) { 22 is_valid_credentials \u0026lt;- check_login_credentials(username = username, password = password, app_name = app_name) 23 if(is_valid_credentials) { 24 user_info$is_logged \u0026lt;- TRUE 25 user_info$username \u0026lt;- username 26 27 output$login_fail \u0026lt;- renderText(\u0026#34;\u0026#34;) 28 29 log_session(username = username, is_in = 1, app_name = app_name) 30 } else { 31 output$login_fail \u0026lt;- renderText(\u0026#34;Login failed, try again or contact admin\u0026#34;) 32 } 33 } 34 }) A try-catch block can also be useful to prevent this type of error due to a missing element. Below the plot of the application page is handled in tryCatch so that the application doesn\u0026rsquo;t stop abruptly with an error although the plot element doesn\u0026rsquo;t exist in the current page.\n1tryCatch({ 2 output$distPlot \u0026lt;- renderPlot({ 3 # generate bins based on input$bins from ui.R 4 x \u0026lt;- faithful[, 2] 5 bins \u0026lt;- seq(min(x), max(x), length.out = input$bins + 1) 6 7 # draw the histogram with the specified number of bins 8 hist(x, breaks = bins, col = \u0026#39;darkgray\u0026#39;, border = \u0026#39;white\u0026#39;) 9 }) 10}) I hope this post is useful.\n","date":"June 27, 2016","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/blog/2016-06-27-shiny-open-source-render-multiple-pages/","series":[],"smallImg":"","tags":[{"title":"R","url":"/tags/r/"},{"title":"Shiny","url":"/tags/shiny/"}],"timestamp":1466985600,"title":"Some Thoughts on Shiny Open Source - Render Multiple Pages"},{"categories":[{"title":"Development","url":"/categories/development/"}],"content":"Shiny is an interesting web framework that helps create a web application quickly. If it targets a large number of users, however, there are several limitations and it is so true when the open source version of Shiny is in use. It would be possible to tackle down some of the limitations with the enterprise version but it is not easy to see enough examples of Shiny applications in production environment. While whether Shiny can be used in production environment is a controversial issue, this series of posts illustrate some ways to use open source Shiny a bit more wisely. Specifically the following topics are going to be covered.\nLoad balancing (and auto scaling) Each application (or folder in /srv/shiny-server) is binded by a single process so that multiple users or sessions are served by the same process. Let say multiple cores exist in the server machine. Then this can be one of the main causes of performance bottleneck as only a single process is reserved for an application. Rendering multiple pages, including authentication An application is served as a single-page web application and thus it is not built to render multiple pages. Application code could be easier to manage if code is split by different pages. Moreover it is highly desirable to implement authentication. Running with a Proxy and SSL configuration for HTTPS By default, an application is served by HTTP with port 3838. A useful use case to serve a Shiny application via HTTPS is it can be integrated with a Tableau dashboard. In this post, a simple way of internal load balancing is demonstrated by redirecting multiple same applications, depending on the number of processes binded to them - this is originally from Huidong Tian\u0026rsquo;s blog.\nFolder structure As an illustration, 5 applications are added to /srv/shiny-server/redirect as shown below. The folders named 1 to 4 are the same application in different folders and the application that redirects a user (or session) to the individual applications is placed in app folder. How many sessions are binded by each application is monitored by monitor.R and the output is recorded in monitor.log.\nProcess monitoring The following script saves records of the number of sessions (users) that belong to each application (app) at the end of each iteration. It begins with filtering processes that is been initiated by shiny user using the top command. Then, for each PID, it extracts a condition if TCP socket is established using the netstat command as well as the corresponding application (folder) using the lsof command.\n1lapply(1:60, function(x) { 2 tops \u0026lt;- system(\u0026#34;top -n 1 -b -u shiny\u0026#34;, intern = TRUE) 3 if(length(tops) \u0026gt; 0) { 4 ids \u0026lt;- grep(\u0026#34;R *$\u0026#34;, tops) 5 header \u0026lt;- grep(\u0026#34;%CPU\u0026#34;, tops) 6 names \u0026lt;- strsplit(gsub(\u0026#34;^ +|%|\\\\+\u0026#34;, \u0026#34;\u0026#34;, tops[header]), \u0026#34; +\u0026#34;)[[1]] 7 8 if(length(ids) \u0026gt; 0) { 9 dat \u0026lt;- as.data.frame(do.call(rbind, strsplit(gsub(\u0026#34;^ *\u0026#34;, \u0026#34;\u0026#34;, tops[ids]), \u0026#34; +\u0026#34;))) 10 names(dat) \u0026lt;- names 11 info \u0026lt;- as.data.frame(do.call(rbind, lapply(dat$PID, function(pid) { 12 netstat \u0026lt;- system(paste(\u0026#34;sudo netstat -p | grep\u0026#34;, pid), intern = TRUE) 13 lsof \u0026lt;- system(paste(\u0026#34;sudo lsof -p\u0026#34;, pid, \u0026#34;| grep /srv\u0026#34;), intern = TRUE) 14 users \u0026lt;- length(grep(\u0026#34;ESTABLISHED\u0026#34;, netstat) \u0026amp; grep(\u0026#34;tcp\u0026#34;, netstat)) 15 app \u0026lt;- regmatches(lsof, regexec(\u0026#34;srv/(.*)\u0026#34;, lsof))[[1]][2] 16 c(app = app, users = users) 17 }))) 18 } else { 19 info \u0026lt;- data.frame(app = \u0026#34;app\u0026#34;, users = 0) 20 } 21 write.table(info, file = \u0026#34;/srv/shiny-server/redirect/monitor.log\u0026#34;) 22 } 23}) The script should be run as root so that all processes are seen. Below shows an example output when two applications are open in a browser.\n1 app users 21 shiny-server/redirect/1 1 32 shiny-server/redirect/2 1 Due to process visibility, the above script cannot be run in a Shiny application and a cron job is created in root (sudo crontab -e). It is executed every minute but a single run of the script completes quite quickly. Therefore it is wrapped in lapply() so that records are saved multiple times until the next run.\n1* * * * * Rscript /srv/shiny-server/redirect/monitor.R Application code The redirect application and associating javascript code are shown below. The application that has the least number of users is selected in the server function (app$app[which.min(app$users)]) and the link to that application is constructed. Then it is added to the input text input and a java script function named setInterval() in redirect.js is triggered.\n1library(shiny) 2library(magrittr) 3library(dplyr) 4 5ui \u0026lt;- fluidPage( 6 fluidRow( 7 #uncomment in practice 8 #tags$style(\u0026#34;#link {visibility: hidden;}\u0026#34;), 9 tags$script(type=\u0026#34;text/javascript\u0026#34;, src = \u0026#34;redirect.js\u0026#34;), 10 column(3, offset = 4, 11 wellPanel( 12 h3(\u0026#34;app info\u0026#34;), 13 tableOutput(\u0026#34;app_info\u0026#34;) 14 ) 15 ) 16 ), 17 fluidRow( 18 column(3, offset = 4, 19 wellPanel( 20 h3(\u0026#34;Redirecting ...\u0026#34;), 21 textInput(inputId = \u0026#34;link\u0026#34;, label = \u0026#34;\u0026#34;, value = \u0026#34;\u0026#34;) 22 ) 23 ) 24 ) 25) 26 27server \u0026lt;- function(input, output, session) { 28 users \u0026lt;- read.table(\u0026#34;/srv/shiny-server/redirect/monitor.log\u0026#34;, header = TRUE, stringsAsFactors = FALSE) 29 app \u0026lt;- data.frame(app = paste0(\u0026#34;shiny-server/redirect/\u0026#34;, 1:4), stringsAsFactors = FALSE) 30 app \u0026lt;- app %\u0026gt;% left_join(users, by = \u0026#34;app\u0026#34;) %\u0026gt;% mutate(app = sub(\u0026#34;shiny-server/\u0026#34;, \u0026#34;\u0026#34;, app), 31 users = ifelse(is.na(users), \u0026#34;0\u0026#34;, as.character(users))) 32 link \u0026lt;- paste0(\u0026#34;hostname-or-ip-address[:port]/\u0026#34;, app$app[which.min(app$users)]) 33 34 # info tables 35 output$app_info \u0026lt;- renderTable(app) 36 37 updateTextInput(session, inputId = \u0026#34;link\u0026#34;, value = link) 38} 39 40shinyApp(ui = ui, server = server) 1# put redirect.js in www folder 2setInterval(function() { 3 var link = document.getElementById(\u0026#39;link\u0026#39;).value; 4 if (link.length \u0026gt; 1) { 5 window.open(link, \u0026#34;_top\u0026#34;) 6 } 7}, 1000) An example of the application is shown below.\nHere is the code for the redirected application.\n1library(shiny) 2 3ui \u0026lt;- fluidPage( 4 fluidRow( 5 column(3, offset = 4, 6 wellPanel( 7 h3(\u0026#34;URL components\u0026#34;), 8 verbatimTextOutput(\u0026#34;urlText\u0026#34;) 9 ) 10 ) 11 ) 12) 13 14server \u0026lt;- function(input, output, session) { 15 output$urlText \u0026lt;- renderText({ 16 paste(sep = \u0026#34;\u0026#34;, 17 \u0026#34;protocol: \u0026#34;, session$clientData$url_protocol, \u0026#34;\\n\u0026#34;, 18 \u0026#34;hostname: \u0026#34;, session$clientData$url_hostname, \u0026#34;\\n\u0026#34;, 19 \u0026#34;pathname: \u0026#34;, session$clientData$url_pathname, \u0026#34;\\n\u0026#34;, 20 \u0026#34;port: \u0026#34;, session$clientData$url_port, \u0026#34;\\n\u0026#34;, 21 \u0026#34;search: \u0026#34;, session$clientData$url_search, \u0026#34;\\n\u0026#34; 22 ) 23 }) 24} 25 26shinyApp(ui = ui, server = server) ","date":"May 23, 2016","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/blog/2016-05-23-shiny-open-source-internal-load-balancing/","series":[],"smallImg":"","tags":[{"title":"R","url":"/tags/r/"},{"title":"Shiny","url":"/tags/shiny/"}],"timestamp":1463961600,"title":"Some Thoughts on Shiny Open Source - Internal Load Balancing"},{"categories":[{"title":"R","url":"/categories/r/"}],"content":"In this post, a way to overcome one of R\u0026rsquo;s limitations (lack of multi-threading) is discussed by job queuing using the jobqueue package - a generic asynchronous job queue implementation for R. See the package description below.\nThe jobqueue package is meant to provide an easy-to-use interface that allows to queue computations for background evaluation while the calling R session remains responsive. It is based on a 1-node socket cluster from the parallel package. The package provides a way to do basic threading in R. The main focus of the package is on an intuitive and easy-to-use interface for the job queue programming construct. \u0026hellip; Typical applications include: background computation of lengthy tasks (such as data sourcing, model fitting, bootstrapping), simple/interactive parallelization (if you have 5 different jobs, move them to up to 5 different job queues), and concurrent task scheduling in more complicated R programs. \u0026hellip;\nAdded to the typical applications indicated above, this package can be quite beneficial with a Shiny application especially when long-running process has to be served.\nThe package is not on CRAN and it can be installed as following.\n1# http://r-forge.r-project.org/R/?group_id=2066 2if(!require(jobqueue)) { 3 pkg_src \u0026lt;- if(grepl(\u0026#34;win\u0026#34;, Sys.info()[\u0026#34;sysname\u0026#34;], ignore.case = TRUE)) { 4 \u0026#34;http://download.r-forge.r-project.org/bin/windows/contrib/3.2/jobqueue_1.0-4.zip\u0026#34; 5 } else { 6 \u0026#34;http://download.r-forge.r-project.org/src/contrib/jobqueue_1.0-4.tar.gz\u0026#34; 7 } 8 9 install.packages(pkg_src, repos = NULL) 10} 11 12library(jobqueue) As can be seen in the description, it is highly related to the parallel package and thus it wouldn\u0026rsquo;t be hard to understand how it works if you know how to do parallel processing using that package - if not, have a look at this post.\nHere is a quick example of job queue. In the following function, execution is suspended for 1 second at each iteration and the processed is blocking until it is executed in base R.\n1fun \u0026lt;- function(max_val) { 2 unlist(lapply(1:max_val, function(x) { 3 Sys.sleep(1) 4 x 5 })) 6} Using the package, however, the function can be executed asynchronously as shown below.\n1# create queue 2# similar to makeCluster() 3queue \u0026lt;- Q.make() 4# send local R object 5# similar to clusterEvalQ() or clusterCall() 6Q.sync(queue, fun) 7# execute function 8# similar to clusterApply() or parLapply() 9Q.push(queue, fun(10)) 10### another job can be done while it is being executed 11# get result - NULL is not complete 12Q.pop(queue) ## NULL 1while (TRUE) { 2 out \u0026lt;- Q.pop(queue) 3 message(paste(\u0026#34;INFO execution not completed?\u0026#34;, is.null(out))) 4 if(!is.null(out)) { 5 break 6 } 7} ## INFO execution not completed? TRUE ## INFO execution not completed? TRUE ## INFO execution not completed? TRUE ## INFO execution not completed? TRUE ## INFO execution not completed? TRUE ## INFO execution not completed? TRUE ## INFO execution not completed? TRUE ## INFO execution not completed? FALSE 1# close queue 2# similar to stopCluster() 3Q.close(queue) 4out ## [1] 1 2 3 4 5 6 7 8 9 10 Another example of applying job queue is fitting a bootstrap-based algorithm. In this example, each of 500 trees are grown and they are combined at the end - note that, in practice, it\u0026rsquo;d be better to save outputs and combine them later.\n1q1 \u0026lt;- Q.make() 2q2 \u0026lt;- Q.make() 3# load library 4Q.push(q1, library(randomForest), mute = TRUE) 5Q.push(q2, library(randomForest), mute = TRUE) 6Q.push(q1, rf \u0026lt;- randomForest(Species ~ ., data=iris, importance=TRUE, proximity=TRUE)) 7Q.push(q2, rf \u0026lt;- randomForest(Species ~ ., data=iris, importance=TRUE, proximity=TRUE)) 8# should be waited until completion in practice 9r1 \u0026lt;- Q.pop(q1) 10r2 \u0026lt;- Q.pop(q2) 11Q.close(q1) 12Q.close(q2) 13 14library(randomForest) 15do.call(\u0026#34;combine\u0026#34;, list(r1, r2)) ## ## Call: ## randomForest(formula = Species ~ ., data = iris, importance = TRUE, proximity = TRUE) ## Type of random forest: classification ## Number of trees: 1000 ## No. of variables tried at each split: 2 I hope this article is useful.\n","date":"May 12, 2016","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/blog/2016-05-12-asynchronous-processing-using-job-queue/","series":[],"smallImg":"","tags":[{"title":"R","url":"/tags/r/"}],"timestamp":1463011200,"title":"Asynchronous Processing Using Job Queue"},{"categories":[{"title":"R","url":"/categories/r/"}],"content":"In the previous post, it is demonstrated how to start SparkR in local and cluster mode. While SparkR is in active development, it is yet to fully support Spark\u0026rsquo;s key libraries such as MLlib and Spark Streaming. Even, as a data processing engine, this R API is still limited as it is not possible to manipulate RDDs directly but only via Spark SQL/DataFrame API. As can be checked in the API doc, SparkR rebuilds many existing R functions to work with Spark DataFrame and notably it borrows some functions from the dplyr package. Also there are some alien functions (eg from_utc_timestamp()) and many of them are from Hive Query Language (HiveQL). In relation to those functions from HiveQL, although some Hive user defined functions (UDFs) are ported, still many useful UDFs and Window functions don\u0026rsquo;t exist.\nIn this circumstances, I consider one option to boost SparkR\u0026rsquo;s performance as a data processing engine is manipulating data in Hive Context rather than in limited SQL Context. There is good and bad news. The good one is existing Hive installation is not necessary to setup Hive Context and the other one is Spark has to be built from source with Hive. In this post, several examples of using Hive UDFs and Window functions are demonstrated, comparing to the dplyr package. Also a summary of Spark build with Hive is discussed.\nSparkR in Hive Context I tried in local mode on my Windows machine and here is how SparkR context (sc) is created - for details, see this post. Here the key difference is spark_home - this is where my pre-built Spark with Hive locates.\n1#### set up environment variables 2base_path \u0026lt;- getwd() 3## SPARK_HOME 4#spark_home \u0026lt;- paste0(base_path, \u0026#39;/spark\u0026#39;) 5spark_home \u0026lt;- paste0(base_path, \u0026#39;/spark-1.6.0-bin-spark-1.6.0-bin-hadoop2.4-hive-yarn\u0026#39;) 6Sys.setenv(SPARK_HOME = spark_home) 7## $SPARK_HOME/bin to PATH 8spark_bin \u0026lt;- paste0(spark_home, \u0026#39;/bin\u0026#39;) 9Sys.setenv(PATH = paste(Sys.getenv(c(\u0026#39;PATH\u0026#39;)), spark_bin, sep=\u0026#39;:\u0026#39;)) 10## HADOOP_HOME 11hadoop_home \u0026lt;- paste0(spark_home, \u0026#39;/hadoop\u0026#39;) # hadoop-common missing on Windows 12Sys.setenv(HADOOP_HOME = hadoop_home) # hadoop-common missing on Windows 13 14#### extra driver jar to be passed 15postgresql_drv \u0026lt;- paste0(getwd(), \u0026#39;/postgresql-9.3-1103.jdbc3.jar\u0026#39;) 16 17#### add SparkR to search path 18sparkr_lib \u0026lt;- paste0(spark_home, \u0026#39;/R/lib\u0026#39;) 19.libPaths(c(.libPaths(), sparkr_lib)) 20 21#### specify master host name or localhost 22spark_link \u0026lt;- \u0026#34;local[*]\u0026#34; 23 24library(magrittr) 25library(dplyr) 26library(SparkR) 27 28## include spark-csv package 29Sys.setenv(\u0026#39;SPARKR_SUBMIT_ARGS\u0026#39;=\u0026#39;\u0026#34;--packages\u0026#34; \u0026#34;com.databricks:spark-csv_2.10:1.3.0\u0026#34; \u0026#34;sparkr-shell\u0026#34;\u0026#39;) 30 31sc \u0026lt;- sparkR.init(master = spark_link, 32 sparkEnvir = list(spark.driver.extraClassPath = postgresql_drv), 33 sparkJars = postgresql_drv) ## Launching java with spark-submit command C:/workspace/sparkr-test/spark-1.6.0-bin-spark-1.6.0-bin-hadoop2.4-hive-yarn/bin/spark-submit.cmd --jars C:\\workspace\\sparkr-test\\postgresql-9.3-1103.jdbc3.jar --driver-class-path \u0026#34;C:/workspace/sparkr-test/postgresql-9.3-1103.jdbc3.jar\u0026#34; \u0026#34;--packages\u0026#34; \u0026#34;com.databricks:spark-csv_2.10:1.3.0\u0026#34; \u0026#34;sparkr-shell\u0026#34; C:\\Users\\jaehyeon\\AppData\\Local\\Temp\\Rtmp2HKUIN\\backend_port295475933bd8 I set up both SQL and Hive contexts for comparison.\n1sqlContext \u0026lt;- sparkRSQL.init(sc) 2sqlContext ## Java ref type org.apache.spark.sql.SQLContext id 1 1hiveContext \u0026lt;- sparkRHive.init(sc) 2hiveContext ## Java ref type org.apache.spark.sql.hive.HiveContext id 4 A synthetic sales data is used in the examples.\n1sales \u0026lt;- data.frame(dealer = c(rep(\u0026#34;xyz\u0026#34;, 9), \u0026#34;abc\u0026#34;), 2 make = c(\u0026#34;highlander\u0026#34;, rep(\u0026#34;prius\u0026#34;, 3), rep(\u0026#34;versa\u0026#34;, 3), \u0026#34;s3\u0026#34;, \u0026#34;s3\u0026#34;, \u0026#34;forrester\u0026#34;), 3 type = c(\u0026#34;suv\u0026#34;, rep(\u0026#34;hatch\u0026#34;, 6), \u0026#34;sedan\u0026#34;, \u0026#34;sedan\u0026#34;, \u0026#34;suv\u0026#34;), 4 day = c(0:3, 1:3, 1:2, 1), stringsAsFactors = FALSE) 5sales ## dealer make type day ## 1 xyz highlander suv 0 ## 2 xyz prius hatch 1 ## 3 xyz prius hatch 2 ## 4 xyz prius hatch 3 ## 5 xyz versa hatch 1 ## 6 xyz versa hatch 2 ## 7 xyz versa hatch 3 ## 8 xyz s3 sedan 1 ## 9 xyz s3 sedan 2 ## 10 abc forrester suv 1 Basic data manipulation The first example is a basic data manipulation, which counts the number of records per dealer, make and type.\n1sales_s \u0026lt;- createDataFrame(sqlContext, sales) 2 3sales_s %\u0026gt;% 4 select(sales_s$dealer, sales_s$make, sales_s$type) %\u0026gt;% 5 group_by(sales_s$dealer, sales_s$make, sales_s$type) %\u0026gt;% 6 summarize(count = count(sales_s$dealer)) %\u0026gt;% 7 arrange(sales_s$dealer, sales_s$make) %\u0026gt;% head() ## dealer make type count ## 1 abc forrester suv 1 ## 2 xyz highlander suv 1 ## 3 xyz prius hatch 3 ## 4 xyz s3 sedan 2 ## 5 xyz versa hatch 3 This kind of manipulation also works in Spark SQL after registering the RDD as a temporary table. Here another Spark DataFrame (sales_h) is created using Hive Context and the equivalent query is applied - this works in both SQL and Hive context.\n1sales_h \u0026lt;- createDataFrame(hiveContext, sales) 2registerTempTable(sales_h, \u0026#34;sales_h\u0026#34;) 3 4qry_h1 \u0026lt;- \u0026#34;SELECT dealer, make, type, count(*) AS count FROM sales_h GROUP BY dealer, type, make ORDER BY dealer, make\u0026#34; 5sql(hiveContext, qry_h1) %\u0026gt;% head() ## dealer make type count ## 1 abc forrester suv 1 ## 2 xyz highlander suv 1 ## 3 xyz prius hatch 3 ## 4 xyz s3 sedan 2 ## 5 xyz versa hatch 3 Window function example Window functions can be useful as data can be summarized by partition but they are not supported in SQL context. Here is an example of adding rank by the number of records per group, followed by dplyr equivalent. Note that some functions in the dplyr package are masked by the SparkR package so that their name space (dplyr) is indicated where appropriate.\n1qry_h2 \u0026lt;- \u0026#34; 2SELECT dealer, make, type, rank() OVER (PARTITION BY dealer ORDER BY make, count DESC) AS rank FROM ( 3 SELECT dealer, make, type, count(*) AS count FROM sales_h GROUP BY dealer, type, make 4) t\u0026#34; 5sql(hiveContext, qry_h2) %\u0026gt;% head() ## dealer make type rank ## 1 abc forrester suv 1 ## 2 xyz highlander suv 1 ## 3 xyz prius hatch 2 ## 4 xyz s3 sedan 3 ## 5 xyz versa hatch 4 1sales %\u0026gt;% dplyr::select(dealer, make, type) %\u0026gt;% 2 dplyr::group_by(dealer, type, make) %\u0026gt;% 3 dplyr::mutate(count = n()) %\u0026gt;% 4 dplyr::distinct(dealer, make, type) %\u0026gt;% 5 dplyr::arrange(dealer, -count) %\u0026gt;% 6 dplyr::ungroup() %\u0026gt;% 7 dplyr::arrange(dealer, make) %\u0026gt;% 8 dplyr::group_by(dealer) %\u0026gt;% 9 dplyr::mutate(rank = row_number()) %\u0026gt;% 10 dplyr::select(-count) ## Source: local data frame [5 x 4] ## Groups: dealer [2] ## ## dealer make type rank ## (chr) (chr) (chr) (int) ## 1 abc forrester suv 1 ## 2 xyz highlander suv 1 ## 3 xyz prius hatch 2 ## 4 xyz s3 sedan 3 ## 5 xyz versa hatch 4 The next window function example is adding cumulative counts per dealer and make.\n1qry_h3 \u0026lt;- \u0026#34;SELECT dealer, make, count, SUM(count) OVER (PARTITION BY dealer ORDER BY dealer, make) as cumsum FROM ( 2 SELECT dealer, make, count(*) AS count FROM sales_h GROUP BY dealer, make 3) t\u0026#34; 4sql(hiveContext, qry_h3) %\u0026gt;% head() ## dealer make count cumsum ## 1 abc forrester 1 1 ## 2 xyz highlander 1 1 ## 3 xyz prius 3 4 ## 4 xyz s3 2 6 ## 5 xyz versa 3 9 1sales %\u0026gt;% dplyr::select(dealer, make) %\u0026gt;% 2 dplyr::group_by(dealer, make) %\u0026gt;% 3 dplyr::mutate(count = n()) %\u0026gt;% 4 dplyr::distinct(dealer, make, count) %\u0026gt;% 5 dplyr::arrange(dealer, make) %\u0026gt;% 6 dplyr::ungroup() %\u0026gt;% 7 dplyr::group_by(dealer) %\u0026gt;% 8 dplyr::mutate(cumsum = cumsum(count)) ## Source: local data frame [5 x 4] ## Groups: dealer [2] ## ## dealer make count cumsum ## (chr) (chr) (int) (int) ## 1 abc forrester 1 1 ## 2 xyz highlander 1 1 ## 3 xyz prius 3 4 ## 4 xyz s3 2 6 ## 5 xyz versa 3 9 UDF example There are lots of useful UDFs in HiveQL and many of them are currently missing in the SparkR package. Here collect_list() is used for illustration where sales paths per dealer and type are created - for further details of this and other functions, see this language manual.\n1qry_h4 \u0026lt;- \u0026#34;SELECT dealer, type, concat_ws(\u0026#39; \u0026gt; \u0026#39;, collect_list(make)) AS sales_order FROM ( 2 SELECT dealer, day, type, make FROM sales_h ORDER BY dealer, type, day 3) t GROUP BY dealer, type ORDER BY dealer, type 4\u0026#34; 5sql(hiveContext, qry_h4) %\u0026gt;% head() ## dealer type sales_order ## 1 abc suv forrester ## 2 xyz hatch prius \u0026gt; versa \u0026gt; prius \u0026gt; versa \u0026gt; prius \u0026gt; versa ## 3 xyz sedan s3 \u0026gt; s3 ## 4 xyz suv highlander 1sales %\u0026gt;% dplyr::arrange(dealer, type, day) %\u0026gt;% 2 dplyr::group_by(dealer, type) %\u0026gt;% 3 dplyr::summarise(sales_order = paste(make, collapse = \u0026#34; \u0026gt; \u0026#34;)) %\u0026gt;% 4 dplyr::arrange(dealer, type) ## Source: local data frame [4 x 3] ## Groups: dealer [2] ## ## dealer type sales_order ## (chr) (chr) (chr) ## 1 abc suv forrester ## 2 xyz hatch prius \u0026gt; versa \u0026gt; prius \u0026gt; versa \u0026gt; prius \u0026gt; versa ## 3 xyz sedan s3 \u0026gt; s3 ## 4 xyz suv highlander Spark build with Hive I built Spark with Hive in the latest LTS Ubuntu - Ubuntu 16.04 Xenial Xerus. I just used the default JAVA version and Scala 2.10.3. The source is built for Hadoop 2.4 (-Phadoop-2.4 and -Dhadoop.version=2.4.0) with YARN (-Pyarn) and Hive (-Phive and -Phive-thriftserver). I also selected to include SparkR (-Psparkr). See the official documentation for further details.\nHere is a summary of steps followed.\nUpdate packages sudo apt-get update Install JAVA and set JAVA_HOME sudo apt-get install default-jdk export JAVA_HOME=\u0026quot;/usr/lib/jvm/java-8-openjdk-amd64\u0026quot; Install Scala 2.10.3 wget http://www.scala-lang.org/files/archive/scala-2.10.3.tgz tar xvf scala-2.10.3.tgz sudo mv scala-2.10.3 /usr/bin sudo ln -s /usr/bin/scala-2.10.3 /usr/bin/scala export PATH=$PATH:/usr/bin/scala/bin Download Spark 1.6.0 and run make-distribution.sh wget http://d3kbcqa49mib13.cloudfront.net/spark-1.6.0.tgz tar xvf spark-1.6.0.tgz cd spark-1.6.0 export MAVEN_OPTS=\u0026quot;-Xmx2g -XX:MaxPermSize=512M -XX:ReservedCodeCacheSize=512m\u0026quot; ./make-distribution.sh --name spark-1.6.0-bin-hadoop2.4-hive-yarn --tgz -Pyarn -Phadoop-2.4 -Dhadoop.version=2.4.0 -Psparkr -Phive -Phive-thriftserver -DskipTests The build was done in a VirtualBox guest where 2 cores and 8 GB of memory were allocated. After about 30 minutes, I was able to see the following output and the pre-built Spark source (spark-1.6.0-bin-spark-1.6.0-bin-hadoop2.4-hive-yarn.tgz).\nI hope this post is useful.\n","date":"April 30, 2016","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/blog/2016-04-30-boost-sparkr-with-hive/","series":[],"smallImg":"","tags":[{"title":"Apache Spark","url":"/tags/apache-spark/"},{"title":"Apache Hive","url":"/tags/apache-hive/"},{"title":"R","url":"/tags/r/"},{"title":"SparkR","url":"/tags/sparkr/"}],"timestamp":1461974400,"title":"Boost SparkR With Hive"},{"categories":[{"title":"R","url":"/categories/r/"}],"content":"In the previous post, a Spark cluster is set up using 2 VirtualBox Ubuntu guests. While this is a viable option for many, it is not always for others. For those who find setting-up such a cluster is not convenient, there\u0026rsquo;s still another option, which is relying on the local mode of Spark. In this post, a BitBucket repository is introduced, which is a R project that includes Spark 1.6.0 Pre-built for Hadoop 2.0 and later and hadoop-common 2.2.0 - the latter is necessary if it is tested on Windows. Then several initialization steps are discussed such as setting-up environment variables and library path as well as including the spark-csv package and a JDBC driver. Finally it shows some examples of reading JSON and CSV files in the cluster mode.\nsparkr-test repo Spark 1.6.0 Pre-built for Hadoop 2.0 and later is downloaded and renamed as spark after decompressing. Also, hadoop-common 2.2.0 is downloaded from this GitHub repo and saved within the spark folder as hadoop. The SparkR package is in R/lib and the bin path includes files that execute spark applications interactively and in batch mode. The conf folder includes a number of configuration templates. At the moment, only one of the templates is modified - log4j.properties.template. This template is renanmed as log4j.properties and log4j.rootCategory is set to be WARN as shown below. Previously it was INFO and it may be distracting as a lot of messages are printed with this option.\n1# Set everything to be logged to the console 2log4j.rootCategory=WARN, console The spark folder in the repository is shown below.\nThere are 2 data files. iris.json is the popular iris data set in JSON format. iris_up.csv is the same data set in CSV format with 3 extra columns - 1 integer, 1 date and 1 integer column with NA values - how to read them will be discussed shortly. postgresql-9.3-1103.jdbc3.jar is a JDBC driver to connect PostgreSQL-like database servers such as PostgreSQL server or Amazon Redshift - you may add another driver for your own DB server.\nInitialization Local mode It sets two environment variables: SPARK_HOME and HADOOP_HOME. The latter is mandatory if you\u0026rsquo;re running this example on Windows (possibly in local mode). Otherwise the following error is thrown: java.lang.NullPointerException. Note that the Spark pre-built distribution doesn\u0026rsquo;t include Hadoop-common and it is downloaded from this repo and added within the spark folder - the foler is named as hadoop. If it is running on Linux, this part can be commented out as in the cluster mode example below.\nAlso the spark bin directory is added to the PATH environment variable - this path is where spark-submit (Spark batch excutor) and Saprk REPL launchers exist. Then the path of the db driver (postgresql-9.3-1103.jdbc3.jar) is specified - see postgresql_drv. As can be seen in sparkR.init(), the path is added to environment variable on the worker node by adding sparkEnvir and the driver is passed to the worker node by setting sparkJars.\nFinally the search path is updated to add the SparkR package (sparkr_lib). For the local mode, the master can be set as local[*] if it is required to use all existing cores or a specific number can be specified - see spark_link. The last environment variable (SPARKR_SUBMIT_ARGS) is for controlling spark-submit. In this setting, the spark-csv package is included at launch.\nAt the end, a spark context (sc) and sql context (sqlContext) are defined. Note that, in this way, it is possible to run a Spark application interactively as well as in batch mode using Rscript, rather than using spark-submit.\n1#### set up environment variables 2base_path \u0026lt;- getwd() 3## SPARK_HOME 4spark_home \u0026lt;- paste0(base_path, \u0026#39;/spark\u0026#39;) 5Sys.setenv(SPARK_HOME = spark_home) 6## $SPARK_HOME/bin to PATH 7spark_bin \u0026lt;- paste0(spark_home, \u0026#39;/bin\u0026#39;) 8Sys.setenv(PATH = paste(Sys.getenv(c(\u0026#39;PATH\u0026#39;)), spark_bin, sep=\u0026#39;:\u0026#39;)) 9## HADOOP_HOME 10# hadoop-common missing on Windows and downloaded from 11# https://github.com/srccodes/hadoop-common-2.2.0-bin/archive/master.zip 12# java.lang.NullPointerException if not set 13hadoop_home \u0026lt;- paste0(spark_home, \u0026#39;/hadoop\u0026#39;) # hadoop-common missing on Windows 14Sys.setenv(HADOOP_HOME = hadoop_home) # hadoop-common missing on Windows 15 16#### extra driver jar to be passed 17postgresql_drv \u0026lt;- paste0(getwd(), \u0026#39;/postgresql-9.3-1103.jdbc3.jar\u0026#39;) 18 19#### add SparkR to search path 20sparkr_lib \u0026lt;- paste0(spark_home, \u0026#39;/R/lib\u0026#39;) 21.libPaths(c(.libPaths(), sparkr_lib)) 22 23#### specify master host name or localhost 24#spark_link \u0026lt;- system(\u0026#39;cat /root/spark-ec2/cluster-url\u0026#39;, intern=TRUE) 25spark_link \u0026lt;- \u0026#34;local[*]\u0026#34; 26#spark_link \u0026lt;- \u0026#39;spark://192.168.1.10:7077\u0026#39; 27 28library(SparkR) 29 30## include spark-csv package 31Sys.setenv(\u0026#39;SPARKR_SUBMIT_ARGS\u0026#39;=\u0026#39;\u0026#34;--packages\u0026#34; \u0026#34;com.databricks:spark-csv_2.10:1.3.0\u0026#34; \u0026#34;sparkr-shell\u0026#34;\u0026#39;) 32 33sc \u0026lt;- sparkR.init(master = spark_link, appName = \u0026#34;SparkR_local\u0026#34;, 34 sparkEnvir = list(spark.driver.extraClassPath = postgresql_drv), 35 sparkJars = postgresql_drv) 36sqlContext \u0026lt;- sparkRSQL.init(sc) 37 38## do something 39 40sparkR.stop() standalone cluster In order to run the script in the cluster mode, the two data files (iris.json and iris_up.csv) are copied to ~/data in both the master and slave machines. (Files should exist in the same location if you\u0026rsquo;re not using HDFS, S3 \u0026hellip;) Note that I started a cluster by ~/spark/sbin/start-all.sh - see this post for further details.\nThe main difference is the Spark master, which is set to be spark://192.168.1.10:7077.\n1#### set up environment variables 2base_path \u0026lt;- \u0026#39;/home/jaehyeon\u0026#39; 3## SPARK_HOME 4spark_home \u0026lt;- paste0(base_path, \u0026#39;/spark\u0026#39;) 5Sys.setenv(SPARK_HOME = spark_home) 6## $SPARK_HOME/bin to PATH 7spark_bin \u0026lt;- paste0(spark_home, \u0026#39;/bin\u0026#39;) 8Sys.setenv(PATH = paste(Sys.getenv(c(\u0026#39;PATH\u0026#39;)), spark_bin, sep=\u0026#39;:\u0026#39;)) 9## HADOOP_HOME 10# hadoop-common missing on Windows and downloaded from 11#\thttps://github.com/srccodes/hadoop-common-2.2.0-bin/archive/master.zip 12# java.lang.NullPointerException if not set 13#hadoop_home \u0026lt;- paste0(spark_home, \u0026#39;/hadoop\u0026#39;) # hadoop-common missing on Windows 14#Sys.setenv(HADOOP_HOME = hadoop_home) # hadoop-common missing on Windows 15 16#### extra driver jar to be passed 17postgresql_drv \u0026lt;- paste0(getwd(), \u0026#39;/postgresql-9.3-1103.jdbc3.jar\u0026#39;) 18 19#### add SparkR to search path 20sparkr_lib \u0026lt;- paste0(spark_home, \u0026#39;/R/lib\u0026#39;) 21.libPaths(c(.libPaths(), sparkr_lib)) 22 23#### specify master host name or localhost 24#spark_link \u0026lt;- system(\u0026#39;cat /root/spark-ec2/cluster-url\u0026#39;, intern=TRUE) 25#spark_link \u0026lt;- \u0026#34;local[*]\u0026#34; 26spark_link \u0026lt;- \u0026#39;spark://192.168.1.10:7077\u0026#39; 27 28library(SparkR) 29 30## include spark-csv package 31Sys.setenv(\u0026#39;SPARKR_SUBMIT_ARGS\u0026#39;=\u0026#39;\u0026#34;--packages\u0026#34; \u0026#34;com.databricks:spark-csv_2.10:1.3.0\u0026#34; \u0026#34;sparkr-shell\u0026#34;\u0026#39;) 32 33sc \u0026lt;- sparkR.init(master = spark_link, appName = \u0026#34;SparkR_cluster\u0026#34;, 34 sparkEnvir = list(spark.driver.extraClassPath = postgresql_drv), 35 sparkJars = postgresql_drv) ## Launching java with spark-submit command /home/jaehyeon/spark/bin/spark-submit --jars /home/jaehyeon/jaehyeon-kim.github.io/_posts/projects/postgresql-9.3-1103.jdbc3.jar --driver-class-path \u0026#34;/home/jaehyeon/jaehyeon-kim.github.io/_posts/projects/postgresql-9.3-1103.jdbc3.jar\u0026#34; \u0026#34;--packages\u0026#34; \u0026#34;com.databricks:spark-csv_2.10:1.3.0\u0026#34; \u0026#34;sparkr-shell\u0026#34; /tmp/RtmpuQHgNQ/backend_port1ee16d63ec86 1sqlContext \u0026lt;- sparkRSQL.init(sc) 2 3data_path \u0026lt;- paste0(base_path, \u0026#39;/data\u0026#39;) The JSON format is built-in so that it suffices to specify the source. By default, read.df() infers the schema (or data type) and it is found that all data types are identified correctly except for the last one where it includes some NA values.\n1iris_js \u0026lt;- read.df(sqlContext, path = paste0(data_path, \u0026#34;/iris.json\u0026#34;), source = \u0026#34;json\u0026#34;) 2head(iris_js) ## Petal_Length Petal_Width Sepal_Length Sepal_Width Species ## 1 1.4 0.2 5.1 3.5 setosa ## 2 1.4 0.2 4.9 3.0 setosa ## 3 1.3 0.2 4.7 3.2 setosa ## 4 1.5 0.2 4.6 3.1 setosa ## 5 1.4 0.2 5.0 3.6 setosa ## 6 1.7 0.4 5.4 3.9 setosa The schema inference is worse on CSV as the date field is identified as string.\n1iris_inf \u0026lt;- read.df(sqlContext, path = paste0(data_path, \u0026#34;/iris_up.csv\u0026#34;), 2 source = \u0026#34;com.databricks.spark.csv\u0026#34;, inferSchema = \u0026#34;true\u0026#34;, header = \u0026#34;true\u0026#34;) 3head(iris_inf) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species int date ## 1 5.1 3.5 1.4 0.2 setosa 1 2016-02-29 ## 2 4.9 3.0 1.4 0.2 setosa 2 2016-02-29 ## 3 4.7 3.2 1.3 0.2 setosa 3 2016-02-29 ## 4 4.6 3.1 1.5 0.2 setosa 4 2016-02-29 ## 5 5.0 3.6 1.4 0.2 setosa 5 2016-02-29 ## 6 5.4 3.9 1.7 0.4 setosa 6 2016-02-29 ## null ## 1 NA ## 2 NA ## 3 NA ## 4 4 ## 5 5 ## 6 6 1schema(iris_inf) ## StructType ## |-name = \u0026#34;Sepal.Length\u0026#34;, type = \u0026#34;DoubleType\u0026#34;, nullable = TRUE ## |-name = \u0026#34;Sepal.Width\u0026#34;, type = \u0026#34;DoubleType\u0026#34;, nullable = TRUE ## |-name = \u0026#34;Petal.Length\u0026#34;, type = \u0026#34;DoubleType\u0026#34;, nullable = TRUE ## |-name = \u0026#34;Petal.Width\u0026#34;, type = \u0026#34;DoubleType\u0026#34;, nullable = TRUE ## |-name = \u0026#34;Species\u0026#34;, type = \u0026#34;StringType\u0026#34;, nullable = TRUE ## |-name = \u0026#34;int\u0026#34;, type = \u0026#34;IntegerType\u0026#34;, nullable = TRUE ## |-name = \u0026#34;date\u0026#34;, type = \u0026#34;StringType\u0026#34;, nullable = TRUE ## |-name = \u0026#34;null\u0026#34;, type = \u0026#34;StringType\u0026#34;, nullable = TRUE It is possible to specify individual data types by constructing a custom schema (customSchema). Note that the last column (null) is set as string in the custom schema and converted into integer using cast() after the data is loaded. The reason is NA is considered as string that the following error will be thrown if it is set as integer: java.lang.NumberFormatException: For input string: \u0026quot;NA\u0026quot;.\n1customSchema \u0026lt;- structType( 2 structField(\u0026#34;Sepal.Length\u0026#34;, \u0026#34;double\u0026#34;), 3 structField(\u0026#34;Sepal.Width\u0026#34;, \u0026#34;double\u0026#34;), 4 structField(\u0026#34;Petal.Length\u0026#34;, \u0026#34;double\u0026#34;), 5 structField(\u0026#34;Petal.Width\u0026#34;, \u0026#34;double\u0026#34;), 6 structField(\u0026#34;Species\u0026#34;, \u0026#34;string\u0026#34;), 7 structField(\u0026#34;integer\u0026#34;, \u0026#34;integer\u0026#34;), 8 structField(\u0026#34;date\u0026#34;, \u0026#34;date\u0026#34;), 9 structField(\u0026#34;null\u0026#34;, \u0026#34;string\u0026#34;) 10) 11 12iris_cus \u0026lt;- read.df(sqlContext, path = paste0(data_path, \u0026#34;/iris_up.csv\u0026#34;), 13 source = \u0026#34;com.databricks.spark.csv\u0026#34;, schema = customSchema, header = \u0026#34;true\u0026#34;) 14cast(iris_cus$null, \u0026#34;integer\u0026#34;) ## Column unresolvedalias(cast(null as int)) 1head(iris_cus) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species integer ## 1 5.1 3.5 1.4 0.2 setosa 1 ## 2 4.9 3.0 1.4 0.2 setosa 2 ## 3 4.7 3.2 1.3 0.2 setosa 3 ## 4 4.6 3.1 1.5 0.2 setosa 4 ## 5 5.0 3.6 1.4 0.2 setosa 5 ## 6 5.4 3.9 1.7 0.4 setosa 6 ## date null ## 1 2016-02-29 NA ## 2 2016-02-29 NA ## 3 2016-02-29 NA ## 4 2016-02-29 4 ## 5 2016-02-29 5 ## 6 2016-02-29 6 1schema(iris_cus) ## StructType ## |-name = \u0026#34;Sepal.Length\u0026#34;, type = \u0026#34;DoubleType\u0026#34;, nullable = TRUE ## |-name = \u0026#34;Sepal.Width\u0026#34;, type = \u0026#34;DoubleType\u0026#34;, nullable = TRUE ## |-name = \u0026#34;Petal.Length\u0026#34;, type = \u0026#34;DoubleType\u0026#34;, nullable = TRUE ## |-name = \u0026#34;Petal.Width\u0026#34;, type = \u0026#34;DoubleType\u0026#34;, nullable = TRUE ## |-name = \u0026#34;Species\u0026#34;, type = \u0026#34;StringType\u0026#34;, nullable = TRUE ## |-name = \u0026#34;integer\u0026#34;, type = \u0026#34;IntegerType\u0026#34;, nullable = TRUE ## |-name = \u0026#34;date\u0026#34;, type = \u0026#34;DateType\u0026#34;, nullable = TRUE ## |-name = \u0026#34;null\u0026#34;, type = \u0026#34;StringType\u0026#34;, nullable = TRUE I hope this post is useful.\n","date":"March 2, 2016","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/blog/2016-03-02-quick-start-sparkr-in-local-and-cluster-mode/","series":[],"smallImg":"","tags":[{"title":"Apache Spark","url":"/tags/apache-spark/"},{"title":"R","url":"/tags/r/"},{"title":"SparkR","url":"/tags/sparkr/"}],"timestamp":1456876800,"title":"Quick Start SparkR in Local and Cluster Mode"},{"categories":[{"title":"R","url":"/categories/r/"}],"content":"We discuss how to set up a Spark cluser between 2 Ubuntu guests. Firstly it begins with machine preparation. Once a machine is baked, its image file (VDI) is be copied for the second one. Then how to launch a cluster by standalone mode is discussed. Let\u0026rsquo;s get started.\nMachine preparation If you haven\u0026rsquo;t read the previous post, I recommend reading as it introduces Putty as well. Also, as Spark need Java Development Kit (JDK), you may need to apt-get it first - see this tutorial for further details.\nI downloaded Spark 1.6.0 Pre-built for Hadoop 2.6 and later and unpacked it in my user directory as following.\ncd ~ wget http://www.us.apache.org/dist/spark/spark-1.6.0/spark-1.6.0-bin-hadoop2.6.tgz tar zxvf spark-*.tgz mv ./spark*/ spark Then I prepared \u0026lsquo;password-less\u0026rsquo; SSH access (no passphrase) from the master machine to the other by generating a ssh key.\nmkdir .ssh ssh-keygen -t rsa Enter file in which to save the key (/home/you/.ssh/id_rsa): [ENTER] Enter passphrase (empty for no passphrase): [EMPTY] Enter same passphrase again: [EMPTY] You will see two files id_rsa (private key) and id_rsa.pub (public key) in .ssh folder. The public key was added to a file called authorized_keys as following.\ncd .ssh cat id_rsa.pub \u0026gt;\u0026gt; authorized_keys chmod 644 authorized_keys Note that, by now, there is only a single machine but its image file will be copied shortly to be used as a slave machine. Therefore adding a public key to the authorized_keys file will be convenient.\nSet up guest machines I named the baked machine\u0026rsquo;s image file as spark-master.vdi and copied it as spark-slave.vdi. When I tried to create a virtual machine with the new image, the following error was encountered, which indicates duplicate UUIDs.\nThis was resolved by setting a differnet UUID using VBOXMANAGE.EXE. On my Windows (host) CMD, I did the following and it was possible to create a virtual machine from the copied image.\ncd \u0026#34;C:\\Program Files\\Oracle\\VirtualBox\u0026#34; VBOXMANAGE.EXE internalcommands sethduuid \u0026#34;D:\\VirtualEnv\\spark-slave.vdi\u0026#34; As in the previous post, I set up 2 network adapters - Bridged Adapter and Host-only Adapter. The latter lets a virtual machine to have a static IP. As the second image is copied from the first, both have the same IP address, which can be problematic. They can have different IP addresses by letting them have different MAC addresses. (Note the refresh button in the right.)\nIn my case, the master and slave machine\u0026rsquo;s IP addresses are set up to be 192.168.1.8 and 192.168.1.11.\nAlso they have the same host name: ubuntu-master. It\u0026rsquo;d be necessary to change the slave machine\u0026rsquo;s host name. I modified the host name in /etc/hostname and /etc/hosts. Basically I changed any ubuntu-master in those files to ubuntu-slave1 and restarted the machine - see further details Note this requires root privilege.\nThe updated host name is shown below in the right.\nFinally I added slave\u0026rsquo;s host information to the master\u0026rsquo;s /etc/hosts and did the other way around to the slave\u0026rsquo;s file.\nStandalone mode setup Firstly I added the slave machine\u0026rsquo;s host name to the master\u0026rsquo;s slaves file in ~/spark/conf as following.\ncd ~/spark/conf/ cp slaves.template slaves I just commented localhost, which is in the last line and added the slave\u0026rsquo;s host name. i.e.\n#localhost ubuntu-slave1 Then I updated spark-env.sh file on each of the machines.\ncp spark-env.sh.template spark-env.sh And, for the master, I added the following\nJAVA_HOME=/usr/lib/jvm/java-7-openjdk-amd64 export SPARK_MASTER_IP=192.168.1.8 export SPARK_WORKER_CORES=1 export SPARK_WORKER_INSTANCES=1 export SPARK_MASTER_PORT=7077 export SPARK_WORKER_MEMORY=4g export MASTER=spark://${SPARK_MASTER_IP}:${SPARK_MASTER_PORT} export SPARK_LOCAL_IP=192.168.1.8 and, for the slave,\nJAVA_HOME=/usr/lib/jvm/java-7-openjdk-amd64 export SPARK_MASTER_IP=192.168.1.8 export SPARK_WORKER_CORES=1 export SPARK_WORKER_INSTANCES=1 export SPARK_MASTER_PORT=7077 export SPARK_WORKER_MEMORY=4g export MASTER=spark://${SPARK_MASTER_IP}:${SPARK_MASTER_PORT} export SPARK_LOCAL_IP=192.168.1.11 That\u0026rsquo;s it. By executing the following command, I was able to create a Spark cluster and to check the status of the cluster on the web UI.\n~/spark/sbin/start-all.sh I hope this post is useful.\n","date":"February 22, 2016","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/blog/2016-02-22-spark-cluster-setup-on-virtualbox/","series":[],"smallImg":"","tags":[{"title":"Apache Spark","url":"/tags/apache-spark/"},{"title":"R","url":"/tags/r/"},{"title":"SparkR","url":"/tags/sparkr/"},{"title":"VirtualBox","url":"/tags/virtualbox/"}],"timestamp":1456099200,"title":"Spark Cluster Setup on VirtualBox"},{"categories":[{"title":"R","url":"/categories/r/"}],"content":"As mentioned in an earlier post, things that are not easy in R can be relatively simple in other languages. Another example would be connecting to Amazon Web Services. In relation to s3, although there are a number of existing packages, many of them seem to be deprecated, premature or platform-dependent. (I consider the cloudyr project looks promising though.)\nIf there isn\u0026rsquo;t a comprehensive R-way of doing something yet, it may be necessary to create it from scratch. Actually there are some options to do so by using AWS Command Line Interface, AWS REST API or wrapping functionality of another language.\nIn this post, a quick summary of the last way using Python is illustrated by introducing the rs3helper package.\nThe reasons why I\u0026rsquo;ve come up with a package are as following.\nFirstly, Python is relatively easy to learn and it has quite a comprehensive interface to Amazon Web Services - boto. Secondly, in order to call Python in R, the rPython package may be used if it only targets UNIX-like platforms. For cross-platform functionality, however, system command has to be executed. Finally, due to the previous reason, it wouldn\u0026rsquo;t be stable to keep the source files locally and it\u0026rsquo;d be necessary to keep them in a package. I use Python 2.7 and the boto library can be installed easily using pip by executing pip install boto.\nUsing RStudio, it is not that complicated to develop a package. (see R packages by Hadley Wickham) Even the folder structure and necessary files are generated if the project type is selected as R Package. R script files should locate in the R folder while Python scripts should be in inst/python.\nIn the package, the s3-related R functions exists in R/s3utils.R while the corresponding python scripts are in inst/python - all Python functions are in inst/python/s3helper.py. As the Python function outputs should be passed to R, a response variable is returned for each function and it is converted into JSON string. The response variable is a Python list, dictionary or list of dictionaries and thus it is parsed as R vector, list or data frame.\nAn example of the wrapper functions, which looks up a bucket, is shown below.\nPython: connect_to_s3() and lookup_bucket() are imported to inst/python/lookup_bucket.py from inst/python/s3helper.py. The script requires 4 mandatory/optional argumens and prints the response after converting it into JSON string.\n1## in inst/python/s3helper.py 2import boto 3from boto.s3.connection import OrdinaryCallingFormat 4 5def connect_to_s3(access_key_id, secret_access_key, region = None): 6 try: 7 if region is None: 8 conn = boto.connect_s3(access_key_id, secret_access_key) 9 else: 10 conn = boto.s3.connect_to_region( 11 region_name = region, 12 aws_access_key_id = access_key_id, 13 aws_secret_access_key = secret_access_key, 14 calling_format = OrdinaryCallingFormat() 15 ) 16 except boto.exception.AWSConnectionError: 17 conn = None 18 return conn 19 20def lookup_bucket(conn, bucket_name): 21 if conn is not None: 22 try: 23 bucket = conn.lookup(bucket_name) 24 if bucket is not None: 25 response = {\u0026#39;bucket\u0026#39;: bucket_name, \u0026#39;is_exist\u0026#39;: True, \u0026#39;message\u0026#39;: None} 26 else: 27 response = {\u0026#39;bucket\u0026#39;: bucket_name, \u0026#39;is_exist\u0026#39;: False, \u0026#39;message\u0026#39;: None} 28 except boto.exception.S3ResponseError as re: 29 response = {\u0026#39;bucket\u0026#39;: bucket_name, \u0026#39;is_exist\u0026#39;: None, \u0026#39;message\u0026#39;: \u0026#39;S3ResponseError = {0} {1}\u0026#39;.format(re[0], re[1])} 30 except: 31 response = {\u0026#39;bucket\u0026#39;: bucket_name, \u0026#39;is_exist\u0026#39;: None, \u0026#39;message\u0026#39;: \u0026#39;Unhandled error occurs\u0026#39;} 32 else: 33 response = {\u0026#39;bucket\u0026#39;: bucket_name, \u0026#39;is_exist\u0026#39;: None, \u0026#39;message\u0026#39;: \u0026#39;connection is not made\u0026#39;} 34 return response 1## in inst/python/lookup_bucket.py 2import json 3import argparse 4 5from s3helper import connect_to_s3, lookup_bucket 6 7parser = argparse.ArgumentParser(description=\u0026#39;lookup a bucket\u0026#39;) 8parser.add_argument(\u0026#39;--access_key_id\u0026#39;, required=True, type=str, help=\u0026#39;AWS access key id\u0026#39;) 9parser.add_argument(\u0026#39;--secret_access_key\u0026#39;, required=True, type=str, help=\u0026#39;AWS secret access key\u0026#39;) 10parser.add_argument(\u0026#39;--bucket_name\u0026#39;, required=True, type=str, help=\u0026#39;S3 bucket name\u0026#39;) 11parser.add_argument(\u0026#39;--region\u0026#39;, required=False, type=str, help=\u0026#39;Region info\u0026#39;) 12 13args = parser.parse_args() 14 15conn = connect_to_s3(args.access_key_id, args.secret_access_key, args.region) 16response = lookup_bucket(conn, args.bucket_name) 17 18print(json.dumps(response)) R: lookup_bucket() generates the path where inst/python/lookup_bucket.py exists and constructs the command to be executed in system() - the intern argument should be TRUE to grap the printed JSON string. Then it parses the returned JSON string into a R object using the jsoinlite package.\n1lookup_bucket \u0026lt;- function(access_key_id, secret_access_key, bucket_name, region = NULL) { 2 if(bucket_name == \u0026#39;\u0026#39;) stop(\u0026#39;bucket_name: expected one argument\u0026#39;) 3 4 path \u0026lt;- system.file(\u0026#39;python\u0026#39;, \u0026#39;lookup_bucket.py\u0026#39;, package = \u0026#39;rs3helper\u0026#39;) 5 command \u0026lt;- paste(\u0026#39;python\u0026#39;, path, \u0026#39;--access_key_id\u0026#39;, access_key_id, \u0026#39;--secret_access_key\u0026#39;, secret_access_key, \u0026#39;--bucket_name\u0026#39;, bucket_name) 6 if(!is.null(region)) command \u0026lt;- paste(command, \u0026#39;--region\u0026#39;, region) 7 8 response \u0026lt;- system(command, intern = TRUE) 9 tryCatch({ 10 fromJSON(response) 11 }, error = function(err) { 12 warning(\u0026#39;fails to parse JSON response\u0026#39;) 13 response 14 }) 15} A quick example of running this function is shown below.\n1if (!require(\u0026#34;devtools\u0026#34;)) 2 install.packages(\u0026#34;devtools\u0026#34;) 3devtools::install_github(\u0026#34;jaehyeon-kim/rs3helper\u0026#34;) 4 5library(rs3helper) 6library(jsonlite) # not sure why it is not loaded at the first place 7 8lookup_bucket(\u0026#39;access-key-id\u0026#39;, \u0026#39;secret-access-key\u0026#39;, \u0026#39;rs3helper\u0026#39;) ## $is_exist ## [1] TRUE ## ## $message ## NULL ## ## $bucket ## [1] \u0026#34;rs3helper\u0026#34; ","date":"November 21, 2015","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/blog/2015-11-21-quick-test-to-wrap-python-in-r/","series":[],"smallImg":"","tags":[{"title":"R","url":"/tags/r/"},{"title":"Python","url":"/tags/python/"}],"timestamp":1448064000,"title":"Quick Test to Wrap Python in R"},{"categories":[{"title":"General","url":"/categories/general/"}],"content":"There seem to be growing interest in Python in the R cummunity. While there can be a range of opinions about using R over Python (or vice versa) for exploratory data analysis, fitting statistical/machine learning algorithms and so on, I consider one of the strongest attractions of using Python comes from the fact that Python is a general purpose programming language. As more developers are involved in, it can provide a way to get jobs done easily, which can be tricky in R. In this article, an example is introduced by illustrating how to connect to SOAP (Simple Object Access Protocol) web services.\nWeb service (or API) is a popular way to connect to a server programmatically and SOAP web service is one type. For those who are interested in it, please see this article. Although R has good packages to connect to a newer type of web service, which is based on REST (Representational state transfer) (eg, httr package), I haven\u0026rsquo;t found a good R package that can be used as a comprehensive SOAP client, which means I have to use the RCurl package at best. On the other hand, as \u0026lsquo;batteries included\u0026rsquo;, one of Python\u0026rsquo;s philosophies, assures, it has a number of SOAP client libraries. Among those, I\u0026rsquo;ve chosen the suds library.\nIn this demo, I\u0026rsquo;m going to connect to Sizmek MDX API where online campaign data can be pulled from it. I\u0026rsquo;ve used the PyDev plugin of Eclipse and the source of this demo can be found in my GitHub repo. It has 4 classes that connect to the API (Authentication, Advertiser, ConvTag and Campaign) and they are kept in the sizmek package. Also 2 extra classes are set up in the utils package (Soap and Helper), which keep common methods for the 4 classes. The advertiser class can be seen as following.\n1from utils.soap import Soap 2from datetime import datetime 3 4class Advertiser: 5 def __init__(self, pid, name, vertical, useConv): 6 \u0026#39;\u0026#39;\u0026#39; 7 Constructor 8 \u0026#39;\u0026#39;\u0026#39; 9 self.id = pid 10 self.name = name 11 self.vertical = vertical 12 self.useConv = useConv 13 self.addedDate = datetime.now().date() 14 15 def __repr__(self): 16 return \u0026#34;id: %s|name: %s|use conv: %s\u0026#34; % (self.id, self.name, self.useConv) 17 18 def __str__(self): 19 return \u0026#34;id: %s|name: %s|use conv: %s\u0026#34; % (self.id, self.name, self.useConv) 20 21 def __len__(self): 22 return 1 23 24 @staticmethod 25 def GetItemRes(wurl, auth, pageIndex, pageSize, showExtInfo=True): 26 client = Soap.SetupClient(wurl, auth, toAddToken=True, toImportMsgSrc=True, toImportArrSrc=False) 27 # update paging info 28 paging = client.factory.create(\u0026#39;ns1:ListPaging\u0026#39;) 29 paging[\u0026#39;PageIndex\u0026#39;] = pageIndex 30 paging[\u0026#39;PageSize\u0026#39;] = pageSize 31 # update filter array - empty 32 filterArrary = client.factory.create(\u0026#39;ns0:ArrayOfAdvertiserServiceFilter\u0026#39;) 33 # get response 34 response = client.service.GetAdvertisers(filterArrary, paging, showExtInfo) 35 return response 36 37 @staticmethod 38 def GetItem(response): 39 objList = [] 40 for r in response[1][\u0026#39;Advertisers\u0026#39;][\u0026#39;AdvertiserInfo\u0026#39;]: 41 obj = Advertiser(r[\u0026#39;ID\u0026#39;], r[\u0026#39;AdvertiserName\u0026#39;], r[\u0026#39;Vertical\u0026#39;], r[\u0026#39;AdvertiserExtendedInfo\u0026#39;][\u0026#39;UsesConversionTags\u0026#39;]) 42 objList.append(obj) 43 return objList 44 45 @staticmethod 46 def GetItemPgn(wurl, auth, pageIndex, pageSize, showExtInfo=True): 47 objList = [] 48 cond = True 49 while cond: 50 response = Advertiser.GetItemRes(wurl, auth, pageIndex, pageSize, showExtInfo) 51 objList = objList + Advertiser.GetItem(response) 52 Soap.ShowProgress(response[1][\u0026#39;TotalCount\u0026#39;], len(objList), pageIndex, pageSize) 53 if len(objList) \u0026lt; response[1][\u0026#39;TotalCount\u0026#39;]: 54 pageIndex += 1 55 else: 56 cond = False 57 return objList 58 59 @staticmethod 60 def GetFilter(objList): 61 filteredList = [obj for obj in objList if obj.useConv == True] 62 print \u0026#34;%s out of %s advertiser where useConv equals True\u0026#34; % (len(filteredList), len(objList)) 63 return filteredList The 4 classes have a number of common methods (GetItemRes(), GetItem(), GetItemPgn(), GetFilter()) to retrieve data from the relevant sections of the API and these methods are not related to an instance of the classes so that they are set to be static (@staticmethod). In R, this class may be constructed as following.\n1advertiser = function(pid, name, vertical, useConv) { 2 out \u0026lt;- list() 3 out$id = pid 4 out$name = name 5 out$vertical = vertical 6 out$useConv = useConv 7 out$addedDate = Sys.Date() 8 class(out) \u0026lt;- append(class(out), \u0026#39;Advertiser\u0026#39;) 9} 10 11GetItemRes \u0026lt;- function(obj) { 12 UseMethod(\u0026#39;GetItemRes\u0026#39;, obj) 13} 14 15GetItemRes.default \u0026lt;- function(obj) { 16 warning(\u0026#39;Default GetItemRes method called on unrecognized object.\u0026#39;) 17 obj 18} 19 20GetItemRes.Advertiser \u0026lt;- function(obj) { 21 response \u0026lt;- \u0026#39;Get response from the API\u0026#39; 22 response 23} 24 25... While it is relatively straightforward to set up corresponding S3 classes, the issue is that there is no comprehensive SOAP client in R. In Python, the client library helps create a proxy class based on the relevant WSDL file so that a request/response can be handled entirely in a \u0026lsquo;Pythonic\u0026rsquo; way. For example, below shows how to retrieve advertiser details from the API.\n1from utils.helper import Helper 2from sizmek.authentication import Auth 3from sizmek.advertiser import Advertiser 4import logging 5 6logging.basicConfig(level=logging.INFO) 7logging.getLogger(\u0026#39;suds.client\u0026#39;).setLevel(logging.DEBUG) 8 9## WSDL urls 10authWSDL = \u0026#39;https://platform.mediamind.com/Eyeblaster.MediaMind.API/V2/AuthenticationService.svc?wsdl\u0026#39; 11advertiserWSDL = \u0026#39;https://platform.mediamind.com/Eyeblaster.MediaMind.API/V2/AdvertiserService.svc?wsdl\u0026#39; 12campaignWSDL = \u0026#39;https://platform.mediamind.com/Eyeblaster.MediaMind.API/V2/CampaignService.svc?wsdl\u0026#39; 13 14## credentials 15username = \u0026#39;user-name\u0026#39; 16password = \u0026#39;password\u0026#39; 17appkey = \u0026#39;application-key\u0026#39; 18 19## path to export API responses 20path = \u0026#39;C:\\\\projects\\\\workspace\\\\sizmek_report\\\\src\\\\csvs\\\\\u0026#39; 21 22## authentication 23auth = Auth(username, password, appkey, authWSDL) 24 25## get advertisers 26advRes = Advertiser.GetItemRes(advertiserWSDL, auth, pageIndex=0, pageSize=50, showExtInfo=True) 27advList = Advertiser.GetItem(advRes) 28Helper.PrintObjects(advList) 29 30#response example 31#id: 78640|name: Roses Only|use conv: True 32#id: 79716|name: Knowledge Source|use conv: True 33#id: 83457|name: Gold Buyers|use conv: True On the other hand, if I use the RCurl package, I have to send the following SOAP message by hacking the relevant WSDL file and its XML response has to be parsed accordingly. Although it is possible, life will be a lot harder.\n1\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; 2\u0026lt;SOAP-ENV:Envelope xmlns:ns0=\u0026#34;http://api.eyeblaster.com/message\u0026#34; xmlns:ns1=\u0026#34;http://api.eyeblaster.com/message\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xmlns:SOAP-ENV=\u0026#34;http://schemas.xmlsoap.org/soap/envelope/\u0026#34;\u0026gt; 3 \u0026lt;SOAP-ENV:Header\u0026gt; 4 \u0026lt;ns1:UserSecurityToken\u0026gt;token-generated-from-authentication-service\u0026lt;/ns1:UserSecurityToken\u0026gt; 5 \u0026lt;/SOAP-ENV:Header\u0026gt; 6 \u0026lt;ns1:Body xmlns:ns1=\u0026#34;http://schemas.xmlsoap.org/soap/envelope/\u0026#34;\u0026gt; 7 \u0026lt;GetAdvertisersRequest xmlns=\u0026#34;http://api.eyeblaster.com/message\u0026#34;\u0026gt; 8 \u0026lt;Paging\u0026gt; 9 \u0026lt;PageIndex\u0026gt;0\u0026lt;/PageIndex\u0026gt; 10 \u0026lt;PageSize\u0026gt;50\u0026lt;/PageSize\u0026gt; 11 \u0026lt;/Paging\u0026gt; 12 \u0026lt;ShowAdvertiserExtendedInfo\u0026gt;true\u0026lt;/ShowAdvertiserExtendedInfo\u0026gt; 13 \u0026lt;/GetAdvertisersRequest\u0026gt; 14 \u0026lt;/ns1:Body\u0026gt; 15\u0026lt;/SOAP-ENV:Envelope\u0026gt; 16DEBUG:suds.client:headers = {\u0026#39;SOAPAction\u0026#39;: \u0026#39;\u0026#34;http://api.eyeblaster.com/IAdvertiserService/GetAdvertisers\u0026#34;\u0026#39;, \u0026#39;Content-Type\u0026#39;: \u0026#39;text/xml; charset=utf-8\u0026#39;} I guess most R users are not programmers but many of them are quite good at understanding how a program works. Therefore, if there is an area that R is not strong, it\u0026rsquo;d be alright to consider another language to make life easier. Among those, I consider Python is easy to learn and it can provide a range of good tools. If you\u0026rsquo;re interested, please see my next article about some thoughts on Python.\n","date":"August 9, 2015","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/blog/2015-08-09-some-thoughts-on-python-for-r-users/","series":[],"smallImg":"","tags":[{"title":"Python","url":"/tags/python/"},{"title":"R","url":"/tags/r/"}],"timestamp":1439078400,"title":"Some Thoughts on Python for R Users"},{"categories":[{"title":"General","url":"/categories/general/"}],"content":"When I bagan to teach myself C# 3 years ago, I only had some experience in interactive analysis tools such as MATLAB and R - I didn\u0026rsquo;t consider R as a programming language at that time. The general purpose programming language shares some common features (data type, loop, if\u0026hellip;) but it is rather different in the way how code is written/organized, which is object oriented. Therefore, while it was not a problem to grap the common features, it took quite some time to understand and keep my code in an object oriented way.\nPython, as another object-oriented programming language, the situation would be similar. The snippets in the following link show an example - (Link). There 4 classes are defined to connect to the advertiser service of Sizmek MDX API.\nSpecifically\nAuth - class to keep authentication token Advertiser - class to keep properties and methods to connect to the advertiser service Soap - class to keep methods that are related to SOAP service Helper - class to keep utility methods\nThen, as seen in example.py, advertiser details can be requested by Advertiser.GetItemRes(), parsed by Advertiser.GetItem() and printed by Helper.PrintObjects(). I admit that the code wouldn\u0026rsquo;t be Pythonic as I\u0026rsquo;m still teaching myself the language but the idea of expressing code in an object oriented way should be valid. In this regard, it would be important to appreciate and adopt this style of coding for successful Python development. (A more extended demo can be found in my GitHub repo.)\nRecently I happened to find a book titled Introducing Python: Modern Computing in Simple Packages. A more generic explanation would be made using it. Its table of contents with some grouping is listed below.\nIntro Chapter 1, A Taste of Py Common Features Chapter 2, Py Ingredients: Numbers, Strings, and Variables Chapter 3, Py Filling: Lists, Tuples, Dictionaries, and Sets Chapter 4, Py Crust: Code Structures Chapter 5, Py Boxes: Modules, Packages, and Programs OOP Chapter 6, Oh Oh: Objects and Classes Application** Chapter 7, Mangle Data Like a Pro Chapter 8, Data Has to Go Somewhere Chapter 9, The Web, Untangled Chapter 10, Systems Chapter 11, Concurrency and Networks A typical development workflow Chapter 12, Be a Pythonista Beginning with an introduction in Chapter 1, it covers the common features in Chapter 2 to 5 and OOP is the topic of Chapter 6. Up to this, the fundamentals of Python are covered and the rest can be considered as applications. Specifically text and Unicode data can be handled (Chapter 7), data can be processed to and from external data sources (Chapter 8), web services can be consumed/produced (Chapter 9), system functions (file, directory\u0026hellip;) can be accessed/modified (Chapter 10) and performance can be improved (Chapter 11). Note that, for the application parts, it should be understood what to do well. For example, it is hardly successful to process data to/from a database if one doesn\u0026rsquo;t know how database works.\nI find this book is well organized and covers enough topics related to Python itself, its standard/3rd party libraries, and development with it. This book can be used as a comprehensive guide for new or intermediate Python programmers. (Note that the book focuses on Python 3 but the different between Python 2 and 3 would be minimal.)\nTo summarise, in order for successful Python development, its fundamentals should be understood well (and don\u0026rsquo;t forget it is an object-oriented language) as well as programming targets (or what to program).\n","date":"August 8, 2015","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/blog/2015-08-08-some-thoughts-on-python/","series":[],"smallImg":"","tags":[{"title":"Python","url":"/tags/python/"}],"timestamp":1438992000,"title":"Some Thoughts on Python"},{"categories":[{"title":"Machine Learning","url":"/categories/machine-learning/"}],"content":"A short while ago I had a chance to perform analysis using the caret package. One of the requirements is to run it parallelly and to work in both Windows and Linux. The requirement can be met by using the parallel and doParallel packages as the caret package trains a model using the foreach package if clusters are registered by the doParallel package - further details about how to implement parallel processing on a single machine can be found in earlier posts (Link 1, Link 2 and Link 3). While it is relatively straightforward to train a model across multiple clusters using the caret package, setting up random seeds may be a bit tricky. As analysis can be more reproducible by random seeds, a way of setting them up is illustrated using a simple function in this post.\nThe following packages are used.\n1library(parallel) 2library(doParallel) 3library(randomForest) 4library(caret) In the caret package, random seeds are set up by adjusting the argument of seeds in trainControl() and the object document illustrates it as following.\nseeds - an optional set of integers that will be used to set the seed at each resampling iteration. This is useful when the models are run in parallel. A value of NA will stop the seed from being set within the worker processes while a value of NULL will set the seeds using a random set of integers. Alternatively, a list can be used. The list should have B+1 elements where B is the number of resamples. The first B elements of the list should be vectors of integers of length M where M is the number of models being evaluated. The last element of the list only needs to be a single integer (for the final model). See the Examples section below and the Details section. Setting seeds to either NA or NULL wouldn\u0026rsquo;t guarantee a full control of resampling so that a custom list would be necessary. Here setSeeds() creats the custom list and it handles only (repeated) cross validation and it returns NA if a different resampling method is specified - this function is based on the source code. Specifically B is determined by the number of folds (numbers) or the number of repeats of it (numbers x repeats). Then a list of B elements are generated where each element is an integer vector of length M. M is the sum of the number of folds (numbers) and the length of the tune grid (tunes). Finally an integer vector of length 1 is added to the list.\n1# function to set up random seeds 2setSeeds \u0026lt;- function(method = \u0026#34;cv\u0026#34;, numbers = 1, repeats = 1, tunes = NULL, seed = 1237) { 3 #B is the number of resamples and integer vector of M (numbers + tune length if any) 4 B \u0026lt;- if (method == \u0026#34;cv\u0026#34;) numbers 5 else if(method == \u0026#34;repeatedcv\u0026#34;) numbers * repeats 6 else NULL 7 8 if(is.null(length)) { 9 seeds \u0026lt;- NULL 10 } else { 11 set.seed(seed = seed) 12 seeds \u0026lt;- vector(mode = \u0026#34;list\u0026#34;, length = B) 13 seeds \u0026lt;- lapply(seeds, function(x) sample.int(n = 1000000, size = numbers + ifelse(is.null(tunes), 0, tunes))) 14 seeds[[length(seeds) + 1]] \u0026lt;- sample.int(n = 1000000, size = 1) 15 } 16 # return seeds 17 seeds 18} Below shows the control variables of the resampling methods used in this post: k-fold cross validation and repeated k-fold cross validation. Here (5 repeats of) 3-fold cross validation is chosen. Also a grid is set up to tune mtry of randomForest() (cvTunes) and rcvTunes is for tuning the number of nearest neighbours of knn().\n1# control variables 2numbers \u0026lt;- 3 3repeats \u0026lt;- 5 4cvTunes \u0026lt;- 4 # tune mtry for random forest 5rcvTunes \u0026lt;- 12 # tune nearest neighbor for KNN 6seed \u0026lt;- 1237 Random seeds for 3-fold cross validation are shown below - B + 1 and M equal to 4 (3 + 1) and 7 (3 + 4) respectively.\n1# cross validation 2cvSeeds \u0026lt;- setSeeds(method = \u0026#34;cv\u0026#34;, numbers = numbers, tunes = cvTunes, seed = seed) 3c(\u0026#39;B + 1\u0026#39; = length(cvSeeds), M = length(cvSeeds[[1]])) ## B + 1 M ## 4 7 1cvSeeds[c(1, length(cvSeeds))] ## [[1]] ## [1] 328213 983891 24236 118267 196003 799071 956708 ## ## [[2]] ## [1] 710446 Random seeds for 5 repeats of 3-fold cross validation are shown below - B + 1 and M equal to 16 (3 x 5 + 1) and 15 (3 + 12) respectively.\n1# repeated cross validation 2rcvSeeds \u0026lt;- setSeeds(method = \u0026#34;repeatedcv\u0026#34;, numbers = numbers, repeats = repeats, 3 tunes = rcvTunes, seed = seed) 4c(\u0026#39;B + 1\u0026#39; = length(rcvSeeds), M = length(rcvSeeds[[1]])) ## B + 1 M ## 16 15 1rcvSeeds[c(1, length(rcvSeeds))] ## [[1]] ## [1] 328213 983891 24236 118267 196003 799071 956708 395670 826198 863837 ## [11] 119602 864078 177322 382935 513432 ## ## [[2]] ## [1] 751240 Given the random seeds, train controls are set up as shown below.\n1# cross validation 2cvCtrl \u0026lt;- trainControl(method = \u0026#34;cv\u0026#34;, number = numbers, classProbs = TRUE, 3 savePredictions = TRUE, seeds = cvSeeds) 4# repeated cross validation 5rcvCtrl \u0026lt;- trainControl(method = \u0026#34;repeatedcv\u0026#34;, number = numbers, repeats = repeats, 6 classProbs = TRUE, savePredictions = TRUE, seeds = rcvSeeds) They are tested by comparing the two learners: knn and randomForest. Each of the two sets of objects are the same except for times elements, which are not related to model reproduciblility.\n1# repeated cross validation 2cl \u0026lt;- makeCluster(detectCores()) 3registerDoParallel(cl) 4set.seed(1) 5KNN1 \u0026lt;- train(Species ~ ., data = iris, method = \u0026#34;knn\u0026#34;, tuneLength = rcvTunes, trControl = rcvCtrl) 6stopCluster(cl) 7 8cl \u0026lt;- makeCluster(detectCores()) 9registerDoParallel(cl) 10set.seed(1) 11KNN2 \u0026lt;- train(Species ~ ., data = iris, method = \u0026#34;knn\u0026#34;, tuneLength = rcvTunes, trControl = rcvCtrl) 12stopCluster(cl) 13 14# same outcome, difference only in times element 15all.equal(KNN1$results[KNN1$results$k == KNN1$bestTune[[1]],], 16 KNN2$results[KNN2$results$k == KNN2$bestTune[[1]],]) ## [1] TRUE 1all.equal(KNN1, KNN2) ## [1] \u0026#34;Component \\\u0026#34;times\\\u0026#34;: Component \\\u0026#34;everything\\\u0026#34;: Mean relative difference: 0.02158273\u0026#34; 1# cross validation 2cl \u0026lt;- makeCluster(detectCores()) 3registerDoParallel(cl) 4set.seed(1) 5rf1 \u0026lt;- train(Species ~ ., data = iris, method = \u0026#34;rf\u0026#34;, ntree = 100, 6 tuneGrid = expand.grid(mtry = seq(1, 2 * as.integer(sqrt(ncol(iris) - 1)), by = 1)), 7 importance = TRUE, trControl = cvCtrl) 8stopCluster(cl) 9 10cl \u0026lt;- makeCluster(detectCores()) 11registerDoParallel(cl) 12set.seed(1) 13rf2 \u0026lt;- train(Species ~ ., data = iris, method = \u0026#34;rf\u0026#34;, ntree = 100, 14 tuneGrid = expand.grid(mtry = seq(1, 2 * as.integer(sqrt(ncol(iris) - 1)), by = 1)), 15 importance = TRUE, trControl = cvCtrl) 16stopCluster(cl) 17 18# same outcome, difference only in times element 19all.equal(rf1$results[rf1$results$mtry == rf1$bestTune[[1]],], 20 rf2$results[rf2$results$mtry == rf2$bestTune[[1]],]) ## [1] TRUE 1all.equal(rf1, rf2) ## [1] \u0026#34;Component \\\u0026#34;times\\\u0026#34;: Component \\\u0026#34;everything\\\u0026#34;: Mean relative difference: 0.01735016\u0026#34; ## [2] \u0026#34;Component \\\u0026#34;times\\\u0026#34;: Component \\\u0026#34;final\\\u0026#34;: Mean relative difference: 0.6666667\u0026#34; I hope this post may be helpful to improve reproducibility of analysis using the caret package.\n","date":"May 30, 2015","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/blog/2015-05-30-setup-random-seeds-on-caret-package/","series":[],"smallImg":"","tags":[{"title":"R","url":"/tags/r/"}],"timestamp":1432944000,"title":"Setup Random Seeds on Caret Package"},{"categories":[{"title":"R","url":"/categories/r/"}],"content":"When I imagine a workflow, it is performing the same or similar tasks regularly (daily or weekly) in an automated way. Although those tasks can be executed in a script or a *source()*d script, it may not be easy to maintain separate scripts while the size of tasks gets bigger or if they have to be executed in different machines. In academia, reproducible research shares similar ideas but the level of reproducibility introduced in Gandrud, 2013 may not suffice in a business environment as the focus is documenting in a reproducible way. A R package, however, can be an effective tool and it can be considered like a portable class library in C# or Java. Like a class library, it can include a set of necessary tasks (usually using functions) and, being portable, its dependency can be managed well - for example, it is possible to set so that dependent packages can also be installed if some of them are not installed already. Moreover the benefit of creating a R package would be significant if it has to be deployed in a production server as it\u0026rsquo;d be a lot easier to convince system admin with the built-in unit tests, object documents and package vignettes. In this article an example of creating a R package is illustrated.\nIntroduction to the treebgg package This package extends the CART model by the rpart package (cartmore()) and implements bagging both sequentially (cartbgg()) and in parallel (cartbggs()). For sequantial bagging implementation, it returns the number of trees, variable importance, out-of-bag/test response and out-of-bag/test prediction of each tree in a list. The outputs of the sequential implementation are combined and it is performed so as to obtain variable importance measures and errors of bagged trees.\nSteps to create the treebgg package The treebgg package is created, following Wickham, 2015 - an overview of package development can also be checked in the Jeff Leek\u0026rsquo;s repo. With RStudio and several packages in Step 1, it is quite straightforward as long as something to be included is clear. Specific steps are listed below.\nInstall necessary packages in Getting Started section of Wickham, 2015. install.packages(c(\u0026quot;devtools\u0026quot;, \u0026quot;roxygen2\u0026quot;, \u0026quot;testthat\u0026quot;, \u0026quot;knitr\u0026quot;)) Create a R package in a new folder via R Studio (link). create README.md - just a new text file where the file extension is md. Create a GitHub repo with the same name (treebgg) empty repo without initialization with README.md Push into the remote GitHub repository HTTPS cd treebgg git status git add * git commit -a -m \u0026quot;initial commit\u0026quot; git remote add origin https://github.com/jaehyeon-kim/treebgg.git git push -u origin master SSH git remote add origin git@github.com:jaehyeon-kim/treebgg.git Otherwise the following error occurs error: The requested URL returned error: 403 Forbidden while accessing https://github.com/jaehyeon-kim/treebgg.git/info/refs Or Modify directly Open .git/confit and update url HTTP: url=https://github.com/jaehyeon-kim/treebgg.git SSH: url=git@github.com:jaehyeon-kim/treebgg.git Update R code, package meta data and object documents. R code - /R Package meta data - DESCRIPTION mostly auto-generated by roxygen2 Object documents - /man [if necessary] Build and reload package: Build \u0026ndash;\u0026gt; Build and Reload (Ctrl + Shift + B) or Clean and Rebuild Create documents: devtools::document() Complete unit tests using the testthat package. Testing devtools::use_testthat() /tests is created and testing files of 5 constructors/functions are created in /tests/testthat Build \u0026ndash;\u0026gt; Test Package or Ctrl + Shift + T Create a vignette document using the knitr package. Vignettes devtools::use_vignette(\u0026quot;intro_treebgg\u0026quot;) /vignettes created with a RMD file named above and DESCRIPTION updated to suggest the knitr package, VignetteBuilder set to knitr Installation The devtools package should be installed as the package exists in a GitHub repository only. For Windows users, Rtools has to be installed to build from source.\nThe package can be installed and loaded as following. Note that the following packages will be installed if they are not installed already: rpart, foreach, doParallel, iterators.\n1library(devtools) 2install_github(\u0026#34;jaehyeon-kim/treebgg\u0026#34;) 3{% endhighlight %} 4 5 6{% highlight r %} 7library(treebgg) CART extension To extend the CART model, a S3 object is instantiated (cartmore) by cartmore(), which fits the model at the least xerror or by the 1-SE rule - both classification and regression trees can be extended. The signature of this constructor is show below.\ncartmore(formula, trainData, testData = NULL) 1data(car90, package=\u0026#34;rpart\u0026#34;) 2# data for regression tree 3cars \u0026lt;- car90[, -match(c(\u0026#34;Rim\u0026#34;, \u0026#34;Tires\u0026#34;, \u0026#34;Model2\u0026#34;), names(car90))] 4cars$Price \u0026lt;- cars$Price/1000 5# data for classification tree 6div = quantile(cars$Price, 0.5, na.rm = TRUE) 7cars.cl \u0026lt;- transform(cars, Price = as.factor(ifelse(Price \u0026gt; div, \u0026#34;High\u0026#34;,\u0026#34;No\u0026#34;))) 8# regression (rg) and classification (cl) tree 9fit.rg \u0026lt;- cartmore(\u0026#34;Price ~ .\u0026#34;, cars, cars) # cars in testData is for illustration only 10fit.cl \u0026lt;- cartmore(\u0026#34;Price ~ .\u0026#34;, cars.cl) The object has 4 groups of elements. train/test means train/test data sets while lst/se means the cp values at the least xerror and by the 1-SE rule. The train elements keeps the model (rpart object), complexity-related values (cp, xerror and xstd), fitted values (fitted), variable importance measure (var.imp) and error (mean misclassification error or root mean squared error) (error). The test elements only hold the fitted values and error if a data set is specified and NULL if not.\n1class(fit.rg) ## [1] \u0026#34;cartmore\u0026#34; 1names(fit.rg) ## [1] \u0026#34;train.lst\u0026#34; \u0026#34;test.lst\u0026#34; \u0026#34;train.se\u0026#34; \u0026#34;test.se\u0026#34; 1names(fit.rg$train.lst) ## [1] \u0026#34;mod\u0026#34; \u0026#34;cp\u0026#34; \u0026#34;xerror\u0026#34; \u0026#34;xstd\u0026#34; \u0026#34;fitted\u0026#34; \u0026#34;var.imp\u0026#34; \u0026#34;error\u0026#34; 1names(fit.rg$test.lst) ## [1] \u0026#34;fitted\u0026#34; \u0026#34;error\u0026#34; 1names(fit.cl$test.lst) ## NULL 1c(fit.cl$train.lst$error, fit.cl$train.se$error) ## [1] 0.1047619 0.1714286 Sequential bagging implementation For sequantial bagging implementation, cartbgg() instantiates the cartbgg object and it returns a list of the number of trees (ntree), variable importance (var.imp), out-of-bag/test response (oob.res and test.res) and out-of-bag/test prediction of each tree (oob.pred and test.pred). The signature of cartbgg() is shown below.\ncartbgg(formula, trainData, testData = NULL, ntree = 1L) As can be seen in the signature, it has one extra argument that specifies the number of trees to generate - note that the type is restricted to be integer so that L should be followed by a numeric value.\n1data(car90, package=\u0026#34;rpart\u0026#34;) 2# data for regression tree 3cars \u0026lt;- car90[, -match(c(\u0026#34;Rim\u0026#34;, \u0026#34;Tires\u0026#34;, \u0026#34;Model2\u0026#34;), names(car90))] 4cars$Price \u0026lt;- cars$Price/1000 5# data for classification tree 6div = quantile(cars$Price, 0.5, na.rm = TRUE) 7cars.cl \u0026lt;- transform(cars, Price = as.factor(ifelse(Price \u0026gt; div, \u0026#34;High\u0026#34;,\u0026#34;No\u0026#34;))) 8# regression (rg) and classification (cl) bagged trees 9bgg.rg \u0026lt;- cartbgg(\u0026#34;Price ~ .\u0026#34;, cars, cars, ntree = 10L) # note integer, not numeric 10bgg.cl \u0026lt;- cartbgg(\u0026#34;Price ~ .\u0026#34;, cars.cl, ntree = 10L) # note integer, not numeric The class and elements can be checked by class() and names(). This object keeps the variable importance, response and prediction values in a data frame for ease of merging. In fact, values from each tree are merge()d. If testData is not specified, NULL is returned.\n1class(bgg.rg) ## [1] \u0026#34;cartbgg\u0026#34; 1names(bgg.rg) ## [1] \u0026#34;ntree\u0026#34; \u0026#34;var.imp\u0026#34; \u0026#34;oob.res\u0026#34; \u0026#34;oob.pred\u0026#34; \u0026#34;test.res\u0026#34; \u0026#34;test.pred\u0026#34; 1lst \u0026lt;- list(bgg.rg$var.imp, bgg.rg$oob.res, bgg.rg$oob.pred) 2sapply(lst, class) ## [1] \u0026#34;data.frame\u0026#34; \u0026#34;data.frame\u0026#34; \u0026#34;data.frame\u0026#34; 1sapply(list(bgg.cl$test.res, bgg.cl$test.pred), c) ## [[1]] ## NULL ## ## [[2]] ## NULL Parallel bagging implementation Basically cartbggs() combines the cartbgg objects in parallel, instantiating the cartbggs object. The parallel processing is performed by utilizing the following packages: parallel, foreach, doParallel and iterators. The signature of cartbggs() is shown below.\ncartbggs(formula, trainData, testData = NULL, eachTree = 1L, ncl = NULL, seed = NULL) It has three extra arguments - eachTree, ncl and seed. Rather than specifying the total number of trees to generate (ntree) in cartbgg(), it requires the number of trees to generate in each cluster (eachTree). The number of clusters (ncl) can be either specified or left out - if the number is left out, it is detected. The last arguments is for specifying a random seed. Note that the type of these argumens is restricted to be integer so that L should be followed by a numeric value.\n1data(car90, package=\u0026#34;rpart\u0026#34;) 2# data for regression tree 3cars \u0026lt;- car90[, -match(c(\u0026#34;Rim\u0026#34;, \u0026#34;Tires\u0026#34;, \u0026#34;Model2\u0026#34;), names(car90))] 4cars$Price \u0026lt;- cars$Price/1000 5# data for classification tree 6div = quantile(cars$Price, 0.5, na.rm = TRUE) 7cars.cl \u0026lt;- transform(cars, Price = as.factor(ifelse(Price \u0026gt; div, \u0026#34;High\u0026#34;,\u0026#34;No\u0026#34;))) 8# regression (rg) and classification (cl) bagged trees in parallel 9bggs.rg \u0026lt;- cartbggs(\u0026#34;Price ~ .\u0026#34;, cars, cars, eachTree = 10L, seed = 1237L) # note integer, not numeric 10bggs.cl \u0026lt;- cartbggs(\u0026#34;Price ~ .\u0026#34;, cars.cl, eachTree = 10L, seed = 1237L) # note integer, not numeric The ntree value is the total number of trees and it is eachTree multiplied by ncl. The bagged trees\u0026rsquo; variable importance measure is converted as percentage so that its sum is 100. Only the out-of-bag/test errors of bagged trees are returned and the test error is NULL if a data set is not entered in testData.\n1class(bggs.rg) ## [1] \u0026#34;cartbggs\u0026#34; 1names(bggs.rg) ## [1] \u0026#34;ntree\u0026#34; \u0026#34;var.imp\u0026#34; \u0026#34;oob.err\u0026#34; \u0026#34;test.err\u0026#34; 1bggs.rg ## $ntree ## [1] 60 ## ## $var.imp ## Country Disp Disp2 Eng.Rev Front.Hd ## 3.550772559 12.950732392 11.824200206 0.079064381 1.567940833 ## Frt.Leg.Room Frt.Shld Gear.Ratio Gear2 Height ## 2.064551549 1.321197687 0.070177629 1.894963262 1.118311638 ## HP HP.revs Length Luggage Mileage ## 16.135667088 1.633101113 6.023700543 1.148433807 0.004728218 ## Rear.Hd Rear.Seating RearShld Reliability Sratio.m ## 0.726666309 1.599366429 1.745734662 0.047711318 0.001935001 ## Sratio.p Steering Tank Trans1 Trans2 ## 0.279495784 0.919217450 11.254315624 0.442100562 0.133071794 ## Turning Type Weight Wheel.base Width ## 1.387680928 3.965640488 9.667418558 4.661085995 1.781016195 ## ## $oob.err ## [1] 1.112348 ## ## $test.err ## [1] 0.1138147 ## ## attr(,\u0026#34;class\u0026#34;) ## [1] \u0026#34;cartbggs\u0026#34; Roughly there would be two ways of performing a task. One is easy to start but hard to maintain and the other is hard to start but easy to maintain. Developing a R package should require more time to start but its benefit will be ongoing with little or no maintenance. Depending on complexity of a task, it\u0026rsquo;d be considerable to turning analysis into a package.\n","date":"March 24, 2015","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/blog/2015-03-24-packaging-analysis/","series":[],"smallImg":"","tags":[{"title":"R","url":"/tags/r/"}],"timestamp":1427155200,"title":"Packaging Analysis"},{"categories":[{"title":"R","url":"/categories/r/"}],"content":"In the previous posts, two groups of ways to implement parallel processing on a single machine are introduced. The first group is provided by the snow or parallel package and the functions are an extension of lapply() (LINK). The second group is based on an extension of the for construct (foreach, %dopar% and %:%). The foreach construct is provided by the foreach package while clusters are made and registered by the parallel and doParallel packages respectively (LINK). To conclude this series, three practical examples are discussed for comparison in this article.\nLet\u0026rsquo;s get started.\nThe following packages are loaded at first. Note that the randomForest, rpart and ISLR packages are necessary for the second and third examples and they are loaded later.\n1library(parallel) 2library(iterators) 3library(foreach) 4library(doParallel) k-means clustering This example is from McCallum and Weston (2012). It is originally created using clusterApply() in the snow package. Firstly a slight modification is made to be used with parLapplyLB() in the parallel package. Also a foreach construct is created for comparison.\nAccording to the document,\nthe data given by x are clustered by the k-means method, which aims to partition the points into k groups such that the sum of squares from points to the assigned cluster centres is minimized At the minimum, all data points are nearest to the cluster centres. The number of centers are specified by centers (4 in this example). The distance value is kept in tot.withinss. As initial clusters are randomly assigned at the beginning, fitting is performed multiple times and it is determined by nstart.\nparallel package The clusters are initialized by clusterEvalQ() as Boston data is available in the MASS package. A list of outputs are returned by parLapplyLB() and tot.withinss is extract by sapply(). The final outcome is what gives the minimum tot.withinss.\n1# parallel 2split = detectCores() 3eachStart = 25 4 5cl = makeCluster(split) 6init = clusterEvalQ(cl, { library(MASS); NULL }) 7results = parLapplyLB(cl 8 ,rep(eachStart, split) 9 ,function(nstart) kmeans(Boston, 4, nstart=nstart)) 10withinss = sapply(results, function(result) result$tot.withinss) 11result = results[[which.min(withinss)]] 12stopCluster(cl) 13 14result$tot.withinss ## [1] 1814438 foreach package The corresponding implementation using the foreach package is shown below. An iterator object is created to repeat the individual nstart value for the number of clusters (iters). A funtion to combine the outcome is created (comb()), which just keeps the outcome that gives the minimum tot.withinss - as .combine doesn\u0026rsquo;t seem to allow an argument, this kind of modification would be necessary.\n1# foreach 2split = detectCores() 3eachStart = 25 4# set up iterators 5iters = iter(rep(eachStart, split)) 6# set up combine function 7comb = function(res1, res2) { 8 if(res1$tot.withinss \u0026lt; res2$tot.withinss) res1 else res2 9} 10 11cl = makeCluster(split) 12registerDoParallel(cl) 13result = foreach(nstart=iters, .combine=\u0026#34;comb\u0026#34;, .packages=\u0026#34;MASS\u0026#34;) %dopar% 14 kmeans(Boston, 4, nstart=nstart) 15stopCluster(cl) 16 17result$tot.withinss ## [1] 1814438 random forest This example is from foreach packages\u0026rsquo;s vignette.\nAccording to the package document,\nrandomForest implements Breiman\u0026rsquo;s random forest algorithm (based on Breiman and Cutler\u0026rsquo;s original Fortran code) for classification and regression. parallel package x and y keep the predictors and response. A function (rf()) is created to implement the algorithm. If data has to be sent to each worker, it can be sent either by clusterCall() or by a function. If clusterApply() or clusterApplyLB() are used, the former should be used to reduce I/O operations time and it\u0026rsquo;d be alright to send by a function if parLapply() or parLapplyLB() are used - single I/O for each task split. (for details, see the first article) As the randomForest package provides a function to combine the objects (combine()), it is used in do.call(). Finally a confusion table is created.\n1## Random forest 2library(randomForest) 3 4# parallel 5rm(list = ls()) 6set.seed(1237) 7x = matrix(runif(500), 100) 8y = gl(2,50) 9 10split = detectCores() 11eachTrees = 250 12# define function to fit random forest given predictors and response 13# data has to be sent to workers using this function 14rf = function(ntree, pred, res) { 15 randomForest(pred, res, ntree=ntree) 16} 17 18cl = makeCluster(split) 19clusterSetRNGStream(cl, iseed=1237) 20init = clusterEvalQ(cl, { library(randomForest); NULL }) 21results = parLapplyLB(cl, rep(eachTrees, split), rf, pred=x, res=y) 22result = do.call(\u0026#34;combine\u0026#34;, results) 23stopCluster(cl) 24 25cm = table(data.frame(actual=y, fitted=result$predicted)) 26cm ## fitted ## actual 1 2 ## 1 23 27 ## 2 28 22 foreach package randomForest() is directly used in the foreach construct and the returned outcomes are combined by combine() (.combine=\u0026ldquo;combine\u0026rdquo;). The fitting function has to be available in each worker and it is set by .packages=\u0026ldquo;randomForest\u0026rdquo;. As there are multiple argument in the combine function, it is necessary to set the multi-combine option to be TRUE (.multicombine=TRUE) - this option will be discussed further in the next example. As the above example, a confusion matrix is created at the end - both the results should be the same as the same streams of random numbers are set to be generated by clusterSetRNGStream().\n1# foreach 2rm(list = ls()) 3set.seed(1237) 4x = matrix(runif(500), 100) 5y = gl(2,50) 6 7split = detectCores() 8eachTrees = 250 9# set up iterators 10iters = iter(rep(eachTrees, split)) 11 12cl = makeCluster(split) 13clusterSetRNGStream(cl, iseed=1237) 14registerDoParallel(cl) 15result = foreach(ntree=iters, .combine=\u0026#34;combine\u0026#34;, .multicombine=TRUE, .packages=\u0026#34;randomForest\u0026#34;) %dopar% 16 randomForest(x, y, ntree=ntree) 17stopCluster(cl) 18 19cm = table(data.frame(actual=y, fitted=result$predicted)) 20cm ## fitted ## actual 1 2 ## 1 23 27 ## 2 28 22 bagging Although bagging can be implemented using the randomForest package, another quick implementation is tried using the rpart package for illustration (cartBGG()). Specifically bootstrap samples can be created for the number of trees specified by ntree. To simplify discussion, only the variable importance values are kept - an rpart object keeps this details in variable.importance. Therefore cartBGG() returns a list where its only element is a data frame where the number of rows is the same to the number of predictors and the number of columns is the same to the number of trees. In fact, cartBGG() is a constructor that generates a S3 object (rpartbgg).\n1## Bagging 2rm(list = ls()) 3# update variable importance measure of bagged trees 4cartBGG = function(formula, trainData, ntree=1, ...) { 5 # extract response name and index 6 res.name = gsub(\u0026#34; \u0026#34;,\u0026#34;\u0026#34;,unlist(strsplit(formula,split=\u0026#34;~\u0026#34;))[[1]]) 7 res.ind = match(res.name, colnames(trainData)) 8 9 # variable importance - merge by \u0026#39;var\u0026#39; 10 var.imp = data.frame(var=colnames(trainData[,-res.ind])) 11 require(rpart) 12 for(i in 1:ntree) { 13 # create in bag and out of bag sample 14 bag = sample(nrow(trainData), size=nrow(trainData), replace=TRUE) 15 inbag = trainData[bag,] 16 outbag = trainData[-bag,] 17 # fit model 18 mod = rpart(formula=formula, data=inbag, control=rpart.control(cp=0)) 19 # set helper variables 20 colname = paste(\u0026#34;s\u0026#34;,i,sep=\u0026#34;.\u0026#34;) 21 pred.type = ifelse(class(trainData[,res.ind])==\u0026#34;factor\u0026#34;,\u0026#34;class\u0026#34;,\u0026#34;vector\u0026#34;) 22 # merge variable importance 23 imp = data.frame(names(mod$variable.importance), mod$variable.importance) 24 colnames(imp) = c(\u0026#34;var\u0026#34;, colname) 25 var.imp = merge(var.imp, imp, by=\u0026#34;var\u0026#34;, all=TRUE) 26 } 27 # adjust outcome 28 rownames(var.imp) = var.imp[,1] 29 var.imp = var.imp[,2:ncol(var.imp)] 30 31 # create outcome as a list 32 result=list(var.imp = var.imp) 33 class(result) = c(\u0026#34;rpartbgg\u0026#34;) 34 result 35} If the total number of trees are split into clusters (eg into 4 clusters), there will be 4 lists and it is possible to combine them. Below is an example of such a function (comBGG()) - it just sums individual variable importance values.\nSpecifically\narguments shouldn\u0026rsquo;t be dertermined (\u0026hellip;) as the number of clusters can vary (an thus the number of lists) a list is created that binds the variable number of arguments (list(...)) only the elements that keeps variable importance are extracted into another list by lapply() the new list of variable importance values can be restructured using do.call() and cbind() finally each row values are added by apply() 1# combine variable importance of bagged trees 2comBGG = function(...) { 3 # add rpart objects in a list 4 bgglist = list(...) 5 # extract variable importance 6 var.imp = lapply(bgglist, function(x) x$var.imp) 7 # combine and sum by row 8 var.imp = do.call(\u0026#34;cbind\u0026#34;, var.imp) 9 var.imp = apply(var.imp, 1, sum, na.rm=TRUE) 10 var.imp 11} parallel package Similar to the above example, bagged trees are generated across clusters using cartBGG(). Then the result is combined by comBGG().\n1# data 2library(ISLR) 3data(Carseats) 4 5# parallel 6split = detectCores() 7eachTree = 250 8 9cl = makeCluster(split) 10clusterSetRNGStream(cl, iseed=1237) 11# rpart is required in cartBGG(), not need to load in each cluster 12results = parLapplyLB(cl 13 ,rep(eachTree, split) 14 ,cartBGG, formula=\u0026#34;Sales ~ .\u0026#34;, trainData=Carseats, ntree=eachTree) 15result = do.call(\u0026#34;comBGG\u0026#34;, results) 16stopCluster(cl) 17 18result ## Advertising Age CompPrice Education Income Population ## 295646.22 366140.36 514889.28 144642.19 271524.31 232959.06 ## Price ShelveLoc Urban US ## 964252.07 978758.83 24794.35 80023.66 foreach package This example is simplar to the random forest example. Note that .multicombine determines how many arguments are combined. For performance, the maximum number is 100 if the value is TRUE and 2 if FALSE by default (.maxcombine=if (.multicombine) 100 else 2). As there are more than 2 lists in this example, it should be set TRUE so that all lists generated by the clusters can be combined.\n1# foreach 2split = detectCores() 3eachTrees = 250 4# set up iterators 5iters = iter(rep(eachTrees, split)) 6 7cl = makeCluster(split) 8clusterSetRNGStream(cl, iseed=1237) 9registerDoParallel(cl) 10# note .multicombine=TRUE as \u0026gt; 2 arguments 11# .maxcombine=if (.multicombine) 100 else 2 12result = foreach(ntree=iters, .combine=\u0026#34;comBGG\u0026#34;, .multicombine=TRUE) %dopar% 13 cartBGG(formula=\u0026#34;Sales ~ .\u0026#34;, trainData=Carseats, ntree=ntree) 14stopCluster(cl) 15 16result ## Advertising Age CompPrice Education Income Population ## 295646.22 366140.36 514889.28 144642.19 271524.31 232959.06 ## Price ShelveLoc Urban US ## 964252.07 978758.83 24794.35 80023.66 Three practical examples of implementing parallel processing in a single machine are discussed in this post. They are relatively easy to implement and all existing packages can be used directly. I hope this series of posts are useful.\n","date":"March 19, 2015","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/blog/2015-03-19-parallel-processing-on-single-machine-3/","series":[{"title":"Parallel Processing on Single Machine","url":"/series/parallel-processing-on-single-machine/"}],"smallImg":"","tags":[{"title":"R","url":"/tags/r/"}],"timestamp":1426723200,"title":"Parallel Processing on Single Machine - Part III"},{"categories":[{"title":"R","url":"/categories/r/"}],"content":"In the previous article, parallel processing on a single machine using the snow and parallel packages are introduced. The four functions are an extension of lapply() with an additional argument that specifies a cluster object. In spite of their effectiveness and ease of use, there may be cases where creating a function that can be sent into clusters is not easy or looping may be more natural. In this article, another way of implementing parallel processing on a single machine is introduced using the foreach and doParallel packages where clusters are created by the parallel package. Finally the iterators package is briefly covered as it can facilitate writing a loop. The examples here are largely based on the individual packages\u0026rsquo; vignettes and further details can be found there.\nLet\u0026rsquo;s get started.\nThe following packages are loaded.\n1library(parallel) 2library(iterators) 3library(foreach) 4library(doParallel) A key difference between the for construct in base R and the foreach construct in the foreach package is as following.\nfor causes a side-effect side-effect means state of something is changed. For example, printing a variable, changing the value of a variable and writing data to disk are side-effects. foreach returns a variable A list is created by default. An equivalent outcome by the two can be created as following.\n1x = list() 2for(i in 1:3) x[[length(x)+1]] = exp(i) # values of x is changed 3x ## [[1]] ## [1] 2.718282 ## ## [[2]] ## [1] 7.389056 ## ## [[3]] ## [1] 20.08554 1x = foreach(i=1:3) %do% exp(i) 2x ## [[1]] ## [1] 2.718282 ## ## [[2]] ## [1] 7.389056 ## ## [[3]] ## [1] 20.08554 The foreach construct has two binary operators for executing a loop: %do% and %dopar%. The first executes a loop sequentially while the latter does it in parallel. By default, the doParallel package uses functionality of the multicore package on Unix-like systems and that of the snow package on Windows. However the default type value (PSOCK) of makeCluster() in the parallel package is brought from the snow package and thus the socket transport by the package will be used in this example regardless of operation systems. The number of cores (or workers in the socket transport) is identified by detectCores() and this function is provided by the parallel package. Note that, if a cluster object is not setup, the loop will be executed sequentially.\n1system.time(foreach(i=1:4) %do% Sys.sleep(i)) ## user system elapsed ## 0.010 0.084 10.006 1cl = makeCluster(detectCores()) 2registerDoParallel(cl) 3system.time(foreach(i=1:4) %dopar% Sys.sleep(i)) ## user system elapsed ## 0.013 0.033 4.056 1stopCluster(cl) Multiple iterators can be used and (1) more than one expressions can be run in parentheses. (2) If more than one iterators are used, the number of iterations is the minimum length of the iterators. Some examples are shown below.\n1x = foreach(i=1:3, j=rep(10,3)) %do% (i + j) 2x = foreach(i=1:3, j=rep(10,3)) %do% { 3 # do something (1) 4 i + j 5} 6x = foreach(i=rep(0,100), j=rep(10,3)) %do% (i + j) 7do.call(\u0026#34;sum\u0026#34;,x) # (2) ## [1] 30 There are some options to control a loop or to change the return data type. Some of them are\n.combine: A function can be specified to reduce the outcome variable. c, + - * / ..., rbind/cbind and min/max are some of useful built-in functions. A user-defined function can also be created. .multicombine and .maxcombine can be set to determine how a function is applied - actually I don\u0026rsquo;t fully understand these options and see the vignette for further details. 1foreach(i=1:2, .combine=\u0026#34;c\u0026#34;) %do% exp(i) ## [1] 2.718282 7.389056 1foreach(i=1:2, .combine=\u0026#34;rbind\u0026#34;) %do% exp(i) ## [,1] ## result.1 2.718282 ## result.2 7.389056 1foreach(i=1:2, .combine=\u0026#34;+\u0026#34;) %do% exp(i) ## [1] 10.10734 1foreach(i=1:2, .combine=\u0026#34;min\u0026#34;) %do% exp(i) ## [1] 2.718282 1# custom min function 2cusMin = function(a, b) if(a \u0026lt; b) a else b 3foreach(i=1:2, .combine=\u0026#34;cusMin\u0026#34;) %do% exp(i) ## [1] 2.718282 .inorder: The order of sequence is reserved if TRUE and it is relevant if a loop is executed in parallel. The default value is TRUE.\n.packages: By specifying one or more pckage names, the package(s) can be loaded in each cluster. This is similar to initialize a worker using clusterEvalQ() in the parallel package.\n1# return row dimension of Boston data in each cluster 2cl = makeCluster(detectCores()) 3registerDoParallel(cl) 4x = foreach(i=1:detectCores(), .combine=\u0026#34;c\u0026#34;, .packages=\u0026#34;MASS\u0026#34;) %dopar% dim(Boston)[1] 5stopCluster(cl) 6x ## [1] 506 506 506 506 The binary operator of %:% can be used for list comprehension (filtering which to loop with when) and nested looping.\nAn example of list comprehension is shown below. It returns a vector of even numbers.\n1# reutrn a vector of even numbers 2foreach(i=1:10, .combine=\u0026#34;c\u0026#34;) %:% when(i %% 2 == 0) %do% i ## [1] 2 4 6 8 10 Below shows an example of nested looping by for and foreach. According to the vignette, it is not necessary to determine which loop (inner or outer) to parallize as %:% turns multiple foreach loops into a single stream of tasks that can be parallelized.\n1# for 2avec = c(10,20) 3bvec = 1:4 4mat = matrix(0, nrow=length(avec), ncol=length(bvec)) 5for(b in bvec) { 6 for(a in 1:length(avec)) { 7 mat[a,b] = avec[a] + bvec[b] 8 } 9} 10mat ## [,1] [,2] [,3] [,4] ## [1,] 11 12 13 14 ## [2,] 21 22 23 24 1# foreach - sequential 2x = foreach(b=1:4, .combine=\u0026#34;cbind\u0026#34;) %:% 3 foreach(a=c(10,20), .combine=\u0026#34;c\u0026#34;) %do% (a + b) 4x ## result.1 result.2 result.3 result.4 ## [1,] 11 12 13 14 ## [2,] 21 22 23 24 1# foreach - parallel 2cl = makeCluster(detectCores()) 3registerDoParallel(cl) 4x = foreach(b=1:4, .combine=\u0026#34;cbind\u0026#34;) %:% 5 foreach(a=c(10,20), .combine=\u0026#34;c\u0026#34;) %dopar% (a + b) 6stopCluster(cl) 7x ## result.1 result.2 result.3 result.4 ## [1,] 11 12 13 14 ## [2,] 21 22 23 24 As a loop is constructed by foreach, the iterators package can be useful as the package allows to create an iterator object from a conventional R objects: vectors, data frames, matrices, lists and even functions. Some examples are shown below.\n1## create iterator object 2# by object 3iters = iter(1:10) 4c(nextElem(iters),nextElem(iters)) ## [1] 1 2 1df = data.frame(number=c(1:26), letter=letters) 2iters = iter(df, by=\u0026#34;row\u0026#34;) 3nextElem(iters) ## number letter ## 1 1 a 1iters = iter(list(1:2, 3:4)) 2for(i in 1:iters$length) print(nextElem(iters)) ## [1] 1 2 ## [1] 3 4 1# by function 2set.seed(1237) 3iters = iter(function() sample(0:9, 4, replace=TRUE)) 4nextElem(iters) ## [1] 3 9 0 1 The following code generates Error: StopIteration error at the end as there is no nextElem available.\n1iters = iter(list(1:2, 3:4)) 2for(i in 1:(iters$length+1)) print(nextElem(iters)) A quick example of using an iterator object with foreach is shown below.\n1set.seed(1237) 2mat = matrix(rnorm(100),nrow=1) 3iters = iter(mat, by=\u0026#34;row\u0026#34;) 4foreach(a=iters, .combine=\u0026#34;c\u0026#34;) %do% mean(a) ## [1] 0.03148661 Finally this package provides some wappers around some of built-in functions: icount(), irnorm(), irunit(), irbinom(), irnbinom() and irpois().\nSo far two groups of ways are introduced to perform parallel processing on a single machine. The first group uses an extended lapply() while the latter is an extension of for construct. In the next article, they will be compared with more realistic examples.\n","date":"March 17, 2015","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/blog/2015-03-17-parallel-processing-on-single-machine-2/","series":[{"title":"Parallel Processing on Single Machine","url":"/series/parallel-processing-on-single-machine/"}],"smallImg":"","tags":[{"title":"R","url":"/tags/r/"}],"timestamp":1426550400,"title":"Parallel Processing on Single Machine - Part II"},{"categories":[{"title":"R","url":"/categories/r/"}],"content":"Lack of multi-threading and memory limitation are two outstanding weaknesses of base R. In fact, however, if the size of data is not so large that it can be read in RAM, the former would be relatively easily handled by parallel processing, provided that multiple processors are equipped. This article introduces to a way of implementing parallel processing on a single machine using the snow and parallel packages - the examples are largely based on McCallum and Weston (2012).\nThe snow and multicore are two of the packages for parallel processing and the parallel package, which has been included in the base R distribution by CRAN (since R 2.14.0), provides functions of both the packages (and more). Only the functions based on the snow package are covered in the article.\nThe functions are similar to lapply() and they just have an additional argument for a cluster object (cl). The following 4 functions will be compared.\nclusterApply(cl, x, fun, ...) pushes tasks to workers by distributing x elements clusterApplyLB(cl, x, fun, ...) clusterApply + load balancing (workers pull tasks as needed) efficient if some tasks take longer or some workers are slower parLapply(cl, x, fun, ...) clusterApply + scheduling tasks by splitting x given clusters docall(c, clusterApply(cl, splitList(x, length(cl)), lapply, fun, \u0026hellip;)) parLapplyLB(cl = NULL, X, fun, ...) clusterApply + tasks scheduling + load balancing available only in the parallel package Let\u0026rsquo;s get started.\nAs the packages share the same function names, the following utility function is used to reset environment at the end of examples.\n1reset = function(package) { 2 unloadPkg = function(pkg) { 3 detach(search()[grep(paste0(\u0026#34;*\u0026#34;,pkg),search())] 4 ,unload=TRUE 5 ,character.only=TRUE) 6 } 7 unloadPkg(package) # unload package 8 rm(list = ls()[ls()!=\u0026#34;knitPost\u0026#34;]) # leave utility function 9 rm(list = ls()[ls()!=\u0026#34;moveFigs\u0026#34;]) # leave utility function 10 par(mfrow=c(1,1),mar=c(5.1, 4.1, 4.1, 2.1)) # reset graphics parameters 11} Make and stop a cluster In order to make a cluster, the socket transport is selected (type=\u0026ldquo;SOCK\u0026rdquo; or type=\u0026ldquo;PSOCK\u0026rdquo;) and the number of workers are set manually using the snow package (spec=4). In the parallel package, it can be detected by detectCores(). stopCluster() stops a cluster.\n1## make and stop cluster 2require(snow) 3spec = 4 4cl = makeCluster(spec, type=\u0026#34;SOCK\u0026#34;) 5stopCluster(cl) 6reset(\u0026#34;snow\u0026#34;) 7 8require(parallel) 9# number of workers can be detected 10# type: \u0026#34;PSOCK\u0026#34; or \u0026#34;FORK\u0026#34;, default - \u0026#34;PSOCK\u0026#34; 11cl = makeCluster(detectCores()) 12stopCluster(cl) 13reset(\u0026#34;parallel\u0026#34;) CASE I - load balancing matters As mentioned earlier, load balancing can be important when some tasks take longer or some workers are slower. In this example, system sleep time is assigned randomly so as to compare how tasks are performed using snow.time() in the snow package and how long it takes by the functions using system.time().\n1# snow 2require(snow) 3set.seed(1237) 4sleep = sample(1:10,10) 5spec = 4 6cl = makeCluster(spec, type=\u0026#34;SOCK\u0026#34;) 7st = snow.time(clusterApply(cl, sleep, Sys.sleep)) 8stLB = snow.time(clusterApplyLB(cl, sleep, Sys.sleep)) 9stPL = snow.time(parLapply(cl, sleep, Sys.sleep)) 10stopCluster(cl) 11par(mfrow=c(3,1),mar=rep(2,4)) 12plot(st, title=\u0026#34;clusterApply\u0026#34;) 13plot(stLB, title=\u0026#34;clusterApplyLB\u0026#34;) 14plot(stPL, title=\u0026#34;parLapply\u0026#34;) Both clusterApplyLB() and parLapply() takes shorter than clusterApply(). The efficiency of the former is due to load balancing (pulling a task when necessary) while that of the latter is because of a lower number of I/O operations thanks to task scheduling, which allows a single I/O operation in a chunk (or split) - its benefit is more outstanding when one or more arguments are sent to workers as shown in the next example. The scheduling can be checked by clusterSplit().\n1sleep ## [1] 4 9 1 8 2 10 5 6 3 7 1clusterSplit(cl, sleep) ## [[1]] ## [1] 4 9 1 ## ## [[2]] ## [1] 8 2 ## ## [[3]] ## [1] 10 5 ## ## [[4]] ## [1] 6 3 7 1# clear env 2reset(\u0026#34;snow\u0026#34;) The above functions can also be executed using the parallel package and it provides an additional function (parLapplyLB()). As snow.time() is not available, their system time is compared below.\n1# parallel 2require(parallel) 3set.seed(1237) 4sleep = sample(1:10,10) 5cl = makeCluster(detectCores()) 6st = system.time(clusterApply(cl, sleep, Sys.sleep)) 7stLB = system.time(clusterApplyLB(cl, sleep, Sys.sleep)) 8stPL = system.time(parLapply(cl, sleep, Sys.sleep)) 9stPLB = system.time(parLapplyLB(cl, sleep, Sys.sleep)) 10stopCluster(cl) 11sysTime = do.call(\u0026#34;rbind\u0026#34;,list(st,stLB,stPL,stPLB)) 12sysTime = cbind(sysTime,data.frame(fun=c(\u0026#34;clusterApply\u0026#34;,\u0026#34;clusterApplyLB\u0026#34; 13 ,\u0026#34;parLapply\u0026#34;,\u0026#34;parLapplyLB\u0026#34;))) 14require(ggplot2) 15ggplot(data=sysTime, aes(x=fun,y=elapsed,fill=fun)) + 16 geom_bar(stat=\u0026#34;identity\u0026#34;) + ggtitle(\u0026#34;Elapsed time of each function\u0026#34;) 1# clear env 2reset(\u0026#34;parallel\u0026#34;) CASE II - I/O operation matters In this example, a case where an argument is sent to workers is considered. While the argument is passed to wokers once for each task by clusterApply() and clusterApplyLB(), it is sent to each chunk once by parLapply() and parLapplyLB(). Therefore the benefit of the latter group of functions can be outstanding in this example - it can be checked workers are idle inbetween in the first two plots while tasks are performed continuously in the last plot.\n1# snow 2require(snow) 3mat = matrix(0, 2000, 2000) 4sleep = rep(1,50) 5fcn = function(st, arg) Sys.sleep(st) 6spec = 4 7cl = makeCluster(spec, type=\u0026#34;SOCK\u0026#34;) 8st = snow.time(clusterApply(cl, sleep, fcn, arg=mat)) 9stLB = snow.time(clusterApplyLB(cl, sleep, fcn, arg=mat)) 10stPL = snow.time(parLapply(cl, sleep, fcn, arg=mat)) 11stopCluster(cl) 12par(mfrow=c(3,1),mar=rep(2,4)) 13plot(st, title=\u0026#34;clusterApply\u0026#34;) 14plot(stLB, title=\u0026#34;clusterApplyLB\u0026#34;) 15plot(stPL, title=\u0026#34;parLapply\u0026#34;) 1# clear env 2reset(\u0026#34;snow\u0026#34;) Although clusterApplyLB() has some improvement over clusterApply(), it is parLapply() which takes the least amount of time. Actually, for the snow package, McCallum and Weston (2012) recommends parLapply() and it\u0026rsquo;d be better to use parLapplyLB() if the parallel package is used. The elapsed time of each function is shown below - the last two functions\u0026rsquo; elapsed time is identical as individual tasks are assumed to take exactly the same amount of time.\n1# parallel 2require(parallel) 3mat = matrix(0, 2000, 2000) 4sleep = rep(1,50) 5fcn = function(st, arg) Sys.sleep(st) 6cl = makeCluster(detectCores()) 7st = system.time(clusterApply(cl, sleep, fcn, arg=mat)) 8stLB = system.time(clusterApplyLB(cl, sleep, fcn, arg=mat)) 9stPL = system.time(parLapply(cl, sleep, fcn, arg=mat)) 10stPLB = system.time(parLapplyLB(cl, sleep, fcn, arg=mat)) 11stopCluster(cl) 12sysTime = do.call(\u0026#34;rbind\u0026#34;,list(st,stLB,stPL,stPLB)) 13sysTime = cbind(sysTime,data.frame(fun=c(\u0026#34;clusterApply\u0026#34;,\u0026#34;clusterApplyLB\u0026#34; 14 ,\u0026#34;parLapply\u0026#34;,\u0026#34;parLapplyLB\u0026#34;))) 15require(ggplot2) 16ggplot(data=sysTime, aes(x=fun,y=elapsed,fill=fun)) + 17 geom_bar(stat=\u0026#34;identity\u0026#34;) + ggtitle(\u0026#34;Elapsed time of each function\u0026#34;) 1# clear env 2reset(\u0026#34;parallel\u0026#34;) Initialization of workers Sometimes workers have to be initialized (eg loading a library) and two functions can be used: clusterEvalQ() and clusterCall(). While the former just executes an expression, it is possible to send a variable using the latter. Note that it is recommended to let an expression or a function return NULL in order not to receive unnecessary data from workers (McCallum and Weston (2012)). Only an example by the snow package is shown below.\n1# snow and parallel 2require(snow) 3spec = 4 4cl = makeCluster(spec, type=\u0026#34;SOCK\u0026#34;) 5# execute expression 6exp = clusterEvalQ(cl, { library(MASS); NULL }) 7# execute expression + pass variables 8worker.init = function(arg) { 9 for(a in arg) library(a, character.only=TRUE) 10 NULL 11} 12expCall = clusterCall(cl, worker.init, arg=c(\u0026#34;MASS\u0026#34;,\u0026#34;boot\u0026#34;)) 13stopCluster(cl) 14 15# clear env 16reset(\u0026#34;snow\u0026#34;) Random number generation In order to ensure that each worker has different random numbers, independent randome number streams have to be set up. In the snow package, either the L\u0026rsquo;Ecuyer\u0026rsquo;s random number generator by the rlecuyer package or the SPRNG generator by the rsprng are used in clusterSetupRNG(). Only the former is implemented in the parallel package in clusterSetRNGStream() and, as it uses its own algorithm, the rlecuyer package is not necessary. For reproducibility, a seed can be specified and, for the function in the snow package, a vector of six integers is necessary (eg seed=rep(1237,6)) while an integer value is required for the function in the parallel package (eg iseed=1237). Each of the examples are shown below.\n1# snow 2require(snow) 3require(rlecuyer) 4# Uniform Random Number Generation in SNOW Clusters 5# seed is six integer values if RNGStream 6spec = 4 7cl = makeCluster(spec, type=\u0026#34;SOCK\u0026#34;) 8rndSeed = function(x) { 9 clusterSetupRNG(cl, type=\u0026#34;RNGstream\u0026#34;, seed=rep(1237,6)) 10 unlist(clusterEvalQ(cl, rnorm(1))) 11} 12t(sapply(1:2,rndSeed)) ## [,1] [,2] [,3] [,4] ## [1,] -0.2184466 1.237636 0.2448028 -0.5889211 ## [2,] -0.2184466 1.237636 0.2448028 -0.5889211 1stopCluster(cl) 2 3# clear env 4reset(\u0026#34;snow\u0026#34;) 5 6# parallel 7require(parallel) 8cl = makeCluster(detectCores()) 9rndSeed = function(x) { 10 clusterSetRNGStream(cl, iseed=1237) 11 unlist(clusterEvalQ(cl, rnorm(1))) 12} 13t(sapply(1:2,rndSeed)) ## [,1] [,2] [,3] [,4] ## [1,] 0.5707716 -0.2752422 0.3562034 -0.08946821 ## [2,] 0.5707716 -0.2752422 0.3562034 -0.08946821 1stopCluster(cl) 2 3# clear env 4reset(\u0026#34;parallel\u0026#34;) A quick introduction to the snow and parallel packages is made in this article. Sometimes it may not be easy to create a function that can be sent into clusters or looping may be more natural for computation. In this case, the foreach package would be used and an introduction to this package will be made in the next article.\n","date":"March 14, 2015","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/blog/2015-03-14-parallel-processing-on-single-machine-1/","series":[{"title":"Parallel Processing on Single Machine","url":"/series/parallel-processing-on-single-machine/"}],"smallImg":"","tags":[{"title":"R","url":"/tags/r/"}],"timestamp":1426291200,"title":"Parallel Processing on Single Machine - Part I"},{"categories":[{"title":"Machine Learning","url":"/categories/machine-learning/"}],"content":" Part I Part II Part III Part IV Part V Part VI (this post) A regression tree is evaluated using bagged trees in the previous article. In this article, the response variable of the same data set is converted into a binary factor variable and a classification tree is evaluated by comparing to bagged trees\u0026rsquo; individual oob/test errors, cumulative oob/test errors and variable importance measures.\nBefore getting started, note that the source of the classes can be found in this gist and, together with the relevant packages (see tags), it requires a utility function (bestParam()) that can be found here.\nData is split as usual.\n1## data 2require(ISLR) 3data(Carseats) 4require(dplyr) 5Carseats = Carseats %\u0026gt;% 6 mutate(High=factor(ifelse(Sales\u0026lt;=8,\u0026#34;No\u0026#34;,\u0026#34;High\u0026#34;),labels=c(\u0026#34;High\u0026#34;,\u0026#34;No\u0026#34;))) 7data.cl = subset(Carseats, select=c(-Sales)) 8data.rg = subset(Carseats, select=c(-High)) 9 10# split data 11require(caret) 12set.seed(1237) 13trainIndex = createDataPartition(Carseats$High, p=0.8, list=FALSE, times=1) 14trainData.cl = data.cl[trainIndex,] 15testData.cl = data.cl[-trainIndex,] Both the single and bagged classification trees are fit. For bagging, 500 trees are generated - as shown below, the cumulative test error settles well before this number of trees but this is not the case for the cumulative oob error. The number of the trees is just set rather than tuned in this article.\n1## run rpartDT 2# import constructors 3source(\u0026#34;src/cart.R\u0026#34;) 4set.seed(12357) 5cl = cartDT(trainData.cl, testData.cl, \u0026#34;High ~ .\u0026#34;, ntree=500) The summary of the cp values of the bagged trees are shown below, followed by the single tree\u0026rsquo;s cp value at the least xerror.\n1## cp values 2# cart 3cl$rpt$cp[1,][[1]] ## [1] 0.01136364 1# bagging 2summary(t(cl$boot.cp)[,2]) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.000000 0.000000 0.002407 0.007940 0.014930 0.048950 Individual Error From the distributions of the oob and test errors, it is found that\nthe distributions are roughly bell-shaped where the centers are closer to 0.3 - interestingly the mean of the oob error (0.29) is higher than that of the test error (0.26) while their standard deviations are not that much different, the single tree\u0026rsquo;s test error is quite away from the means of the bagged trees\u0026rsquo; oob and test errors - it is 2.38 and 1.43 standard deviation away repectively and the test error of the single tree (0.19) seems to be quite optimistic. 1## individual errors 2# cart test error 3crt.err = cl$rpt$test.lst$error$error 4crt.err ## [1] 0.19 1# bagging error at least xerror - se to see 1-SE rule 2ind.oob.err = data.frame(type=\u0026#34;oob\u0026#34;,error=unlist(cl$ind.oob.lst.err)) 3ind.tst.err = data.frame(type=\u0026#34;test\u0026#34;,error=unlist(cl$ind.tst.lst.err)) 4ind.err = rbind(ind.oob.err,ind.tst.err) 5ind.err.summary = as.data.frame(rbind(summary(ind.err$error[ind.err$type==\u0026#34;oob\u0026#34;]) 6 ,summary(ind.err$error[ind.err$type==\u0026#34;test\u0026#34;]))) 7rownames(ind.err.summary) \u0026lt;- c(\u0026#34;oob\u0026#34;,\u0026#34;test\u0026#34;) 8ind.err.summary ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## oob 0.1695 0.2605 0.2881 0.2890 0.3162 0.4182 ## test 0.1392 0.2278 0.2532 0.2593 0.2911 0.4557 A graphical illustration of the error distributions and the location of the single tree\u0026rsquo;s test error are shown below.\n1# plot error distribution 2ggplot(ind.err, aes(x=error,fill=type)) + 3 geom_histogram() + geom_vline(xintercept=crt.err, color=\u0026#34;blue\u0026#34;) + 4 ggtitle(\u0026#34;Error distribution\u0026#34;) + theme(plot.title=element_text(face=\u0026#34;bold\u0026#34;)) Cumulative Error The plots of cumulative oob and test errors are shown below. It is found that, while the cumulative test error (0.152) settles just after the 300th tree and it is lower than the single tree\u0026rsquo;s test error (0.19), the cumulative oob error fluctuates and it is higher than that of the single tree - the error at the 500th tree is 0.218 (the oob error doesn\u0026rsquo;t settle even a higher number of trees are tried).\nIt would be understood as following.\nHastie et al.(2008) demonstrates that an aggregate estimator tends to decrease mean-squared error as averaging can lower variance and leaves bias unchanged. However, as bias and variance are non-additive, it does not hold for classification under 0-1 loss and an aggregate estimator\u0026rsquo;s performance depends on how good the classifer is. Also, compared to the cumulative test error, the single tree seems to overfit the train data so that its prediction power is impared. According to the above demonstration and comparison to the cumulative test error, the quality of the CART model as a classifier might be questionable for this data set. 1bgg.oob.err = data.frame(type=\u0026#34;oob\u0026#34; 2 ,ntree=1:length(cl$cum.oob.lst.err) 3 ,error=unlist(cl$cum.oob.lst.err)) 4bgg.tst.err = data.frame(type=\u0026#34;test\u0026#34; 5 ,ntree=1:length(cl$cum.tst.lst.err) 6 ,error=unlist(cl$cum.tst.lst.err)) 7bgg.err = rbind(bgg.oob.err,bgg.tst.err) 8 9# plot bagging errors 10ggplot(data=bgg.err,aes(x=ntree,y=error,colour=type)) + 11 geom_line() + geom_abline(intercept=crt.err,slope=0,color=\u0026#34;blue\u0026#34;) + 12 ggtitle(\u0026#34;Bagging error\u0026#34;) + theme(plot.title=element_text(face=\u0026#34;bold\u0026#34;)) Variable Importance Like the regression task, Price and ShelveLoc are shown to be the two most important variables although their ranks are reversed. In the single tree, these variables are more valued than bagging and the bottom 4 variables are measured having little value - even US and Urban are not employed to reduce impurity.\nDifference in composition of the variable importance measures and concentration to the top 2 variables seems to bring more questions with regard to its effectiveness as a classifier for this data set.\n1# cart 2cart.varImp = data.frame(method=\u0026#34;cart\u0026#34; 3 ,variable=names(cl$rpt$mod$variable.importance) 4 ,value=cl$rpt$mod$variable.importance/sum(cl$rpt$mod$variable.importance) 5 ,row.names=NULL) 6# bagging 7ntree = length(cl$cum.varImp.lst) 8bgg.varImp = data.frame(method=\u0026#34;bagging\u0026#34; 9 ,variable=rownames(cl$cum.varImp.lst) 10 ,value=cl$cum.varImp.lst[,ntree]) 11# plot variable importance measure 12cl.varImp = rbind(cart.varImp,bgg.varImp) 13cl.varImp$variable = reorder(cl.varImp$variable, 1/cl.varImp$value) 14ggplot(data=cl.varImp,aes(x=variable,y=value,fill=method)) + geom_bar(stat=\u0026#34;identity\u0026#34;) + 15 ggtitle(\u0026#34;Variable importance\u0026#34;) + theme(plot.title=element_text(face=\u0026#34;bold\u0026#34;)) In this article, a classification tree is evaluated comparing to bagged trees. In comparison to individual oob/test errors, the single tree\u0026rsquo;s test error seems to be quite optimistic. Also oob samples doesn\u0026rsquo;t improve prediction performance as the tree generating process might be dominated by a few predictors. Comparing to the cumulative test error, it seems that the single tree overfits the train data so that its prediction power is not competitive. Although the CART model as a classifier doesn\u0026rsquo;t seem to be attractive for this data set, it may be a bit early to discard it. What seems to be necessary is to check the cases where the dominant predictors\u0026rsquo; impacts are reduced and subsequent articles would head toward that direction.\n","date":"March 7, 2015","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/blog/2015-03-07-tree-based-methods-6/","series":[{"title":"Tree Based Methods in R","url":"/series/tree-based-methods-in-r/"}],"smallImg":"","tags":[{"title":"R","url":"/tags/r/"}],"timestamp":1425686400,"title":"Tree Based Methods in R - Part VI"},{"categories":[{"title":"Machine Learning","url":"/categories/machine-learning/"}],"content":" Part I Part II Part III Part IV Part V (this post) Part VI This article evaluates a single regression tree\u0026rsquo;s performance by comparing to bagged trees\u0026rsquo; individual oob/test errors, cumulative oob/test errors and variable importance measures.\nBefore getting started, note that the source of the classes can be found in this gist and, together with the relevant packages (see tags), it requires a utility function (bestParam()) that can be found here.\nData is split as usual.\n1## data 2require(ISLR) 3data(Carseats) 4require(dplyr) 5Carseats = Carseats %\u0026gt;% 6 mutate(High=factor(ifelse(Sales\u0026lt;=8,\u0026#34;No\u0026#34;,\u0026#34;High\u0026#34;),labels=c(\u0026#34;High\u0026#34;,\u0026#34;No\u0026#34;))) 7data.cl = subset(Carseats, select=c(-Sales)) 8data.rg = subset(Carseats, select=c(-High)) 9 10# split data 11require(caret) 12set.seed(1237) 13trainIndex = createDataPartition(Carseats$High, p=0.8, list=FALSE, times=1) 14trainData.rg = data.rg[trainIndex,] 15testData.rg = data.rg[-trainIndex,] Both the single and bagged regression trees are fit. For bagging, 2000 trees are generated.\n1## run rpartDT 2# import constructors 3source(\u0026#34;src/cart.R\u0026#34;) 4set.seed(12357) 5rg = cartDT(trainData.rg, testData.rg, \u0026#34;Sales ~ .\u0026#34;, ntree=2000) The summary of the cp values of the bagged trees are shown below, followed by the single tree\u0026rsquo;s cp value at the least xerror.\n1## cp values 2# cart 3rg$rpt$cp[1,][[1]] ## [1] 0.004852267 1# bagging 2summary(t(rg$boot.cp)[,2]) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.000000 0.000000 0.000000 0.001430 0.002568 0.018640 Individual Error From the distributions of the oob and test errors, it is found that\nthe single tree\u0026rsquo;s test error is not far away from the means of the bagged trees\u0026rsquo; oob and test errors - it is only 0.92 and 0.65 standard deviation away repectively and the distributions are dense in the left hand side where the single tree\u0026rsquo;s test error locates so that it may be considered that the test error is a likely value. 1## individual errors 2# cart test error 3crt.err = rg$rpt$test.lst$error$error 4crt.err ## [1] 0.737656 1# bagging error at least xerror - se to see 1-SE rule 2ind.oob.err = data.frame(type=\u0026#34;oob\u0026#34;,error=unlist(rg$ind.oob.lst.err)) 3ind.tst.err = data.frame(type=\u0026#34;test\u0026#34;,error=unlist(rg$ind.tst.lst.err)) 4ind.err = rbind(ind.oob.err,ind.tst.err) 5ind.err.summary = as.data.frame(rbind(summary(ind.err$error[ind.err$type==\u0026#34;oob\u0026#34;]) 6 ,summary(ind.err$error[ind.err$type==\u0026#34;test\u0026#34;]))) 7rownames(ind.err.summary) \u0026lt;- c(\u0026#34;oob\u0026#34;,\u0026#34;test\u0026#34;) 8ind.err.summary ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## oob 0.0053970 0.9026 1.972 2.247 3.207 10.670 ## test 0.0005156 0.5734 1.217 1.442 2.107 6.738 A graphical illustration of the error distributions and the location of the single tree\u0026rsquo;s test error are shown below.\n1# plot error distributions 2ggplot(ind.err, aes(x=error,fill=type)) + 3 geom_histogram() + geom_vline(xintercept=crt.err, color=\u0026#34;blue\u0026#34;) + 4 ggtitle(\u0026#34;Error distribution\u0026#34;) + theme(plot.title=element_text(face=\u0026#34;bold\u0026#34;)) Cumulative Error The plots of cumulative oob and test errors are shown below and both the errors seem to settle around 1000th tree. The oob and test errors at the 1000th tree are 0.018 and 0.512 respectively. Compared to the single tree\u0026rsquo;s test error of 0.738, the bagged trees deliveres imporved results.\n1bgg.oob.err = data.frame(type=\u0026#34;oob\u0026#34; 2 ,ntree=1:length(rg$cum.oob.lst.err) 3 ,error=unlist(rg$cum.oob.lst.err)) 4bgg.tst.err = data.frame(type=\u0026#34;test\u0026#34; 5 ,ntree=1:length(rg$cum.tst.lst.err) 6 ,error=unlist(rg$cum.tst.lst.err)) 7bgg.err = rbind(bgg.oob.err,bgg.tst.err) 8# plot bagging errors 9ggplot(data=bgg.err,aes(x=ntree,y=error,colour=type)) + 10 geom_line() + geom_abline(intercept=crt.err,slope=0,color=\u0026#34;blue\u0026#34;) + 11 ggtitle(\u0026#34;Bagging error\u0026#34;) + theme(plot.title=element_text(face=\u0026#34;bold\u0026#34;)) Variable Importance While ShelveLoc and Price are shown to be the two most important variables, both the single and bagged trees show similar variable importance profiles. For evaluating a single tree, this would be a positive result as it could show that the splits of the single tree is reinforced to be valid by the bagged trees.\nFor prediction, however, there may be some room for improvement as the bagged trees might be correlated to some extent, especially due to the existence of the two important variables. For example, if earlier splits are determined by these predictors, there are not enough chances for the remaining predictors and it is not easy to identify local systematic patterns.\n1# cart 2cart.varImp = data.frame(method=\u0026#34;cart\u0026#34; 3 ,variable=names(rg$rpt$mod$variable.importance) 4 ,value=rg$rpt$mod$variable.importance/sum(rg$rpt$mod$variable.importance) 5 ,row.names=NULL) 6# bagging 7ntree = 1000 8bgg.varImp = data.frame(method=\u0026#34;bagging\u0026#34; 9 ,variable=rownames(rg$cum.varImp.lst) 10 ,value=rg$cum.varImp.lst[,ntree]) 11# plot variable importance measure 12rg.varImp = rbind(cart.varImp,bgg.varImp) 13rg.varImp$variable = reorder(rg.varImp$variable, 1/rg.varImp$value) 14ggplot(data=rg.varImp,aes(x=variable,y=value,fill=method)) + geom_bar(stat=\u0026#34;identity\u0026#34;) In this article, a single regression tree is evaluated by bagged trees. Comparing to individual oob/test errors, the single tree\u0026rsquo;s test error seems to be a likely value. Also, while bagged trees improve prediction performance, the single tree may not be a bad choice especially if more focus is on interpretation. Despite the performance improvement of the bagged trees, there seems to be a chance for additional improvement and the right direction of subsequence articles would be looking into it.\n","date":"March 5, 2015","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/blog/2015-03-05-tree-based-methods-5/","series":[{"title":"Tree Based Methods in R","url":"/series/tree-based-methods-in-r/"}],"smallImg":"","tags":[{"title":"R","url":"/tags/r/"}],"timestamp":1425513600,"title":"Tree Based Methods in R - Part V"},{"categories":[{"title":"Machine Learning","url":"/categories/machine-learning/"}],"content":" Part I Part II Part III Part IV (this post) Part V Part VI While the last three articles illustrated the CART model for both classification (with equal/unequal costs) and regression tasks, this article is rather technical as it compares three packages: rpart, caret and mlr. For those who are not familiar with the last two packages, they are wrappers (or frameworks) that implement a range of models (or algorithms) in a unified way. They are useful because inconsistent API could be a drawback of R (like other open source tools) and it would be quite beneficial if there is a way to implement different models in a standardized way. In line with the earlier articles, the Carseats data is used for a classification task.\nBefore getting started, I should admit the names are not defined effectively. I hope the below list may be helpful to follow the script.\npackage: rpt - rpart, crt - caret, mlr - mlr model fitting: ftd - fit on training data, ptd - fit on test data parameter selection bst - best cp by 1-SE rule (recommended by rpart) lst - best cp by highest accuracy (caret) or lowest mmce (mlr) cost: eq - equal cost, uq - unequal cost (uq case not added in this article) etc: cp - complexity parameter, mmce - mean misclassification error, acc - Accuracy (caret), cm - confusion matrix Also the data is randomly split into trainData and testData. In practice, the latter is not observed and it is used here for evaludation.\nLet\u0026rsquo;s get started.\nThe following packages are used.\n1library(reshape2) 2library(plyr) 3library(dplyr) 4library(ggplot2) 5library(rpart) 6library(caret) 7library(mlr) The Sales column is converted into a binary variables.\n1## data 2require(ISLR) 3data(Carseats) 4Carseats = Carseats %\u0026gt;% 5 mutate(High=factor(ifelse(Sales\u0026lt;=8,\u0026#34;No\u0026#34;,\u0026#34;High\u0026#34;),labels=c(\u0026#34;High\u0026#34;,\u0026#34;No\u0026#34;))) 6data = subset(Carseats, select=c(-Sales)) Balanced splitting of data can be performed in either of the packages as shown below.\n1## split data 2# caret 3set.seed(1237) 4trainIndex = createDataPartition(Carseats$High, p=0.8, list=FALSE, times=1) 5trainData.caret = data[trainIndex,] 6testData.caret = data[-trainIndex,] 7 8# mlr 9set.seed(1237) 10split.desc = makeResampleDesc(method=\u0026#34;Holdout\u0026#34;, stratify=TRUE, split=0.8) 11split.task = makeClassifTask(data=data,target=\u0026#34;High\u0026#34;) 12resampleIns = makeResampleInstance(desc=split.desc, task=split.task) 13trainData.mlr = data[resampleIns$train.inds[[1]],] 14testData.mlr = data[-resampleIns$train.inds[[1]],] 15 16# response summaries 17train.res.caret = with(trainData.caret,rbind(table(High),table(High)/length(High))) 18train.res.caret ## High No ## [1,] 132.000000 189.000000 ## [2,] 0.411215 0.588785 1train.res.mlr = with(trainData.mlr,rbind(table(High),table(High)/length(High))) 2train.res.mlr ## High No ## [1,] 131.0000000 188.0000000 ## [2,] 0.4106583 0.5893417 1test.res.caret = with(testData.caret,rbind(table(High),table(High)/length(High))) 2test.res.caret ## High No ## [1,] 32.0000000 47.0000000 ## [2,] 0.4050633 0.5949367 1test.res.mlr = with(testData.mlr,rbind(table(High),table(High)/length(High))) 2test.res.mlr ## High No ## [1,] 33.0000000 48.0000000 ## [2,] 0.4074074 0.5925926 Same to the previous articles, the split by the caret package is taken.\n1# data from caret is taken in line with previous articles 2trainData = trainData.caret 3testData = testData.caret Note that two custom functions are used: bestParam() and updateCM(). The former searches the cp values by the 1-SE rule (bst) and at the lowest xerror (lst) from the cp table of a rpart object. The latter produces a confusion matrix with model and use error, added to the last column and row respectively. Their sources can be seen here.\n1source(\u0026#34;src/mlUtils.R\u0026#34;) At first, the model is fit using the rpart package and bst and lst cp values are obtained.\n1### rpart 2## train on training data 3set.seed(12357) 4mod.rpt.eq = rpart(High ~ ., data=trainData, control=rpart.control(cp=0)) 5mod.rpt.eq.par = bestParam(mod.rpt.eq$cptable,\u0026#34;CP\u0026#34;,\u0026#34;xerror\u0026#34;,\u0026#34;xstd\u0026#34;) 6mod.rpt.eq.par ## lowest best ## param 0.01136364 0.10606061 ## error 0.65909091 0.70454545 ## errStd 0.06033108 0.06157189 The selected cp values can be check graphically below.\n1# plot xerror vs cp 2df = as.data.frame(mod.rpt.eq$cptable) 3best = mod.rpt.eq.par 4ubound = ifelse(best[2,1]+best[3,1]\u0026gt;max(df$xerror),max(df$xerror),best[2,1]+best[3,1]) 5lbound = ifelse(best[2,1]-best[3,1]\u0026lt;min(df$xerror),min(df$xerror),best[2,1]-best[3,1]) 6 7ggplot(data=df[1:nrow(df),], aes(x=CP,y=xerror)) + 8 geom_line() + geom_point() + 9 geom_abline(intercept=ubound,slope=0, color=\u0026#34;purple\u0026#34;) + 10 geom_abline(intercept=lbound,slope=0, color=\u0026#34;purple\u0026#34;) + 11 geom_point(aes(x=best[1,2],y=best[2,2]),color=\u0026#34;red\u0026#34;,size=3) + 12 geom_point(aes(x=best[1,1],y=best[2,1]),color=\u0026#34;blue\u0026#34;,size=3) The original tree is pruned with the 2 cp values, resulting in 2 separate trees, and they are fit on the training data.\n1## performance on train data 2mod.rpt.eq.lst.cp = mod.rpt.eq.par[1,1] 3mod.rpt.eq.bst.cp = mod.rpt.eq.par[1,2] 4# prune 5mod.rpt.eq.lst = prune(mod.rpt.eq, cp=mod.rpt.eq.lst.cp) 6mod.rpt.eq.bst = prune(mod.rpt.eq, cp=mod.rpt.eq.bst.cp) 7 8# fit to train data 9mod.rpt.eq.lst.ftd = predict(mod.rpt.eq.lst, type=\u0026#34;class\u0026#34;) 10mod.rpt.eq.bst.ftd = predict(mod.rpt.eq.bst, type=\u0026#34;class\u0026#34;) Details of the fitting is kept in a list (mmce).\npkg: package name isTest: fit on test data? isBest: cp by 1-SE rule? isEq: equal cost? cp: cp value used mmce: mean misclassification error 1# fit to train data 2mod.rpt.eq.lst.ftd = predict(mod.rpt.eq.lst, type=\u0026#34;class\u0026#34;) 3mod.rpt.eq.bst.ftd = predict(mod.rpt.eq.bst, type=\u0026#34;class\u0026#34;) 4 5# confusion matrix 6mod.rpt.eq.lst.ftd.cm = table(actual=trainData$High,fitted=mod.rpt.eq.lst.ftd) 7mod.rpt.eq.lst.ftd.cm = updateCM(mod.rpt.eq.lst.ftd.cm, type=\u0026#34;Fitted\u0026#34;) 8 9mod.rpt.eq.bst.ftd.cm = table(actual=trainData$High,fitted=mod.rpt.eq.bst.ftd) 10mod.rpt.eq.bst.ftd.cm = updateCM(mod.rpt.eq.bst.ftd.cm, type=\u0026#34;Fitted\u0026#34;) 11 12# misclassification error 13mod.rpt.eq.lst.ftd.mmce = list(pkg=\u0026#34;rpart\u0026#34;,isTest=FALSE,isBest=FALSE,isEq=TRUE 14 ,cp=round(mod.rpt.eq.lst.cp,4) 15 ,mmce=mod.rpt.eq.lst.ftd.cm[3,3]) 16mmce = list(unlist(mod.rpt.eq.lst.ftd.mmce)) 17 18mod.rpt.eq.bst.ftd.mmce = list(pkg=\u0026#34;rpart\u0026#34;,isTest=FALSE,isBest=TRUE,isEq=TRUE 19 ,cp=round(mod.rpt.eq.bst.cp,4) 20 ,mmce=mod.rpt.eq.bst.ftd.cm[3,3]) 21mmce[[length(mmce)+1]] = unlist(mod.rpt.eq.bst.ftd.mmce) 22 23ldply(mmce) ## pkg isTest isBest isEq cp mmce ## 1 rpart FALSE FALSE TRUE 0.0114 0.16 ## 2 rpart FALSE TRUE TRUE 0.1061 0.29 The pruned trees are fit into the test data and the same details are added to the list (mmce).\n1## performance of test data 2# fit to test data 3mod.rpt.eq.lst.ptd = predict(mod.rpt.eq.lst, newdata=testData, type=\u0026#34;class\u0026#34;) 4mod.rpt.eq.bst.ptd = predict(mod.rpt.eq.bst, newdata=testData, type=\u0026#34;class\u0026#34;) 5 6# confusion matrix 7mod.rpt.eq.lst.ptd.cm = table(actual=testData$High, fitted=mod.rpt.eq.lst.ptd) 8mod.rpt.eq.lst.ptd.cm = updateCM(mod.rpt.eq.lst.ptd.cm) 9 10mod.rpt.eq.bst.ptd.cm = table(actual=testData$High, fitted=mod.rpt.eq.bst.ptd) 11mod.rpt.eq.bst.ptd.cm = updateCM(mod.rpt.eq.bst.ptd.cm) 12 13# misclassification error 14mod.rpt.eq.lst.ptd.mmce = list(pkg=\u0026#34;rpart\u0026#34;,isTest=TRUE,isBest=FALSE,isEq=TRUE 15 ,cp=round(mod.rpt.eq.lst.cp,4) 16 ,mmce=mod.rpt.eq.lst.ptd.cm[3,3]) 17mmce[[length(mmce)+1]] = unlist(mod.rpt.eq.lst.ptd.mmce) 18 19mod.rpt.eq.bst.ptd.mmce = list(pkg=\u0026#34;rpart\u0026#34;,isTest=TRUE,isBest=TRUE,isEq=TRUE 20 ,cp=round(mod.rpt.eq.bst.cp,4) 21 ,mmce=mod.rpt.eq.bst.ptd.cm[3,3]) 22mmce[[length(mmce)+1]] = unlist(mod.rpt.eq.bst.ptd.mmce) 23 24ldply(mmce) ## pkg isTest isBest isEq cp mmce ## 1 rpart FALSE FALSE TRUE 0.0114 0.16 ## 2 rpart FALSE TRUE TRUE 0.1061 0.29 ## 3 rpart TRUE FALSE TRUE 0.0114 0.19 ## 4 rpart TRUE TRUE TRUE 0.1061 0.3 Secondly the caret package is employed to implement the CART model.\n1### caret 2## train on training data 3trControl = trainControl(method=\u0026#34;repeatedcv\u0026#34;, number=10, repeats=5) 4set.seed(12357) 5mod.crt.eq = caret::train(High ~ . 6 ,data=trainData 7 ,method=\u0026#34;rpart\u0026#34; 8 ,tuneLength=20 9 ,trControl=trControl) Note that the caret package select the best cp value that corresponds to the lowest Accuracy. Therefore the best cp by this package is labeled as lst to be consistent with the rpart package. And the bst cp is selected by the 1-SE rule. Note that, as the standard error of Accuracy is relatively wide, an adjustment is maded to select the best cp value and it can be checked in the graph below.\n1# lowest CP 2# caret: maximum accuracy, rpart: 1-SE 3# according to rpart, caret\u0026#39;s best is lowest 4df = mod.crt.eq$results 5mod.crt.eq.lst.cp = mod.crt.eq$bestTune$cp 6mod.crt.eq.lst.acc = subset(df,cp==mod.crt.eq.lst.cp)[[2]] 7 8# best cp by 1-SE rule - values are adjusted from graph 9mod.crt.eq.bst.cp = df[17,1] 10mod.crt.eq.bst.acc = df[17,2] 1# CP by 1-SE rule 2maxAcc = subset(df,Accuracy==max(Accuracy))[[2]] 3stdAtMaxAcc = subset(df,subset=Accuracy==max(Accuracy))[[4]] 4# max cp within 1 SE 5maxCP = subset(df,Accuracy\u0026gt;=maxAcc-stdAtMaxAcc)[nrow(subset(df,Accuracy\u0026gt;=maxAcc-stdAtMaxAcc)),][[1]] 6accAtMaxCP = subset(df,cp==maxCP)[[2]] 7 8# plot Accuracy vs cp 9ubound = ifelse(maxAcc+stdAtMaxAcc\u0026gt;max(df$Accuracy),max(df$Accuracy),maxAcc+stdAtMaxAcc) 10lbound = ifelse(maxAcc-stdAtMaxAcc\u0026lt;min(df$Accuracy),min(df$Accuracy),maxAcc-stdAtMaxAcc) 11 12ggplot(data=df[1:nrow(df),], aes(x=cp,y=Accuracy)) + 13 geom_line() + geom_point() + 14 geom_abline(intercept=ubound,slope=0, color=\u0026#34;purple\u0026#34;) + 15 geom_abline(intercept=lbound,slope=0, color=\u0026#34;purple\u0026#34;) + 16 geom_point(aes(x=mod.crt.eq.bst.cp,y=mod.crt.eq.bst.acc),color=\u0026#34;red\u0026#34;,size=3) + 17 geom_point(aes(x=mod.crt.eq.lst.cp,y=mod.crt.eq.lst.acc),color=\u0026#34;blue\u0026#34;,size=3) Similar to above, 2 trees with the respective cp values are fit into the train and test data and the details are kept in mmce. Below is the update by fitting from the train data.\n1## performance on train data 2# refit from rpart for best cp - cp by 1-SE not fitted by caret 3# note no cross-validation necessary - xval=0 (default: xval=10) 4set.seed(12357) 5mod.crt.eq.bst = rpart(High ~ ., data=trainData, control=rpart.control(xval=0,cp=mod.crt.eq.bst.cp)) 6 7# fit to train data - lowest from caret, best (1-SE) from rpart 8mod.crt.eq.lst.ftd = predict(mod.crt.eq,newdata=trainData) 9mod.crt.eq.bst.ftd = predict(mod.crt.eq.bst, type=\u0026#34;class\u0026#34;) 10 11# confusion matrix 12mod.crt.eq.lst.ftd.cm = table(actual=trainData$High,fitted=mod.crt.eq.lst.ftd) 13mod.crt.eq.lst.ftd.cm = updateCM(mod.crt.eq.lst.ftd.cm, type=\u0026#34;Fitted\u0026#34;) 14 15mod.crt.eq.bst.ftd.cm = table(actual=trainData$High,fitted=mod.crt.eq.bst.ftd) 16mod.crt.eq.bst.ftd.cm = updateCM(mod.crt.eq.bst.ftd.cm, type=\u0026#34;Fitted\u0026#34;) 17 18# misclassification error 19mod.crt.eq.lst.ftd.mmce = list(pkg=\u0026#34;caret\u0026#34;,isTest=FALSE,isBest=FALSE,isEq=TRUE 20 ,cp=round(mod.crt.eq.lst.cp,4) 21 ,mmce=mod.crt.eq.lst.ftd.cm[3,3]) 22mmce[[length(mmce)+1]] = unlist(mod.crt.eq.lst.ftd.mmce) 23 24mod.crt.eq.bst.ftd.mmce = list(pkg=\u0026#34;caret\u0026#34;,isTest=FALSE,isBest=TRUE,isEq=TRUE 25 ,cp=round(mod.crt.eq.bst.cp,4) 26 ,mmce=mod.crt.eq.bst.ftd.cm[3,3]) 27mmce[[length(mmce)+1]] = unlist(mod.crt.eq.bst.ftd.mmce) Below is the update by fitting from the test data. The updated fitting details can be checked.\n1## performance of test data 2# fit to test data - lowest from caret, best (1-SE) from rpart 3mod.crt.eq.lst.ptd = predict(mod.crt.eq,newdata=testData) 4mod.crt.eq.bst.ptd = predict(mod.crt.eq.bst, newdata=testData) 5 6# confusion matrix 7mod.crt.eq.lst.ptd.cm = table(actual=testData$High, fitted=mod.rpt.eq.lst.ptd) 8mod.crt.eq.lst.ptd.cm = updateCM(mod.crt.eq.lst.ptd.cm) 9 10mod.crt.eq.bst.ptd.cm = table(actual=testData$High, fitted=mod.rpt.eq.bst.ptd) 11mod.crt.eq.bst.ptd.cm = updateCM(mod.crt.eq.bst.ptd.cm) 12 13# misclassification error 14mod.crt.eq.lst.ptd.mmce = list(pkg=\u0026#34;caret\u0026#34;,isTest=TRUE,isBest=FALSE,isEq=TRUE 15 ,cp=round(mod.crt.eq.lst.cp,4) 16 ,mmce=mod.crt.eq.lst.ptd.cm[3,3]) 17mmce[[length(mmce)+1]] = unlist(mod.crt.eq.lst.ptd.mmce) 18 19mod.crt.eq.bst.ptd.mmce = list(pkg=\u0026#34;caret\u0026#34;,isTest=TRUE,isBest=TRUE,isEq=TRUE 20 ,cp=round(mod.crt.eq.bst.cp,4) 21 ,mmce=mod.crt.eq.bst.ptd.cm[3,3]) 22mmce[[length(mmce)+1]] = unlist(mod.crt.eq.bst.ptd.mmce) 23 24ldply(mmce) ## pkg isTest isBest isEq cp mmce ## 1 rpart FALSE FALSE TRUE 0.0114 0.16 ## 2 rpart FALSE TRUE TRUE 0.1061 0.29 ## 3 rpart TRUE FALSE TRUE 0.0114 0.19 ## 4 rpart TRUE TRUE TRUE 0.1061 0.3 ## 5 caret FALSE FALSE TRUE 0.0156 0.16 ## 6 caret FALSE TRUE TRUE 0.2488 0.29 ## 7 caret TRUE FALSE TRUE 0.0156 0.19 ## 8 caret TRUE TRUE TRUE 0.2488 0.3 Finally the mlr package is employed.\nAt first, a taks and learner are set up.\n1### mlr 2## task and learner 3tsk.mlr.eq = makeClassifTask(data=trainData,target=\u0026#34;High\u0026#34;) 4lrn.mlr.eq = makeLearner(\u0026#34;classif.rpart\u0026#34;,par.vals=list(cp=0)) Then a grid of cp values is generated followed by tuning the parameter. Note that, as the tuning optimization path does not include a standard-error-like variable, only the best cp values are taken into consideration.\n1## tune parameter 2cpGrid = function(index) { 3 start=0 4 end=0.3 5 len=20 6 inc = (end-start)/len 7 grid = c(start) 8 while(start \u0026lt; end) { 9 start = start + inc 10 grid = c(grid,start) 11 } 12 grid[index] 13} 14# create tune control grid 15ps.mlr.eq = makeParamSet( 16 makeNumericParam(\u0026#34;cp\u0026#34;,lower=1,upper=20,trafo=cpGrid) 17 ) 18ctrl.mlr.eq = makeTuneControlGrid(resolution=c(cp=(20))) 19 20# tune cp 21rdesc.mlr.eq = makeResampleDesc(\u0026#34;RepCV\u0026#34;,reps=5,folds=10,stratify=TRUE) 22set.seed(12357) 23tune.mlr.eq = tuneParams(learner=lrn.mlr.eq 24 ,task=tsk.mlr.eq 25 ,resampling=rdesc.mlr.eq 26 ,par.set=ps.mlr.eq 27 ,control=ctrl.mlr.eq) 28# tuned cp 29tune.mlr.eq$x ## $cp ## [1] 0.015 1# optimization path 2# no standard error like values, only best cp is considered 3path.mlr.eq = as.data.frame(tune.mlr.eq$opt.path) 4path.mlr.eq = transform(path.mlr.eq,cp=cpGrid(cp)) 5head(path.mlr.eq,3) ## cp mmce.test.mean dob eol error.message exec.time ## 1 0.000 0.2623790 1 NA \u0026lt;NA\u0026gt; 0.948 ## 2 0.015 0.2555407 2 NA \u0026lt;NA\u0026gt; 0.944 ## 3 0.030 0.2629069 3 NA \u0026lt;NA\u0026gt; 0.945 Using the best cp value, the learner is updated followed by training the model.\n1# obtain fitted and predicted responses 2# update cp 3lrn.mlr.eq.bst = setHyperPars(lrn.mlr.eq, par.vals=tune.mlr.eq$x) 4 5# train model 6trn.mlr.eq.bst = train(lrn.mlr.eq.bst, tsk.mlr.eq) Then the model is fit into the train and test data and the fitting details are updated in mmce. The overall fitting results can be checked below.\n1# fitted responses 2mod.mlr.eq.bst.ftd = predict(trn.mlr.eq.bst, tsk.mlr.eq)$data 3mod.mlr.eq.bst.ptd = predict(trn.mlr.eq.bst, newdata=testData)$data 4 5# confusion matrix 6mod.mlr.eq.bst.ftd.cm = table(actual=mod.mlr.eq.bst.ftd$truth, fitted=mod.mlr.eq.bst.ftd$response) 7mod.mlr.eq.bst.ftd.cm = updateCM(mod.mlr.eq.bst.ftd.cm) 8 9mod.mlr.eq.bst.ptd.cm = table(actual=mod.mlr.eq.bst.ptd$truth, fitted=mod.mlr.eq.bst.ptd$response) 10mod.mlr.eq.bst.ptd.cm = updateCM(mod.mlr.eq.bst.ptd.cm) 11 12# misclassification error 13mod.mlr.eq.bst.ftd.mmce = list(pkg=\u0026#34;mlr\u0026#34;,isTest=FALSE,isBest=FALSE,isEq=TRUE 14 ,cp=round(tune.mlr.eq$x[[1]],4) 15 ,mmce=mod.mlr.eq.bst.ftd.cm[3,3]) 16mmce[[length(mmce)+1]] = unlist(mod.mlr.eq.bst.ftd.mmce) 17 18mod.mlr.eq.bst.ptd.mmce = list(pkg=\u0026#34;mlr\u0026#34;,isTest=TRUE,isBest=FALSE,isEq=TRUE 19 ,cp=round(tune.mlr.eq$x[[1]],4) 20 ,mmce=mod.mlr.eq.bst.ptd.cm[3,3]) 21mmce[[length(mmce)+1]] = unlist(mod.mlr.eq.bst.ptd.mmce) 22 23ldply(mmce) ## pkg isTest isBest isEq cp mmce ## 1 rpart FALSE FALSE TRUE 0.0114 0.16 ## 2 rpart FALSE TRUE TRUE 0.1061 0.29 ## 3 rpart TRUE FALSE TRUE 0.0114 0.19 ## 4 rpart TRUE TRUE TRUE 0.1061 0.3 ## 5 caret FALSE FALSE TRUE 0.0156 0.16 ## 6 caret FALSE TRUE TRUE 0.2488 0.29 ## 7 caret TRUE FALSE TRUE 0.0156 0.19 ## 8 caret TRUE TRUE TRUE 0.2488 0.3 ## 9 mlr FALSE FALSE TRUE 0.015 0.16 ## 10 mlr TRUE FALSE TRUE 0.015 0.19 It is shown that the mmce values are identical and it seems to be because the model is quite stable with respect to cp. It can be checked in the following graph.\n1# mmce vs cp 2ftd.mmce = c() 3ptd.mmce = c() 4cps = mod.crt.eq$results[[1]] 5for(i in 1:length(cps)) { 6 if(i %% 2 == 0) { 7 set.seed(12357) 8 mod = rpart(High ~ ., data=trainData, control=rpart.control(cp=cps[i])) 9 ftd = predict(mod,type=\u0026#34;class\u0026#34;) 10 ptd = predict(mod,newdata=testData,type=\u0026#34;class\u0026#34;) 11 ftd.mmce = c(ftd.mmce,updateCM(table(trainData$High,ftd))[3,3]) 12 ptd.mmce = c(ptd.mmce,updateCM(table(testData$High,ptd))[3,3]) 13 } 14} 15mmce.crt = data.frame(cp=as.factor(round(cps[seq(2,length(cps),2)],3)) 16 ,fitted=ftd.mmce 17 ,predicted=ptd.mmce) 18mmce.crt = melt(mmce.crt,id=c(\u0026#34;cp\u0026#34;),variable.name=\u0026#34;data\u0026#34;,value.name=\u0026#34;mmce\u0026#34;) 19ggplot(data=mmce.crt,aes(x=cp,y=mmce,fill=data)) + 20 geom_bar(stat=\u0026#34;identity\u0026#34;, position=position_dodge()) It may not be convicing to use a wrapper by this article about a single model. For example, however, if there are multiple models with a variety of tuning parameters to compare, the benefit of having one can be considerable. In the following articles, a similar approach would be taken, which is comparing individual packages to the wrappers.\n","date":"February 15, 2015","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/blog/2015-02-15-tree-based-methods-4/","series":[{"title":"Tree Based Methods in R","url":"/series/tree-based-methods-in-r/"}],"smallImg":"","tags":[{"title":"R","url":"/tags/r/"}],"timestamp":1423958400,"title":"Tree Based Methods in R - Part IV"},{"categories":[{"title":"Machine Learning","url":"/categories/machine-learning/"}],"content":" Part I Part II Part III (this post) Part IV Part V Part VI While classification tasks are implemented in the last two articles (Part I and Part II), a regression task is the topic of this article. While the caret package selects the tuning parameter (cp) that minimizes the error (RMSE), the rpart packages recommends the 1-SE rule, which selects the smallest tree within 1 standard error of the minimum cross validation error (xerror). The models with 2 complexity parameters that are suggested by the packages are compared.\nThe bold-cased sections of the tutorial of the caret package are covered in this article.\nVisualizations Pre-Processing Data Splitting Miscellaneous Model Functions Model Training and Tuning Using Custom Models Variable Importance Feature Selection: RFE, Filters, GA, SA Other Functions Parallel Processing Adaptive Resampling Let\u0026rsquo;s get started.\nThe following packages are used.\n1library(knitr) 2library(dplyr) 3library(ggplot2) 4library(grid) 5library(gridExtra) 6library(rpart) 7library(rpart.plot) 8library(caret) The Carseats data from the ISLR package is used - a dummy variable (High) is created from Sales for classification. Note that the order of the labels is changed from the previous articles for comparison with the regression model.\n1## data 2require(ISLR) 3data(Carseats) 4# label order changed (No=1) 5Carseats = Carseats %\u0026gt;% 6 mutate(High=factor(ifelse(Sales\u0026lt;=8,\u0026#34;No\u0026#34;,\u0026#34;High\u0026#34;),labels=c(\u0026#34;No\u0026#34;,\u0026#34;High\u0026#34;))) 7# structure of predictors 8str(subset(Carseats,select=c(-High,-Sales))) ## \u0026#39;data.frame\u0026#39;:\t400 obs. of 10 variables: ## $ CompPrice : num 138 111 113 117 141 124 115 136 132 132 ... ## $ Income : num 73 48 35 100 64 113 105 81 110 113 ... ## $ Advertising: num 11 16 10 4 3 13 0 15 0 0 ... ## $ Population : num 276 260 269 466 340 501 45 425 108 131 ... ## $ Price : num 120 83 80 97 128 72 108 120 124 124 ... ## $ ShelveLoc : Factor w/ 3 levels \u0026#34;Bad\u0026#34;,\u0026#34;Good\u0026#34;,\u0026#34;Medium\u0026#34;: 1 2 3 3 1 1 3 2 3 3 ... ## $ Age : num 42 65 59 55 38 78 71 67 76 76 ... ## $ Education : num 17 10 12 14 13 16 15 10 10 17 ... ## $ Urban : Factor w/ 2 levels \u0026#34;No\u0026#34;,\u0026#34;Yes\u0026#34;: 2 2 2 2 2 1 2 2 1 1 ... ## $ US : Factor w/ 2 levels \u0026#34;No\u0026#34;,\u0026#34;Yes\u0026#34;: 2 2 2 2 1 2 1 2 1 2 ... 1# response summaries 2res.cl.summary = with(Carseats,rbind(table(High),table(High)/length(High))) 3res.cl.summary ## No High ## [1,] 164.00 236.00 ## [2,] 0.41 0.59 1res.reg.summary = summary(Carseats$Sales) 2res.reg.summary ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.000 5.390 7.490 7.496 9.320 16.270 The data is split as following.\n1## split data 2set.seed(1237) 3trainIndex = createDataPartition(Carseats$High, p=.8, list=FALSE, times=1) 4# classification 5trainData.cl = subset(Carseats, select=c(-Sales))[trainIndex,] 6testData.cl = subset(Carseats, select=c(-Sales))[-trainIndex,] 7# regression 8trainData.reg = subset(Carseats, select=c(-High))[trainIndex,] 9testData.reg = subset(Carseats, select=c(-High))[-trainIndex,] 10 11# response summary 12train.res.cl.summary = with(trainData.cl,rbind(table(High),table(High)/length(High))) 13test.res.cl.summary = with(testData.cl,rbind(table(High),table(High)/length(High))) 14 15train.res.reg.summary = summary(trainData.reg$Sales) 16test.res.reg.summary = summary(testData.reg$Sales) 1## train model 2# set up train control 3trControl = trainControl(method=\u0026#34;repeatedcv\u0026#34;,number=10,repeats=5) Having 5 times of 10-fold cross-validation set above, both the CART is fit as both classification and regression tasks.\n1# classification 2set.seed(12357) 3mod.cl = train(High ~ . 4 ,data=trainData.cl 5 ,method=\u0026#34;rpart\u0026#34; 6 ,tuneLength=20 7 ,trControl=trControl) Note that the package developer informs that, in spite of the warning messages, the function fits the training data without a problem so that the outcome can be relied upon (Link). Note that the criterion is selecting the cp that has the lowest RMSE.\n1# regression - caret 2set.seed(12357) 3mod.reg.caret = train(Sales ~ . 4 ,data=trainData.reg 5 ,method=\u0026#34;rpart\u0026#34; 6 ,tuneLength=20 7 ,trControl=trControl) ## Warning in nominalTrainWorkflow(x = x, y = y, wts = weights, info = ## trainInfo, : There were missing values in resampled performance measures. For comparison, the data is fit using rpart() where the cp value is set to 0. This cp value results in an unpruned tree and cp can be selected by the 1-SE rule, which selects the smallest tree within 1 standard error of the minimum cross validation error (xerror), which is recommended by the package. Note that a custom function (bestParam()) is used to search the best parameter by the 1-SE rule and the source can be found here.\n1# regression - rpart 2source(\u0026#34;src/mlUtils.R\u0026#34;) 3set.seed(12357) 4mod.reg.rpart = rpart(Sales ~ ., data=trainData.reg, control=rpart.control(cp=0)) 5mod.reg.rpart.param = bestParam(mod.reg.rpart$cptable,\u0026#34;CP\u0026#34;,\u0026#34;xerror\u0026#34;,\u0026#34;xstd\u0026#34;) 6mod.reg.rpart.param ## lowest best ## param 0.004852267 0.009349806 ## error 0.585672544 0.610102155 ## errStd 0.041710792 0.041305185 If it is necessary to apply the 1-SE rule on the result by the caret package, the bestParam() can be used by setting isDesc to be FALSE. The result is shown below as reference.\n1mod.reg.caret.param = bestParam(mod.reg.caret$results,\u0026#34;cp\u0026#34;,\u0026#34;RMSE\u0026#34;,\u0026#34;RMSESD\u0026#34;,isDesc=FALSE) 2mod.reg.caret.param ## lowest best ## param 0.00828502 0.05661803 ## error 2.23430550 2.38751487 ## errStd 0.23521777 0.27267509 A graphical display of the best cp is shown below.\n1# plot best CP 2df = as.data.frame(mod.reg.rpart$cptable) 3best = bestParam(mod.reg.rpart$cptable,\u0026#34;CP\u0026#34;,\u0026#34;xerror\u0026#34;,\u0026#34;xstd\u0026#34;) 4ubound = ifelse(best[2,1]+best[3,1]\u0026gt;max(df$xerror),max(df$xerror),best[2,1]+best[3,1]) 5lbound = ifelse(best[2,1]-best[3,1]\u0026lt;min(df$xerror),min(df$xerror),best[2,1]-best[3,1]) 6 7ggplot(data=df[3:nrow(df),], aes(x=CP,y=xerror)) + 8 geom_line() + geom_point() + 9 geom_abline(intercept=ubound,slope=0, color=\u0026#34;blue\u0026#34;) + 10 geom_abline(intercept=lbound,slope=0, color=\u0026#34;blue\u0026#34;) + 11 geom_point(aes(x=best[1,2],y=best[2,2]),color=\u0026#34;red\u0026#34;,size=3) The best cp values for each of the models are shown below\n1## show best tuned cp 2# classification 3subset(mod.cl$results,subset=cp==mod.cl$bestTune$cp) ## cp Accuracy Kappa AccuracySD KappaSD ## 1 0 0.7262793 0.4340102 0.07561158 0.153978 1# regression - caret 2subset(mod.reg.caret$results,subset=cp==mod.reg.caret$bestTune$cp) ## cp RMSE Rsquared RMSESD RsquaredSD ## 1 0.00828502 2.234305 0.4342292 0.2352178 0.1025357 1# regression - rpart 2mod.reg.rpart.summary = data.frame(t(mod.reg.rpart.param[,2])) 3colnames(mod.reg.rpart.summary) = c(\u0026#34;CP\u0026#34;,\u0026#34;xerror\u0026#34;,\u0026#34;xstd\u0026#34;) 4mod.reg.rpart.summary ## CP xerror xstd ## 1 0.009349806 0.6101022 0.04130518 The training data is refit with the best cp values.\n1## refit the model to the entire training data 2# classification 3cp.cl = mod.cl$bestTune$cp 4mod.cl = rpart(High ~ ., data=trainData.cl, control=rpart.control(cp=cp.cl)) 5 6# regression - caret 7cp.reg.caret = mod.reg.caret$bestTune$cp 8mod.reg.caret = rpart(Sales ~ ., data=trainData.reg, control=rpart.control(cp=cp.reg.caret)) 9 10# regression - rpart 11cp.reg.rpart = mod.reg.rpart.param[1,2] 12mod.reg.rpart = rpart(Sales ~ ., data=trainData.reg, control=rpart.control(cp=cp.reg.rpart)) Initially it was planned to compare the regression model to the classification model. Specifically, as the response is converted as a binary variable and the break is at the value of 8.0, it is possible to create a regression version of confusion matrix by splitting the data at the equivalent percentile, which is about 0.59 in this data. Then the outcomes can be compared. However it turns out that they cannot be compared directly as the regression outcome is too good as shown below. Note updateCM() and regCM() are custom functions and their sources can be found here.\n1## generate confusion matrix on training data 2# fit models 3fit.cl = predict(mod.cl, type=\u0026#34;class\u0026#34;) 4fit.reg.caret = predict(mod.reg.caret) 5fit.reg.rpart = predict(mod.reg.rpart) 6 7# classification 8# percentile that Sales is divided by No and High 9eqPercentile = with(trainData.reg,length(Sales[Sales\u0026lt;=8])/length(Sales)) 10 11# classification 12fit.cl.cm = table(data.frame(actual=trainData.cl$High,response=fit.cl)) 13fit.cl.cm = updateCM(fit.cl.cm,type=\u0026#34;Fitted\u0026#34;) 14fit.cl.cm ## Fitted: No Fitted: High Model Error ## actual: No 111.00 21.00 0.16 ## actual: High 26.00 163.00 0.14 ## Use Error 0.19 0.11 0.15 1# regression with equal percentile is not comparable 2probs = eqPercentile 3fit.reg.caret.cm = regCM(trainData.reg$Sales, fit.reg.caret, probs=probs, type=\u0026#34;Fitted\u0026#34;) 4fit.reg.caret.cm ## Fitted: 59%- Fitted: 59%+ Model Error ## actual: 59%- 185 4.00 0.02 ## actual: 59%+ 0 132.00 0.00 ## Use Error 0 0.03 0.01 As it is not easy to compare the classification and regression models directly, only the 2 regression models are compared from now on. At first, the regression version of confusion matrices are compared by every 20th percentile followed by the residual mean sqaured error (RMSE) values.\n1# regression with selected percentiles 2probs = seq(0.2,0.8,0.2) 3 4# regression - caret 5# caret produces a better outcome on training data - note lower cp 6fit.reg.caret.cm = regCM(trainData.reg$Sales, fit.reg.caret, probs=probs, type=\u0026#34;Fitted\u0026#34;) 7kable(fit.reg.caret.cm) Fitted: 20%- Fitted: 40%- Fitted: 60%- Fitted: 80%- Fitted: 80%+ Model Error actual: 20%- 65.00 0 0.0 0.00 0 0.00 actual: 40%- 1.00 49 14.0 0.00 0 0.23 actual: 60%- 0.00 0 56.0 8.00 0 0.12 actual: 80%- 0.00 0 0.0 64.00 0 0.00 actual: 80%+ 0.00 0 0.0 16.00 48 0.25 Use Error 0.02 0 0.2 0.27 0 0.12 1# regression - rpart 2fit.reg.rpart.cm = regCM(trainData.reg$Sales, fit.reg.rpart, probs=probs, type=\u0026#34;Fitted\u0026#34;) 3kable(fit.reg.rpart.cm) Fitted: 20%- Fitted: 40%- Fitted: 60%- Fitted: 80%- Fitted: 80%+ Model Error actual: 20%- 59 6.00 0.00 0.00 0 0.09 actual: 40%- 0 50.00 14.00 0.00 0 0.22 actual: 60%- 0 0.00 64.00 0.00 0 0.00 actual: 80%- 0 0.00 24.00 40.00 0 0.38 actual: 80%+ 0 0.00 0.00 33.00 31 0.52 Use Error 0 0.11 0.37 0.45 0 0.24 1# fitted RMSE 2fit.reg.caret.rmse = sqrt(sum(trainData.reg$Sales-fit.reg.caret)^2/length(trainData.reg$Sales)) 3fit.reg.caret.rmse ## [1] 2.738924e-15 1fit.reg.rpart.rmse = sqrt(sum(trainData.reg$Sales-fit.reg.rpart)^2/length(trainData.reg$Sales)) 2fit.reg.rpart.rmse ## [1] 2.788497e-15 It turns out that the model by the caret package produces a better fit and it can also be checked by RMSE values. This is understandable as the model by the caret package takes the cp that minimizes RMSE while the one by the rpart package accepts some more error in favor of a smaller tree by the 1-SE rule. Besides their different resampling strateges can be a source that makes it difficult to compare the outcomes directly.\nAs their performance on the test data is more important, they are compared on it.\n1## generate confusion matrix on test data 2# fit models 3pred.reg.caret = predict(mod.reg.caret, newdata=testData.reg) 4pred.reg.rpart = predict(mod.reg.rpart, newdata=testData.reg) 5 6# regression - caret 7pred.reg.caret.cm = regCM(testData.reg$Sales, pred.reg.caret, probs=probs) 8kable(pred.reg.caret.cm) Pred: 20%- Pred: 40%- Pred: 60%- Pred: 80%- Pred: 80%+ Model Error actual: 20%- 11 5.00 0.00 0 0.00 0.31 actual: 40%- 0 15.00 1.00 0 0.00 0.06 actual: 60%- 0 0.00 15.00 0 0.00 0.00 actual: 80%- 0 0.00 0.00 15 1.00 0.06 actual: 80%+ 0 0.00 0.00 0 16.00 0.00 Use Error 0 0.25 0.06 0 0.06 0.09 1# regression - rpart 2pred.reg.rpart.cm = regCM(testData.reg$Sales, pred.reg.rpart, probs=probs) 3kable(pred.reg.rpart.cm) Pred: 20%- Pred: 40%- Pred: 60%- Pred: 80%- Pred: 80%+ Model Error actual: 20%- 10 6.0 0.0 0 0.00 0.38 actual: 40%- 0 9.0 7.0 0 0.00 0.44 actual: 60%- 0 0.0 15.0 0 0.00 0.00 actual: 80%- 0 0.0 8.0 6 2.00 0.62 actual: 80%+ 0 0.0 0.0 0 16.00 0.00 Use Error 0 0.4 0.5 0 0.11 0.29 1pred.reg.caret.rmse = sqrt(sum(testData.reg$Sales-pred.reg.caret)^2/length(testData.reg$Sales)) 2pred.reg.caret.rmse ## [1] 1.178285 1pred.reg.rpart.rmse = sqrt(sum(testData.reg$Sales-pred.reg.rpart)^2/length(testData.reg$Sales)) 2pred.reg.rpart.rmse ## [1] 1.955439 Even on the test data, the model by the caret package performs well and it seems that the cost of selecting a smaller tree by the 1-SE rule may be too much on this data set.\nBelow shows some plots on the model by the caret package on the test data.\n1## plot actual vs prediced and resid vs fitted 2mod.reg.caret.test = rpart(Sales ~ ., data=testData.reg, control=rpart.control(cp=cp.reg.caret)) 3predDF = data.frame(actual=testData.reg$Sales 4 ,predicted=pred.reg.caret 5 ,resid=resid(mod.reg.caret.test)) 6 7# actual vs predicted 8actual.plot = ggplot(predDF, aes(x=predicted,y=actual)) + 9 geom_point(shape=1,position=position_jitter(width=0.1,height=0.1)) + 10 geom_smooth(method=lm,se=FALSE) 11 12# resid vs predicted 13resid.plot = ggplot(predDF, aes(x=predicted,y=resid)) + 14 geom_point(shape=1,position=position_jitter(width=0.1,height=0.1)) + 15 geom_smooth(method=lm,se=FALSE) 16 17grid.arrange(actual.plot, resid.plot, ncol = 2) Finally the following shows the CART model tree on the training data.\n1# plot tree 2cols \u0026lt;- ifelse(mod.reg.caret$frame$yval \u0026gt; 8,\u0026#34;green4\u0026#34;,\u0026#34;darkred\u0026#34;) # green if high 3prp(mod.reg.caret 4 ,main=\u0026#34;CART Model Tree\u0026#34; 5 #,extra=106 # display prob of survival and percent of obs 6 ,nn=TRUE # display the node numbers 7 ,fallen.leaves=TRUE # put the leaves on the bottom of the page 8 ,branch=.5 # change angle of branch lines 9 ,faclen=0 # do not abbreviate factor levels 10 ,trace=1 # print the automatically calculated cex 11 ,shadow.col=\u0026#34;gray\u0026#34; # shadows under the leaves 12 ,branch.lty=3 # draw branches using dotted lines 13 ,split.cex=1.2 # make the split text larger than the node text 14 ,split.prefix=\u0026#34;is \u0026#34; # put \u0026#34;is \u0026#34; before split text 15 ,split.suffix=\u0026#34;?\u0026#34; # put \u0026#34;?\u0026#34; after split text 16 ,col=cols, border.col=cols # green if survived 17 ,split.box.col=\u0026#34;lightgray\u0026#34; # lightgray split boxes (default is white) 18 ,split.border.col=\u0026#34;darkgray\u0026#34; # darkgray border on split boxes 19 ,split.round=.5) ## cex 0.65 xlim c(0, 1) ylim c(0, 1) ","date":"February 14, 2015","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/blog/2015-02-14-tree-based-methods-3/","series":[{"title":"Tree Based Methods in R","url":"/series/tree-based-methods-in-r/"}],"smallImg":"","tags":[{"title":"R","url":"/tags/r/"}],"timestamp":1423872000,"title":"Tree Based Methods in R - Part III"},{"categories":[{"title":"Machine Learning","url":"/categories/machine-learning/"}],"content":" Part I Part II (this post) Part III Part IV Part V Part VI In the previous article (Tree Based Methods in R - Part I), a decision tree is created on the Carseats data which is in the chapter 8 lab of ISLR. In that article, potentially asymetric costs due to misclassification are not taken into account. When unbalance between false positive and false negative can have a significant impact, it can be explicitly adjusted either by altering prior (or empirical) probabilities or by adding a loss matrix.\nA comprehensive summary of this topic, as illustrated in Berk (2008), is shown below.\n\u0026hellip;when the CART solution is determined solely by the data, the prior distribution is empirically determined, and the costs in the loss matrix of all classification errors are the same. Costs are being assigned even if the data analyst makes no conscious decision about them. Should the balance of false negatives to false positives that results be unsatisfactory, that balance can be changed. Either the costs in the loss matrix can be directly altered, leaving the prior distribution to be empirically determined, or the prior distribution can be altered leaving the default costs untouched. Much of the software currently available makes it easier to change the prior in the binary response case. When there are more than two response categories, it will usually be easier in practice to change the costs in the loss matrix directly.\nIn this article, cost-sensitive classification is implemented, assuming that misclassifying the High class is twice as expensive, both by altering the priors and by adjusting the loss matrix.\nThe following loss matrix is implemented.\nThe corresponding altered priors can be obtained by\nThe bold-cased sections of the tutorial of the caret package are covered in this article.\nVisualizations Pre-Processing Data Splitting Miscellaneous Model Functions Model Training and Tuning Using Custom Models Variable Importance Feature Selection: RFE, Filters, GA, SA Other Functions Parallel Processing Adaptive Resampling Let\u0026rsquo;s get started.\nThe following packages are used.\n1library(dplyr) # data minipulation 2library(rpart) # fit tree 3library(rpart.plot) # plot tree 4library(caret) # tune model Carseats data is created as following while the response (Sales) is converted into a binary variable.\n1require(ISLR) 2data(Carseats) 3Carseats = Carseats %\u0026gt;% 4 mutate(High=factor(ifelse(Sales\u0026lt;=8,\u0026#34;No\u0026#34;,\u0026#34;High\u0026#34;),labels=c(\u0026#34;High\u0026#34;,\u0026#34;No\u0026#34;))) 5# structure of predictors 6str(subset(Carseats,select=c(-High,-Sales))) ## \u0026#39;data.frame\u0026#39;:\t400 obs. of 10 variables: ## $ CompPrice : num 138 111 113 117 141 124 115 136 132 132 ... ## $ Income : num 73 48 35 100 64 113 105 81 110 113 ... ## $ Advertising: num 11 16 10 4 3 13 0 15 0 0 ... ## $ Population : num 276 260 269 466 340 501 45 425 108 131 ... ## $ Price : num 120 83 80 97 128 72 108 120 124 124 ... ## $ ShelveLoc : Factor w/ 3 levels \u0026#34;Bad\u0026#34;,\u0026#34;Good\u0026#34;,\u0026#34;Medium\u0026#34;: 1 2 3 3 1 1 3 2 3 3 ... ## $ Age : num 42 65 59 55 38 78 71 67 76 76 ... ## $ Education : num 17 10 12 14 13 16 15 10 10 17 ... ## $ Urban : Factor w/ 2 levels \u0026#34;No\u0026#34;,\u0026#34;Yes\u0026#34;: 2 2 2 2 2 1 2 2 1 1 ... ## $ US : Factor w/ 2 levels \u0026#34;No\u0026#34;,\u0026#34;Yes\u0026#34;: 2 2 2 2 1 2 1 2 1 2 ... 1# classification response summary 2res.summary = with(Carseats,rbind(table(High),table(High)/length(High))) 3res.summary ## High No ## [1,] 164.00 236.00 ## [2,] 0.41 0.59 The train and test data sets are split using createDataPartition().\n1# split data 2set.seed(1237) 3trainIndex = createDataPartition(Carseats$High, p=.8, list=FALSE, times=1) 4trainData = subset(Carseats, select=c(-Sales))[trainIndex,] 5testData = subset(Carseats, select=c(-Sales))[-trainIndex,] 6 7# response summary 8train.res.summary = with(trainData,rbind(table(High),table(High)/length(High))) 9test.res.summary = with(testData,rbind(table(High),table(High)/length(High))) 5 repeats of 10-fold cross validation is set up.\n1# set up train control 2trControl = trainControl(method=\u0026#34;repeatedcv\u0026#34;,number=10,repeats=5) Rather than tuning the complexity parameter (cp) using the built-in tuneLength, a grid is created. At first, it was intended to use this grid together with altered priors in the expand.grid() function of the caret package as rpart() has an argument named parms to enter altered priors (prior) or a loss matrix (loss) as a list. Later, however, it was found that the function does not accept an argument if it is not set as a tuning parameter. Therefore cp is not tuned when each of parms values is modified. (Although it is not considered in this article, the mlr package seems to support cost sensitive classification by adding a loss matrix as can be checked here)\n1# generate tune grid 2cpGrid = function(start,end,len) { 3 inc = if(end \u0026gt; start) (end-start)/len else 0 4 # update grid 5 if(inc \u0026gt; 0) { 6 grid = c(start) 7 while(start \u0026lt; end) { 8 start = start + inc 9 grid = c(grid,start) 10 } 11 } else { 12 message(\u0026#34;start \u0026gt; end, default cp value is taken\u0026#34;) 13 grid = c(0.1) 14 } 15 grid 16} 17 18grid = expand.grid(cp=cpGrid(0,0.3,20)) The default model is fit below.\n1# train model with equal cost 2set.seed(12357) 3mod.eq.cost = train(High ~ . 4 ,data=trainData 5 ,method=\u0026#34;rpart\u0026#34; 6 ,tuneGrid=grid 7 ,trControl=trControl) 8 9# select results at best tuned cp 10subset(mod.eq.cost$results,subset=cp==mod.eq.cost$bestTune$cp) ## cp Accuracy Kappa AccuracySD KappaSD ## 2 0.015 0.7303501 0.4383323 0.0836073 0.1743789 The model is refit with the tuned cp value.\n1# refit the model to the entire training data 2cp = mod.eq.cost$bestTune$cp 3mod.eq.cost = rpart(High ~ ., data=trainData, control=rpart.control(cp=cp)) Confusion matrices are obtained from both the training and test data sets. Here the matrices are transposed to the previous article and this is to keep the same structure as used in Berk (2008) - the source of getUpdatedCM() can be found in this gist.\nThe model error means how successful fitting or prediction is on each class given data and it is shown that the High class is more misclassified. The use error is to see how useful the model is given fitted or predicted values. It is also found that misclassification of the High class becomes worse when the model is applied to the test data.\n1source(\u0026#34;src/mlUtils.R\u0026#34;) 2fit.eq.cost = predict(mod.eq.cost, type=\u0026#34;class\u0026#34;) 3fit.cm.eq.cost = table(data.frame(actual=trainData$High,response=fit.eq.cost)) 4fit.cm.eq.cost = getUpdatedCM(cm=fit.cm.eq.cost, type=\u0026#34;Fitted\u0026#34;) 5fit.cm.eq.cost ## Fitted: High Fitted: No Model Error ## Actual: High 106.00 26.00 0.20 ## Actual: No 24.00 165.00 0.13 ## Use Error 0.18 0.14 0.16 1pred.eq.cost = predict(mod.eq.cost, newdata=testData, type=\u0026#34;class\u0026#34;) 2pred.cm.eq.cost = table(data.frame(actual=testData$High,response=pred.eq.cost)) 3pred.cm.eq.cost = getUpdatedCM(pred.cm.eq.cost) 4pred.cm.eq.cost ## Pred: High Pred: No Model Error ## Actual: High 21.00 11.0 0.34 ## Actual: No 4.00 43.0 0.09 ## Use Error 0.16 0.2 0.19 As mentioned earlier, either althered priors or a loss matrix can be entered into rpart(). They are created below.\n1# update prior probabilities 2costs = c(2,1) 3train.res.summary ## High No ## [1,] 132.000000 189.000000 ## [2,] 0.411215 0.588785 1prior.w.weight = c(train.res.summary[2,1] * costs[1] 2 ,train.res.summary[2,2] * costs[2]) 3priorUp = c(prior.w.weight[1]/sum(prior.w.weight) 4 ,prior.w.weight[2]/sum(prior.w.weight)) 5priorUp ## High No ## 0.5827815 0.4172185 1# loss matrix 2loss.mat = matrix(c(0,2,1,0),nrow=2,byrow=TRUE) 3loss.mat ## [,1] [,2] ## [1,] 0 2 ## [2,] 1 0 Both will deliver the same outcome.\n1# refit the model with the updated priors 2# fit with updated prior 3mod.uq.cost = rpart(High ~ ., data=trainData, parms=list(prior=priorUp), control=rpart.control(cp=cp)) 4 5# fit with loss matrix 6# mod.uq.cost = rpart(High ~ ., data=trainData, parms=list(loss=loss.mat), control=rpart.control(cp=cp)) Confusion matrices are obtained again. It is shown that more values are classified as the High class. Note that, although the overall misclassification error is increased, it does not reflect costs. In a situation, the cost adjusted CART may be more beneficial.\n1fit.cm.eq.cost ## Fitted: High Fitted: No Model Error ## Actual: High 106.00 26.00 0.20 ## Actual: No 24.00 165.00 0.13 ## Use Error 0.18 0.14 0.16 1fit.uq.cost = predict(mod.uq.cost, type=\u0026#34;class\u0026#34;) 2fit.cm.uq.cost = table(data.frame(actual=trainData$High,response=fit.uq.cost)) 3fit.cm.uq.cost = getUpdatedCM(fit.cm.uq.cost, type=\u0026#34;Fitted\u0026#34;) 4fit.cm.uq.cost ## Fitted: High Fitted: No Model Error ## Actual: High 123.00 9.00 0.07 ## Actual: No 44.00 145.00 0.23 ## Use Error 0.26 0.06 0.17 1pred.cm.eq.cost ## Pred: High Pred: No Model Error ## Actual: High 21.00 11.0 0.34 ## Actual: No 4.00 43.0 0.09 ## Use Error 0.16 0.2 0.19 1pred.uq.cost = predict(mod.uq.cost, newdata=testData, type=\u0026#34;class\u0026#34;) 2pred.cm.uq.cost = table(data.frame(actual=testData$High,response=pred.uq.cost)) 3pred.cm.uq.cost = getUpdatedCM(pred.cm.uq.cost) 4pred.cm.uq.cost ## Pred: High Pred: No Model Error ## Actual: High 26.00 6.00 0.19 ## Actual: No 14.00 33.00 0.30 ## Use Error 0.35 0.15 0.25 ","date":"February 8, 2015","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/blog/2015-02-08-tree-based-methods-2/","series":[{"title":"Tree Based Methods in R","url":"/series/tree-based-methods-in-r/"}],"smallImg":"","tags":[{"title":"R","url":"/tags/r/"}],"timestamp":1423353600,"title":"Tree Based Methods in R - Part II"},{"categories":[{"title":"Machine Learning","url":"/categories/machine-learning/"}],"content":" Part I (this post) Part II Part III Part IV Part V Part VI This is the first article about tree based methods using R. Carseats data in the chapter 8 lab of ISLR is used to perform classification analysis. Unlike the lab example, the rpart package is used to fit the CART model on the data and the caret package is used for tuning the pruning parameter (cp).\nThe bold-cased sections of the tutorial are covered in this article.\nVisualizations Pre-Processing Data Splitting Miscellaneous Model Functions Model Training and Tuning Using Custom Models Variable Importance Feature Selection: RFE, Filters, GA, SA Other Functions Parallel Processing Adaptive Resampling The pruning parameter in the rpart package is scaled so that its values are from 0 to 1. Specifically the formula is\n$$ R_{cp}\\left(T\\right)\\equiv R\\left(T\\right) + cp*|T|*R\\left(T_{1}\\right) $$\nwhere $$T_{1}$$ is the tree with no splits, $$\\mid T\\mid$$ is the number of splits for a tree and R is the risk.\nDue to the inclusion of $$R\\left(T_{1}\\right)$$, when cp=1, the tree will result in no splits while it is not pruned when cp=0. On the other hand, in the original setup without the term, the pruning parameter ($$\\alpha$$) can range from 0 to infinity.\nLet\u0026rsquo;s get started.\nThe following packages are used.\n1library(dplyr) # data minipulation 2library(rpart) # fit tree 3library(rpart.plot) # plot tree 4library(caret) # tune model Carseats data is created as following while the response (Sales) is converted into a binary variable.\n1require(ISLR) 2data(Carseats) 3Carseats = Carseats %\u0026gt;% 4 mutate(High=factor(ifelse(Sales\u0026lt;=8,\u0026#34;No\u0026#34;,\u0026#34;High\u0026#34;),labels=c(\u0026#34;High\u0026#34;,\u0026#34;No\u0026#34;))) 5# structure of predictors 6str(subset(Carseats,select=c(-High,-Sales))) ## \u0026#39;data.frame\u0026#39;:\t400 obs. of 10 variables: ## $ CompPrice : num 138 111 113 117 141 124 115 136 132 132 ... ## $ Income : num 73 48 35 100 64 113 105 81 110 113 ... ## $ Advertising: num 11 16 10 4 3 13 0 15 0 0 ... ## $ Population : num 276 260 269 466 340 501 45 425 108 131 ... ## $ Price : num 120 83 80 97 128 72 108 120 124 124 ... ## $ ShelveLoc : Factor w/ 3 levels \u0026#34;Bad\u0026#34;,\u0026#34;Good\u0026#34;,\u0026#34;Medium\u0026#34;: 1 2 3 3 1 1 3 2 3 3 ... ## $ Age : num 42 65 59 55 38 78 71 67 76 76 ... ## $ Education : num 17 10 12 14 13 16 15 10 10 17 ... ## $ Urban : Factor w/ 2 levels \u0026#34;No\u0026#34;,\u0026#34;Yes\u0026#34;: 2 2 2 2 2 1 2 2 1 1 ... ## $ US : Factor w/ 2 levels \u0026#34;No\u0026#34;,\u0026#34;Yes\u0026#34;: 2 2 2 2 1 2 1 2 1 2 ... 1# classification response summary 2with(Carseats,table(High)) ## High ## High No ## 164 236 1with(Carseats,table(High) / length(High)) ## High ## High No ## 0.41 0.59 The train and test data sets are split using createDataPartition().\n1## Data Splitting 2set.seed(1237) 3trainIndex.cl = createDataPartition(Carseats$High, p=.8, list=FALSE, times=1) 4trainData.cl = subset(Carseats, select=c(-Sales))[trainIndex.cl,] 5testData.cl = subset(Carseats, select=c(-Sales))[-trainIndex.cl,] Stratify sampling is performed by default.\n1# training response summary 2with(trainData.cl,table(High)) ## High ## High No ## 132 189 1with(trainData.cl,table(High) / length(High)) ## High ## High No ## 0.411215 0.588785 1# test response summary 2with(testData.cl,table(High)) ## High ## High No ## 32 47 1with(testData.cl,table(High) / length(High)) ## High ## High No ## 0.4050633 0.5949367 The following resampling strategies are considered: cross-validation, repeated cross-validation and bootstrap.\n1## train control 2trControl.cv = trainControl(method=\u0026#34;cv\u0026#34;,number=10) 3trControl.recv = trainControl(method=\u0026#34;repeatedcv\u0026#34;,number=10,repeats=5) 4trControl.boot = trainControl(method=\u0026#34;boot\u0026#34;,number=50) There are two methods in the caret package: rpart and repart2. The first method allows the pruning parameter to be tuned. The tune grid is not set up explicitly and it is adjusted by tuneLength - equally spaced cp values are created from 0 to 0.3 in the package.\n1set.seed(12357) 2fit.cl.cv = train(High ~ . 3 ,data=trainData.cl 4 ,method=\u0026#34;rpart\u0026#34; 5 ,tuneLength=20 6 ,trControl=trControl.cv) 7 8fit.cl.recv = train(High ~ . 9 ,data=trainData.cl 10 ,method=\u0026#34;rpart\u0026#34; 11 ,tuneLength=20 12 ,trControl=trControl.recv) 13 14fit.cl.boot = train(High ~ . 15 ,data=trainData.cl 16 ,method=\u0026#34;rpart\u0026#34; 17 ,tuneLength=20 18 ,trControl=trControl.boot) Repeated cross-validation and bootstrap produce the same best tuned cp value while cross-validation returns a higher value.\n1# results at best tuned cp 2subset(fit.cl.cv$results,subset=cp==fit.cl.cv$bestTune$cp) ## cp Accuracy Kappa AccuracySD KappaSD ## 3 0.03110048 0.7382148 0.4395888 0.06115425 0.1355713 1subset(fit.cl.recv$results,subset=cp==fit.cl.recv$bestTune$cp) ## cp Accuracy Kappa AccuracySD KappaSD ## 2 0.01555024 0.7384537 0.456367 0.08602102 0.1821254 1subset(fit.cl.boot$results,subset=cp==fit.cl.boot$bestTune$cp) ## cp Accuracy Kappa AccuracySD KappaSD ## 2 0.01555024 0.7178692 0.4163337 0.04207806 0.08445416 The one from repeated cross-validation is taken to fit to the entire training data.\nUpdated on Feb 10, 2015\nAs a value of cp is entered in rpart(), the function fits the model up to the value and takes the result. Therefore it produces a pruned tree. If it is not set or set to be a low value (eg, 0), pruning can be done using the prune() function. 1## refit the model to the entire training data 2cp.cl = fit.cl.recv$bestTune$cp 3fit.cl = rpart(High ~ ., data=trainData.cl, control=rpart.control(cp=cp.cl)) 4 5# Updated on Feb 10, 2015 6# prune if cp not set or too low 7# fit.cl = prune(tree=fit.cl, cp=cp.cl) The resulting tree is shown as following. The plot shows expected losses and node probabilities in the final nodes. For example, the leftmost node has\nexpected loss of 0.13 (= 8/60 (0.13333)) node probability of 19% (= 60/321) 1# plot tree 2cols \u0026lt;- ifelse(fit.cl$frame$yval == 1,\u0026#34;green4\u0026#34;,\u0026#34;darkred\u0026#34;) # green if high 3prp(fit.cl 4 ,main=\u0026#34;CART model tree\u0026#34; 5 ,extra=106 # display prob of survival and percent of obs 6 ,nn=TRUE # display the node numbers 7 ,fallen.leaves=TRUE # put the leaves on the bottom of the page 8 ,branch=.5 # change angle of branch lines 9 ,faclen=0 # do not abbreviate factor levels 10 ,trace=1 # print the automatically calculated cex 11 ,shadow.col=\u0026#34;gray\u0026#34; # shadows under the leaves 12 ,branch.lty=3 # draw branches using dotted lines 13 ,split.cex=1.2 # make the split text larger than the node text 14 ,split.prefix=\u0026#34;is \u0026#34; # put \u0026#34;is \u0026#34; before split text 15 ,split.suffix=\u0026#34;?\u0026#34; # put \u0026#34;?\u0026#34; after split text 16 ,col=cols, border.col=cols # green if survived 17 ,split.box.col=\u0026#34;lightgray\u0026#34; # lightgray split boxes (default is white) 18 ,split.border.col=\u0026#34;darkgray\u0026#34; # darkgray border on split boxes 19 ,split.round=.5) ## cex 0.7 xlim c(0, 1) ylim c(-0.1, 1.1) The fitted model is applied to the test data.\n1## apply to the test data 2pre.cl = predict(fit.cl, newdata=testData.cl, type=\u0026#34;class\u0026#34;) 3 4# confusion matrix 5cMat.cl = table(data.frame(response=pre.cl,truth=testData.cl$High)) 6cMat.cl ## truth ## response High No ## High 21 4 ## No 11 43 1# mean misclassification error 2mmse.cl = 1 - sum(diag(cMat.cl))/sum(cMat.cl) 3mmse.cl ## [1] 0.1898734 More models are going to be implemented/compared in the subsequent articles.\n","date":"February 1, 2015","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/blog/2015-02-01-tree-based-methods-1/","series":[{"title":"Tree Based Methods in R","url":"/series/tree-based-methods-in-r/"}],"smallImg":"","tags":[{"title":"R","url":"/tags/r/"}],"timestamp":1422748800,"title":"Tree Based Methods in R - Part I"},{"categories":[{"title":"R","url":"/categories/r/"}],"content":"This is a quick trial of adding overall and conditional (by user) average columns in a data frame. base,plyr,dplyr,data.table,dplyr + data.table packages are used. Personally I perfer dplyr + data.table - dplyr for comperhensive syntax and data.table for speed.\n1## set up variables 2size \u0026lt;- 36000 3numUsers \u0026lt;- 4900 4# roughly each user has 7 sessions 5numSessions \u0026lt;- (numUsers / 7) - ((numUsers / 7) %% 1) 6 7## create data frame 8set.seed(123457) 9userIds \u0026lt;- sample.int(numUsers, size=size, replace=TRUE) 10ssIds \u0026lt;- sample.int(numSessions, size=size, replace=TRUE) 11scores \u0026lt;- sample.int(10, size=size, replace=TRUE) 12 13preDf \u0026lt;- data.frame(User=userIds, Session=ssIds, Score=scores) 14preDf$User \u0026lt;- as.factor(preDf$User) Adding overall average As calculating overall average is not complicated, I don\u0026rsquo;t find a big difference among packages.\n1# base 2system.time(overallDf1 \u0026lt;- transform(preDf, MeanScore=mean(Score, na.rm=TRUE))) ## user system elapsed ## 0.001 0.000 0.002 1# plyr 2require(plyr) 3system.time(overallDf2 \u0026lt;- mutate(preDf, MeanScore=mean(Score, na.rm=TRUE))) ## user system elapsed ## 0.002 0.000 0.002 1# dplyr 2require(dplyr) 3system.time(overallDf3 \u0026lt;- preDf %\u0026gt;% 4 mutate(MeanScore=mean(Score, na.rm=TRUE))) ## user system elapsed ## 0.007 0.000 0.007 1# data.table 2require(data.table) 3preDt \u0026lt;- data.table(preDf) 4setkey(preDt, User) 5system.time(overallDt \u0026lt;- preDt[,list(User=User 6 ,Session=Session 7 ,Score=Score 8 ,MeanScore=mean(Score, na.rm=T))]) ## user system elapsed ## 0.007 0.000 0.007 1# dplyr + data.table 2system.time(overallDf4 \u0026lt;- preDt %\u0026gt;% 3 mutate(MeanScore=mean(Score, na.rm=TRUE))) ## user system elapsed ## 0.003 0.000 0.003 Adding average by user It takes quite long using plyr and other packages would be more practial - base is not considered even.\n1# plyr 2require(plyr) 3system.time(postDf1 \u0026lt;- ddply(preDf 4 ,.(User) 5 ,mutate,MeanScore=mean(Score, na.rm=TRUE))) ## user system elapsed ## 76.488 0.522 76.990 1# dplyr 2require(dplyr) 3system.time(postDf2 \u0026lt;- preDf %\u0026gt;% 4 group_by(User) %\u0026gt;% 5 mutate(MeanScore=mean(Score, na.rm=TRUE)) %\u0026gt;% 6 arrange(User)) ## user system elapsed ## 0.022 0.006 0.028 1# data.table 2require(data.table) 3preDt \u0026lt;- data.table(preDf) 4setkey(preDt, User) 5system.time(postDt \u0026lt;- preDt[,list(Session=Session 6 ,Score=Score 7 ,MeanScore=mean(Score, na.rm=T)) 8 ,by=User]) ## user system elapsed ## 0.005 0.004 0.009 1# dplyr + data.table 2system.time(postDf3 \u0026lt;- preDt %\u0026gt;% 3 group_by(User) %\u0026gt;% 4 mutate(MeanScore=mean(Score, na.rm=TRUE)) %\u0026gt;% 5 arrange(User)) ## user system elapsed ## 0.008 0.004 0.012 ","date":"January 14, 2015","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/blog/2015-01-14-quick-trial-of-adding-column/","series":[],"smallImg":"","tags":[{"title":"R","url":"/tags/r/"}],"timestamp":1421193600,"title":"Quick Trial of Adding Column"},{"categories":[{"title":"R","url":"/categories/r/"}],"content":"Purely programming point of view, I consider for-loops would be better to be avoided in R as\nthe script can be more readable it is easier to handle errors Some articles on the web indicate that looping functions (or apply family of functions) don\u0026rsquo;t guarantee faster execution and sometimes even slower. Although, assuming that the experiments are correct, in my opinion, code readability itself is beneficial enough to avoid for-loops. Even worse, R\u0026rsquo;s dynamic typing system coupled with poor readability can result in a frustrating consequence as the code grows.\nAlso one of the R\u0026rsquo;s best IDE (RStudio) doesn\u0026rsquo;t seem to provide an incremental debugging - it may be wrong as I\u0026rsquo;m new to it. Therefore it is not easy to debug pieces in a for-loop. In this regard, it would be a good idea to refactor for-loops into pieces.\nIf one has decided to avoid for-loops, the way how to code would need to be changed. With for-loops, the focus is \u0026lsquo;how to get the job done\u0026rsquo;. One the other hand, if it is replaced with looping functions, the focus should be \u0026lsquo;what does the outcome look like\u0026rsquo;. In other words, the way of thinking should be declarative, rather than imperative.\nBelow shows two examples from The R Project for Statistical Computing in LinkedIn. Instead of using for-loop, apply family of functions or plyr package are used for recursive computation.\nThe following packages are used.\n1library(knitr) 2library(plyr) How can I set up for function for the following codes? In this post, the goal is to create a function that creates a simulated vector with the following code.\n1n=100 2m=1000 3mu=c() 4sigma2=c() 5for(i in 1:m){ 6 x = rnorm(n) 7 s=sum((x-mean(x))^2) 8 v=sum(x[1:n-1]^2) 9 sigma2[i]=s/v 10 mu[i]=mean(x)+x[n]*sqrt(sigma2[i]/n) 11} The above for-loop can be replaced easily with apply and mapply as following.\n1sim \u0026lt;- function(n, m) { 2 # create internal functions 3 sigmaSquared \u0026lt;- function(x) { 4 sum((x-mean(x))^2) / sum(x[1:length(x)-1]^2) 5 } 6 mu \u0026lt;- function(x) { 7 mean(x) + x[length(x)] + sqrt(sigmaSquared(x) / length(x)) 8 } 9 10 # create random numbers 11 set.seed(123457) 12 mat \u0026lt;- mapply(rnorm, rep(n, m)) 13 14 apply(mat, 2, mu) 15} The function named sim is evaluated below.\n1n \u0026lt;- 100 2m \u0026lt;- 1000 3simVec \u0026lt;- sim(n, m) 4 5head(simVec) ## [1] 0.90176825 -0.55948429 0.23258654 0.08713173 0.49159634 0.71904362 Automatically selecting groups of numeric values in a data frame I\u0026rsquo;m trying to extract subsets of values from my dataset which are grouped by a value (which could be any number). This column is set by another piece of software and so the code needs to be flexible enough to identify groups of identical numbers without the number being specified. I.e. if value in row10 = row11 then group. For that I have used: 1for (n in 1:length(data$BPM)){ 2 for(i in 1:length(data$Date)){ 3 bird\u0026lt;-subset(data, Date == Date[i] \u0026amp; BPM == BPM[n], select = c(Date:Power)) 4 head(bird) 5 } 6} This seems to work. I then need to identify all of the groups which have \u0026gt;4 rows and separate those from each other. Here the goal is to select records that have preset Date and BPM pairs. Also it is necessary that the numbers of each group are greater than 4.\nInitial selection conditions are shown below. Note that, for simplicity, the groups will be selected only if the numbers are greater than or equal to 2.\n1# create initial conditions 2numRec \u0026lt;- 2 # assumed to be 2 for simplicity 3dateFilter \u0026lt;- c(as.Date(\u0026#34;2014/12/1\u0026#34;), as.Date(\u0026#34;2014/12/3\u0026#34;)) 4numFilter \u0026lt;- c(1) Then a data frame is created.\n1# create a data frame 2set.seed(123457) 3days \u0026lt;- seq(as.Date(\u0026#34;2014/12/1\u0026#34;), as.Date(\u0026#34;2014/12/3\u0026#34;), \u0026#34;day\u0026#34;) 4randDays \u0026lt;- sample(days, size=10, replace=TRUE) 5randBpm \u0026lt;- sample(1:3, size=10, replace=TRUE) 6df \u0026lt;- data.frame(Date=randDays, BPM = randBpm) At first, the data frame is converted into a list by Date and BPM using ldply.\n1# data frame to list by Date and BPM 2lst \u0026lt;- dlply(df, .(Date, BPM)) Then dimensions of each list elements are checked using sapply. Then a Boolean vector is created so that the value will be TRUE only if the row dimension is greater than 2.\n1# find which list elements \u0026gt;= numRec 2dimDf \u0026lt;- sapply(lst, dim) 3isGreater \u0026lt;- dimDf[1,] \u0026gt;= numRec Finally the list is converted into a data frame using ldply by subsetting the Boolean vector.\n1# convert into data frame only if # elements \u0026gt;= numRec 2exDf \u0026lt;- ldply(lst[isGreater]) 3kable(subset(exDf, subset = Date %in% dateFilter \u0026amp; BPM %in% numFilter, select = c(-.id))) Date BPM 3 2014-12-03 1 4 2014-12-03 1 5 2014-12-03 1 ","date":"December 17, 2014","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/blog/2014-12-17-looping-without-for/","series":[],"smallImg":"","tags":[{"title":"R","url":"/tags/r/"}],"timestamp":1418774400,"title":"Looping Without For"},{"categories":[{"title":"R","url":"/categories/r/"}],"content":"As I don\u0026rsquo;t use R at work yet, I haven\u0026rsquo;t got enough chances to learn R by doing. Together with teaching myself machine learning with R, I consider it would be a good idea to collect examples on the web. Recently I have been visiting a Linkedin group called The R Project for Statistical Computing and suggesting scripts on data manipulation or programming topics. Actually I expected some of the topics may also be helpful to learn machine learning/statistics but, possibly due to lack of understaning of the topics in context, I\u0026rsquo;ve found only a few - it may be necessary to look for another source. Anyway below is a summary of two examples.\nSummarise a data frame group by a column This seems to be a part of an assignment of a Coursera class and the original posting is as following.\nProblem Statement : To find the \u0026lsquo;Best\u0026rsquo; hospital in a particular city under one of the three outcomes(heart attack, heart failure, pneumonia). The data set (dat) contains 54 unique states and a total of 4706 observations with 26 variable columns. Column 11 contains Mortality rate for each hospital(4706) due to heart attack. Logic is simple as the best hospital would be one with least mortality. But something is going awry which i\u0026rsquo;m not able to identify. Please help it out! As far as I understand, the one who posted this article tried to split the data by state into a list as x\u0026lt;- split(as.character(dat$Hospital.Name),dat$State). Then he was going to combine the list elements after calculating the minimum mortality rate by state.\nFocusing on the logic in bold-cased above, I just created a simple data frame and converted it into another data frame which shows state, name and mininum number - an interger vector was assumed rather than mortality rates.\nSome notes about the code is\nIt would be good to use a library that provides a comprehensive syntax plyr is used ddply converts a data frame (ddply) into another (ddply) .data requires a data frame - dat .(state) will group by state when applying a function - eg mininum by state name = name[num==min(num, na.rm=TRUE)][1] selects name when num is the minimum of num last indexing ([1]) is necessary in case there is a tie for example, it\u0026rsquo;ll fail if num = c(3, 2, 6, 2, 3) (same number in state1) the idea is just selecting the first record when there is a tie 1library(knitr) 2library(plyr) 1dat \u0026lt;- data.frame(name=c(\u0026#34;name1\u0026#34;,\u0026#34;name2\u0026#34;,\u0026#34;name3\u0026#34;,\u0026#34;name4\u0026#34;,\u0026#34;name5\u0026#34;), 2 num = c(3, 2, 6, 2, 9), 3 state = c(\u0026#34;state1\u0026#34;,\u0026#34;state2\u0026#34;,\u0026#34;state3\u0026#34;,\u0026#34;state3\u0026#34;,\u0026#34;state1\u0026#34;)) 4 5# summarise dat by state 6sumDat \u0026lt;- ddply(.data=dat, .(state), .drop=FALSE, # missing combination kept 7 summarise, name = name[num==min(num, na.rm=TRUE)][1], minNum = min(num, na.rm=TRUE)) 8 9# show output 10kable(sumDat) state name minNum state1 name1 3 state2 name2 2 state3 name4 2 A quick way of simulation This is a quick way of simulating exponential random variables where the number of variables in each trial is 8 and the rate is fixed to be 1/5. 5000 trials are supposed to be performed and their means and variances are calculated.\nThe simulation is done using mapply, which is a multivariate version of sapply. Then apply is ued to obtain the sample properties.\n1# set local variables 2n \u0026lt;- 8 3times \u0026lt;- 500 4rate \u0026lt;- 1/5 5 6set.seed(12347) 7# mapply is a multivariate version of sapply 8# it allow to apply rexp on a vector rather than a single value 9mat \u0026lt;- mapply(rexp, rep(n,times=times), rate=rate) 10 11# apply to get mean and variance 12df \u0026lt;- as.data.frame(cbind(apply(mat, 2, mean),apply(mat, 2, var))) 13names(df) \u0026lt;- c(\u0026#34;mean\u0026#34;,\u0026#34;var\u0026#34;) 14 15# show output 16kable(head(df)) mean var 5.755293 39.998841 7.378336 64.692667 12.036352 68.976558 3.454449 16.025874 5.289533 11.723384 3.641087 6.539533 ","date":"December 3, 2014","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/blog/2014-12-03-short-r-examples/","series":[],"smallImg":"","tags":[{"title":"R","url":"/tags/r/"}],"timestamp":1417564800,"title":"Short R Examples"},{"categories":[{"title":"R","url":"/categories/r/"}],"content":"This post is a slight extension of the previous two articles (Download Stock Data - Part I, Download Stock Data - Part II) and we discuss how to produce gross returns, standard deviation and correlation of multiple shares.\nThe following packages are used.\n1library(knitr) 2library(lubridate) 3library(stringr) 4library(reshape2) 5library(plyr) 6library(dplyr) The script begins with creating a data folder in the format of data_YYYY-MM-DD.\n1# create data folder 2dataDir \u0026lt;- paste0(\u0026#34;data\u0026#34;,\u0026#34;_\u0026#34;,format(Sys.Date(),\u0026#34;%Y-%m-%d\u0026#34;)) 3if(file.exists(dataDir)) { 4 unlink(dataDir, recursive = TRUE) 5 dir.create(dataDir) 6} else { 7 dir.create(dataDir) 8} Given company codes, URLs and file paths are created. Then data files are downloaded by Map, which is a wrapper of mapply. Note that R\u0026rsquo;s download.file function is wrapped by downloadFile so that the function does not break when an error occurs.\n1# assumes codes are known beforehand 2codes \u0026lt;- c(\u0026#34;MSFT\u0026#34;, \u0026#34;TCHC\u0026#34;) 3urls \u0026lt;- paste0(\u0026#34;http://www.google.com/finance/historical?q=NASDAQ:\u0026#34;, 4 codes,\u0026#34;\u0026amp;output=csv\u0026#34;) 5paths \u0026lt;- paste0(dataDir,\u0026#34;/\u0026#34;,codes,\u0026#34;.csv\u0026#34;) # backward slash on windows (\\) 6 7# simple error handling in case file doesn\u0026#39;t exists 8downloadFile \u0026lt;- function(url, path, ...) { 9 # remove file if exists already 10 if(file.exists(path)) file.remove(path) 11 # download file 12 tryCatch( 13 download.file(url, path, ...), error = function(c) { 14 # remove file if error 15 if(file.exists(path)) file.remove(path) 16 # create error message 17 c$message \u0026lt;- paste(substr(path, 1, 4),\u0026#34;failed\u0026#34;) 18 message(c$message) 19 } 20 ) 21} 22# wrapper of mapply 23Map(downloadFile, urls, paths) Once the files are downloaded, they are read back to combine using rbind_all. Some more details about this step is listed below.\nonly Date, Close and Code columns are taken codes are extracted from file paths by matching a regular expression data is arranged by date as the raw files are sorted in a descending order error is handled by returning a dummy data frame where its code value is NA. individual data files are merged in a long format \u0026lsquo;NA\u0026rsquo; is filtered out 1# read all csv files and merge 2files \u0026lt;- dir(dataDir, full.name = TRUE) 3dataList \u0026lt;- llply(files, function(file){ 4 # get code from file path 5 pattern \u0026lt;- \u0026#34;/[A-Z][A-Z][A-Z][A-Z]\u0026#34; 6 code \u0026lt;- substr(str_extract(file, pattern), 2, nchar(str_extract(file, pattern))) 7 tryCatch({ 8 data \u0026lt;- read.csv(file, stringsAsFactors = FALSE) 9 # first column\u0026#39;s name is funny 10 names(data) \u0026lt;- c(\u0026#34;Date\u0026#34;,\u0026#34;Open\u0026#34;,\u0026#34;High\u0026#34;,\u0026#34;Low\u0026#34;,\u0026#34;Close\u0026#34;,\u0026#34;Volume\u0026#34;) 11 data$Date \u0026lt;- dmy(data$Date) 12 data$Close \u0026lt;- as.numeric(data$Close) 13 data$Code \u0026lt;- code 14 # optional 15 data$Open \u0026lt;- as.numeric(data$Open) 16 data$High \u0026lt;- as.numeric(data$High) 17 data$Low \u0026lt;- as.numeric(data$Low) 18 data$Volume \u0026lt;- as.integer(data$Volume) 19 # select only \u0026#39;Date\u0026#39;, \u0026#39;Close\u0026#39; and \u0026#39;Code\u0026#39; 20 # raw data should be arranged in an ascending order 21 arrange(subset(data, select = c(Date, Close, Code)), Date) 22 }, 23 error = function(c){ 24 c$message \u0026lt;- paste(code,\u0026#34;failed\u0026#34;) 25 message(c$message) 26 # return a dummy data frame not to break function 27 data \u0026lt;- data.frame(Date=dmy(format(Sys.Date(),\u0026#34;%d%m%Y\u0026#34;)), Close=0, Code=\u0026#34;NA\u0026#34;) 28 data 29 }) 30}, .progress = \u0026#34;text\u0026#34;) 31 32# data is combined to create a long format 33# dummy data frame values are filtered out 34data \u0026lt;- filter(rbind_all(dataList), Code != \u0026#34;NA\u0026#34;) Some values of this long format data is shown below.\nDate Close Code 2013-11-29 38.13 MSFT 2013-12-02 38.45 MSFT 2013-12-03 38.31 MSFT 2013-12-04 38.94 MSFT 2013-12-05 38.00 MSFT 2013-12-06 38.36 MSFT The data is converted into a wide format data where the x and y variables are Date and Code respectively (Date ~ Code) while the value variable is Close (value.var=\u0026quot;Close\u0026quot;). Some values of the wide format data is shown below.\n1# data is converted into a wide format 2data \u0026lt;- dcast(data, Date ~ Code, value.var=\u0026#34;Close\u0026#34;) 3kable(head(data)) Date MSFT TCHC 2013-11-29 38.13 13.52 2013-12-02 38.45 13.81 2013-12-03 38.31 13.48 2013-12-04 38.94 13.71 2013-12-05 38.00 13.55 2013-12-06 38.36 13.95 The remaining steps are just differencing close price values after taking log and applying sum, sd, and cor.\n1# select except for Date column 2data \u0026lt;- select(data, -Date) 3 4# apply log difference column wise 5dailyRet \u0026lt;- apply(log(data), 2, diff, lag=1) 6 7# obtain daily return, variance and correlation 8returns \u0026lt;- apply(dailyRet, 2, sum, na.rm = TRUE) 9std \u0026lt;- apply(dailyRet, 2, sd, na.rm = TRUE) 10correlation \u0026lt;- cor(dailyRet) 11 12returns ## MSFT TCHC ## 0.2249777 0.6293973 1std ## MSFT TCHC ## 0.01167381 0.03203031 1correlation ## MSFT TCHC ## MSFT 1.0000000 0.1481043 ## TCHC 0.1481043 1.0000000 Finally the data folder is deleted.\n1# delete data folder 2if(file.exists(dataDir)) { unlink(dataDir, recursive = TRUE) } ","date":"November 27, 2014","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/blog/2014-11-27-summarise-stock-returns-from-multiple-files/","series":[],"smallImg":"","tags":[{"title":"R","url":"/tags/r/"}],"timestamp":1417046400,"title":"Summarise Stock Returns From Multiple Files"},{"categories":[{"title":"R","url":"/categories/r/"}],"content":"In an earlier article, a way to download stock price data files from Google, save it into a local drive and merge them into a single data frame. If files are not large, however, it wouldn\u0026rsquo;t be effective and, in this article, files are downloaded and merged internally.\nThe following packages are used.\n1library(knitr) 2library(lubridate) 3library(stringr) 4library(plyr) 5library(dplyr) Taking urls as file locations, files are directly read using llply and they are combined using rbind_all. As the merged data has multiple stocks\u0026rsquo; records, Code column is created. Note that, when an error occurrs, the function returns a dummy data frame in order not to break the loop - values of the dummy data frame(s) are filtered out at the end.\n1# assumes codes are known beforehand 2codes \u0026lt;- c(\u0026#34;MSFT\u0026#34;, \u0026#34;TCHC\u0026#34;) # codes \u0026lt;- c(\u0026#34;MSFT\u0026#34;, \u0026#34;1234\u0026#34;) for testing 3files \u0026lt;- paste0(\u0026#34;http://www.google.com/finance/historical?q=NASDAQ:\u0026#34;, 4 codes,\u0026#34;\u0026amp;output=csv\u0026#34;) 5 6dataList \u0026lt;- llply(files, function(file, ...) { 7 # get code from file url 8 pattern \u0026lt;- \u0026#34;Q:[0-9a-zA-Z][0-9a-zA-Z][0-9a-zA-Z][0-9a-zA-Z]\u0026#34; 9 code \u0026lt;- substr(str_extract(file, pattern), 3, nchar(str_extract(file, pattern))) 10 11 # read data directly from a URL with only simple error handling 12 # for further error handling: http://adv-r.had.co.nz/Exceptions-Debugging.html 13 tryCatch({ 14 data \u0026lt;- read.csv(file, stringsAsFactors = FALSE) 15 # first column\u0026#39;s name is funny 16 names(data) \u0026lt;- c(\u0026#34;Date\u0026#34;,\u0026#34;Open\u0026#34;,\u0026#34;High\u0026#34;,\u0026#34;Low\u0026#34;,\u0026#34;Close\u0026#34;,\u0026#34;Volume\u0026#34;) 17 data$Date \u0026lt;- dmy(data$Date) 18 data$Open \u0026lt;- as.numeric(data$Open) 19 data$High \u0026lt;- as.numeric(data$High) 20 data$Low \u0026lt;- as.numeric(data$Low) 21 data$Close \u0026lt;- as.numeric(data$Close) 22 data$Volume \u0026lt;- as.integer(data$Volume) 23 data$Code \u0026lt;- code 24 data 25 }, 26 error = function(c) { 27 c$message \u0026lt;- paste(code,\u0026#34;failed\u0026#34;) 28 message(c$message) 29 # return a dummy data frame 30 data \u0026lt;- data.frame(Date=dmy(format(Sys.Date(),\u0026#34;%d%m%Y\u0026#34;)), Open=0, High=0, 31 Low=0, Close=0, Volume=0, Code=\u0026#34;NA\u0026#34;) 32 data 33 }) 34}) 35 36# dummy data frame values are filtered out 37data \u0026lt;- filter(rbind_all(dataList), Code != \u0026#34;NA\u0026#34;) Some of the values are shown below.\nDate Open High Low Close Volume Code 2014-11-26 47.49 47.99 47.28 47.75 27164877 MSFT 2014-11-25 47.66 47.97 47.45 47.47 28007993 MSFT 2014-11-24 47.99 48.00 47.39 47.59 35434245 MSFT 2014-11-21 49.02 49.05 47.57 47.98 42884795 MSFT 2014-11-20 48.00 48.70 47.87 48.70 21510587 MSFT 2014-11-19 48.66 48.75 47.93 48.22 26177450 MSFT It took a bit longer to complete the script as I had to teach myself how to handle errors in R. And this is why I started to write articles in this blog.\nI hope this article is useful.\n","date":"November 21, 2014","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/blog/2014-11-21-download-stock-data-2/","series":[],"smallImg":"","tags":[{"title":"R","url":"/tags/r/"}],"timestamp":1416528000,"title":"Download Stock Data - Part II"},{"categories":[{"title":"R","url":"/categories/r/"}],"content":"This article illustrates how to download stock price data files from Google, save it into a local drive and merge them into a single data frame. This script is slightly modified from a script which downloads RStudio package download log data. The original source can be found here.\nFirst of all, the following three packages are used.\n1library(knitr) 2library(lubridate) 3library(stringr) 4library(plyr) 5library(dplyr) The script begins with creating a folder to save data files.\n1# create data folder 2dataDir \u0026lt;- paste0(\u0026#34;data\u0026#34;,\u0026#34;_\u0026#34;,\u0026#34;2014-11-20-Download-Stock-Data-1\u0026#34;) 3if(file.exists(dataDir)) { 4 unlink(dataDir, recursive = TRUE) 5 dir.create(dataDir) 6} else { 7 dir.create(dataDir) 8} After creating urls and file paths, files are downloaded using Map function - it is a warpper of mapply. Note that, in case the function breaks by an error (eg when a file doesn\u0026rsquo;t exist), download.file is wrapped by another function that includes an error handler (tryCatch).\n1# assumes codes are known beforehand 2codes \u0026lt;- c(\u0026#34;MSFT\u0026#34;, \u0026#34;TCHC\u0026#34;) # codes \u0026lt;- c(\u0026#34;MSFT\u0026#34;, \u0026#34;1234\u0026#34;) for testing 3urls \u0026lt;- paste0(\u0026#34;http://www.google.com/finance/historical?q=NASDAQ:\u0026#34;, 4 codes,\u0026#34;\u0026amp;output=csv\u0026#34;) 5paths \u0026lt;- paste0(dataDir,\u0026#34;/\u0026#34;,codes,\u0026#34;.csv\u0026#34;) # back slash on windows (\\\\) 6 7# simple error handling in case file doesn\u0026#39;t exists 8downloadFile \u0026lt;- function(url, path, ...) { 9 # remove file if exists already 10 if(file.exists(path)) file.remove(path) 11 # download file 12 tryCatch( 13 download.file(url, path, ...), error = function(c) { 14 # remove file if error 15 if(file.exists(path)) file.remove(path) 16 # create error message 17 c$message \u0026lt;- paste(substr(path, 1, 4),\u0026#34;failed\u0026#34;) 18 message(c$message) 19 } 20 ) 21} 22# wrapper of mapply 23Map(downloadFile, urls, paths) Finally files are read back using llply and they are combined using rbind_all. Note that, as the merged data has multiple stocks\u0026rsquo; records, Code column is created.\n1# read all csv files and merge 2files \u0026lt;- dir(dataDir, full.name = TRUE) 3dataList \u0026lt;- llply(files, function(file){ 4 data \u0026lt;- read.csv(file, stringsAsFactors = FALSE) 5 # get code from file path 6 pattern \u0026lt;- \u0026#34;/[A-Z][A-Z][A-Z][A-Z]\u0026#34; 7 code \u0026lt;- substr(str_extract(file, pattern), 2, nchar(str_extract(file, pattern))) 8 # first column\u0026#39;s name is funny 9 names(data) \u0026lt;- c(\u0026#34;Date\u0026#34;,\u0026#34;Open\u0026#34;,\u0026#34;High\u0026#34;,\u0026#34;Low\u0026#34;,\u0026#34;Close\u0026#34;,\u0026#34;Volume\u0026#34;) 10 data$Date \u0026lt;- dmy(data$Date) 11 data$Open \u0026lt;- as.numeric(data$Open) 12 data$High \u0026lt;- as.numeric(data$High) 13 data$Low \u0026lt;- as.numeric(data$Low) 14 data$Close \u0026lt;- as.numeric(data$Close) 15 data$Volume \u0026lt;- as.integer(data$Volume) 16 data$Code \u0026lt;- code 17 data 18}, .progress = \u0026#34;text\u0026#34;) 19 20data \u0026lt;- rbind_all(dataList) Some of the values are shown below.\nDate Open High Low Close Volume Code 2014-11-26 47.49 47.99 47.28 47.75 27164877 MSFT 2014-11-25 47.66 47.97 47.45 47.47 28007993 MSFT 2014-11-24 47.99 48.00 47.39 47.59 35434245 MSFT 2014-11-21 49.02 49.05 47.57 47.98 42884795 MSFT 2014-11-20 48.00 48.70 47.87 48.70 21510587 MSFT 2014-11-19 48.66 48.75 47.93 48.22 26177450 MSFT This way wouldn\u0026rsquo;t be efficient compared to the way where files are read directly without being saved into a local drive. This option may be useful, however, if files are large and the API server breaks connection abrubtly.\nI hope this article is useful and I\u0026rsquo;m going to write an article to show the second way.\n","date":"November 20, 2014","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/blog/2014-11-20-download-stock-data-1/","series":[],"smallImg":"","tags":[{"title":"R","url":"/tags/r/"}],"timestamp":1416441600,"title":"Download Stock Data - Part I"},{"categories":[],"content":" ","date":"January 1, 1","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/categories/_index.zh-cn/","series":[],"smallImg":"","tags":[],"timestamp":-62135596800,"title":""},{"categories":[],"content":" ","date":"January 1, 1","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/news/1/01/","series":[],"smallImg":"","tags":[],"timestamp":-62135596800,"title":""},{"categories":[],"content":" ","date":"January 1, 1","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/series/_index.zh-cn/","series":[],"smallImg":"","tags":[],"timestamp":-62135596800,"title":""},{"categories":[],"content":" ","date":"January 1, 1","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/tags/_index.zh-cn/","series":[],"smallImg":"","tags":[],"timestamp":-62135596800,"title":""},{"categories":[],"content":"","date":"January 1, 1","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/offline/","series":[],"smallImg":"","tags":[],"timestamp":-62135596800,"title":"Offline"},{"categories":[],"content":"","date":"January 1, 1","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/search/_index.zh-cn/","series":[],"smallImg":"","tags":[],"timestamp":-62135596800,"title":"搜索"}]
