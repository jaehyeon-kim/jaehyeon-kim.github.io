---
title: Data Build Tool (dbt) Pizza Shop Demo - Part 1 Modelling on PostgreSQL
date: 2024-01-18
draft: false
featured: true
comment: true
toc: true
reward: false
pinned: false
carousel: false
featuredImage: false
series:
  - DBT Pizza Shop Demo
categories:
  - Data Engineering
tags: 
  - Data Build Tool (DBT)
  - PostgreSQL
  - Docker
  - Docker Compose
  - Python
authors:
  - JaehyeonKim
images: []
description: The data build tool (dbt) is a popular data transformation tool for data warehouse development. Moreover, it can be used for data lakehouse development thanks to open table formats such as Apache Iceberg, Apache Hudi and Delta Lake. dbt supports key AWS analytics services and I wrote a series of posts that discuss how to utilise dbt with Redshift, Glue, EMR on EC2, EMR on EKS, and Athena. Those posts focus on platform integration, however, they do not show realistic ETL scenarios. In this series of posts, we discuss practical data warehouse/lakehouse examples including ETL orchestration with Apache Airflow. As a starting point, we develop a dbt project on PostgreSQL using fictional pizza shop data in this post.
---

The [data build tool (dbt)](https://docs.getdbt.com/docs/introduction) is a popular data transformation tool for data warehouse development. Moreover, it can be used for [data lakehouse](https://www.databricks.com/glossary/data-lakehouse) development thanks to open table formats such as Apache Iceberg, Apache Hudi and Delta Lake. *dbt* supports key AWS analytics services and I wrote a series of posts that discuss how to utilise *dbt* with [Redshift](/blog/2022-09-28-dbt-on-aws-part-1-redshift), [Glue](/blog/2022-10-09-dbt-on-aws-part-2-glue), [EMR on EC2](/blog/2022-10-19-dbt-on-aws-part-3-emr-ec2), [EMR on EKS](/blog/2022-11-01-dbt-on-aws-part-4-emr-eks), and [Athena](/blog/2023-04-12-integrate-glue-schema-registry). Those posts focus on platform integration, however, they do not show realistic ETL scenarios. 

In this series of posts, we discuss practical data warehouse/lakehouse examples including ETL orchestration with Apache Airflow. As a starting point, we develop a *dbt* project on PostgreSQL using fictional pizza shop data in this post.


* [Part 1 Modelling on PostgreSQL](#) (this post)
* [Part 2 ETL on PostgreSQL via Airflow](/blog/2024-01-25-dbt-pizza-shop-2)
* [Part 3 Modelling on BigQuery](/blog/2024-02-08-dbt-pizza-shop-3)
* [Part 4 ETL on BigQuery via Airflow](/blog/2024-02-22-dbt-pizza-shop-4)
* Part 5 Modelling on Amazon Athena
* Part 6 ETL on Amazon Athena via Airflow

## Setup Database

PostgreSQL is used in the post, and it is deployed locally using Docker Compose. The source can be found in the [**GitHub repository**](https://github.com/jaehyeon-kim/general-demos/tree/master/dbt-postgres-demo) of this post.

### Prepare Data

Fictional pizza shop data from [Building Real-Time Analytics Systems](https://www.oreilly.com/library/view/building-real-time-analytics/9781098138783/) is used in this post. There are three data sets - *products*, *users* and *orders*. The first two data sets are copied from the book's [GitHub repository](https://github.com/mneedham/real-time-analytics-book/tree/main/mysql/data) and saved into *initdb/data* folder. The last order data set is generated by the following Python script.

```python
# initdb/generate_orders.py
import os
import csv
import dataclasses
import random
import json


@dataclasses.dataclass
class Order:
    user_id: int
    items: str

    @staticmethod
    def create():
        order_items = [
            {"product_id": id, "quantity": random.randint(1, 5)}
            for id in set(random.choices(range(1, 82), k=random.randint(1, 10)))
        ]
        return Order(
            user_id=random.randint(1, 10000),
            items=json.dumps([item for item in order_items]),
        )


if __name__ == "__main__":
    """
    Generate random orders given by the NUM_ORDERS environment variable.
        - orders.csv will be written to ./data folder
    
    Example:
        python generate_orders.py
        NUM_ORDERS=10000 python generate_orders.py
    """
    NUM_ORDERS = int(os.getenv("NUM_ORDERS", "20000"))
    CURRENT_DIR = os.path.dirname(os.path.realpath(__file__))
    orders = [Order.create() for _ in range(NUM_ORDERS)]

    filepath = os.path.join(CURRENT_DIR, "data", "orders.csv")
    if os.path.exists(filepath):
        os.remove(filepath)

    with open(os.path.join(CURRENT_DIR, "data", "orders.csv"), "w") as f:
        writer = csv.writer(f)
        writer.writerow(["user_id", "items"])
        for order in orders:
            writer.writerow(dataclasses.asdict(order).values())
```

Below shows sample order records generated by the script. It includes user ID and order items. As discussed further later, the items will be kept as the *JSONB* type in the staging table and transposed into rows in the fact table.

```txt
user_id,items
6845,"[{""product_id"": 52, ""quantity"": 4}, {""product_id"": 68, ""quantity"": 5}]"
6164,"[{""product_id"": 77, ""quantity"": 4}]"
9303,"[{""product_id"": 5, ""quantity"": 2}, {""product_id"": 71, ""quantity"": 3}, {""product_id"": 74, ""quantity"": 2}, {""product_id"": 10, ""quantity"": 5}, {""product_id"": 12, ""quantity"": 2}]"
```

The folder structure of source data sets and order generation script can be found below.

```bash
$ tree initdb/ -P  "*.csv|generate*" -I "scripts"
initdb/
├── data
│   ├── orders.csv
│   ├── products.csv
│   └── users.csv
└── generate_orders.py
```

### Run Database

A PostgreSQL server is deployed using Docker Compose. Also, the compose file creates a database (*devdb*) and user (*devuser*) by specifying corresponding environment variables (*POSTGRES*_*). Moreover, the database bootstrap script (*bootstrap.sql*) is volume-mapped into the docker entry point directory, and it creates necessary schemas and tables followed by loading initial records into the staging tables at startup. See below for details about the bootstrap script.

```yaml
# compose-postgres.yml
version: "3"

services:
  postgres:
    image: postgres:13
    container_name: postgres
    ports:
      - 5432:5432
    volumes:
      - ./initdb/scripts:/docker-entrypoint-initdb.d
      - ./initdb/data:/tmp
      - postgres_data:/var/lib/postgresql/data
    environment:
      - POSTGRES_DB=devdb
      - POSTGRES_USER=devuser
      - POSTGRES_PASSWORD=password
      - TZ=Australia/Sydney

volumes:
  postgres_data:
    driver: local
    name: postgres_data
```

#### Database Bootstrap Script

The bootstrap script begins with creating schemas and granting permission to the development user. Then the tables for the pizza shop data are created in the *staging* schema, and initial records are copied from data files into corresponding tables.

```sql
-- initdb/scripts/bootstrap.sql

-- // create schemas and grant permission
CREATE SCHEMA staging;
GRANT ALL ON SCHEMA staging TO devuser;

CREATE SCHEMA dev;
GRANT ALL ON SCHEMA dev TO devuser;

-- // create tables
DROP TABLE IF EXISTS staging.users;
DROP TABLE IF EXISTS staging.products;
DROP TABLE IF EXISTS staging.orders;

CREATE TABLE staging.users
(
    id SERIAL,
    first_name VARCHAR(255),
    last_name VARCHAR(255),
    email VARCHAR(255),
    residence VARCHAR(500),
    lat DECIMAL(10, 8),
    lon DECIMAL(10, 8),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE IF NOT EXISTS staging.products
(
    id SERIAL,
    name VARCHAR(100),
    description VARCHAR(500),
    price FLOAT,
    category VARCHAR(100),
    image VARCHAR(200),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE IF NOT EXISTS staging.orders
(
    id SERIAL,
    user_id INT,
    items JSONB,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- // copy data
COPY staging.users(first_name, last_name, email, residence, lat, lon)
FROM '/tmp/users.csv' DELIMITER ',' CSV HEADER;

COPY staging.products(name, description, price, category, image)
FROM '/tmp/products.csv' DELIMITER ',' CSV HEADER;

COPY staging.orders(user_id, items)
FROM '/tmp/orders.csv' DELIMITER ',' CSV HEADER;
```

## Setup DBT Project

A *dbt* project named *pizza_shop* is created using the *dbt-postgres* package (*dbt-postgres==1.7.4*). Specifically, it is created using the `dbt init` command, and it bootstraps the project in the *pizza_shop* folder as well as adds the project profile to the *dbt* profiles file as shown below.

```yaml
# $HOME/.dbt/profiles.yml
pizza_shop:
  outputs:
    dev:
      dbname: devdb
      host: localhost
      pass: password
      port: 5432
      schema: dev
      threads: 4
      type: postgres
      user: devuser
  target: dev
```

### Project Sources

Recall that three tables are created in the *staging* schema by the database bootstrap script. They are used as sources of the project and their details are kept in *sources.yml* to be referred easily in other models.

```yaml
# pizza_shop/models/sources.yml
version: 2

sources:
  - name: raw
    schema: staging
    tables:
      - name: users
        identifier: users
      - name: products
        identifier: products
      - name: orders
        identifier: orders
```

Using the raw sources, three models are created by performing simple transformations such as adding [surrogate keys](https://docs.getdbt.com/terms/surrogate-key) using the *dbt_utils* package and changing column names. Note that, as the *products* and *users* dimension tables are kept by [Type 2 slowly changing dimension (SCD type 2)](https://en.wikipedia.org/wiki/Slowly_changing_dimension), the surrogate keys are used to uniquely identify relevant dimension records.

```sql
-- pizza_shop/models/src/src_products.sql
WITH raw_products AS (
  SELECT * FROM {{ source('raw', 'products') }}
)
SELECT
  {{ dbt_utils.generate_surrogate_key(['name', 'description', 'price', 'category', 'image']) }} as product_key,
  id AS product_id,
  name,
  description,
  price,
  category,
  image,
  created_at
FROM raw_products
```

```sql
-- pizza_shop/models/src/src_users.sql
WITH raw_users AS (
  SELECT * FROM {{ source('raw', 'users') }}
)
SELECT
  {{ dbt_utils.generate_surrogate_key(['first_name', 'last_name', 'email', 'residence', 'lat', 'lon']) }} as user_key,
  id AS user_id,
  first_name,
  last_name,
  email,
  residence,
  lat AS latitude,
  lon AS longitude,
  created_at
FROM raw_users
```

```sql
-- pizza_shop/models/src/src_orders.sql
WITH raw_orders AS (
  SELECT * FROM {{ source('raw', 'orders') }}
)
SELECT
  id AS order_id,
  user_id,
  items,
  created_at
FROM raw_orders
```

### Data Modelling

For SCD type 2, the dimension tables are materialized as *table* and two additional columns are included - *valid_from* and *valid_to*. The extra columns are for setting up a time range where a record is applicable, and they are used to map a relevant surrogate key in the fact table when there are multiple dimension records according to the same natural key. Note that SCD type 2 tables can also be maintained by [*dbt* snapshots](https://docs.getdbt.com/docs/build/snapshots).

```sql
-- pizza_shop/models/dim/dim_products.sql
{{
  config(
    materialized = 'table',
    )
}}
WITH src_products AS (
  SELECT * FROM {{ ref('src_products') }}
)
SELECT
    *, 
    created_at AS valid_from,
    COALESCE(
      LEAD(created_at, 1) OVER (PARTITION BY product_id ORDER BY created_at), 
      '2199-12-31'::TIMESTAMP
    ) AS valid_to
FROM src_products
```

```sql
-- pizza_shop/models/dim/dim_users.sql
{{
  config(
    materialized = 'table',
    )
}}
WITH src_users AS (
  SELECT * FROM {{ ref('src_users') }}
)
SELECT
    *, 
    created_at AS valid_from,
    COALESCE(
      LEAD(created_at, 1) OVER (PARTITION BY user_id ORDER BY created_at), 
      '2199-12-31'::TIMESTAMP
    ) AS valid_to
FROM src_users
```

The transactional fact table is materialized as *incremental* so that only new records are appended. Also, order items are transposed into rows using the *jsonb_array_elements* function. Finally, the records are joined with the dimension tables to add relevant surrogate keys from them. Note that the surrogate key of the fact table is constructed by a combination of all natural keys.

```sql
-- pizza_shop/models/fct/fct_orders.sql
{{
  config(
    materialized = 'incremental'
    )
}}
WITH dim_products AS (
  SELECT * FROM {{ ref('dim_products') }}
), dim_users AS (
  SELECT * FROM {{ ref('dim_users') }}
), src_orders AS (
  SELECT 
    order_id,
    user_id,
    jsonb_array_elements(items) AS order_item,
    created_at    
  FROM {{ ref('src_orders') }}
), expanded_orders AS (
  SELECT 
    order_id,
    user_id,
    (order_item ->> 'product_id')::INT AS product_id,
    (order_item ->> 'quantity')::INT AS quantity,    
    created_at
  FROM src_orders
)
SELECT
  {{ dbt_utils.generate_surrogate_key(['order_id', 'p.product_id', 'u.user_id']) }} as order_key,
  p.product_key,
  u.user_key,
  o.order_id,
  o.user_id,
  o.product_id,
  o.quantity,
  o.created_at
FROM expanded_orders o
JOIN dim_products p 
  ON o.product_id = p.product_id
    AND o.created_at >= p.valid_from
    AND o.created_at < p.valid_to
JOIN dim_users u 
  ON o.user_id = u.user_id
    AND o.created_at >= u.valid_from
    AND o.created_at < u.valid_to
{% if is_incremental() %}
  WHERE o.created_at > (SELECT created_at from {{ this }} ORDER BY created_at DESC LIMIT 1)
{% endif %}
```

We can keep the final models in a separate YAML file for testing and enhanced documentation.

```yaml
# pizza_shop/models/schema.yml
version: 2

models:
  - name: dim_products
    description: Products table, which is converted into SCD type 2
    columns:
      - name: product_key
        description: |
          Primary key of the table
          Surrogate key, which is generated by md5 hash using the following columns
            - name, description, price, category, image
        tests:
          - not_null
          - unique
      - name: product_id
        description: Natural key of products
      - name: name
        description: Porduct name
      - name: description
        description: Product description
      - name: price
        description: Product price
      - name: category
        description: Product category
      - name: image
        description: Product image
      - name: created_at
        description: Timestamp when the record is loaded
      - name: valid_from
        description: Effective start timestamp of the corresponding record (inclusive)
      - name: valid_to
        description: Effective end timestamp of the corresponding record (exclusive)
  - name: dim_users
    description: Users table, which is converted into SCD type 2
    columns:
      - name: user_key
        description: |
          Primary key of the table
          Surrogate key, which is generated by md5 hash using the following columns
            - first_name, last_name, email, residence, lat, lon
        tests:
          - not_null
          - unique
      - name: user_id
        description: Natural key of users
      - name: first_name
        description: First name
      - name: last_name
        description: Last name
      - name: email
        description: Email address
      - name: residence
        description: User address
      - name: latitude
        description: Latitude of user address
      - name: longitude
        description: Longitude of user address
      - name: created_at
        description: Timestamp when the record is loaded
      - name: valid_from
        description: Effective start timestamp of the corresponding record (inclusive)
      - name: valid_to
        description: Effective end timestamp of the corresponding record (exclusive)
  - name: fct_orders
    description: Orders fact table. Order items are exploded into rows
    columns:
      - name: order_key
        description: |
          Primary key of the table
          Surrogate key, which is generated by md5 hash using the following columns
            - order_id, product_id, user_id
        tests:
          - not_null
          - unique
      - name: product_key
        description: Product surrogate key which matches the product dimension record
        tests:
          - not_null
          - relationships:
              to: ref('dim_products')
              field: product_key
      - name: user_key
        description: User surrogate key which matches the user dimension record
        tests:
          - not_null
          - relationships:
              to: ref('dim_users')
              field: user_key
      - name: order_id
        description: Natural key of orders
      - name: user_id
        description: Natural key of users
      - name: product_id
        description: Natural key of products
      - name: quantity
        description: Amount of products ordered
      - name: created_at
        description: Timestamp when the record is loaded
```

The project can be executed using the `dbt run` command as shown below.

```bash
$ dbt run
04:39:32  Running with dbt=1.7.4
04:39:33  Registered adapter: postgres=1.7.4
04:39:33  Found 6 models, 10 tests, 3 sources, 0 exposures, 0 metrics, 515 macros, 0 groups, 0 semantic models
04:39:33  
04:39:33  Concurrency: 4 threads (target='dev')
04:39:33  
04:39:33  1 of 6 START sql view model dev.src_orders ..................................... [RUN]
04:39:33  2 of 6 START sql view model dev.src_products ................................... [RUN]
04:39:33  3 of 6 START sql view model dev.src_users ...................................... [RUN]
04:39:33  2 of 6 OK created sql view model dev.src_products .............................. [CREATE VIEW in 0.14s]
04:39:33  1 of 6 OK created sql view model dev.src_orders ................................ [CREATE VIEW in 0.15s]
04:39:33  3 of 6 OK created sql view model dev.src_users ................................. [CREATE VIEW in 0.14s]
04:39:33  4 of 6 START sql table model dev.dim_products .................................. [RUN]
04:39:33  5 of 6 START sql table model dev.dim_users ..................................... [RUN]
04:39:33  5 of 6 OK created sql table model dev.dim_users ................................ [SELECT 10000 in 0.12s]
04:39:33  4 of 6 OK created sql table model dev.dim_products ............................. [SELECT 81 in 0.13s]
04:39:33  6 of 6 START sql incremental model dev.fct_orders .............................. [RUN]
04:39:33  6 of 6 OK created sql incremental model dev.fct_orders ......................... [SELECT 105789 in 0.33s]
04:39:33  
04:39:33  Finished running 3 view models, 2 table models, 1 incremental model in 0 hours 0 minutes and 0.70 seconds (0.70s).
04:39:33  
04:39:33  Completed successfully
04:39:33  
04:39:33  Done. PASS=6 WARN=0 ERROR=0 SKIP=0 TOTAL=6
```

Also, the project can be tested using the `dbt test` command.

```bash
$ dbt test
04:40:53  Running with dbt=1.7.4
04:40:53  Registered adapter: postgres=1.7.4
04:40:53  Found 6 models, 10 tests, 3 sources, 0 exposures, 0 metrics, 515 macros, 0 groups, 0 semantic models
04:40:53  
04:40:53  Concurrency: 4 threads (target='dev')
04:40:53  
04:40:53  1 of 10 START test not_null_dim_products_product_key ........................... [RUN]
04:40:53  2 of 10 START test not_null_dim_users_user_key ................................. [RUN]
04:40:53  3 of 10 START test not_null_fct_orders_order_key ............................... [RUN]
04:40:53  4 of 10 START test not_null_fct_orders_product_key ............................. [RUN]
04:40:53  1 of 10 PASS not_null_dim_products_product_key ................................. [PASS in 0.11s]
04:40:53  5 of 10 START test not_null_fct_orders_user_key ................................ [RUN]
04:40:53  2 of 10 PASS not_null_dim_users_user_key ....................................... [PASS in 0.11s]
04:40:53  4 of 10 PASS not_null_fct_orders_product_key ................................... [PASS in 0.11s]
04:40:53  3 of 10 PASS not_null_fct_orders_order_key ..................................... [PASS in 0.12s]
04:40:53  6 of 10 START test relationships_fct_orders_product_key__product_key__ref_dim_products_  [RUN]
04:40:53  7 of 10 START test relationships_fct_orders_user_key__user_key__ref_dim_users_ . [RUN]
04:40:53  8 of 10 START test unique_dim_products_product_key ............................. [RUN]
04:40:53  5 of 10 PASS not_null_fct_orders_user_key ...................................... [PASS in 0.09s]
04:40:53  9 of 10 START test unique_dim_users_user_key ................................... [RUN]
04:40:53  8 of 10 PASS unique_dim_products_product_key ................................... [PASS in 0.06s]
04:40:53  10 of 10 START test unique_fct_orders_order_key ................................ [RUN]
04:40:53  6 of 10 PASS relationships_fct_orders_product_key__product_key__ref_dim_products_  [PASS in 0.10s]
04:40:53  7 of 10 PASS relationships_fct_orders_user_key__user_key__ref_dim_users_ ....... [PASS in 0.11s]
04:40:53  9 of 10 PASS unique_dim_users_user_key ......................................... [PASS in 0.06s]
04:40:53  10 of 10 PASS unique_fct_orders_order_key ...................................... [PASS in 0.11s]
04:40:53  
04:40:53  Finished running 10 tests in 0 hours 0 minutes and 0.42 seconds (0.42s).
04:40:53  
04:40:53  Completed successfully
04:40:53  
04:40:53  Done. PASS=10 WARN=0 ERROR=0 SKIP=0 TOTAL=10
```

## Update Records

Although we will discuss ETL orchestration with Apache Airflow in the next post, here I illustrate how the dimension and fact tables change when records are updated.

### Product

First, a new record is inserted into the *products* table in the *staging* schema, and the price is set to increase by 10.

```sql
-- // update a product record
INSERT INTO staging.products (id, name, description, price, category, image)
    SELECT 1, name, description, price + 10, category, image
    FROM staging.products
    WHERE id = 1;

SELECT id, name, price, category, created_at 
FROM staging.products 
WHERE id = 1;

id|name                            |price|category  |created_at         |
--+--------------------------------+-----+----------+-------------------+
 1|Moroccan Spice Pasta Pizza - Veg|335.0|veg pizzas|2024-01-14 15:38:30|
 1|Moroccan Spice Pasta Pizza - Veg|345.0|veg pizzas|2024-01-14 15:43:15|
```

When we execute the `dbt run` command again, we see the corresponding dimension table reflects the change by adding a new record and updating *valid_from* and *valid_to* columns accordingly. With this change, any later order record that has this product should be mapped into the new product surrogate key.

```sql
SELECT product_key, price, created_at, valid_from, valid_to 
FROM dev.dim_products 
WHERE product_id = 1;

product_key                     |price|created_at         |valid_from         |valid_to           |
--------------------------------+-----+-------------------+-------------------+-------------------+
a8c5f8c082bcf52a164f2eccf2b493f6|335.0|2024-01-14 15:38:30|2024-01-14 15:38:30|2024-01-14 15:43:15|
c995d7e1ec035da116c0f37e6284d1d5|345.0|2024-01-14 15:43:15|2024-01-14 15:43:15|2199-12-31 00:00:00|
```

### User

Also, a new record is inserted into the *users* table in the *staging* schema while modifying the email address.

```sql
-- // update a user record
INSERT INTO staging.users (id, first_name, last_name, email, residence, lat, lon)
    SELECT 1, first_name, last_name, 'john.doe@example.com', residence, lat, lon
    FROM staging.users
    WHERE id = 1;

SELECT id, first_name, last_name, email, created_at 
FROM staging.users 
WHERE id = 1;

id|first_name|last_name|email                     |created_at         |
--+----------+---------+--------------------------+-------------------+
 1|Kismat    |Shroff   |drishyamallick@hotmail.com|2024-01-14 15:38:30|
 1|Kismat    |Shroff   |john.doe@example.com      |2024-01-14 15:43:35|
```

Again the corresponding dimension table reflects the change by adding a new record and updating *valid_from* and *valid_to* columns accordingly.

```sql
SELECT user_key, email, valid_from, valid_to 
FROM dev.dim_users 
WHERE user_id = 1;

user_key                        |email                     |valid_from         |valid_to           |
--------------------------------+--------------------------+-------------------+-------------------+
7f530277c15881c328b67c4764205a9c|drishyamallick@hotmail.com|2024-01-14 15:38:30|2024-01-14 15:43:35|
f4b2344f893f50597cdc4a12c7e87e81|john.doe@example.com      |2024-01-14 15:43:35|2199-12-31 00:00:00|
```

### Order

We insert a new order record that has two items where the IDs of the first and second products are 1 and 2 respectively. In this example, we expect the first product maps to the updated product surrogate key while the surrogate key of the second product remains the same. We can check it by querying the new order record together with an existing record that has corresponding products. As expected, the query result shows the product surrogate key is updated only in the new order record.

```sql
INSERT INTO staging.orders(user_id, items)
VALUES (1,'[{"product_id": 1, "quantity": 2}, {"product_id": 2, "quantity": 3}]');

SELECT o.order_key, o.product_key, o.order_id, o.product_id, p.price, o.quantity, o.created_at 
FROM dev.fct_orders o
JOIN dev.dim_products p ON o.product_key = p.product_key
WHERE o.order_id IN (249, 20001)
ORDER BY o.order_id, o.product_id;

order_key                       |  product_key                     |order_id|product_id|price|quantity|created_at         |
--------------------------------+----------------------------------+--------+----------+-----+--------+-------------------+
5c8f7a8bc27d825efdb2e8167c9ae481|* a8c5f8c082bcf52a164f2eccf2b493f6|     249|         1|335.0|       1|2024-01-14 15:38:30|
42834bd2b8f847cbd182765b43094be0|  8dd51b3981692c787baa9d4335f15345|     249|         2| 60.0|       1|2024-01-14 15:38:30|
28d937c571bd9601c2e719e618e56f67|* c995d7e1ec035da116c0f37e6284d1d5|   20001|         1|345.0|       2|2024-01-14 15:51:04|
75e1957ce20f188ab7502c322e2ab7d9|  8dd51b3981692c787baa9d4335f15345|   20001|         2| 60.0|       3|2024-01-14 15:51:04|
```

## Summary

The data build tool (dbt) is a popular data transformation tool. In this series, we discuss practical examples of data warehouse and lakehouse development where data transformation is performed by the data build tool (dbt) and ETL is managed by Apache Airflow. As a starting point, we developed a dbt project on PostgreSQL using fictional pizza shop data in this post. Two SCD type 2 dimension tables and a single transaction tables are modelled on a dbt project and impacts of record updates are discussed in detail.