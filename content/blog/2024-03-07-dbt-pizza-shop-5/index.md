---
title: Data Build Tool (dbt) Pizza Shop Demo - Part 5 Modelling on Amazon Athena
date: 2024-03-07
draft: false
featured: true
comment: true
toc: true
reward: false
pinned: false
carousel: false
featuredImage: false
series:
  - DBT Pizza Shop Demo
categories:
  - Data Engineering
tags: 
  - Data Build Tool (DBT)
  - AWS
  - Amazon Athena
  - Apache Iceberg
  - Python
authors:
  - JaehyeonKim
images: []
description: In Part 1 and Part 3, we developed data build tool (dbt) projects that target PostgreSQL and BigQuery using fictional pizza shop data. The data is modelled by SCD type 2 dimension tables and one transactional fact table. While the order records should be joined with dimension tables to get complete details for PostgreSQL, the fact table is denormalized using nested and repeated fields to improve query performance for BigQuery. Open Table Formats such as Apache Iceberg bring a new opportunity that implements data warehousing features in a data lake (i.e. data lakehouse) and Amazon Athena is probably the easiest way to perform such tasks on AWS. In this post, we create a new dbt project that targets Apache Iceberg where transformations are performed on Amazon Athena. Data modelling is similar to the BigQuery project where the dimension tables are modelled by the SCD type 2 approach and the fact table is denormalized using the array and struct data types.
---

In [Part 1]((/blog/2024-01-18-dbt-pizza-shop-1)) and [Part 3]((/blog/2024-02-08-dbt-pizza-shop-3)), we developed [data build tool (dbt)](https://docs.getdbt.com/docs/introduction) projects that target *PostgreSQL* and *BigQuery* using fictional pizza shop data. The data is modelled by [SCD type 2](https://en.wikipedia.org/wiki/Slowly_changing_dimension) dimension tables and one transactional fact table. While the order records should be joined with dimension tables to get complete details for *PostgreSQL*, the fact table is denormalized using [nested and repeated fields](https://cloud.google.com/bigquery/docs/best-practices-performance-nested) to improve query performance for *BigQuery*. 

Open Table Formats such as [Apache Iceberg](https://iceberg.apache.org/) bring a new opportunity that implements data warehousing features in a data lake (i.e. data lakehouse) and [Amazon Athena](https://aws.amazon.com/athena/) is probably the easiest way to perform such tasks on AWS. In this post, we create a new *dbt* project that targets *Apache Iceberg* where transformations are performed on *Amazon Athena*. Data modelling is similar to the *BigQuery* project where the dimension tables are modelled by the *SCD type 2* approach and the fact table is denormalized using the *array* and *struct* data types. 

* [Part 1 Modelling on PostgreSQL](/blog/2024-01-18-dbt-pizza-shop-1)
* [Part 2 ETL on PostgreSQL via Airflow](/blog/2024-01-25-dbt-pizza-shop-2)
* [Part 3 Modelling on BigQuery](/blog/2024-02-08-dbt-pizza-shop-3)
* [Part 4 ETL on BigQuery via Airflow](/blog/2024-02-22-dbt-pizza-shop-4)
* [Part 5 Modelling on Amazon Athena](#) (this post)
* Part 6 ETL on Amazon Athena via Airflow

## Setup Amazon Athena

*Amazon Athena* is used in the post, and the source can be found in the [**GitHub repository**](https://github.com/jaehyeon-kim/general-demos/tree/master/dbt-athena-demo) of this post.

### Prepare Data

Fictional pizza shop data from [Building Real-Time Analytics Systems](https://www.oreilly.com/library/view/building-real-time-analytics/9781098138783/) is used in this post. There are three data sets - *products*, *users* and *orders*. The first two data sets are copied from the book's [GitHub repository](https://github.com/mneedham/real-time-analytics-book/tree/main/mysql/data) and saved into the *setup/data* folder. The last order data set is generated by the following Python script.

```python
# setup/generate_orders.py
import os
import csv
import dataclasses
import random
import json


@dataclasses.dataclass
class Order:
    user_id: int
    items: str

    @classmethod
    def create(self):
        order_items = [
            {"product_id": id, "quantity": random.randint(1, 5)}
            for id in set(random.choices(range(1, 82), k=random.randint(1, 10)))
        ]
        return Order(
            user_id=random.randint(1, 10000),
            items=json.dumps([item for item in order_items]),
        )


if __name__ == "__main__":
    """
    Generate random orders given by the NUM_ORDERS environment variable.
        - orders.csv will be written to ./data folder
    
    Example:
        python generate_orders.py
        NUM_ORDERS=10000 python generate_orders.py
    """
    NUM_ORDERS = int(os.getenv("NUM_ORDERS", "20000"))
    CURRENT_DIR = os.path.dirname(os.path.realpath(__file__))
    orders = [Order.create() for _ in range(NUM_ORDERS)]

    filepath = os.path.join(CURRENT_DIR, "data", "orders.csv")
    if os.path.exists(filepath):
        os.remove(filepath)

    with open(os.path.join(CURRENT_DIR, "data", "orders.csv"), "w") as f:
        writer = csv.writer(f)
        writer.writerow(["user_id", "items"])
        for order in orders:
            writer.writerow(dataclasses.asdict(order).values())
```

Below shows sample order records generated by the script. It includes user ID and order items.

```txt
user_id,items
2911,"[{""product_id"": 57, ""quantity"": 3}, {""product_id"": 66, ""quantity"": 4}, {""product_id"": 27, ""quantity"": 4}]"
4483,"[{""product_id"": 13, ""quantity"": 3}]"
894,"[{""product_id"": 59, ""quantity"": 1}, {""product_id"": 22, ""quantity"": 5}]"
```

The folder structure of source data sets and order generation script can be found below.

```bash
$ tree setup/ -P  "*.csv|generate*"
setup/
├── data
│   ├── orders.csv
│   ├── products.csv
│   └── users.csv
└── generate_orders.py
```

### Insert Source Data

The source data is inserted using the [AWS SDK for pandas (awswrangler)](https://aws-sdk-pandas.readthedocs.io/en/stable/) package. It is a simple process that reads records from the source data files as Pandas DataFrame, adds incremental ID/creation datetime and inserts to S3. Upon successful insertion, we can check the source records are saved into S3 and their table metadata is registered in a Glue database named *pizza_shop*.

```python
# setup/insert_records.py
import os
import datetime

import boto3
import botocore
import pandas as pd
import awswrangler as wr


class QueryHelper:
    def __init__(self, db_name: str, bucket_name: str, current_dir: str = None):
        self.db_name = db_name
        self.bucket_name = bucket_name
        self.current_dir = current_dir or os.path.dirname(os.path.realpath(__file__))
        self.glue_client = boto3.client(
            "glue", region_name=os.getenv("AWS_REGION", "ap-southeast-2")
        )

    def check_db(self):
        try:
            self.glue_client.get_database(Name=self.db_name)
            return True
        except botocore.exceptions.ClientError as err:
            if err.response["Error"]["Code"] == "EntityNotFoundException":
                return False
            else:
                raise err

    def create_db(self):
        try:
            self.glue_client.create_database(
                DatabaseInput={
                    "Name": self.db_name,
                }
            )
            return True
        except botocore.exceptions.ClientError as err:
            if err.response["Error"]["Code"] == "AlreadyExistsException":
                return True
            else:
                raise err

    def read_source(self, file_name: str):
        df = pd.read_csv(os.path.join(self.current_dir, "data", file_name))
        df.insert(0, "id", range(1, len(df) + 1))
        df.insert(
            df.shape[1],
            "created_at",
            datetime.datetime.now(),
        )
        return df

    def load_source(self, df: pd.DataFrame, obj_name: str):
        if obj_name not in ["users", "products", "orders"]:
            raise ValueError("object name should be one of users, products, orders")
        wr.s3.to_parquet(
            df=df,
            path=f"s3://{self.bucket_name}/staging/{obj_name}/",
            dataset=True,
            database=self.db_name,
            table=f"staging_{obj_name}",
            boto3_session=boto3.Session(
                region_name=os.getenv("AWS_REGION", "ap-southeast-2")
            ),
        )


if __name__ == "__main__":
    query_helper = QueryHelper(db_name="pizza_shop", bucket_name="dbt-pizza-shop-demo")
    if not query_helper.check_db():
        query_helper.create_db()
    print("inserting products...")
    products = query_helper.read_source("products.csv")
    query_helper.load_source(products, "products")
    print("inserting users...")
    users = query_helper.read_source("users.csv")
    query_helper.load_source(users, "users")
    print("inserting orders...")
    orders = query_helper.read_source("orders.csv")
    query_helper.load_source(orders, "orders")
```

## Setup DBT Project

A *dbt* project named *pizza_shop* is created using the *dbt-athena-community* package (*dbt-athena-community==1.7.1*). Specifically, it is created using the `dbt init` command, and it bootstraps the project in the *pizza_shop* folder as well as adds the project profile to the *dbt* profiles file. See [this page](https://dbt-athena.github.io/docs/getting-started/installation) for details about how to set up *Amazon Athena* for a *dbt* project.

```yaml
# $HOME/.dbt/profiles.yml
pizza_shop:
  outputs:
    dev:
      type: athena
      database: awsdatacatalog
      schema: pizza_shop
      region_name: ap-southeast-2
      s3_data_dir: s3://dbt-pizza-shop-demo/dbt-data/
      s3_staging_dir: s3://dbt-pizza-shop-demo/dbt-staging/
      threads: 4
  target: dev
```

### Project Sources

Recall that three staging tables are created earlier, and they are used as sources of the project. Their details are kept in *sources.yml* to be referred easily in other models.

```yaml
# pizza_shop/models/sources.yml
version: 2

sources:
  - name: raw
    schema: pizza_shop
    tables:
      - name: users
        identifier: staging_users
      - name: products
        identifier: staging_products
      - name: orders
        identifier: staging_orders
```

Using the raw sources, three models are created by performing simple transformations such as adding [surrogate keys](https://docs.getdbt.com/terms/surrogate-key) using the *dbt_utils* package and changing column names. Note that, as the *products* and *users* dimension tables are kept by [Type 2 slowly changing dimension (SCD type 2)](https://en.wikipedia.org/wiki/Slowly_changing_dimension), the surrogate keys are used to uniquely identify relevant dimension records.

```sql
-- pizza_shop/models/src/src_products.sql
WITH raw_products AS (
  SELECT * FROM {{ source('raw', 'products') }}
)
SELECT
  {{ dbt_utils.generate_surrogate_key(['name', 'description', 'price', 'category', 'image']) }} as product_key,
  id AS product_id,
  name,
  description,
  price,
  category,
  image,
  created_at
FROM raw_products
```

```sql
-- pizza_shop/models/src/src_users.sql
WITH raw_users AS (
  SELECT * FROM {{ source('raw', 'users') }}
)
SELECT
  {{ dbt_utils.generate_surrogate_key(['first_name', 'last_name', 'email', 'residence', 'lat', 'lon']) }} as user_key,
  id AS user_id,
  first_name,
  last_name,
  email,
  residence,
  lat AS latitude,
  lon AS longitude,
  created_at
FROM raw_users
```

```sql
-- pizza_shop/models/src/src_orders.sql
WITH raw_orders AS (
  SELECT * FROM {{ source('raw', 'orders') }}
)
SELECT
  id AS order_id,
  user_id,
  items,
  created_at
FROM raw_orders
```

### Data Modelling

The dimension tables are materialized as *table* where the table type is chosen as *iceberg*. Also, for SCD type 2, two additional columns are created - *valid_from* and *valid_to*. The extra columns are for setting up a time range where a record is applicable, and they are used to map a relevant record in the fact table when there are multiple dimension records, having the same natural key. Note that SCD type 2 tables can also be maintained by [*dbt* snapshots](https://dbt-athena.github.io/docs/configuration/snapshots).

```sql
-- pizza_shop/models/dim/dim_products.sql
{{
  config(
    materialized = 'table',
    table_type='iceberg',
    format='parquet',
    table_properties={
      'optimize_rewrite_delete_file_threshold': '2'
    })
}}
WITH src_products AS (
  SELECT
    product_key,
    product_id,
    name,
    description,
    price,
    category,
    image,
    CAST(created_at AS TIMESTAMP(6)) AS created_at
  FROM {{ ref('src_products') }}
)
SELECT
    *, 
    created_at AS valid_from,
    COALESCE(
      LEAD(created_at, 1) OVER (PARTITION BY product_id ORDER BY created_at), 
      CAST('2199-12-31' AS TIMESTAMP(6))
    ) AS valid_to
FROM src_products
```

```sql
-- pizza_shop/models/dim/dim_users.sql
{{
  config(
    materialized = 'table',
    table_type='iceberg',
    format='parquet',
    table_properties={
      'optimize_rewrite_delete_file_threshold': '2'
    })
}}
WITH src_users AS (
  SELECT
    user_key,
    user_id,
    first_name,
    last_name,
    email,
    residence,
    latitude,
    longitude,
    CAST(created_at AS TIMESTAMP(6)) AS created_at
  FROM {{ ref('src_users') }}
)
SELECT
    *, 
    created_at AS valid_from,
    COALESCE(
      LEAD(created_at, 1) OVER (PARTITION BY user_id ORDER BY created_at), 
      CAST('2199-12-31' AS TIMESTAMP(6))
    ) AS valid_to
FROM src_users
```

When it comes to the transactional fact table, its materialization and incremental strategy are chosen to be *incremental* and *append* respectively as only new records need to be added to it. Also, it is created as a partitioned table for improving query performance by applying the [partition transform](https://iceberg.apache.org/spec/#partition-transforms) of *day* on the *created_at* column. Finally, the user and order items records are pre-joined from the relevant dimension tables. As can be seen later, the order items (*product*) and *user* fields are converted into an array of struct and struct data types.

```sql
-- pizza_shop/models/fct/fct_orders.sql
{{
  config(
    materialized = 'incremental',
    table_type='iceberg',
    format='parquet',
    partitioned_by=['day(created_at)'],
    incremental_strategy='append',
    unique_key='order_id',
    table_properties={
      'optimize_rewrite_delete_file_threshold': '2'
    })
}}
WITH dim_products AS (
  SELECT * FROM {{ ref('dim_products') }}
), dim_users AS (
  SELECT * FROM {{ ref('dim_users') }}
), expanded_orders AS (
  SELECT 
    o.order_id,
    o.user_id,
    row.product_id,
    row.quantity,
    CAST(created_at AS TIMESTAMP(6)) AS created_at
  FROM {{ ref('src_orders') }} AS o
  CROSS JOIN UNNEST(CAST(JSON_PARSE(o.items) as ARRAY(ROW(product_id INTEGER, quantity INTEGER)))) as t(row)
)
SELECT
  o.order_id,
  ARRAY_AGG(
    CAST(
      ROW(p.product_key, o.product_id, p.name, p.price, o.quantity, p.description, p.category, p.image) 
        AS ROW(key VARCHAR, id BIGINT, name VARCHAR, price DOUBLE, quantity INTEGER, description VARCHAR, category VARCHAR, image VARCHAR))
  ) AS product,
  CAST(
    ROW(u.user_key, o.user_id, u.first_name, u.last_name, u.email, u.residence, u.latitude, u.longitude) 
      AS ROW(key VARCHAR, id BIGINT, first_name VARCHAR, last_name VARCHAR, email VARCHAR, residence VARCHAR, latitude DOUBLE, longitude DOUBLE)) AS user,
  o.created_at
FROM expanded_orders o
JOIN dim_products p 
  ON o.product_id = p.product_id
    AND o.created_at >= p.valid_from
    AND o.created_at < p.valid_to
JOIN dim_users u 
  ON o.user_id = u.user_id
    AND o.created_at >= u.valid_from
    AND o.created_at < u.valid_to
{% if is_incremental() %}
  WHERE o.created_at > (SELECT max(created_at) from {{ this }})
{% endif %}
GROUP BY 
  o.order_id, 
  u.user_key, 
  o.user_id, 
  u.first_name, 
  u.last_name, 
  u.email, 
  u.residence, 
  u.latitude, 
  u.longitude, 
  o.created_at
```

We can keep the final models in a separate YAML file for testing and enhanced documentation.

```yaml
# pizza_shop/models/schema.yml
version: 2

models:
  - name: dim_products
    description: Products table, which is converted into SCD Type 2
    columns:
      - name: product_key
        description: |
          Primary key of the table
          Surrogate key, which is generated by md5 hash using the following columns
            - name, description, price, category, image
        tests:
          - not_null
          - unique
      - name: product_id
        description: Natural key of products
      - name: name
        description: Porduct name
      - name: description
        description: Product description
      - name: price
        description: Product price
      - name: category
        description: Product category
      - name: image
        description: Product image
      - name: created_at
        description: Timestamp when the record is loaded
      - name: valid_from
        description: Effective start timestamp of the corresponding record (inclusive)
      - name: valid_to
        description: Effective end timestamp of the corresponding record (exclusive)
  - name: dim_users
    description: Users table, which is converted into SCD Type 2
    columns:
      - name: user_key
        description: |
          Primary key of the table
          Surrogate key, which is generated by md5 hash using the following columns
            - first_name, last_name, email, residence, lat, lon
        tests:
          - not_null
          - unique
      - name: user_id
        description: Natural key of users
      - name: first_name
        description: First name
      - name: last_name
        description: Last name
      - name: email
        description: Email address
      - name: residence
        description: User address
      - name: latitude
        description: Latitude of user address
      - name: longitude
        description: Longitude of user address
      - name: created_at
        description: Timestamp when the record is loaded
      - name: valid_from
        description: Effective start timestamp of the corresponding record (inclusive)
      - name: valid_to
        description: Effective end timestamp of the corresponding record (exclusive)
  - name: fct_orders
    description: Orders fact table. Order items are exploded into rows
    columns:
      - name: order_id
        description: Natural key of orders
      - name: product
        description: |
          Array of products in an order.
          A product is an array of struct where the following attributes are pre-joined from the dim_products table:
            key (product_key), id (product_id), name, price, quantity, description, category, and image
      - name: user
        description: |
          A struct where the following attributes are pre-joined from the dim_users table:
            key (user_key), id (user_id), first_name, last_name, email, residence, latitude, and longitude
      - name: created_at
        description: Timestamp when the record is loaded
```

The project can be executed using the `dbt run` command as shown below.

```bash
$ dbt run
08:55:00  Running with dbt=1.7.9
08:55:00  Registered adapter: athena=1.7.1
08:55:00  Found 6 models, 4 tests, 3 sources, 0 exposures, 0 metrics, 537 macros, 0 groups, 0 semantic models
08:55:00  
08:55:02  Concurrency: 4 threads (target='dev')
08:55:02  
08:55:02  1 of 6 START sql view model pizza_shop.src_orders .............................. [RUN]
08:55:02  2 of 6 START sql view model pizza_shop.src_products ............................ [RUN]
08:55:02  3 of 6 START sql view model pizza_shop.src_users ............................... [RUN]
08:55:05  1 of 6 OK created sql view model pizza_shop.src_orders ......................... [OK -1 in 3.28s]
08:55:05  3 of 6 OK created sql view model pizza_shop.src_users .......................... [OK -1 in 3.27s]
08:55:05  2 of 6 OK created sql view model pizza_shop.src_products ....................... [OK -1 in 3.28s]
08:55:05  4 of 6 START sql table model pizza_shop.dim_users .............................. [RUN]
08:55:05  5 of 6 START sql table model pizza_shop.dim_products ........................... [RUN]
08:55:10  5 of 6 OK created sql table model pizza_shop.dim_products ...................... [OK 81 in 5.18s]
08:55:11  4 of 6 OK created sql table model pizza_shop.dim_users ......................... [OK 10000 in 6.16s]
08:55:11  6 of 6 START sql incremental model pizza_shop.fct_orders ....................... [RUN]
08:55:22  6 of 6 OK created sql incremental model pizza_shop.fct_orders .................. [OK 20000 in 10.23s]
08:55:22  
08:55:22  Finished running 3 view models, 2 table models, 1 incremental model in 0 hours 0 minutes and 21.41 seconds (21.41s).
08:55:22  
08:55:22  Completed successfully
08:55:22  
08:55:22  Done. PASS=6 WARN=0 ERROR=0 SKIP=0 TOTAL=6
```

Also, the project can be tested using the `dbt test` command.

```bash
$ dbt test
08:55:29  Running with dbt=1.7.9
08:55:30  Registered adapter: athena=1.7.1
08:55:30  Found 6 models, 4 tests, 3 sources, 0 exposures, 0 metrics, 537 macros, 0 groups, 0 semantic models
08:55:30  
08:55:31  Concurrency: 4 threads (target='dev')
08:55:31  
08:55:31  1 of 4 START test not_null_dim_products_product_key ............................ [RUN]
08:55:31  2 of 4 START test not_null_dim_users_user_key .................................. [RUN]
08:55:31  3 of 4 START test unique_dim_products_product_key .............................. [RUN]
08:55:31  4 of 4 START test unique_dim_users_user_key .................................... [RUN]
08:55:33  3 of 4 PASS unique_dim_products_product_key .................................... [PASS in 2.17s]
08:55:33  2 of 4 PASS not_null_dim_users_user_key ........................................ [PASS in 2.23s]
08:55:34  4 of 4 PASS unique_dim_users_user_key .......................................... [PASS in 3.16s]
08:55:34  1 of 4 PASS not_null_dim_products_product_key .................................. [PASS in 3.20s]
08:55:34  
08:55:34  Finished running 4 tests in 0 hours 0 minutes and 4.37 seconds (4.37s).
08:55:34  
08:55:34  Completed successfully
08:55:34  
08:55:34  Done. PASS=4 WARN=0 ERROR=0 SKIP=0 TOTAL=4
```

#### Fact Table Structure

The schema of the fact table can be found below. The *product* and *user* are marked as the *array* and *struct* type respectively.

![](fct-orders-schema-01.png#center)

When we click an individual link, its detailed schema appears in a pop-up window as shown below.

![](fct-orders-schema-02.png#center)

In Athena, we can [flatten the product array](https://docs.aws.amazon.com/athena/latest/ug/flattening-arrays.html) into multiple rows by using *CROSS JOIN* in conjunction with the *UNNEST* operator.

![](fct-orders-query.png#center)

## Update Records

Although we will discuss ETL orchestration with Apache Airflow in the next post, here I illustrate how the dimension and fact tables change when records are updated.

### Product

First, a new record is inserted into the *staging_products* table, and the price is set to increase by 10.

```sql
-- // update a product record
INSERT INTO pizza_shop.staging_products (id, name, description, price, category, image, created_at)
    SELECT 1, name, description, price + 10, category, image, CAST(date_add('hour', 11, CURRENT_TIMESTAMP) AS TIMESTAMP)
    FROM pizza_shop.staging_products
    WHERE id = 1;

SELECT id, name, price, category, created_at 
FROM pizza_shop.staging_products
WHERE id = 1
ORDER BY created_at;

#	id  name                                price  category    created_at
1	 1  Moroccan  Spice  Pasta Pizza - Veg	335.0  veg pizzas  2024-02-29 19:54:03.999
2	 1  Moroccan  Spice  Pasta Pizza - Veg	345.0  veg pizzas  2024-02-29 20:02:17.304
```

When we execute the `dbt run` command again, we see the corresponding dimension table reflects the change by adding a new record and updating *valid_from* and *valid_to* columns accordingly. With this change, any later order record that has this product should be mapped into the new product record.

```sql
SELECT product_key, price, created_at, valid_from, valid_to 
FROM pizza_shop.dim_products 
WHERE product_id = 1
ORDER BY created_at;

# product_key                       price  created_at                  valid_from                  valid_to
1 b8c187845db8b7e55626659cfbb8aea1  335.0  2024-02-29 19:54:03.999000  2024-02-29 19:54:03.999000  2024-02-29 20:02:17.304000
2 590d4eb8831d78ae84f44b90e930d2f3  345.0  2024-02-29 20:02:17.304000  2024-02-29 20:02:17.304000  2199-12-31 00:00:00.000000
```

### User

Also, a new record is inserted into the *staging_users* table while modifying the email address.

```sql
-- // update a user record
INSERT INTO pizza_shop.staging_users (id, first_name, last_name, email, residence, lat, lon, created_at)
    SELECT 1, first_name, last_name, 'john.doe@example.com', residence, lat, lon, CAST(date_add('hour', 11, CURRENT_TIMESTAMP) AS TIMESTAMP)
    FROM pizza_shop.staging_users
    WHERE id = 1;

SELECT id, first_name, last_name, email, created_at 
FROM pizza_shop.staging_users
WHERE id = 1
ORDER BY created_at;

# id  first_name  last_name  email                       created_at
1  1  Kismat      Shroff     drishyamallick@hotmail.com  2024-02-29 19:54:04.846
2  1  Kismat      Shroff     john.doe@example.com        2024-02-29 20:04:05.915
```

Again the corresponding dimension table reflects the change by adding a new record and updating *valid_from* and *valid_to* columns accordingly.

```sql
SELECT user_key, email, valid_from, valid_to 
FROM pizza_shop.dim_users 
WHERE user_id = 1
ORDER BY created_at;

# user_key                          email                       valid_from                  valid_to
1 48afd277176d7aed4e0d03ab15033f28  drishyamallick@hotmail.com  2024-02-29 19:54:04.846000  2024-02-29 20:04:05.915000
2 ee798a3c387582d3581a6f907c25af98  john.doe@example.com        2024-02-29 20:04:05.915000  2199-12-31 00:00:00.000000
```

### Order

We insert a new order record that has two items where the IDs of the first and second products are 1 and 2 respectively. In this example, we expect the first product maps to the updated product record while the record of the second product remains the same. We can check it by querying the new order record together with an existing record that has corresponding products. As expected, the query result shows the product record is updated only in the new order record.

```sql
-- // add an order record
INSERT INTO pizza_shop.staging_orders(id, user_id, items, created_at)
VALUES (
  20001, 
  1,
  '[{"product_id": 1, "quantity": 2}, {"product_id": 2, "quantity": 3}]',
  CAST(date_add('hour', 11, CURRENT_TIMESTAMP) AS TIMESTAMP)
);

SELECT o.order_id, p.key, p.id, p.price, p.quantity, o.created_at
FROM pizza_shop.fct_orders AS o
CROSS JOIN UNNEST(o.product) as t(p)
WHERE o.order_id in (11146, 20001) AND p.id IN (1, 2)
ORDER BY o.order_id;

# order_id  key                                 id  price  quantity  created_at
1    11146  * b8c187845db8b7e55626659cfbb8aea1   1  335.0         1  2024-02-29 19:54:05.803000
2    11146  8311b52111a924582c0fe5cb566cfa9a     2   60.0         2  2024-02-29 19:54:05.803000
3    20001  * 590d4eb8831d78ae84f44b90e930d2f3   1  345.0         2  2024-02-29 20:08:32.756000
4    20001  8311b52111a924582c0fe5cb566cfa9a     2   60.0         3  2024-02-29 20:08:32.756000
```

## Summary

In this series, we discuss practical examples of data warehouse and lakehouse development where data transformation is performed by the data build tool (dbt) and ETL is managed by Apache Airflow. Open Table Formats such as Apache Iceberg bring a new opportunity that implements data warehousing features in a data lake (i.e. data lakehouse). In this post, we created a new dbt project that targets Apache Iceberg where transformations are performed on Amazon Athena. Data modelling was similar to the BigQuery project in Part 3 where the dimension tables were modelled by the SCD type 2 approach and the fact table was denormalized using the array and struct data types. Finally, impacts of record updates were discussed in detail.
