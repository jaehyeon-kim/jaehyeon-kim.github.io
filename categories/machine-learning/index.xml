<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Machine Learning on Jaehyeon Kim</title><link>https://jaehyeon.me/categories/machine-learning/</link><description>Recent content in Machine Learning on Jaehyeon Kim</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>Copyright Â© 2023-2024 Jaehyeon Kim. All Rights Reserved.</copyright><lastBuildDate>Sat, 30 May 2015 00:00:00 +0000</lastBuildDate><atom:link href="https://jaehyeon.me/categories/machine-learning/index.xml" rel="self" type="application/rss+xml"/><item><title>Setup Random Seeds on Caret Package</title><link>https://jaehyeon.me/blog/2015-05-30-setup-random-seeds-on-caret-package/</link><pubDate>Sat, 30 May 2015 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2015-05-30-setup-random-seeds-on-caret-package/</guid><description><![CDATA[<p>A short while ago I had a chance to perform analysis using the <strong>caret</strong> package. One of the requirements is to run it parallelly and to work in both Windows and Linux. The requirement can be met by using the <strong>parallel</strong> and <strong>doParallel</strong> packages as the <strong>caret</strong> package trains a model using the <strong>foreach</strong> package if clusters are registered by the <strong>doParallel</strong> package - further details about how to implement parallel processing on a single machine can be found in earlier posts (<a href="/blog/2015-02-01-tree-based-methods-1">Link 1</a>, <a href="/blog/2015-02-08-tree-based-methods-2">Link 2</a> and <a href="/blog/2015-02-14-tree-based-methods-3">Link 3</a>). While it is relatively straightforward to train a model across multiple clusters using the <strong>caret</strong> package, setting up random seeds may be a bit tricky. As analysis can be more reproducible by random seeds, a way of setting them up is illustrated using a simple function in this post.</p>]]></description></item><item><title>Tree Based Methods in R - Part VI</title><link>https://jaehyeon.me/blog/2015-03-07-tree-based-methods-6/</link><pubDate>Sat, 07 Mar 2015 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2015-03-07-tree-based-methods-6/</guid><description><![CDATA[<ul>
<li><a href="/blog/2015-02-01-tree-based-methods-1">Part I</a></li>
<li><a href="/blog/2015-02-08-tree-based-methods-2">Part II</a></li>
<li><a href="/blog/2015-02-14-tree-based-methods-3">Part III</a></li>
<li><a href="/blog/2015-02-15-tree-based-methods-4">Part IV</a></li>
<li><a href="/blog/2015-03-05-tree-based-methods-5">Part V</a></li>
<li><a href="/blog/2015-03-07-tree-based-methods-6/#">Part VI</a> (this post)</li>
</ul>
<p>A regression tree is evaluated using bagged trees in the <a href="/blog/2015-03-05-tree-based-methods-5">previous article</a>. In this article, the response variable of the same data set is converted into a binary factor variable and a classification tree is evaluated by comparing to bagged trees&rsquo; individual oob/test errors, cumulative oob/test errors and variable importance measures.</p>]]></description></item><item><title>Tree Based Methods in R - Part V</title><link>https://jaehyeon.me/blog/2015-03-05-tree-based-methods-5/</link><pubDate>Thu, 05 Mar 2015 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2015-03-05-tree-based-methods-5/</guid><description><![CDATA[<ul>
<li><a href="/blog/2015-02-01-tree-based-methods-1">Part I</a></li>
<li><a href="/blog/2015-02-08-tree-based-methods-2">Part II</a></li>
<li><a href="/blog/2015-02-14-tree-based-methods-3">Part III</a></li>
<li><a href="/blog/2015-02-15-tree-based-methods-4">Part IV</a></li>
<li><a href="/blog/2015-03-05-tree-based-methods-5/#">Part V</a> (this post)</li>
<li><a href="/blog/2015-03-07-tree-based-methods-6">Part VI</a></li>
</ul>
<p>This article evaluates a single regression tree&rsquo;s performance by comparing to bagged trees&rsquo; individual oob/test errors, cumulative oob/test errors and variable importance measures.</p>
<p>Before getting started, note that the source of the classes can be found in <a href="https://gist.github.com/jaehyeon-kim/b89dcbd2fb0b84fd236e" target="_blank" rel="noopener noreferrer">this gist<i class="fas fa-external-link-square-alt ms-1"></i></a> and, together with the relevant packages (see <em>tags</em>), it requires a utility function (<code>bestParam()</code>) that can be found <a href="https://gist.github.com/jaehyeon-kim/5622ae9fa982e0b46550" target="_blank" rel="noopener noreferrer">here<i class="fas fa-external-link-square-alt ms-1"></i></a>.</p>]]></description></item><item><title>Tree Based Methods in R - Part IV</title><link>https://jaehyeon.me/blog/2015-02-15-tree-based-methods-4/</link><pubDate>Sun, 15 Feb 2015 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2015-02-15-tree-based-methods-4/</guid><description><![CDATA[<ul>
<li><a href="/blog/2015-02-01-tree-based-methods-1">Part I</a></li>
<li><a href="/blog/2015-02-08-tree-based-methods-2">Part II</a></li>
<li><a href="/blog/2015-02-14-tree-based-methods-3">Part III</a></li>
<li><a href="/blog/2015-02-15-tree-based-methods-4/#">Part IV</a> (this post)</li>
<li><a href="/blog/2015-03-05-tree-based-methods-5">Part V</a></li>
<li><a href="/blog/2015-03-07-tree-based-methods-6">Part VI</a></li>
</ul>
<p>While the last three articles illustrated the CART model for both classification (with equal/unequal costs) and regression tasks, this article is rather technical as it compares three packages: <strong>rpart</strong>, <strong>caret</strong> and <strong>mlr</strong>. For those who are not familiar with the last two packages, they are wrappers (or frameworks) that implement a range of models (or algorithms) in a unified way. They are useful because inconsistent API could be a drawback of R (like other open source tools) and it would be quite beneficial if there is a way to implement different models in a standardized way. In line with the earlier articles, the <em>Carseats</em> data is used for a classification task.</p>]]></description></item><item><title>Tree Based Methods in R - Part III</title><link>https://jaehyeon.me/blog/2015-02-14-tree-based-methods-3/</link><pubDate>Sat, 14 Feb 2015 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2015-02-14-tree-based-methods-3/</guid><description><![CDATA[<ul>
<li><a href="/blog/2015-02-01-tree-based-methods-1">Part I</a></li>
<li><a href="/blog/2015-02-08-tree-based-methods-2">Part II</a></li>
<li><a href="/blog/2015-02-14-tree-based-methods-3/#">Part III</a> (this post)</li>
<li><a href="/blog/2015-02-15-tree-based-methods-4">Part IV</a></li>
<li><a href="/blog/2015-03-05-tree-based-methods-5">Part V</a></li>
<li><a href="/blog/2015-03-07-tree-based-methods-6">Part VI</a></li>
</ul>
<p>While classification tasks are implemented in the last two articles (<a href="/blog/2015-02-01-tree-based-methods-1">Part I</a> and <a href="/blog/2015-02-08-tree-based-methods-2">Part II</a>), a regression task is the topic of this article. While the <strong>caret</strong> package selects the tuning parameter (<em>cp</em>) that minimizes the error (<em>RMSE</em>), the <strong>rpart</strong> packages recommends the <em>1-SE rule</em>, which selects the smallest tree within 1 standard error of the minimum cross validation error (<em>xerror</em>). The models with 2 complexity parameters that are suggested by the packages are compared.</p>]]></description></item><item><title>Tree Based Methods in R - Part II</title><link>https://jaehyeon.me/blog/2015-02-08-tree-based-methods-2/</link><pubDate>Sun, 08 Feb 2015 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2015-02-08-tree-based-methods-2/</guid><description><![CDATA[<ul>
<li><a href="/blog/2015-02-01-tree-based-methods-1">Part I</a></li>
<li><a href="/blog/2015-02-08-tree-based-methods-2/#">Part II</a> (this post)</li>
<li><a href="/blog/2015-02-14-tree-based-methods-3">Part III</a></li>
<li><a href="/blog/2015-02-15-tree-based-methods-4">Part IV</a></li>
<li><a href="/blog/2015-03-05-tree-based-methods-5">Part V</a></li>
<li><a href="/blog/2015-03-07-tree-based-methods-6">Part VI</a></li>
</ul>
<p>In the previous article (<a href="/blog/2015-02-01-tree-based-methods-1">Tree Based Methods in R - Part I</a>), a decision tree is created on the <em>Carseats</em> data which is in the chapter 8 lab of <a href="https://www.statlearning.com/" target="_blank" rel="noopener noreferrer">ISLR<i class="fas fa-external-link-square-alt ms-1"></i></a>. In that article, potentially asymetric costs due to misclassification are not taken into account. When unbalance between false positive and false negative can have a significant impact, it can be explicitly adjusted either by altering prior (or empirical) probabilities or by adding a loss matrix.</p>]]></description></item><item><title>Tree Based Methods in R - Part I</title><link>https://jaehyeon.me/blog/2015-02-01-tree-based-methods-1/</link><pubDate>Sun, 01 Feb 2015 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2015-02-01-tree-based-methods-1/</guid><description><![CDATA[<ul>
<li><a href="/blog/2015-02-01-tree-based-methods-1/#">Part I</a> (this post)</li>
<li><a href="/blog/2015-02-08-tree-based-methods-2">Part II</a></li>
<li><a href="/blog/2015-02-14-tree-based-methods-3">Part III</a></li>
<li><a href="/blog/2015-02-15-tree-based-methods-4">Part IV</a></li>
<li><a href="/blog/2015-03-05-tree-based-methods-5">Part V</a></li>
<li><a href="/blog/2015-03-07-tree-based-methods-6">Part VI</a></li>
</ul>
<p>This is the first article about tree based methods using R. <em>Carseats</em> data in the chapter 8 lab of <a href="https://www.statlearning.com/" target="_blank" rel="noopener noreferrer">ISLR<i class="fas fa-external-link-square-alt ms-1"></i></a> is used to perform classification analysis. Unlike the lab example, the <strong>rpart</strong> package is used to fit the CART model on the data and the <strong>caret</strong> package is used for tuning the pruning parameter (<code>cp</code>).</p>]]></description></item></channel></rss>