<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Apache Kafka on Jaehyeon Kim</title><link>https://jaehyeon.me/categories/apache-kafka/</link><description>Recent content in Apache Kafka on Jaehyeon Kim</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>Copyright Â© 2023-2024 Jaehyeon Kim. All Rights Reserved.</copyright><lastBuildDate>Thu, 11 Jan 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://jaehyeon.me/categories/apache-kafka/index.xml" rel="self" type="application/rss+xml"/><item><title>Kafka Development on Kubernetes - Part 3 Kafka Connect</title><link>https://jaehyeon.me/blog/2024-01-11-kafka-development-on-k8s-part-3/</link><pubDate>Thu, 11 Jan 2024 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2024-01-11-kafka-development-on-k8s-part-3/</guid><description><![CDATA[<p><a href="https://kafka.apache.org/documentation/#connect" target="_blank" rel="noopener noreferrer">Kafka Connect<i class="fas fa-external-link-square-alt ms-1"></i></a> is a tool for scalably and reliably streaming data between Apache Kafka and other systems. It makes it simple to quickly define connectors that move large collections of data into and out of Kafka. In this post, we discuss how to set up a data ingestion pipeline using Kafka connectors. Fake customer and order data is ingested into Kafka topics using the <a href="https://github.com/awslabs/amazon-msk-data-generator" target="_blank" rel="noopener noreferrer">MSK Data Generator<i class="fas fa-external-link-square-alt ms-1"></i></a>. Also, we use the <a href="https://www.confluent.io/hub/confluentinc/kafka-connect-s3" target="_blank" rel="noopener noreferrer">Confluent S3<i class="fas fa-external-link-square-alt ms-1"></i></a> sink connector to save the messages of the topics into a S3 bucket. The Kafka Connect servers and individual connectors are deployed using the custom resources of <a href="https://strimzi.io/" target="_blank" rel="noopener noreferrer">Strimzi<i class="fas fa-external-link-square-alt ms-1"></i></a> on Kubernetes.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2024-01-11-kafka-development-on-k8s-part-3/featured.png" length="97270" type="image/png"/></item><item><title>Kafka Development on Kubernetes - Part 2 Producer and Consumer</title><link>https://jaehyeon.me/blog/2024-01-04-kafka-development-on-k8s-part-2/</link><pubDate>Thu, 04 Jan 2024 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2024-01-04-kafka-development-on-k8s-part-2/</guid><description><![CDATA[<p>Apache Kafka has five <a href="https://kafka.apache.org/documentation/#api" target="_blank" rel="noopener noreferrer">core APIs<i class="fas fa-external-link-square-alt ms-1"></i></a>, and we can develop applications to send/read streams of data to/from topics in a Kafka cluster using the producer and consumer APIs. While the main Kafka project maintains only the Java APIs, there are several <a href="https://cwiki.apache.org/confluence/display/KAFKA/Clients#Clients-Python" target="_blank" rel="noopener noreferrer">open source projects<i class="fas fa-external-link-square-alt ms-1"></i></a> that provide the Kafka client APIs in Python. In this post, we discuss how to develop Kafka client applications using the <a href="https://kafka-python.readthedocs.io/en/master/index.html" target="_blank" rel="noopener noreferrer">kafka-python<i class="fas fa-external-link-square-alt ms-1"></i></a> package on Kubernetes.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2024-01-04-kafka-development-on-k8s-part-2/featured.png" length="75889" type="image/png"/></item><item><title>Kafka Development on Kubernetes - Part 1 Cluster Setup</title><link>https://jaehyeon.me/blog/2023-12-21-kafka-development-on-k8s-part-1/</link><pubDate>Thu, 21 Dec 2023 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2023-12-21-kafka-development-on-k8s-part-1/</guid><description><![CDATA[<p><a href="https://kafka.apache.org/" target="_blank" rel="noopener noreferrer">Apache Kafka<i class="fas fa-external-link-square-alt ms-1"></i></a> is one of the key technologies for implementing data streaming architectures. <a href="https://strimzi.io/" target="_blank" rel="noopener noreferrer">Strimzi<i class="fas fa-external-link-square-alt ms-1"></i></a> provides a way to run an Apache Kafka cluster and related resources on Kubernetes in various deployment configurations. In this series of posts, we will discuss how to create a Kafka cluster, to develop Kafka client applications in Python and to build a data pipeline using Kafka connectors on Kubernetes.</p>
<ul>
<li><a href="/blog/2023-12-21-kafka-development-on-k8s-part-1/#">Part 1 Cluster Setup</a> (this post)</li>
<li><a href="/blog/2024-01-04-kafka-development-on-k8s-part-2">Part 2 Producer and Consumer</a></li>
<li><a href="/blog/2024-01-11-kafka-development-on-k8s-part-3">Part 3 Kafka Connect</a></li>
</ul>

<h2 id="setup-kafka-cluster" data-numberify>Setup Kafka Cluster<a class="anchor ms-1" href="#setup-kafka-cluster"></a></h2>
<p>The Kafka cluster is deployed using the <a href="https://strimzi.io/" target="_blank" rel="noopener noreferrer">Strimzi Operator<i class="fas fa-external-link-square-alt ms-1"></i></a> on a <a href="https://minikube.sigs.k8s.io/docs/" target="_blank" rel="noopener noreferrer">Minikube<i class="fas fa-external-link-square-alt ms-1"></i></a> cluster. We install Strimzi version 0.27.1 and Kubernetes version 1.24.7 as we use Kafka version 2.8.1 - see <a href="https://strimzi.io/downloads/" target="_blank" rel="noopener noreferrer">this page<i class="fas fa-external-link-square-alt ms-1"></i></a> for details about Kafka version compatibility. Once the <a href="https://minikube.sigs.k8s.io/docs/start/" target="_blank" rel="noopener noreferrer">Minikube CLI<i class="fas fa-external-link-square-alt ms-1"></i></a> and <a href="https://www.docker.com/" target="_blank" rel="noopener noreferrer">Docker<i class="fas fa-external-link-square-alt ms-1"></i></a> are installed, a Minikube cluster can be created by specifying the desired Kubernetes version as shown below.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2023-12-21-kafka-development-on-k8s-part-1/featured.png" length="108975" type="image/png"/></item><item><title>Kafka Development with Docker - Part 11 Kafka Authorization</title><link>https://jaehyeon.me/blog/2023-07-20-kafka-development-with-docker-part-11/</link><pubDate>Thu, 20 Jul 2023 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2023-07-20-kafka-development-with-docker-part-11/</guid><description>&lt;p>In the previous posts, we discussed how to implement client authentication by TLS (SSL or TLS/SSL) and SASL authentication. One of the key benefits of client authentication is achieving user access control. Kafka ships with a pluggable, out-of-the box authorization framework, which is configured with the &lt;em>authorizer.class.name&lt;/em> property in the server configuration and stores Access Control Lists (ACLs) in the cluster metadata (either Zookeeper or the KRaft metadata log). In this post, we will discuss how to configure Kafka authorization with Java and Python client examples while SASL is kept for client authentication.&lt;/p></description><enclosure url="https://jaehyeon.me/blog/2023-07-20-kafka-development-with-docker-part-11/featured.png" length="458848" type="image/png"/></item><item><title>Kafka Development with Docker - Part 10 SASL Authentication</title><link>https://jaehyeon.me/blog/2023-07-13-kafka-development-with-docker-part-10/</link><pubDate>Thu, 13 Jul 2023 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2023-07-13-kafka-development-with-docker-part-10/</guid><description><![CDATA[<p>In the previous post, we discussed TLS (SSL or TLS/SSL) authentication to improve security. It enforces two-way verification where a client certificate is verified by Kafka brokers. Client authentication can also be enabled by <a href="https://en.wikipedia.org/wiki/Simple_Authentication_and_Security_Layer" target="_blank" rel="noopener noreferrer">Simple Authentication and Security Layer (SASL)<i class="fas fa-external-link-square-alt ms-1"></i></a>, and we will discuss how to implement SASL authentication with Java and Python client examples in this post.</p>
<ul>
<li><a href="/blog/2023-05-04-kafka-development-with-docker-part-1">Part 1 Cluster Setup</a></li>
<li><a href="/blog/2023-05-18-kafka-development-with-docker-part-2">Part 2 Management App</a></li>
<li><a href="/blog/2023-05-25-kafka-development-with-docker-part-3">Part 3 Kafka Connect</a></li>
<li><a href="/blog/2023-06-01-kafka-development-with-docker-part-4">Part 4 Producer and Consumer</a></li>
<li><a href="/blog/2023-06-08-kafka-development-with-docker-part-5">Part 5 Glue Schema Registry</a></li>
<li><a href="/blog/2023-06-15-kafka-development-with-docker-part-6">Part 6 Kafka Connect with Glue Schema Registry</a></li>
<li><a href="/blog/2023-06-22-kafka-development-with-docker-part-7">Part 7 Producer and Consumer with Glue Schema Registry</a></li>
<li><a href="/blog/2023-06-29-kafka-development-with-docker-part-8">Part 8 SSL Encryption</a></li>
<li><a href="/blog/2023-07-06-kafka-development-with-docker-part-9">Part 9 SSL Authentication</a></li>
<li><a href="/blog/2023-07-13-kafka-development-with-docker-part-10/#">Part 10 SASL Authentication</a> (this post)</li>
<li><a href="/blog/2023-07-20-kafka-development-with-docker-part-11">Part 11 Kafka Authorization</a></li>
</ul>

<h2 id="certificate-setup" data-numberify>Certificate Setup<a class="anchor ms-1" href="#certificate-setup"></a></h2>
<p>As we will leave Kafka communication to remain encrypted, we need to keep the components for SSL encryption. The details can be found in <a href="/blog/2023-06-29-kafka-development-with-docker-part-8">Part 8</a>, and those components can be generated by <a href="https://github.com/jaehyeon-kim/kafka-pocs/blob/main/kafka-dev-with-docker/part-10/generate.sh" target="_blank" rel="noopener noreferrer"><em>generate.sh</em><i class="fas fa-external-link-square-alt ms-1"></i></a>. Once we execute the script, the following files are created.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2023-07-13-kafka-development-with-docker-part-10/featured.png" length="471947" type="image/png"/></item><item><title>Kafka Development with Docker - Part 9 SSL Authentication</title><link>https://jaehyeon.me/blog/2023-07-06-kafka-development-with-docker-part-9/</link><pubDate>Thu, 06 Jul 2023 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2023-07-06-kafka-development-with-docker-part-9/</guid><description><![CDATA[<p>In the previous post, we discussed how to configure TLS (SSL or TLS/SSL) encryption with Java and Python client examples. SSL encryption is a one-way verification process where a server certificate is verified by a client via <a href="https://en.wikipedia.org/wiki/Transport_Layer_Security#TLS_handshake" target="_blank" rel="noopener noreferrer">SSL Handshake<i class="fas fa-external-link-square-alt ms-1"></i></a>. To improve security, we can add client authentication either by enforcing two-way verification where a client certificate is verified by Kafka brokers (SSL authentication). Or we can choose a separate authentication mechanism, which is typically <a href="https://en.wikipedia.org/wiki/Simple_Authentication_and_Security_Layer" target="_blank" rel="noopener noreferrer">Simple Authentication and Security Layer (SASL)<i class="fas fa-external-link-square-alt ms-1"></i></a>. In this post, we will discuss how to implement SSL authentication with Java and Python client examples while SASL authentication is covered in the next post.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2023-07-06-kafka-development-with-docker-part-9/featured.png" length="471471" type="image/png"/></item><item><title>Kafka Development with Docker - Part 8 SSL Encryption</title><link>https://jaehyeon.me/blog/2023-06-29-kafka-development-with-docker-part-8/</link><pubDate>Thu, 29 Jun 2023 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2023-06-29-kafka-development-with-docker-part-8/</guid><description><![CDATA[<p>By default, Apache Kafka communicates in <em>PLAINTEXT</em>, which means that all data is sent without being encrypted. To secure communication, we can configure Kafka clients and other components to use <a href="https://en.wikipedia.org/wiki/Transport_Layer_Security" target="_blank" rel="noopener noreferrer">Transport Layer Security (TLS)<i class="fas fa-external-link-square-alt ms-1"></i></a> encryption. Note that TLS is also referred to <a href="https://en.wikipedia.org/wiki/Transport_Layer_Security#SSL_1.0,_2.0,_and_3.0" target="_blank" rel="noopener noreferrer">Secure Sockets Layer (SSL)<i class="fas fa-external-link-square-alt ms-1"></i></a> or TLS/SSL. SSL is the predecessor of TLS, and has been deprecated since June 2015. However, it is used in configuration and code instead of TLS for historical reasons. In this post, SSL, TLS and TLS/SSL will be used interchangeably. SSL encryption is a one-way verification process where a server certificate is verified by a client via <a href="https://en.wikipedia.org/wiki/Transport_Layer_Security#TLS_handshake" target="_blank" rel="noopener noreferrer">SSL Handshake<i class="fas fa-external-link-square-alt ms-1"></i></a>. Moreover, we can improve security by adding client authentication. For example, we can enforce two-way verification so that a client certificate is verified by Kafka brokers as well (<em>SSL Authentication</em>). Alternatively we can choose a separate authentication mechanism and typically <a href="https://en.wikipedia.org/wiki/Simple_Authentication_and_Security_Layer" target="_blank" rel="noopener noreferrer">Simple Authentication and Security Layer (SASL)<i class="fas fa-external-link-square-alt ms-1"></i></a> is used (<em>SASL Authentication</em>). In this post, we will discuss how to configure SSL encryption with Java and Python client examples while SSL and SASL client authentication will be covered in later posts.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2023-06-29-kafka-development-with-docker-part-8/featured.png" length="469311" type="image/png"/></item><item><title>Kafka Development with Docker - Part 7 Producer and Consumer with Glue Schema Registry</title><link>https://jaehyeon.me/blog/2023-06-22-kafka-development-with-docker-part-7/</link><pubDate>Thu, 22 Jun 2023 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2023-06-22-kafka-development-with-docker-part-7/</guid><description><![CDATA[<p>In <a href="/blog/2023-06-01-kafka-development-with-docker-part-4">Part 4</a>, we developed Kafka producer and consumer applications using the <a href="https://kafka-python.readthedocs.io/en/master/index.html" target="_blank" rel="noopener noreferrer">kafka-python<i class="fas fa-external-link-square-alt ms-1"></i></a> package. The Kafka messages are serialized as Json, but are not associated with a schema as there was not an integrated schema registry. Later we discussed how producers and consumers to Kafka topics can use schemas to ensure data consistency and compatibility as schemas evolve in <a href="/blog/2023-06-08-kafka-development-with-docker-part-5">Part 5</a>. In this post, I&rsquo;ll demonstrate how to enhance the existing applications by integrating <a href="https://docs.aws.amazon.com/glue/latest/dg/schema-registry.html" target="_blank" rel="noopener noreferrer"><em>AWS Glue Schema Registry</em><i class="fas fa-external-link-square-alt ms-1"></i></a>.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2023-06-22-kafka-development-with-docker-part-7/featured.png" length="57175" type="image/png"/></item><item><title>Kafka Development with Docker - Part 6 Kafka Connect with Glue Schema Registry</title><link>https://jaehyeon.me/blog/2023-06-15-kafka-development-with-docker-part-6/</link><pubDate>Thu, 15 Jun 2023 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2023-06-15-kafka-development-with-docker-part-6/</guid><description><![CDATA[<p>In <a href="/blog/2023-05-25-kafka-development-with-docker-part-3">Part 3</a>, we developed a data ingestion pipeline with fake online order data using Kafka Connect source and sink connectors. Schemas are not enabled on both of them as there was not an integrated schema registry. Later we discussed how producers and consumers to Kafka topics can use schemas to ensure data consistency and compatibility as schemas evolve in <a href="/blog/2023-06-08-kafka-development-with-docker-part-5">Part 5</a>. In this post, I&rsquo;ll demonstrate how to enhance the existing data ingestion pipeline by integrating <a href="https://docs.aws.amazon.com/glue/latest/dg/schema-registry.html" target="_blank" rel="noopener noreferrer"><em>AWS Glue Schema Registry</em><i class="fas fa-external-link-square-alt ms-1"></i></a>.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2023-06-15-kafka-development-with-docker-part-6/featured.png" length="60354" type="image/png"/></item><item><title>Kafka Development with Docker - Part 5 Glue Schema Registry</title><link>https://jaehyeon.me/blog/2023-06-08-kafka-development-with-docker-part-5/</link><pubDate>Thu, 08 Jun 2023 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2023-06-08-kafka-development-with-docker-part-5/</guid><description><![CDATA[<p>As described in the <a href="https://docs.confluent.io/platform/current/schema-registry/index.html#sr-overview" target="_blank" rel="noopener noreferrer">Confluent document<i class="fas fa-external-link-square-alt ms-1"></i></a>, <em>Schema Registry</em> provides a centralized repository for managing and validating schemas for topic message data, and for serialization and deserialization of the data over the network. Producers and consumers to Kafka topics can use schemas to ensure data consistency and compatibility as schemas evolve. In AWS, the <a href="https://docs.aws.amazon.com/glue/latest/dg/schema-registry.html" target="_blank" rel="noopener noreferrer">Glue Schema Registry<i class="fas fa-external-link-square-alt ms-1"></i></a> supports features to manage and enforce schemas on data streaming applications using convenient integrations with Apache Kafka, <a href="https://aws.amazon.com/msk/" target="_blank" rel="noopener noreferrer">Amazon Managed Streaming for Apache Kafka<i class="fas fa-external-link-square-alt ms-1"></i></a>, <a href="https://aws.amazon.com/kinesis/data-streams/" target="_blank" rel="noopener noreferrer">Amazon Kinesis Data Streams<i class="fas fa-external-link-square-alt ms-1"></i></a>, <a href="https://aws.amazon.com/kinesis/data-analytics/" target="_blank" rel="noopener noreferrer">Amazon Kinesis Data Analytics for Apache Flink<i class="fas fa-external-link-square-alt ms-1"></i></a>, and <a href="https://aws.amazon.com/lambda/" target="_blank" rel="noopener noreferrer">AWS Lambda<i class="fas fa-external-link-square-alt ms-1"></i></a>.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2023-06-08-kafka-development-with-docker-part-5/featured.png" length="51170" type="image/png"/></item><item><title>Kafka Development with Docker - Part 4 Producer and Consumer</title><link>https://jaehyeon.me/blog/2023-06-01-kafka-development-with-docker-part-4/</link><pubDate>Thu, 01 Jun 2023 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2023-06-01-kafka-development-with-docker-part-4/</guid><description><![CDATA[<p>In the previous post, we discussed <a href="https://kafka.apache.org/documentation/#connect" target="_blank" rel="noopener noreferrer">Kafka Connect<i class="fas fa-external-link-square-alt ms-1"></i></a> to stream data to/from a Kafka cluster. Kafka also includes the <a href="https://kafka.apache.org/documentation/#api" target="_blank" rel="noopener noreferrer">Producer/Consumer APIs<i class="fas fa-external-link-square-alt ms-1"></i></a> that allow client applications to send/read streams of data to/from topics in a Kafka cluster. While the main Kafka project maintains only the Java clients, there are several <a href="https://cwiki.apache.org/confluence/display/KAFKA/Clients#Clients-Python" target="_blank" rel="noopener noreferrer">open source projects<i class="fas fa-external-link-square-alt ms-1"></i></a> that provide the Kafka client APIs in Python. In this post, I&rsquo;ll demonstrate how to develop producer/consumer applications using the <a href="https://kafka-python.readthedocs.io/en/master/index.html" target="_blank" rel="noopener noreferrer">kafka-python<i class="fas fa-external-link-square-alt ms-1"></i></a> package.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2023-06-01-kafka-development-with-docker-part-4/featured.png" length="75255" type="image/png"/></item><item><title>Kafka Development with Docker - Part 3 Kafka Connect</title><link>https://jaehyeon.me/blog/2023-05-25-kafka-development-with-docker-part-3/</link><pubDate>Thu, 25 May 2023 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2023-05-25-kafka-development-with-docker-part-3/</guid><description><![CDATA[<p>According to the documentation of <a href="https://kafka.apache.org/documentation/#connect" target="_blank" rel="noopener noreferrer">Apache Kafka<i class="fas fa-external-link-square-alt ms-1"></i></a>, <em>Kafka Connect is a tool for scalably and reliably streaming data between Apache Kafka and other systems. It makes it simple to quickly define connectors that move large collections of data into and out of Kafka</em>. Kafka Connect supports two types of connectors - source and sink. Source connectors are used to ingest messages from external systems into Kafka topics while messages are ingested into external systems form Kafka topics with sink connectors. In this post, I will illustrate how to set up a data ingestion pipeline using Kafka connectors. Fake customer and order data will be ingested into the corresponding topics using the <a href="https://github.com/awslabs/amazon-msk-data-generator" target="_blank" rel="noopener noreferrer">MSK Data Generator<i class="fas fa-external-link-square-alt ms-1"></i></a> source connector. The topic messages will then be saved into a S3 bucket using the <a href="https://www.confluent.io/hub/confluentinc/kafka-connect-s3" target="_blank" rel="noopener noreferrer">Confluent S3<i class="fas fa-external-link-square-alt ms-1"></i></a> sink connector.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2023-05-25-kafka-development-with-docker-part-3/featured.png" length="69998" type="image/png"/></item><item><title>Kafka Development with Docker - Part 2 Management App</title><link>https://jaehyeon.me/blog/2023-05-18-kafka-development-with-docker-part-2/</link><pubDate>Thu, 18 May 2023 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2023-05-18-kafka-development-with-docker-part-2/</guid><description><![CDATA[<p>In the previous post, I illustrated how to create a topic and to produce/consume messages using the command utilities provided by Apache Kafka. It is not convenient, however, for example, when you consume serialised messages where their schemas are stored in a schema registry. Also, the utilities don&rsquo;t support to browse or manage related resources such as connectors and schemas. Therefore, a Kafka management app can be a good companion for development, which helps monitor and manage resources on an easy-to-use user interface. An app can be more useful if it supports features that are desirable for Kafka development on AWS. Those features cover <a href="https://docs.aws.amazon.com/msk/latest/developerguide/iam-access-control.html" target="_blank" rel="noopener noreferrer">IAM access control<i class="fas fa-external-link-square-alt ms-1"></i></a> of <a href="https://aws.amazon.com/msk/" target="_blank" rel="noopener noreferrer">Amazon MSK<i class="fas fa-external-link-square-alt ms-1"></i></a> and integration with <a href="https://aws.amazon.com/msk/features/msk-connect/" target="_blank" rel="noopener noreferrer">Amazon MSK Connect<i class="fas fa-external-link-square-alt ms-1"></i></a> and <a href="https://docs.aws.amazon.com/glue/latest/dg/schema-registry.html" target="_blank" rel="noopener noreferrer">AWS Glue Schema Registry<i class="fas fa-external-link-square-alt ms-1"></i></a>. In this post, I&rsquo;ll introduce several management apps that meet those requirements.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2023-05-18-kafka-development-with-docker-part-2/featured.png" length="59675" type="image/png"/></item><item><title>Kafka Development with Docker - Part 1 Cluster Setup</title><link>https://jaehyeon.me/blog/2023-05-04-kafka-development-with-docker-part-1/</link><pubDate>Thu, 04 May 2023 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2023-05-04-kafka-development-with-docker-part-1/</guid><description><![CDATA[<p>I&rsquo;m teaching myself <a href="https://docs.aws.amazon.com/whitepapers/latest/build-modern-data-streaming-analytics-architectures/build-modern-data-streaming-analytics-architectures.html" target="_blank" rel="noopener noreferrer">modern data streaming architectures<i class="fas fa-external-link-square-alt ms-1"></i></a> on AWS, and <a href="https://kafka.apache.org/" target="_blank" rel="noopener noreferrer">Apache Kafka<i class="fas fa-external-link-square-alt ms-1"></i></a> is one of the key technologies, which can be used for messaging, activity tracking, stream processing and so on. While applications tend to be deployed to cloud, it can be much easier if we develop and test those with <a href="https://www.docker.com/" target="_blank" rel="noopener noreferrer">Docker<i class="fas fa-external-link-square-alt ms-1"></i></a> and <a href="https://docs.docker.com/compose/" target="_blank" rel="noopener noreferrer">Docker Compose<i class="fas fa-external-link-square-alt ms-1"></i></a> locally. As the series title indicates, I plan to publish articles that demonstrate Kafka and related tools in <em>Dockerized</em> environments. Although I covered some of them in previous posts, they are implemented differently in terms of the Kafka Docker image, the number of brokers, Docker volume mapping etc. It can be confusing, and one of the purposes of this series is to illustrate reference implementations that can be applied to future development projects. Also, I can extend my knowledge while preparing for this series. In fact Kafka security is one of the areas that I expect to learn further. Below shows a list of posts that I plan for now.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2023-05-04-kafka-development-with-docker-part-1/featured.png" length="98355" type="image/png"/></item></channel></rss>