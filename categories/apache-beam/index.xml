<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Apache Beam on Jaehyeon Kim</title><link>https://jaehyeon.me/categories/apache-beam/</link><description>Recent content in Apache Beam on Jaehyeon Kim</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>Copyright © 2023-2025 Jaehyeon Kim. All Rights Reserved.</copyright><lastBuildDate>Thu, 19 Dec 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://jaehyeon.me/categories/apache-beam/index.xml" rel="self" type="application/rss+xml"/><item><title>Apache Beam Python Examples - Part 10 Develop Streaming File Reader using Splittable DoFn</title><link>https://jaehyeon.me/blog/2024-12-19-beam-examples-10/</link><pubDate>Thu, 19 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2024-12-19-beam-examples-10/</guid><description><![CDATA[<p>In <a href="/blog/2024-12-05-beam-examples-9">Part 9</a>, we developed two Apache Beam pipelines using <a href="https://beam.apache.org/documentation/programming-guide/#splittable-dofns" target="_blank" rel="noopener noreferrer"><em>Splittable DoFn (SDF)</em><i class="fas fa-external-link-square-alt ms-1"></i></a>. One of them is a batch file reader, which reads a list of files in an input folder followed by processing them in parallel. We can extend the I/O connector so that, instead of listing files once at the beginning, it scans an input folder periodically for new files and processes whenever new files are created in the folder. The techniques used in this post can be quite useful as they can be applied to developing I/O connectors that target other unbounded (or streaming) data sources (eg Kafka) using the Python SDK.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2024-12-19-beam-examples-10/featured.png" length="305211" type="image/png"/></item><item><title>Apache Beam Python Examples - Part 9 Develop Batch File Reader and PiSampler using Splittable DoFn</title><link>https://jaehyeon.me/blog/2024-12-05-beam-examples-9/</link><pubDate>Thu, 05 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2024-12-05-beam-examples-9/</guid><description><![CDATA[<p>A <a href="https://beam.apache.org/documentation/programming-guide/#splittable-dofns" target="_blank" rel="noopener noreferrer"><em>Splittable DoFn (SDF)</em><i class="fas fa-external-link-square-alt ms-1"></i></a> is a generalization of a <em>DoFn</em> that enables Apache Beam developers to create modular and composable I/O components. Also, it can be applied in advanced non-I/O scenarios such as Monte Carlo simulation. In this post, we develop two Apache Beam pipelines. The first pipeline is an I/O connector, and it reads a list of files in a folder followed by processing each of the file objects in parallel. The second pipeline estimates the value of $\pi$ by performing Monte Carlo simulation.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2024-12-05-beam-examples-9/featured.png" length="309371" type="image/png"/></item><item><title>Apache Beam Python Examples - Part 8 Enhance Sport Activity Tracker with Runner Motivation</title><link>https://jaehyeon.me/blog/2024-11-21-beam-examples-8/</link><pubDate>Thu, 21 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2024-11-21-beam-examples-8/</guid><description>&lt;p>In &lt;a href="/blog/2024-08-01-beam-examples-3">Part 3&lt;/a>, we developed a Beam pipeline that tracks sport activities of users and outputs their speeds periodically. While reporting such values is useful for users on its own, we can provide more engaging information to users if we have a pipeline that reports pacing of their activities over periods. For example, we can send a message to encourage a user to work harder if he/she has a performance goal and is underperforming for some periods. In this post, we develop a new pipeline that tracks user activities and reports pacing details by comparing short term metrics to their long term counterparts.&lt;/p></description><enclosure url="https://jaehyeon.me/blog/2024-11-21-beam-examples-8/featured.png" length="402888" type="image/png"/></item><item><title>Apache Beam Python Examples - Part 7 Separate Droppable Data into Side Output</title><link>https://jaehyeon.me/blog/2024-10-24-beam-examples-7/</link><pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2024-10-24-beam-examples-7/</guid><description><![CDATA[<p>We develop an Apache Beam pipeline that separates <em>droppable</em> elements from the rest of the data. <em>Droppable</em> elements are those that come later when the watermark passes the window max timestamp plus allowed lateness. Using a timer in a <em>Stateful</em> DoFn, <em>droppable</em> data is separated from normal data and dispatched into a side output rather than being discarded silently, which is the default behaviour. Note that this pipeline works in a situation where <em>droppable</em> elements do not appear often, and thus the chance that a <em>droppable</em> element is delivered as the first element in a particular window is low.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2024-10-24-beam-examples-7/featured.png" length="214574" type="image/png"/></item><item><title>Apache Beam Python Examples - Part 6 Call RPC Service in Batch with Defined Batch Size using Stateful DoFn</title><link>https://jaehyeon.me/blog/2024-10-02-beam-examples-6/</link><pubDate>Wed, 02 Oct 2024 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2024-10-02-beam-examples-6/</guid><description><![CDATA[<p>In the <a href="/blog/2024-09-25-beam-examples-5">previous post</a>, we continued discussing an Apache Beam pipeline that arguments input data by calling a <strong>Remote Procedure Call (RPC)</strong> service. A pipeline was developed that makes a single RPC call for a bundle of elements. The bundle size is determined by the runner, however, we may encounter an issue e.g. if an RPC service becomes quite slower if many elements are included in a single request. We can improve the pipeline using stateful <code>DoFn</code> where the number elements to process and maximum wait seconds can be controlled by <em>state</em> and <em>timers</em>. Note that, although the stateful <code>DoFn</code> used in this post solves the data augmentation task well, in practice, we should use the built-in transforms such as <a href="https://beam.apache.org/documentation/transforms/python/aggregation/batchelements/" target="_blank" rel="noopener noreferrer">BatchElements<i class="fas fa-external-link-square-alt ms-1"></i></a> and <a href="https://beam.apache.org/documentation/transforms/python/aggregation/groupintobatches/" target="_blank" rel="noopener noreferrer">GroupIntoBatches<i class="fas fa-external-link-square-alt ms-1"></i></a> whenever possible.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2024-10-02-beam-examples-6/featured.png" length="99452" type="image/png"/></item><item><title>Apache Beam Python Examples - Part 5 Call RPC Service in Batch using Stateless DoFn</title><link>https://jaehyeon.me/blog/2024-09-18-beam-examples-5/</link><pubDate>Wed, 18 Sep 2024 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2024-09-18-beam-examples-5/</guid><description><![CDATA[<p>In the <a href="/blog/2024-08-15-beam-examples-4">previous post</a>, we developed an Apache Beam pipeline where the input data is augmented by a <strong>Remote Procedure Call (RPC)</strong> service. Each input element performs an RPC call and the output is enriched by the response. This is not an efficient way of accessing an external service provided that the service can accept more than one element. In this post, we discuss how to enhance the pipeline so that a single RPC call is made for a bundle of elements, which can save a significant amount time compared to making a call for each element.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2024-09-18-beam-examples-5/featured.png" length="95285" type="image/png"/></item><item><title>Cache Data on Apache Beam Pipelines Using a Shared Object</title><link>https://jaehyeon.me/blog/2024-08-22-cache-using-shared-object/</link><pubDate>Thu, 22 Aug 2024 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2024-08-22-cache-using-shared-object/</guid><description><![CDATA[<p>I recently contributed to Apache Beam by adding a common pipeline pattern - <a href="https://beam.apache.org/documentation/patterns/shared-class/" target="_blank" rel="noopener noreferrer"><em>Cache data using a shared object</em><i class="fas fa-external-link-square-alt ms-1"></i></a>. Both batch and streaming pipelines are introduced, and they utilise the <a href="https://beam.apache.org/releases/pydoc/current/_modules/apache_beam/utils/shared.html#Shared" target="_blank" rel="noopener noreferrer"><code>Shared</code> class<i class="fas fa-external-link-square-alt ms-1"></i></a> of the Python SDK to enrich <code>PCollection</code> elements. This pattern can be more memory-efficient than side inputs, simpler than a stateful <code>DoFn</code>, and more performant than calling an external service, because it does not have to access an external service for every element or bundle of elements. In this post, we discuss this pattern in more details with batch and streaming use cases. For the latter, we configure the cache gets refreshed periodically.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2024-08-22-cache-using-shared-object/featured.png" length="49574" type="image/png"/></item><item><title>Apache Beam Python Examples - Part 4 Call RPC Service for Data Augmentation</title><link>https://jaehyeon.me/blog/2024-08-15-beam-examples-4/</link><pubDate>Thu, 15 Aug 2024 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2024-08-15-beam-examples-4/</guid><description>&lt;p>In this post, we develop an Apache Beam pipeline where the input data is augmented by a &lt;strong>Remote Procedure Call (RPC)&lt;/strong> service. Each input element performs an RPC call and the output is enriched by the response. This is not an efficient way of accessing an external service provided that the service can accept more than one element. In the subsequent two posts, we will discuss updated pipelines that make RPC calls more efficiently. We begin with illustrating how to manage development resources followed by demonstrating the RPC service that we use in this series. Finally, we develop a Beam pipeline that accesses the external service to augment the input elements.&lt;/p></description><enclosure url="https://jaehyeon.me/blog/2024-08-15-beam-examples-4/featured.png" length="93408" type="image/png"/></item><item><title>Apache Beam Python Examples - Part 3 Build Sport Activity Tracker with/without SQL</title><link>https://jaehyeon.me/blog/2024-08-01-beam-examples-3/</link><pubDate>Thu, 01 Aug 2024 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2024-08-01-beam-examples-3/</guid><description><![CDATA[<p>In this post, we develop two Apache Beam pipelines that track sport activities of users and output their speed periodically. The first pipeline uses native transforms and <a href="https://beam.apache.org/documentation/dsls/sql/overview/" target="_blank" rel="noopener noreferrer">Beam SQL<i class="fas fa-external-link-square-alt ms-1"></i></a> is used for the latter. While <em>Beam SQL</em> can be useful in some situations, its features in the Python SDK are not complete compared to the Java SDK. Therefore, we are not able to build the required tracking pipeline using it. We end up discussing potential improvements of <em>Beam SQL</em> so that it can be used for building competitive applications with the Python SDK.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2024-08-01-beam-examples-3/featured.png" length="94507" type="image/png"/></item><item><title>Apache Beam Python Examples - Part 2 Calculate Average Word Length with/without Fixed Look back</title><link>https://jaehyeon.me/blog/2024-07-18-beam-examples-2/</link><pubDate>Thu, 18 Jul 2024 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2024-07-18-beam-examples-2/</guid><description>&lt;p>In this post, we develop two Apache Beam pipelines that calculate average word lengths from input texts that are ingested by a Kafka topic. They obtain the statistics in different angles. The first pipeline emits the global average lengths whenever a new input text arrives while the latter triggers those values in a sliding time window.&lt;/p></description><enclosure url="https://jaehyeon.me/blog/2024-07-18-beam-examples-2/featured.png" length="96924" type="image/png"/></item><item><title>Apache Beam Python Examples - Part 1 Calculate K Most Frequent Words and Max Word Length</title><link>https://jaehyeon.me/blog/2024-07-04-beam-examples-1/</link><pubDate>Thu, 04 Jul 2024 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2024-07-04-beam-examples-1/</guid><description><![CDATA[<p>In this series, we develop <a href="https://beam.apache.org/" target="_blank" rel="noopener noreferrer">Apache Beam<i class="fas fa-external-link-square-alt ms-1"></i></a> Python pipelines. The majority of them are from <a href="https://www.packtpub.com/en-us/product/building-big-data-pipelines-with-apache-beam-9781800564930" target="_blank" rel="noopener noreferrer">Building Big Data Pipelines with Apache Beam by Jan Lukavský<i class="fas fa-external-link-square-alt ms-1"></i></a>. Mainly relying on the Java SDK, the book teaches fundamentals of Apache Beam using hands-on tasks, and we convert those tasks using the Python SDK. We focus on streaming pipelines, and they are deployed on a local (or embedded) <a href="https://flink.apache.org/" target="_blank" rel="noopener noreferrer">Apache Flink<i class="fas fa-external-link-square-alt ms-1"></i></a> cluster using the <a href="https://beam.apache.org/documentation/runners/flink/" target="_blank" rel="noopener noreferrer">Apache Flink Runner<i class="fas fa-external-link-square-alt ms-1"></i></a>. Beginning with setting up the development environment, we build two pipelines that obtain top K most frequent words and the word that has the longest word length in this post.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2024-07-04-beam-examples-1/featured.png" length="96881" type="image/png"/></item><item><title>Apache Beam Local Development with Python - Part 5 Testing Pipelines</title><link>https://jaehyeon.me/blog/2024-05-09-beam-local-dev-5/</link><pubDate>Thu, 09 May 2024 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2024-05-09-beam-local-dev-5/</guid><description>We developed batch and streaming pipelines in Part 2 and Part 4. Often it is faster and simpler to identify and fix bugs on the pipeline code by performing local unit testing. Moreover, especially when it comes to creating a streaming pipeline, unit testing cases can facilitate development further by using TestStream as it allows us to advance watermarks or processing time according to different scenarios. In this post, we discuss how to perform unit testing of the batch and streaming pipelines that we developed earlier.</description><enclosure url="https://jaehyeon.me/blog/2024-05-09-beam-local-dev-5/featured.png" length="53603" type="image/png"/></item><item><title>Apache Beam Local Development with Python - Part 4 Streaming Pipelines</title><link>https://jaehyeon.me/blog/2024-05-02-beam-local-dev-4/</link><pubDate>Thu, 02 May 2024 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2024-05-02-beam-local-dev-4/</guid><description>In Part 3, we discussed the portability layer of Apache Beam as it helps understand (1) how Python pipelines run on the Flink Runner and (2) how multiple SDKs can be used in a single pipeline, followed by demonstrating local Flink and Kafka cluster creation for developing streaming pipelines. In this post, we build a streaming pipeline that aggregates page visits by user in a fixed time window of 20 seconds.</description><enclosure url="https://jaehyeon.me/blog/2024-05-02-beam-local-dev-4/featured.png" length="54556" type="image/png"/></item><item><title>Apache Beam Local Development with Python - Part 3 Flink Runner</title><link>https://jaehyeon.me/blog/2024-04-18-beam-local-dev-3/</link><pubDate>Thu, 18 Apr 2024 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2024-04-18-beam-local-dev-3/</guid><description>In this series, we discuss local development of Apache Beam pipelines using Python. In the previous posts, we mainly talked about Batch pipelines with/without Beam SQL. Beam pipelines are portable between batch and streaming semantics, and we will discuss streaming pipeline development in this and the next posts. While there are multiple Beam Runners, not every Runner supports Python or some Runners have too limited features in streaming semantics - see Beam Capability Matrix for details.</description><enclosure url="https://jaehyeon.me/blog/2024-04-18-beam-local-dev-3/featured.png" length="262307" type="image/png"/></item><item><title>Apache Beam Local Development with Python - Part 2 Batch Pipelines</title><link>https://jaehyeon.me/blog/2024-04-04-beam-local-dev-2/</link><pubDate>Thu, 04 Apr 2024 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2024-04-04-beam-local-dev-2/</guid><description>In this series, we discuss local development of Apache Beam pipelines using Python. A basic Beam pipeline was introduced in Part 1, followed by demonstrating how to utilise Jupyter notebooks, Beam SQL and Beam DataFrames. In this post, we discuss Batch pipelines that aggregate website visit log by user and time. The pipelines are developed with and without Beam SQL. Additionally, each pipeline is implemented on a Jupyter notebook for demonstration.</description><enclosure url="https://jaehyeon.me/blog/2024-04-04-beam-local-dev-2/featured.png" length="55405" type="image/png"/></item><item><title>Apache Beam Local Development with Python - Part 1 Pipeline, Notebook, SQL and DataFrame</title><link>https://jaehyeon.me/blog/2024-03-28-beam-local-dev-1/</link><pubDate>Thu, 28 Mar 2024 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2024-03-28-beam-local-dev-1/</guid><description>Apache Beam and Apache Flink are open-source frameworks for parallel, distributed data processing at scale. Flink has DataStream and Table/SQL APIs and the former has more capacity to develop sophisticated data streaming applications. The DataStream API of PyFlink, Flink&amp;rsquo;s Python API, however, is not as complete as its Java counterpart, and it doesn&amp;rsquo;t provide enough capability to extend when there are missing features in Python. Recently I had a chance to look through Apache Beam and found it supports more possibility to extend and/or customise its features.</description><enclosure url="https://jaehyeon.me/blog/2024-03-28-beam-local-dev-1/featured.png" length="88260" type="image/png"/></item></channel></rss>