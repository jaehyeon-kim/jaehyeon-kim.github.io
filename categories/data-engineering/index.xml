<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Data Engineering on Jaehyeon Kim</title><link>https://jaehyeon.me/categories/data-engineering/</link><description>Recent content in Data Engineering on Jaehyeon Kim</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>Copyright © 2023-2024 Jaehyeon Kim. All Rights Reserved.</copyright><lastBuildDate>Thu, 05 Sep 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://jaehyeon.me/categories/data-engineering/index.xml" rel="self" type="application/rss+xml"/><item><title>DBT CI/CD Demo with BigQuery and GitHub Actions</title><link>https://jaehyeon.me/blog/2024-09-05-dbt-cicd-demo/</link><pubDate>Thu, 05 Sep 2024 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2024-09-05-dbt-cicd-demo/</guid><description><![CDATA[<p>Continuous integration (CI) is the process of ensuring new code integrates with the larger code base, and it puts a great emphasis on testing automation to check that the application is not broken whenever new commits are integrated into the main branch. Continuous delivery (CD) is an extension of continuous integration since it automatically deploys all code changes to a testing and/or production environment after the build stage. CI/CD helps development teams avoid bugs and code failures while maintaining a continuous cycle of software development and updates. In this post, we discuss how to set up a CI/CD pipeline for a <a href="https://www.getdbt.com/" target="_blank" rel="noopener noreferrer">data build tool (<em>dbt</em>)<i class="fas fa-external-link-square-alt ms-1"></i></a> project using <a href="https://github.com/features/actions" target="_blank" rel="noopener noreferrer">GitHub Actions<i class="fas fa-external-link-square-alt ms-1"></i></a> where <a href="https://cloud.google.com/bigquery?hl=en" target="_blank" rel="noopener noreferrer">BigQuery<i class="fas fa-external-link-square-alt ms-1"></i></a> is used as the target data warehouse.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2024-09-05-dbt-cicd-demo/featured.png" length="60835" type="image/png"/></item><item><title>Data Build Tool (dbt) Pizza Shop Demo - Part 6 ETL on Amazon Athena via Airflow</title><link>https://jaehyeon.me/blog/2024-03-14-dbt-pizza-shop-6/</link><pubDate>Thu, 14 Mar 2024 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2024-03-14-dbt-pizza-shop-6/</guid><description><![CDATA[<p>In <a href="/blog/2024-03-07-dbt-pizza-shop-5">Part 5</a>, we developed a <a href="https://docs.getdbt.com/docs/introduction" target="_blank" rel="noopener noreferrer">dbt<i class="fas fa-external-link-square-alt ms-1"></i></a> project that that targets <a href="https://iceberg.apache.org/" target="_blank" rel="noopener noreferrer">Apache Iceberg<i class="fas fa-external-link-square-alt ms-1"></i></a> where transformations are performed on <a href="https://aws.amazon.com/athena/" target="_blank" rel="noopener noreferrer">Amazon Athena<i class="fas fa-external-link-square-alt ms-1"></i></a>. Two dimension tables that keep product and user records are created as <a href="https://en.wikipedia.org/wiki/Slowly_changing_dimension" target="_blank" rel="noopener noreferrer">Type 2 slowly changing dimension (SCD Type 2)<i class="fas fa-external-link-square-alt ms-1"></i></a> tables, and one transactional fact table is built to keep pizza orders. To improve query performance, the fact table is denormalized to pre-join records from the dimension tables using the array and struct data types. In this post, we discuss how to set up an ETL process on the project using Apache Airflow.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2024-03-14-dbt-pizza-shop-6/featured.png" length="82921" type="image/png"/></item><item><title>Data Build Tool (dbt) Pizza Shop Demo - Part 5 Modelling on Amazon Athena</title><link>https://jaehyeon.me/blog/2024-03-07-dbt-pizza-shop-5/</link><pubDate>Thu, 07 Mar 2024 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2024-03-07-dbt-pizza-shop-5/</guid><description><![CDATA[<p>In <a href="%28/blog/2024-01-18-dbt-pizza-shop-1%29">Part 1</a> and <a href="%28/blog/2024-02-08-dbt-pizza-shop-3%29">Part 3</a>, we developed <a href="https://docs.getdbt.com/docs/introduction" target="_blank" rel="noopener noreferrer">data build tool (dbt)<i class="fas fa-external-link-square-alt ms-1"></i></a> projects that target <em>PostgreSQL</em> and <em>BigQuery</em> using fictional pizza shop data. The data is modelled by <a href="https://en.wikipedia.org/wiki/Slowly_changing_dimension" target="_blank" rel="noopener noreferrer">SCD type 2<i class="fas fa-external-link-square-alt ms-1"></i></a> dimension tables and one transactional fact table. While the order records should be joined with dimension tables to get complete details for <em>PostgreSQL</em>, the fact table is denormalized using <a href="https://cloud.google.com/bigquery/docs/best-practices-performance-nested" target="_blank" rel="noopener noreferrer">nested and repeated fields<i class="fas fa-external-link-square-alt ms-1"></i></a> to improve query performance for <em>BigQuery</em>.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2024-03-07-dbt-pizza-shop-5/featured.png" length="61499" type="image/png"/></item><item><title>Data Build Tool (dbt) Pizza Shop Demo - Part 4 ETL on BigQuery via Airflow</title><link>https://jaehyeon.me/blog/2024-02-22-dbt-pizza-shop-4/</link><pubDate>Thu, 22 Feb 2024 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2024-02-22-dbt-pizza-shop-4/</guid><description><![CDATA[<p>In <a href="/blog/2024-02-08-dbt-pizza-shop-3">Part 3</a>, we developed a <a href="https://docs.getdbt.com/docs/introduction" target="_blank" rel="noopener noreferrer">dbt<i class="fas fa-external-link-square-alt ms-1"></i></a> project that targets Google BigQuery with fictional pizza shop data. Two dimension tables that keep product and user records are created as <a href="https://en.wikipedia.org/wiki/Slowly_changing_dimension" target="_blank" rel="noopener noreferrer">Type 2 slowly changing dimension (SCD Type 2)<i class="fas fa-external-link-square-alt ms-1"></i></a> tables, and one transactional fact table is built to keep pizza orders. The fact table is denormalized using <a href="https://cloud.google.com/bigquery/docs/best-practices-performance-nested" target="_blank" rel="noopener noreferrer">nested and repeated fields<i class="fas fa-external-link-square-alt ms-1"></i></a> for improving query performance. In this post, we discuss how to set up an ETL process on the project using Apache Airflow.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2024-02-22-dbt-pizza-shop-4/featured.png" length="89588" type="image/png"/></item><item><title>Data Build Tool (dbt) Pizza Shop Demo - Part 3 Modelling on BigQuery</title><link>https://jaehyeon.me/blog/2024-02-08-dbt-pizza-shop-3/</link><pubDate>Thu, 08 Feb 2024 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2024-02-08-dbt-pizza-shop-3/</guid><description><![CDATA[<p>In this series, we discuss practical examples of data warehouse and lakehouse development where data transformation is performed by the <a href="https://docs.getdbt.com/docs/introduction" target="_blank" rel="noopener noreferrer">data build tool (dbt)<i class="fas fa-external-link-square-alt ms-1"></i></a> and ETL is managed by <a href="https://airflow.apache.org/" target="_blank" rel="noopener noreferrer">Apache Airflow<i class="fas fa-external-link-square-alt ms-1"></i></a>. In <a href="/blog/2024-01-18-dbt-pizza-shop-1">Part 1</a>, we developed a <em>dbt</em> project on PostgreSQL using fictional pizza shop data. At the end, the data sets are modelled by two <a href="https://en.wikipedia.org/wiki/Slowly_changing_dimension" target="_blank" rel="noopener noreferrer">SCD type 2<i class="fas fa-external-link-square-alt ms-1"></i></a> dimension tables and one transactional fact table. In this post, we create a new <em>dbt</em> project that targets <a href="https://cloud.google.com/bigquery" target="_blank" rel="noopener noreferrer">Google BigQuery<i class="fas fa-external-link-square-alt ms-1"></i></a>. While the dimension tables are kept by the same SCD type 2 approach, the fact table is denormalized using <a href="https://cloud.google.com/bigquery/docs/best-practices-performance-nested" target="_blank" rel="noopener noreferrer">nested and repeated fields<i class="fas fa-external-link-square-alt ms-1"></i></a>, which potentially can improve query performance by pre-joining corresponding dimension records.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2024-02-08-dbt-pizza-shop-3/featured.png" length="70297" type="image/png"/></item><item><title>Data Build Tool (dbt) Pizza Shop Demo - Part 2 ETL on PostgreSQL via Airflow</title><link>https://jaehyeon.me/blog/2024-01-25-dbt-pizza-shop-2/</link><pubDate>Thu, 25 Jan 2024 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2024-01-25-dbt-pizza-shop-2/</guid><description><![CDATA[<p>In this series of posts, we discuss data warehouse/lakehouse examples using <a href="https://docs.getdbt.com/docs/introduction" target="_blank" rel="noopener noreferrer">data build tool (dbt)<i class="fas fa-external-link-square-alt ms-1"></i></a> including ETL orchestration with Apache Airflow. In Part 1, we developed a <em>dbt</em> project on PostgreSQL with fictional pizza shop data. Two dimension tables that keep product and user records are created as <a href="https://en.wikipedia.org/wiki/Slowly_changing_dimension" target="_blank" rel="noopener noreferrer">Type 2 slowly changing dimension (SCD Type 2)<i class="fas fa-external-link-square-alt ms-1"></i></a> tables, and one transactional fact table is built to keep pizza orders. In this post, we discuss how to set up an ETL process on the project using Apache Airflow.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2024-01-25-dbt-pizza-shop-2/featured.png" length="77355" type="image/png"/></item><item><title>Data Build Tool (dbt) Pizza Shop Demo - Part 1 Modelling on PostgreSQL</title><link>https://jaehyeon.me/blog/2024-01-18-dbt-pizza-shop-1/</link><pubDate>Thu, 18 Jan 2024 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2024-01-18-dbt-pizza-shop-1/</guid><description><![CDATA[<p>The <a href="https://docs.getdbt.com/docs/introduction" target="_blank" rel="noopener noreferrer">data build tool (dbt)<i class="fas fa-external-link-square-alt ms-1"></i></a> is a popular data transformation tool for data warehouse development. Moreover, it can be used for <a href="https://www.databricks.com/glossary/data-lakehouse" target="_blank" rel="noopener noreferrer">data lakehouse<i class="fas fa-external-link-square-alt ms-1"></i></a> development thanks to open table formats such as Apache Iceberg, Apache Hudi and Delta Lake. <em>dbt</em> supports key AWS analytics services and I wrote a series of posts that discuss how to utilise <em>dbt</em> with <a href="/blog/2022-09-28-dbt-on-aws-part-1-redshift">Redshift</a>, <a href="/blog/2022-10-09-dbt-on-aws-part-2-glue">Glue</a>, <a href="/blog/2022-10-19-dbt-on-aws-part-3-emr-ec2">EMR on EC2</a>, <a href="/blog/2022-11-01-dbt-on-aws-part-4-emr-eks">EMR on EKS</a>, and <a href="/blog/2023-04-12-integrate-glue-schema-registry">Athena</a>. Those posts focus on platform integration, however, they do not show realistic ETL scenarios.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2024-01-18-dbt-pizza-shop-1/featured.png" length="85093" type="image/png"/></item><item><title>Setup Local Development Environment for Apache Flink and Spark Using EMR Container Images</title><link>https://jaehyeon.me/blog/2023-12-07-flink-spark-local-dev/</link><pubDate>Thu, 07 Dec 2023 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2023-12-07-flink-spark-local-dev/</guid><description><![CDATA[<p><a href="https://aws.amazon.com/about-aws/whats-new/2023/11/apache-flink-available-amazon-emr-eks/" target="_blank" rel="noopener noreferrer">Apache Flink became generally available<i class="fas fa-external-link-square-alt ms-1"></i></a> for <a href="https://aws.amazon.com/emr/features/eks/" target="_blank" rel="noopener noreferrer">Amazon EMR on EKS<i class="fas fa-external-link-square-alt ms-1"></i></a> from the <a href="https://docs.aws.amazon.com/emr/latest/EMR-on-EKS-DevelopmentGuide/emr-eks-6.15.0.html" target="_blank" rel="noopener noreferrer">EMR 6.15.0 releases<i class="fas fa-external-link-square-alt ms-1"></i></a>, and we are able to pull the Flink (as well as Spark) container images from the <a href="https://gallery.ecr.aws/emr-on-eks" target="_blank" rel="noopener noreferrer">ECR Public Gallery<i class="fas fa-external-link-square-alt ms-1"></i></a>. As both of them can be integrated with the <em>Glue Data Catalog</em>, it can be particularly useful if we develop real time data ingestion/processing via Flink and build analytical queries using Spark (or any other tools or services that can access to the Glue Data Catalog).</p>]]></description><enclosure url="https://jaehyeon.me/blog/2023-12-07-flink-spark-local-dev/featured.png" length="133053" type="image/png"/></item><item><title>Data Build Tool (dbt) for Effective Data Transformation on AWS – Part 5 Athena</title><link>https://jaehyeon.me/blog/2022-12-06-dbt-on-aws-part-5-athena/</link><pubDate>Tue, 06 Dec 2022 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2022-12-06-dbt-on-aws-part-5-athena/</guid><description><![CDATA[<p>The <a href="https://docs.getdbt.com/docs/introduction" target="_blank" rel="noopener noreferrer">data build tool (dbt)<i class="fas fa-external-link-square-alt ms-1"></i></a> is an effective data transformation tool and it supports key AWS analytics services - Redshift, Glue, EMR and Athena. In the previous posts, we discussed benefits of a common data transformation tool and the potential of dbt to cover a wide range of data projects from data warehousing to data lake to data lakehouse. Demo data projects that target Redshift Serverless, Glue, EMR on EC2 and EMR on EKS are illustrated as well. In the last part of the dbt on AWS series, we discuss data transformation pipelines using dbt on <a href="https://aws.amazon.com/athena" target="_blank" rel="noopener noreferrer">Amazon Athena<i class="fas fa-external-link-square-alt ms-1"></i></a>. <a href="https://www.imdb.com/interfaces/" target="_blank" rel="noopener noreferrer">Subsets of IMDb data<i class="fas fa-external-link-square-alt ms-1"></i></a> are used as source and data models are developed in multiple layers according to the <a href="https://docs.getdbt.com/guides/best-practices/how-we-structure/1-guide-overview" target="_blank" rel="noopener noreferrer">dbt best practices<i class="fas fa-external-link-square-alt ms-1"></i></a>. A list of posts of this series can be found below.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2022-12-06-dbt-on-aws-part-5-athena/featured.png" length="91796" type="image/png"/></item><item><title>Data Build Tool (dbt) for Effective Data Transformation on AWS – Part 4 EMR on EKS</title><link>https://jaehyeon.me/blog/2022-11-01-dbt-on-aws-part-4-emr-eks/</link><pubDate>Tue, 01 Nov 2022 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2022-11-01-dbt-on-aws-part-4-emr-eks/</guid><description><![CDATA[<p>The <a href="https://docs.getdbt.com/docs/introduction" target="_blank" rel="noopener noreferrer">data build tool (dbt)<i class="fas fa-external-link-square-alt ms-1"></i></a> is an effective data transformation tool and it supports key AWS analytics services - Redshift, Glue, EMR and Athena. In the previous posts, we discussed benefits of a common data transformation tool and the potential of dbt to cover a wide range of data projects from data warehousing to data lake to data lakehouse. Demo data projects that target Redshift Serverless, Glue and EMR on EC2 are illustrated as well. In part 4 of the dbt on AWS series, we discuss data transformation pipelines using dbt on <a href="https://aws.amazon.com/emr/features/eks/" target="_blank" rel="noopener noreferrer">Amazon EMR on EKS<i class="fas fa-external-link-square-alt ms-1"></i></a>. As Spark Submit does not allow the spark thrift server to run in cluster mode on Kubernetes, a simple wrapper class is created to overcome the limitation and it makes the thrift server run indefinitely. <a href="https://www.imdb.com/interfaces/" target="_blank" rel="noopener noreferrer">Subsets of IMDb data<i class="fas fa-external-link-square-alt ms-1"></i></a> are used as source and data models are developed in multiple layers according to the <a href="https://docs.getdbt.com/guides/best-practices/how-we-structure/1-guide-overview" target="_blank" rel="noopener noreferrer">dbt best practices<i class="fas fa-external-link-square-alt ms-1"></i></a>. A list of posts of this series can be found below.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2022-11-01-dbt-on-aws-part-4-emr-eks/featured.png" length="91067" type="image/png"/></item><item><title>Data Build Tool (dbt) for Effective Data Transformation on AWS – Part 3 EMR on EC2</title><link>https://jaehyeon.me/blog/2022-10-19-dbt-on-aws-part-3-emr-ec2/</link><pubDate>Wed, 19 Oct 2022 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2022-10-19-dbt-on-aws-part-3-emr-ec2/</guid><description><![CDATA[<p>The <a href="https://docs.getdbt.com/docs/introduction" target="_blank" rel="noopener noreferrer">data build tool (dbt)<i class="fas fa-external-link-square-alt ms-1"></i></a> is an effective data transformation tool and it supports key AWS analytics services - Redshift, Glue, EMR and Athena. In the previous posts, we discussed benefits of a common data transformation tool and the potential of dbt to cover a wide range of data projects from data warehousing to data lake to data lakehouse. Demo data projects that target Redshift Serverless and Glue are illustrated as well. In part 3 of the dbt on AWS series, we discuss data transformation pipelines using dbt on <a href="https://aws.amazon.com/emr/" target="_blank" rel="noopener noreferrer">Amazon EMR<i class="fas fa-external-link-square-alt ms-1"></i></a>. <a href="https://www.imdb.com/interfaces/" target="_blank" rel="noopener noreferrer">Subsets of IMDb data<i class="fas fa-external-link-square-alt ms-1"></i></a> are used as source and data models are developed in multiple layers according to the <a href="https://docs.getdbt.com/guides/best-practices/how-we-structure/1-guide-overview" target="_blank" rel="noopener noreferrer">dbt best practices<i class="fas fa-external-link-square-alt ms-1"></i></a>. A list of posts of this series can be found below.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2022-10-19-dbt-on-aws-part-3-emr-ec2/featured.png" length="91067" type="image/png"/></item><item><title>Data Build Tool (dbt) for Effective Data Transformation on AWS – Part 2 Glue</title><link>https://jaehyeon.me/blog/2022-10-09-dbt-on-aws-part-2-glue/</link><pubDate>Sun, 09 Oct 2022 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2022-10-09-dbt-on-aws-part-2-glue/</guid><description><![CDATA[<p>The <a href="https://docs.getdbt.com/docs/introduction" target="_blank" rel="noopener noreferrer">data build tool (dbt)<i class="fas fa-external-link-square-alt ms-1"></i></a> is an effective data transformation tool and it supports key AWS analytics services - Redshift, Glue, EMR and Athena. In <a href="/blog/2022-09-28-dbt-on-aws-part-1-redshift">part 1</a>, we discussed benefits of a common data transformation tool and the potential of dbt to cover a wide range of data projects from data warehousing to data lake to data lakehouse. A demo data project that targets Redshift Serverless is illustrated as well. In part 2 of the dbt on AWS series, we discuss data transformation pipelines using dbt on <a href="https://aws.amazon.com/glue/" target="_blank" rel="noopener noreferrer">AWS Glue<i class="fas fa-external-link-square-alt ms-1"></i></a>. <a href="https://www.imdb.com/interfaces/" target="_blank" rel="noopener noreferrer">Subsets of IMDb data<i class="fas fa-external-link-square-alt ms-1"></i></a> are used as source and data models are developed in multiple layers according to the <a href="https://docs.getdbt.com/guides/best-practices/how-we-structure/1-guide-overview" target="_blank" rel="noopener noreferrer">dbt best practices<i class="fas fa-external-link-square-alt ms-1"></i></a>. A list of posts of this series can be found below.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2022-10-09-dbt-on-aws-part-2-glue/featured.png" length="90647" type="image/png"/></item><item><title>Data Build Tool (dbt) for Effective Data Transformation on AWS – Part 1 Redshift</title><link>https://jaehyeon.me/blog/2022-09-28-dbt-on-aws-part-1-redshift/</link><pubDate>Wed, 28 Sep 2022 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2022-09-28-dbt-on-aws-part-1-redshift/</guid><description><![CDATA[<p>The <a href="https://docs.getdbt.com/docs/introduction" target="_blank" rel="noopener noreferrer">data build tool (dbt)<i class="fas fa-external-link-square-alt ms-1"></i></a> is an effective data transformation tool and it supports key AWS analytics services - Redshift, Glue, EMR and Athena. In part 1 of the dbt on AWS series, we discuss data transformation pipelines using dbt on <a href="https://aws.amazon.com/redshift/redshift-serverless/" target="_blank" rel="noopener noreferrer">Redshift Serverless<i class="fas fa-external-link-square-alt ms-1"></i></a>. <a href="https://www.imdb.com/interfaces/" target="_blank" rel="noopener noreferrer">Subsets of IMDb data<i class="fas fa-external-link-square-alt ms-1"></i></a> are used as source and data models are developed in multiple layers according to the <a href="https://docs.getdbt.com/guides/best-practices/how-we-structure/1-guide-overview" target="_blank" rel="noopener noreferrer">dbt best practices<i class="fas fa-external-link-square-alt ms-1"></i></a>.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2022-09-28-dbt-on-aws-part-1-redshift/featured.png" length="97234" type="image/png"/></item><item><title>Develop and Test Apache Spark Apps for EMR Remotely Using Visual Studio Code</title><link>https://jaehyeon.me/blog/2022-09-07-emr-remote-dev/</link><pubDate>Wed, 07 Sep 2022 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2022-09-07-emr-remote-dev/</guid><description><![CDATA[<p>When we develop a Spark application on EMR, we can use <a href="/blog/2022-05-08-emr-local-dev">docker for local development</a> or notebooks via <a href="https://aws.amazon.com/emr/features/studio/" target="_blank" rel="noopener noreferrer">EMR Studio<i class="fas fa-external-link-square-alt ms-1"></i></a> (or <a href="https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-managed-notebooks.html" target="_blank" rel="noopener noreferrer">EMR Notebooks<i class="fas fa-external-link-square-alt ms-1"></i></a>). However, the local development option is not viable if the size of data is large. Also, I am not a fan of notebooks as it is not possible to utilise the features my editor supports such as syntax highlighting, autocomplete and code formatting. Moreover, it is not possible to organise code into modules and to perform unit testing properly with that option. In this post, We will discuss how to set up a remote development environment on an EMR cluster deployed in a private subnet with VPN and the <a href="https://code.visualstudio.com/docs/remote/ssh" target="_blank" rel="noopener noreferrer">VS Code remote SSH extension<i class="fas fa-external-link-square-alt ms-1"></i></a>. Typical Spark development examples will be illustrated while sharing the cluster with multiple users. Overall it brings another effective way of developing Spark apps on EMR, which improves developer experience significantly.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2022-09-07-emr-remote-dev/featured.png" length="72448" type="image/png"/></item><item><title>Manage EMR on EKS with Terraform</title><link>https://jaehyeon.me/blog/2022-08-26-emr-on-eks-with-terraform/</link><pubDate>Fri, 26 Aug 2022 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2022-08-26-emr-on-eks-with-terraform/</guid><description><![CDATA[<p><a href="https://aws.amazon.com/emr/features/eks/" target="_blank" rel="noopener noreferrer">Amazon EMR on EKS<i class="fas fa-external-link-square-alt ms-1"></i></a> is a deployment option for Amazon EMR that allows you to automate the provisioning and management of open-source big data frameworks on EKS. While <a href="https://eksctl.io/" target="_blank" rel="noopener noreferrer">eksctl<i class="fas fa-external-link-square-alt ms-1"></i></a> is popular for working with <a href="https://aws.amazon.com/eks/" target="_blank" rel="noopener noreferrer">Amazon EKS<i class="fas fa-external-link-square-alt ms-1"></i></a> clusters, it has limitations when it comes to building infrastructure that integrates multiple AWS services. Also, it is not straightforward to update EKS cluster resources incrementally with it. On the other hand <a href="https://www.terraform.io/" target="_blank" rel="noopener noreferrer">Terraform<i class="fas fa-external-link-square-alt ms-1"></i></a> can be an effective tool for managing infrastructure that includes not only EKS and EMR virtual clusters but also other AWS resources. Moreover, Terraform has a wide range of <a href="https://www.terraform.io/language/modules" target="_blank" rel="noopener noreferrer">modules<i class="fas fa-external-link-square-alt ms-1"></i></a>, and it can even be simpler to build and manage infrastructure using those compared to the CLI tool. In this post, we’ll discuss how to provision and manage Spark jobs on EMR on EKS with Terraform. <a href="https://aws-ia.github.io/terraform-aws-eks-blueprints/" target="_blank" rel="noopener noreferrer">Amazon EKS Blueprints for Terraform<i class="fas fa-external-link-square-alt ms-1"></i></a> will be used for provisioning EKS, EMR virtual cluster and related resources. Also, Spark job autoscaling will be managed by <a href="https://karpenter.sh/" target="_blank" rel="noopener noreferrer">Karpenter<i class="fas fa-external-link-square-alt ms-1"></i></a> where two Spark jobs with and without <a href="https://spark.apache.org/docs/latest/job-scheduling.html#dynamic-resource-allocation" target="_blank" rel="noopener noreferrer">Dynamic Resource Allocation (DRA)<i class="fas fa-external-link-square-alt ms-1"></i></a> will be compared.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2022-08-26-emr-on-eks-with-terraform/featured.png" length="67936" type="image/png"/></item><item><title>Revisit AWS Lambda Invoke Function Operator of Apache Airflow</title><link>https://jaehyeon.me/blog/2022-08-06-revisit-lambda-operator/</link><pubDate>Sat, 06 Aug 2022 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2022-08-06-revisit-lambda-operator/</guid><description><![CDATA[<p><a href="https://airflow.apache.org/" target="_blank" rel="noopener noreferrer">Apache Airflow<i class="fas fa-external-link-square-alt ms-1"></i></a> is a popular workflow management platform. A wide range of AWS services are integrated with the platform by <a href="https://airflow.apache.org/docs/apache-airflow-providers-amazon/stable/operators/index.html" target="_blank" rel="noopener noreferrer">Amazon AWS Operators<i class="fas fa-external-link-square-alt ms-1"></i></a>. AWS Lambda is one of the integrated services, and it can be used to develop workflows efficiently. The current <a href="https://airflow.apache.org/docs/apache-airflow-providers-amazon/stable/operators/lambda.html" target="_blank" rel="noopener noreferrer">Lambda Operator<i class="fas fa-external-link-square-alt ms-1"></i></a>, however, just invokes a Lambda function, and it can fail to report the invocation result of a function correctly and to record the exact error message from failure. In this post, we’ll discuss a custom Lambda operator that handles those limitations.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2022-08-06-revisit-lambda-operator/featured.png" length="24814" type="image/png"/></item><item><title>Data Warehousing ETL Demo with Apache Iceberg on EMR Local Environment</title><link>https://jaehyeon.me/blog/2022-06-26-iceberg-etl-demo/</link><pubDate>Sun, 26 Jun 2022 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2022-06-26-iceberg-etl-demo/</guid><description><![CDATA[<p>Unlike traditional Data Lake, new table formats (<a href="https://iceberg.apache.org/" target="_blank" rel="noopener noreferrer">Iceberg<i class="fas fa-external-link-square-alt ms-1"></i></a>, <a href="https://hudi.apache.org/" target="_blank" rel="noopener noreferrer">Hudi<i class="fas fa-external-link-square-alt ms-1"></i></a> and <a href="https://delta.io/" target="_blank" rel="noopener noreferrer">Delta Lake<i class="fas fa-external-link-square-alt ms-1"></i></a>) support <a href="https://iceberg.apache.org/docs/latest/spark-writes/" target="_blank" rel="noopener noreferrer">features<i class="fas fa-external-link-square-alt ms-1"></i></a> that can be used to apply data warehousing patterns, which can bring a way to be rescued from <a href="https://www.gartner.com/en/newsroom/press-releases/2014-07-28-gartner-says-beware-of-the-data-lake-fallacy" target="_blank" rel="noopener noreferrer">Data Swamp<i class="fas fa-external-link-square-alt ms-1"></i></a>. In this post, we&rsquo;ll discuss how to implement ETL using retail analytics data. It has two dimension data (user and product) and a single fact data (order). The dimension data sets have different ETL strategies depending on whether to track historical changes. For the fact data, the primary keys of the dimension data are added to facilitate later queries. We&rsquo;ll use Iceberg for data storage/management and Spark for data processing. Instead of provisioning an EMR cluster, a local development environment will be used. Finally, the ETL results will be queried by Athena for verification.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2022-06-26-iceberg-etl-demo/featured.png" length="43604" type="image/png"/></item><item><title>Develop and Test Apache Spark Apps for EMR Locally Using Docker</title><link>https://jaehyeon.me/blog/2022-05-08-emr-local-dev/</link><pubDate>Sun, 08 May 2022 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2022-05-08-emr-local-dev/</guid><description><![CDATA[<p>[<strong>UPDATE 2023-12-07</strong>]</p>
<ul>
<li>I wrote a <a href="/blog/2023-12-07-flink-spark-local-dev">new post</a> that simplifies the Spark configuration dramatically. Besides, the log configuration is based on Log4J2, which applies to newer Spark versions. Moreover, the container is configured to run the Spark History Server, and it allows us to debug and diagnose completed and running Spark applications. I recommend referring to the new post.</li>
</ul>
<p><a href="https://aws.amazon.com/emr/" target="_blank" rel="noopener noreferrer">Amazon EMR<i class="fas fa-external-link-square-alt ms-1"></i></a> is a managed service that simplifies running Apache Spark on AWS. It has multiple deployment options that cover EC2, <a href="https://aws.amazon.com/emr/features/eks/" target="_blank" rel="noopener noreferrer">EKS<i class="fas fa-external-link-square-alt ms-1"></i></a>, <a href="https://aws.amazon.com/emr/features/outposts/" target="_blank" rel="noopener noreferrer">Outposts<i class="fas fa-external-link-square-alt ms-1"></i></a> and <a href="https://aws.amazon.com/emr/serverless/" target="_blank" rel="noopener noreferrer">Serverless<i class="fas fa-external-link-square-alt ms-1"></i></a>. For development and testing, <a href="https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-managed-notebooks.html" target="_blank" rel="noopener noreferrer">EMR Notebooks<i class="fas fa-external-link-square-alt ms-1"></i></a> or <a href="https://aws.amazon.com/emr/features/studio/" target="_blank" rel="noopener noreferrer">EMR Studio<i class="fas fa-external-link-square-alt ms-1"></i></a> can be an option. Both provide a Jupyter Notebook environment and the former is only available for EMR on EC2. There are cases, however, that development (and learning) is performed in a local environment more efficiently. The AWS Glue team understands this demand, and they illustrate how to make use of a custom Docker image for Glue in a <a href="https://aws.amazon.com/blogs/big-data/develop-and-test-aws-glue-version-3-0-jobs-locally-using-a-docker-container/" target="_blank" rel="noopener noreferrer">recent blog post<i class="fas fa-external-link-square-alt ms-1"></i></a>. However, we don’t hear similar news from the EMR team. In order to fill the gap, we’ll discuss how to create a Spark local development environment for EMR using Docker and/or VSCode. Typical Spark development examples will be demonstrated, which covers Spark Submit, Pytest, PySpark shell, Jupyter Notebook and Spark Structured Streaming. For the Spark Submit and Jupyter Notebook examples, <a href="https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-spark-glue.html" target="_blank" rel="noopener noreferrer">Glue Catalog integration<i class="fas fa-external-link-square-alt ms-1"></i></a> will be illustrated as well. And both the cases of utilising <a href="https://code.visualstudio.com/docs/remote/containers" target="_blank" rel="noopener noreferrer">Visual Studio Code Remote - Containers<i class="fas fa-external-link-square-alt ms-1"></i></a> extension and running as an isolated container will be covered in some key examples.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2022-05-08-emr-local-dev/featured.png" length="25693" type="image/png"/></item><item><title>EMR on EKS by Example</title><link>https://jaehyeon.me/blog/2022-01-17-emr-on-eks-by-example/</link><pubDate>Mon, 17 Jan 2022 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2022-01-17-emr-on-eks-by-example/</guid><description><![CDATA[<p><a href="https://aws.amazon.com/emr/features/eks/" target="_blank" rel="noopener noreferrer">EMR on EKS<i class="fas fa-external-link-square-alt ms-1"></i></a> provides a deployment option for <a href="https://aws.amazon.com/emr/" target="_blank" rel="noopener noreferrer">Amazon EMR<i class="fas fa-external-link-square-alt ms-1"></i></a> that allows you to automate the provisioning and management of open-source big data frameworks on <a href="https://aws.amazon.com/eks/" target="_blank" rel="noopener noreferrer">Amazon EKS<i class="fas fa-external-link-square-alt ms-1"></i></a>. While a wide range of open source big data components are available in EMR on EC2, only Apache Spark is available in EMR on EKS. It is more flexible, however, that applications of different EMR versions can be run in multiple availability zones on either EC2 or Fargate. Also, other types of containerized applications can be deployed on the same EKS cluster. Therefore, if you have or plan to have, for example, <a href="https://airflow.apache.org/" target="_blank" rel="noopener noreferrer">Apache Airflow<i class="fas fa-external-link-square-alt ms-1"></i></a>, <a href="https://superset.apache.org/" target="_blank" rel="noopener noreferrer">Apache Superset<i class="fas fa-external-link-square-alt ms-1"></i></a> or <a href="https://www.kubeflow.org/" target="_blank" rel="noopener noreferrer">Kubeflow<i class="fas fa-external-link-square-alt ms-1"></i></a> as your analytics toolkits, it can be an effective way to manage big data (as well as non-big data) workloads. While Glue is more for ETL, EMR on EKS can also be used for other types of tasks such as machine learning. Moreover, it allows you to build a Spark application, not a <em>Gluish</em> Spark application. For example, while you have to use custom connectors for <a href="https://aws.amazon.com/marketplace/pp/prodview-zv3vmwbkuat2e" target="_blank" rel="noopener noreferrer">Hudi<i class="fas fa-external-link-square-alt ms-1"></i></a> or <a href="https://aws.amazon.com/marketplace/pp/prodview-iicxofvpqvsio" target="_blank" rel="noopener noreferrer">Iceberg<i class="fas fa-external-link-square-alt ms-1"></i></a> for Glue, you can use their native libraries with EMR on EKS. In this post, we&rsquo;ll discuss EMR on EKS with simple and elaborated examples.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2022-01-17-emr-on-eks-by-example/featured.png" length="76740" type="image/png"/></item><item><title>Data Lake Demo using Change Data Capture (CDC) on AWS – Part 3 Implement Data Lake</title><link>https://jaehyeon.me/blog/2021-12-19-datalake-demo-part3/</link><pubDate>Sun, 19 Dec 2021 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2021-12-19-datalake-demo-part3/</guid><description><![CDATA[<p>In the <a href="/blog/2021-12-12-datalake-demo-part2">previous post</a>, we created a VPC that has private and public subnets in 2 availability zones in order to build and deploy the data lake solution on AWS. NAT instances are created to forward outbound traffic to the internet and a VPN bastion host is set up to facilitate deployment. An <a href="https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.AuroraPostgreSQL.html" target="_blank" rel="noopener noreferrer">Aurora PostgreSQL<i class="fas fa-external-link-square-alt ms-1"></i></a> cluster is deployed to host the source database and a Python command line app is used to create the database. To develop data ingestion using CDC, an <a href="https://aws.amazon.com/msk/" target="_blank" rel="noopener noreferrer">Amazon MSK<i class="fas fa-external-link-square-alt ms-1"></i></a> cluster is deployed and the <a href="https://debezium.io/documentation/reference/stable/connectors/postgresql.html" target="_blank" rel="noopener noreferrer">Debezium source<i class="fas fa-external-link-square-alt ms-1"></i></a> and <a href="https://lenses.io/blog/2020/11/new-kafka-to-S3-connector/" target="_blank" rel="noopener noreferrer">Lenses S3 sink<i class="fas fa-external-link-square-alt ms-1"></i></a> connectors are created on <a href="https://aws.amazon.com/msk/features/msk-connect/" target="_blank" rel="noopener noreferrer">MSK Connect<i class="fas fa-external-link-square-alt ms-1"></i></a>. We also confirmed the order creation and update events are captured as expected. As the last part of this series, we&rsquo;ll build an <a href="https://hudi.apache.org/docs/writing_data/#deltastreamer" target="_blank" rel="noopener noreferrer">Apache Hudi DeltaStreamer<i class="fas fa-external-link-square-alt ms-1"></i></a> app on <a href="https://aws.amazon.com/emr/" target="_blank" rel="noopener noreferrer">Amazon EMR<i class="fas fa-external-link-square-alt ms-1"></i></a> and use the resulting Hudi table with <a href="https://aws.amazon.com/athena/" target="_blank" rel="noopener noreferrer">Amazon Athena<i class="fas fa-external-link-square-alt ms-1"></i></a> and <a href="https://aws.amazon.com/quicksight/" target="_blank" rel="noopener noreferrer">Amazon Quicksight<i class="fas fa-external-link-square-alt ms-1"></i></a> to build a dashboard.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2021-12-19-datalake-demo-part3/featured.png" length="164526" type="image/png"/></item><item><title>Data Lake Demo using Change Data Capture (CDC) on AWS – Part 2 Implement CDC</title><link>https://jaehyeon.me/blog/2021-12-12-datalake-demo-part2/</link><pubDate>Sun, 12 Dec 2021 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2021-12-12-datalake-demo-part2/</guid><description><![CDATA[<p>In the <a href="/blog/2021-12-05-datalake-demo-part1">previous post</a>, we discussed a data lake solution where data ingestion is performed using <a href="https://www.redhat.com/en/topics/integration/what-is-change-data-capture#what-is-cdc" target="_blank" rel="noopener noreferrer">change data capture (CDC)<i class="fas fa-external-link-square-alt ms-1"></i></a> and the output files are <em>upserted</em> to an <a href="https://hudi.apache.org/" target="_blank" rel="noopener noreferrer">Apache Hudi<i class="fas fa-external-link-square-alt ms-1"></i></a> table. Being registered to <a href="https://docs.aws.amazon.com/glue/latest/dg/populate-data-catalog.html" target="_blank" rel="noopener noreferrer">Glue Data Catalog<i class="fas fa-external-link-square-alt ms-1"></i></a>, it can be used for ad-hoc queries and report/dashboard creation. The <a href="https://docs.yugabyte.com/latest/sample-data/northwind/" target="_blank" rel="noopener noreferrer">Northwind database<i class="fas fa-external-link-square-alt ms-1"></i></a> is used as the source database and, following the <a href="https://microservices.io/patterns/data/transactional-outbox.html" target="_blank" rel="noopener noreferrer">transactional outbox pattern<i class="fas fa-external-link-square-alt ms-1"></i></a>, order-related changes are _upserted _to an outbox table by triggers. The data ingestion is developed using Kafka connectors in the <a href="https://docs.confluent.io/platform/current/quickstart/ce-docker-quickstart.html" target="_blank" rel="noopener noreferrer">local Confluent platform<i class="fas fa-external-link-square-alt ms-1"></i></a> where the <a href="https://debezium.io/documentation/reference/stable/connectors/postgresql.html" target="_blank" rel="noopener noreferrer">Debezium for PostgreSQL<i class="fas fa-external-link-square-alt ms-1"></i></a> is used as the source connector and the <a href="https://lenses.io/blog/2020/11/new-kafka-to-S3-connector/" target="_blank" rel="noopener noreferrer">Lenses S3 sink connector<i class="fas fa-external-link-square-alt ms-1"></i></a> is used as the sink connector. We confirmed the order creation and update events are captured as expected, and it is ready for production deployment. In this post, we&rsquo;ll build the CDC part of the solution on AWS using <a href="https://aws.amazon.com/msk/" target="_blank" rel="noopener noreferrer">Amazon MSK<i class="fas fa-external-link-square-alt ms-1"></i></a> and <a href="https://aws.amazon.com/msk/features/msk-connect/" target="_blank" rel="noopener noreferrer">MSK Connect<i class="fas fa-external-link-square-alt ms-1"></i></a>.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2021-12-12-datalake-demo-part2/featured.png" length="164526" type="image/png"/></item><item><title>Data Lake Demo using Change Data Capture (CDC) on AWS – Part 1 Local Development</title><link>https://jaehyeon.me/blog/2021-12-05-datalake-demo-part1/</link><pubDate>Sun, 05 Dec 2021 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2021-12-05-datalake-demo-part1/</guid><description><![CDATA[<p><a href="https://www.redhat.com/en/topics/integration/what-is-change-data-capture#what-is-cdc" target="_blank" rel="noopener noreferrer">Change data capture (CDC)<i class="fas fa-external-link-square-alt ms-1"></i></a> is a proven data integration pattern that has a wide range of applications. Among those, data replication to data lakes is a good use case in data engineering. Coupled with <a href="https://lakefs.io/hudi-iceberg-and-delta-lake-data-lake-table-formats-compared/" target="_blank" rel="noopener noreferrer">best-in-breed data lake formats<i class="fas fa-external-link-square-alt ms-1"></i></a> such as <a href="https://hudi.apache.org/" target="_blank" rel="noopener noreferrer">Apache Hudi<i class="fas fa-external-link-square-alt ms-1"></i></a>, we can build an efficient data replication solution. This is the first post of the data lake demo series. Over time, we&rsquo;ll build a data lake that uses CDC. As a starting point, we&rsquo;ll discuss the source database and CDC streaming infrastructure in the local environment.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2021-12-05-datalake-demo-part1/featured.png" length="164526" type="image/png"/></item><item><title>Local Development of AWS Glue 3.0 and Later</title><link>https://jaehyeon.me/blog/2021-11-14-glue-3-local-development/</link><pubDate>Sun, 14 Nov 2021 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2021-11-14-glue-3-local-development/</guid><description><![CDATA[<p>In an <a href="/blog/2021-08-20-glue-local-development">earlier post</a>, I demonstrated how to set up a local development environment for AWS Glue 1.0 and 2.0 using a <a href="https://aws.amazon.com/blogs/big-data/developing-aws-glue-etl-jobs-locally-using-a-container/" target="_blank" rel="noopener noreferrer">docker image that is published by the AWS Glue team<i class="fas fa-external-link-square-alt ms-1"></i></a> and the <a href="https://code.visualstudio.com/docs/remote/containers" target="_blank" rel="noopener noreferrer">Visual Studio Code Remote – Containers<i class="fas fa-external-link-square-alt ms-1"></i></a> extension. Recently <a href="https://aws.amazon.com/about-aws/whats-new/2021/08/spark-3-1-runtime-aws-glue-3-0/" target="_blank" rel="noopener noreferrer">AWS Glue 3.0 was released<i class="fas fa-external-link-square-alt ms-1"></i></a>, but a docker image for this version is not published. In this post, I&rsquo;ll illustrate how to create a development environment for AWS Glue 3.0 (and later versions) by building a custom docker image.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2021-11-14-glue-3-local-development/featured.png" length="30923" type="image/png"/></item><item><title>AWS Glue Local Development with Docker and Visual Studio Code</title><link>https://jaehyeon.me/blog/2021-08-20-glue-local-development/</link><pubDate>Fri, 20 Aug 2021 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2021-08-20-glue-local-development/</guid><description><![CDATA[<p>As described in the product page, <a href="https://aws.amazon.com/glue" target="_blank" rel="noopener noreferrer">AWS Glue<i class="fas fa-external-link-square-alt ms-1"></i></a> is a <em>serverless</em> data integration service that makes it easy to discover, prepare, and combine data for analytics, machine learning, and application development. For development, a development endpoint is recommended, but it can be costly, inconvenient or <a href="https://docs.aws.amazon.com/glue/latest/dg/reduced-start-times-spark-etl-jobs.html" target="_blank" rel="noopener noreferrer">unavailable (for Glue 2.0)<i class="fas fa-external-link-square-alt ms-1"></i></a>. The <a href="https://aws.amazon.com/blogs/big-data/developing-aws-glue-etl-jobs-locally-using-a-container/" target="_blank" rel="noopener noreferrer">AWS Glue team published a Docker image<i class="fas fa-external-link-square-alt ms-1"></i></a> that includes the AWS Glue binaries and all the dependencies packaged together. After inspecting it, I find some modifications are necessary in order to build a development environment on it. In this post, I&rsquo;ll demonstrate how to build development environments for AWS Glue 1.0 and 2.0 using the Docker image and the <a href="https://code.visualstudio.com/docs/remote/containers" target="_blank" rel="noopener noreferrer">Visual Studio Code Remote - Containers<i class="fas fa-external-link-square-alt ms-1"></i></a> extension.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2021-08-20-glue-local-development/featured.png" length="19535" type="image/png"/></item><item><title>Thoughts on Apache Airflow AWS Lambda Operator</title><link>https://jaehyeon.me/blog/2020-04-13-airflow-lambda-operator/</link><pubDate>Mon, 13 Apr 2020 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2020-04-13-airflow-lambda-operator/</guid><description><![CDATA[<p><a href="https://airflow.apache.org/" target="_blank" rel="noopener noreferrer">Apache Airflow<i class="fas fa-external-link-square-alt ms-1"></i></a> is a popular open-source workflow management platform. Typically tasks run remotely by <a href="http://www.celeryproject.org/" target="_blank" rel="noopener noreferrer">Celery<i class="fas fa-external-link-square-alt ms-1"></i></a> workers for scalability. In AWS, however, scalability can also be achieved using serverless computing services in a simpler way. For example, the <a href="https://airflow.apache.org/docs/apache-airflow-providers-amazon/stable/operators/ecs.html" target="_blank" rel="noopener noreferrer">ECS Operator<i class="fas fa-external-link-square-alt ms-1"></i></a> allows to run <em>dockerized</em> tasks and, with the <em>Fargate</em> launch type, they can run in a serverless environment.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2020-04-13-airflow-lambda-operator/featured.png" length="44994" type="image/png"/></item></channel></rss>