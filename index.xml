<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Jaehyeon Kim</title><link>https://jaehyeon.me/</link><description>Recent content on Jaehyeon Kim</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>Copyright © 2023-2024 Jaehyeon Kim. All Rights Reserved.</copyright><lastBuildDate>Wed, 02 Oct 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://jaehyeon.me/index.xml" rel="self" type="application/rss+xml"/><item><title>Apache Beam Python Examples - Part 5 Call RPC Service in Batch using Stateless DoFn</title><link>https://jaehyeon.me/blog/2024-09-18-beam-examples-5/</link><pubDate>Wed, 18 Sep 2024 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2024-09-18-beam-examples-5/</guid><description><![CDATA[<p>In the <a href="/blog/2024-08-15-beam-examples-4">previous post</a>, we developed an Apache Beam pipeline where the input data is augmented by an <strong>Remote Procedure Call (RPC)</strong> service. Each input element performs an RPC call and the output is enriched by the response. This is not an efficient way of accessing an external service provided that the service can accept more than one element. In this post, we discuss how to enhance the pipeline so that a single RPC call is made for a bundle of elements, which can save a significant amount time compared to making a call for each element.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2024-09-18-beam-examples-5/featured.png" length="95285" type="image/png"/></item><item><title>Guide to Running DBT in Production</title><link>https://jaehyeon.me/blog/2024-09-13-dbt-guide/</link><pubDate>Fri, 13 Sep 2024 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2024-09-13-dbt-guide/</guid><description><![CDATA[<p>In the <a href="/blog/2024-09-05-dbt-cicd-demo">previous post</a>, we started discussing a <em>continuous integration/continuous delivery (CI/CD)</em> process of a <em>dbt</em> project by introducing two GitHub Actions workflows - <code>slim-ci</code> and <code>deploy</code>. The former is triggered when a pull request is created to the main branch, and it builds only modified models and its first-order children in a <em>ci</em> dataset, followed by performing tests on them. The second workflow gets triggered once a pull request is merged. Beginning with running unit tests, it packages the <em>dbt</em> project as a Docker container and publishes to <em>Artifact Registry</em>. In this post, we focus on how to deploy a <em>dbt</em> project in multiple environments while walking through the entire CI/CD process step-by-step.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2024-09-13-dbt-guide/featured.png" length="71185" type="image/png"/></item><item><title>DBT CI/CD Demo with BigQuery and GitHub Actions</title><link>https://jaehyeon.me/blog/2024-09-05-dbt-cicd-demo/</link><pubDate>Thu, 05 Sep 2024 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2024-09-05-dbt-cicd-demo/</guid><description><![CDATA[<p>Continuous integration (CI) is the process of ensuring new code integrates with the larger code base, and it puts a great emphasis on testing automation to check that the application is not broken whenever new commits are integrated into the main branch. Continuous delivery (CD) is an extension of continuous integration since it automatically deploys all code changes to a testing and/or production environment after the build stage. CI/CD helps development teams avoid bugs and code failures while maintaining a continuous cycle of software development and updates. In this post, we discuss how to set up a CI/CD pipeline for a <a href="https://www.getdbt.com/" target="_blank" rel="noopener noreferrer">data build tool (<em>dbt</em>)<i class="fas fa-external-link-square-alt ms-1"></i></a> project using <a href="https://github.com/features/actions" target="_blank" rel="noopener noreferrer">GitHub Actions<i class="fas fa-external-link-square-alt ms-1"></i></a> where <a href="https://cloud.google.com/bigquery?hl=en" target="_blank" rel="noopener noreferrer">BigQuery<i class="fas fa-external-link-square-alt ms-1"></i></a> is used as the target data warehouse.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2024-09-05-dbt-cicd-demo/featured.png" length="60835" type="image/png"/></item><item><title>Cache Data on Apache Beam Pipelines Using a Shared Object</title><link>https://jaehyeon.me/blog/2024-08-22-cache-using-shared-object/</link><pubDate>Thu, 22 Aug 2024 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2024-08-22-cache-using-shared-object/</guid><description><![CDATA[<p>I recently contributed to Apache Beam by adding a common pipeline pattern - <a href="https://beam.apache.org/documentation/patterns/shared-class/" target="_blank" rel="noopener noreferrer"><em>Cache data using a shared object</em><i class="fas fa-external-link-square-alt ms-1"></i></a>. Both batch and streaming pipelines are introduced, and they utilise the <a href="https://beam.apache.org/releases/pydoc/current/_modules/apache_beam/utils/shared.html#Shared" target="_blank" rel="noopener noreferrer"><code>Shared</code> class<i class="fas fa-external-link-square-alt ms-1"></i></a> of the Python SDK to enrich <code>PCollection</code> elements. This pattern can be more memory-efficient than side inputs, simpler than a stateful <code>DoFn</code>, and more performant than calling an external service, because it does not have to access an external service for every element or bundle of elements. In this post, we discuss this pattern in more details with batch and streaming use cases. For the latter, we configure the cache gets refreshed periodically.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2024-08-22-cache-using-shared-object/featured.png" length="49574" type="image/png"/></item><item><title>Apache Beam Python Examples - Part 4 Call RPC Service for Data Augmentation</title><link>https://jaehyeon.me/blog/2024-08-15-beam-examples-4/</link><pubDate>Thu, 15 Aug 2024 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2024-08-15-beam-examples-4/</guid><description>&lt;p>In this post, we develop an Apache Beam pipeline where the input data is augmented by an &lt;strong>Remote Procedure Call (RPC)&lt;/strong> service. Each input element performs an RPC call and the output is enriched by the response. This is not an efficient way of accessing an external service provided that the service can accept more than one element. In the subsequent two posts, we will discuss updated pipelines that make RPC calls more efficiently. We begin with illustrating how to manage development resources followed by demonstrating the RPC service that we use in this series. Finally, we develop a Beam pipeline that accesses the external service to augment the input elements.&lt;/p></description><enclosure url="https://jaehyeon.me/blog/2024-08-15-beam-examples-4/featured.png" length="93408" type="image/png"/></item><item><title>Apache Beam Python Examples - Part 3 Build Sport Activity Tracker with/without SQL</title><link>https://jaehyeon.me/blog/2024-08-01-beam-examples-3/</link><pubDate>Thu, 01 Aug 2024 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2024-08-01-beam-examples-3/</guid><description><![CDATA[<p>In this post, we develop two Apache Beam pipelines that track sport activities of users and output their speed periodically. The first pipeline uses native transforms and <a href="https://beam.apache.org/documentation/dsls/sql/overview/" target="_blank" rel="noopener noreferrer">Beam SQL<i class="fas fa-external-link-square-alt ms-1"></i></a> is used for the latter. While <em>Beam SQL</em> can be useful in some situations, its features in the Python SDK are not complete compared to the Java SDK. Therefore, we are not able to build the required tracking pipeline using it. We end up discussing potential improvements of <em>Beam SQL</em> so that it can be used for building competitive applications with the Python SDK.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2024-08-01-beam-examples-3/featured.png" length="94507" type="image/png"/></item><item><title>Apache Beam Python Examples - Part 2 Calculate Average Word Length with/without Fixed Look back</title><link>https://jaehyeon.me/blog/2024-07-18-beam-examples-2/</link><pubDate>Thu, 18 Jul 2024 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2024-07-18-beam-examples-2/</guid><description>&lt;p>In this post, we develop two Apache Beam pipelines that calculate average word lengths from input texts that are ingested by a Kafka topic. They obtain the statistics in different angles. The first pipeline emits the global average lengths whenever a new input text arrives while the latter triggers those values in a sliding time window.&lt;/p></description><enclosure url="https://jaehyeon.me/blog/2024-07-18-beam-examples-2/featured.png" length="96924" type="image/png"/></item><item><title>Apache Beam Python Examples - Part 1 Calculate K Most Frequent Words and Max Word Length</title><link>https://jaehyeon.me/blog/2024-07-04-beam-examples-1/</link><pubDate>Thu, 04 Jul 2024 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2024-07-04-beam-examples-1/</guid><description><![CDATA[<p>In this series, we develop <a href="https://beam.apache.org/" target="_blank" rel="noopener noreferrer">Apache Beam<i class="fas fa-external-link-square-alt ms-1"></i></a> Python pipelines. The majority of them are from <a href="https://www.packtpub.com/en-us/product/building-big-data-pipelines-with-apache-beam-9781800564930" target="_blank" rel="noopener noreferrer">Building Big Data Pipelines with Apache Beam by Jan Lukavský<i class="fas fa-external-link-square-alt ms-1"></i></a>. Mainly relying on the Java SDK, the book teaches fundamentals of Apache Beam using hands-on tasks, and we convert those tasks using the Python SDK. We focus on streaming pipelines, and they are deployed on a local (or embedded) <a href="https://flink.apache.org/" target="_blank" rel="noopener noreferrer">Apache Flink<i class="fas fa-external-link-square-alt ms-1"></i></a> cluster using the <a href="https://beam.apache.org/documentation/runners/flink/" target="_blank" rel="noopener noreferrer">Apache Flink Runner<i class="fas fa-external-link-square-alt ms-1"></i></a>. Beginning with setting up the development environment, we build two pipelines that obtain top K most frequent words and the word that has the longest word length in this post.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2024-07-04-beam-examples-1/featured.png" length="96881" type="image/png"/></item><item><title>Deploy Python Stream Processing App on Kubernetes - Part 2 Beam Pipeline on Flink Runner</title><link>https://jaehyeon.me/blog/2024-06-06-beam-deploy-2/</link><pubDate>Thu, 06 Jun 2024 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2024-06-06-beam-deploy-2/</guid><description><![CDATA[<p>In this post, we develop an <a href="https://beam.apache.org/" target="_blank" rel="noopener noreferrer">Apache Beam<i class="fas fa-external-link-square-alt ms-1"></i></a> pipeline using the <a href="https://beam.apache.org/documentation/sdks/python/" target="_blank" rel="noopener noreferrer">Python SDK<i class="fas fa-external-link-square-alt ms-1"></i></a> and deploy it on an <a href="https://flink.apache.org/" target="_blank" rel="noopener noreferrer">Apache Flink<i class="fas fa-external-link-square-alt ms-1"></i></a> cluster via the <a href="https://beam.apache.org/documentation/runners/flink/" target="_blank" rel="noopener noreferrer">Apache Flink Runner<i class="fas fa-external-link-square-alt ms-1"></i></a>. Same as <a href="/blog/2024-05-30-beam-deploy-1">Part I</a>, we deploy a Kafka cluster using the <a href="https://strimzi.io/" target="_blank" rel="noopener noreferrer">Strimzi Operator<i class="fas fa-external-link-square-alt ms-1"></i></a> on a <a href="https://minikube.sigs.k8s.io/docs/" target="_blank" rel="noopener noreferrer">minikube<i class="fas fa-external-link-square-alt ms-1"></i></a> cluster as the pipeline uses <a href="https://kafka.apache.org/" target="_blank" rel="noopener noreferrer">Apache Kafka<i class="fas fa-external-link-square-alt ms-1"></i></a> topics for its data source and sink. Then, we develop the pipeline as a Python package and add the package to a custom Docker image so that Python user code can be executed externally. For deployment, we create a Flink session cluster via the <a href="https://nightlies.apache.org/flink/flink-kubernetes-operator-docs-main/" target="_blank" rel="noopener noreferrer">Flink Kubernetes Operator<i class="fas fa-external-link-square-alt ms-1"></i></a>, and deploy the pipeline using a Kubernetes job. Finally, we check the output of the application by sending messages to the input Kafka topic using a Python producer application.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2024-06-06-beam-deploy-2/featured.png" length="58020" type="image/png"/></item><item><title>Deploy Python Stream Processing App on Kubernetes - Part 1 PyFlink Application</title><link>https://jaehyeon.me/blog/2024-05-30-beam-deploy-1/</link><pubDate>Thu, 30 May 2024 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2024-05-30-beam-deploy-1/</guid><description><![CDATA[<p><a href="https://nightlies.apache.org/flink/flink-kubernetes-operator-docs-main/docs/concepts/overview/" target="_blank" rel="noopener noreferrer">Flink Kubernetes Operator<i class="fas fa-external-link-square-alt ms-1"></i></a> acts as a control plane to manage the complete deployment lifecycle of Apache Flink applications. With the operator, we can simplify deployment and management of Python stream processing applications. In this series, we discuss how to deploy a PyFlink application and Python Apache Beam pipeline on the <a href="https://beam.apache.org/documentation/runners/flink/" target="_blank" rel="noopener noreferrer">Flink Runner<i class="fas fa-external-link-square-alt ms-1"></i></a> on Kubernetes. In Part 1, we first deploy a Kafka cluster on a <a href="https://minikube.sigs.k8s.io/docs/" target="_blank" rel="noopener noreferrer">minikube<i class="fas fa-external-link-square-alt ms-1"></i></a> cluster as the source and sink of the PyFlink application are Kafka topics. Then, the application source is packaged in a custom Docker image and deployed on the minikube cluster using the Flink Kubernetes Operator. Finally, the output of the application is checked by sending messages to the input Kafka topic using a Python producer application.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2024-05-30-beam-deploy-1/featured.png" length="64457" type="image/png"/></item><item><title>Apache Beam Local Development with Python - Part 5 Testing Pipelines</title><link>https://jaehyeon.me/blog/2024-05-09-beam-local-dev-5/</link><pubDate>Thu, 09 May 2024 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2024-05-09-beam-local-dev-5/</guid><description><![CDATA[<p>We developed batch and streaming pipelines in <a href="/blog/2024-04-04-beam-local-dev-2">Part 2</a> and <a href="/blog/2024-05-02-beam-local-dev-4">Part 4</a>. Often it is faster and simpler to identify and fix bugs on the pipeline code by performing local unit testing. Moreover, especially when it comes to creating a streaming pipeline, unit testing cases can facilitate development further by using <a href="https://beam.apache.org/releases/pydoc/2.22.0/_modules/apache_beam/testing/test_stream.html" target="_blank" rel="noopener noreferrer">TestStream<i class="fas fa-external-link-square-alt ms-1"></i></a> as it allows us to advance <a href="https://beam.apache.org/documentation/basics/#watermark" target="_blank" rel="noopener noreferrer">watermarks<i class="fas fa-external-link-square-alt ms-1"></i></a> or processing time according to different scenarios. In this post, we discuss how to perform unit testing of the batch and streaming pipelines that we developed earlier.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2024-05-09-beam-local-dev-5/featured.png" length="53603" type="image/png"/></item><item><title>Apache Beam Local Development with Python - Part 4 Streaming Pipelines</title><link>https://jaehyeon.me/blog/2024-05-02-beam-local-dev-4/</link><pubDate>Thu, 02 May 2024 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2024-05-02-beam-local-dev-4/</guid><description><![CDATA[<p>In <a href="/blog/2024-04-18-beam-local-dev-3">Part 3</a>, we discussed the portability layer of <a href="https://beam.apache.org/" target="_blank" rel="noopener noreferrer">Apache Beam<i class="fas fa-external-link-square-alt ms-1"></i></a> as it helps understand (1) how Python pipelines run on the <a href="https://beam.apache.org/documentation/runners/flink/" target="_blank" rel="noopener noreferrer">Flink Runner<i class="fas fa-external-link-square-alt ms-1"></i></a> and (2) how multiple SDKs can be used in a single pipeline, followed by demonstrating local Flink and Kafka cluster creation for developing streaming pipelines. In this post, we build a streaming pipeline that aggregates page visits by user in a <a href="https://beam.apache.org/documentation/programming-guide/#fixed-time-windows" target="_blank" rel="noopener noreferrer">fixed time window<i class="fas fa-external-link-square-alt ms-1"></i></a> of 20 seconds. Two versions of the pipeline are created with/without relying on <a href="https://beam.apache.org/documentation/dsls/sql/overview/" target="_blank" rel="noopener noreferrer">Beam SQL<i class="fas fa-external-link-square-alt ms-1"></i></a>.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2024-05-02-beam-local-dev-4/featured.png" length="54556" type="image/png"/></item><item><title>Apache Beam Local Development with Python - Part 3 Flink Runner</title><link>https://jaehyeon.me/blog/2024-04-18-beam-local-dev-3/</link><pubDate>Thu, 18 Apr 2024 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2024-04-18-beam-local-dev-3/</guid><description><![CDATA[<p>In this series, we discuss local development of <a href="https://beam.apache.org/" target="_blank" rel="noopener noreferrer">Apache Beam<i class="fas fa-external-link-square-alt ms-1"></i></a> pipelines using Python. In the previous posts, we mainly talked about Batch pipelines with/without Beam SQL. Beam pipelines are portable between batch and streaming semantics, and we will discuss streaming pipeline development in this and the next posts. While there are multiple Beam Runners, not every Runner supports Python or some Runners have too limited features in streaming semantics - see <a href="https://beam.apache.org/documentation/runners/capability-matrix/" target="_blank" rel="noopener noreferrer">Beam Capability Matrix<i class="fas fa-external-link-square-alt ms-1"></i></a> for details. So far, the Apache Flink and Google Cloud Dataflow Runners are the best options, and we will use the <a href="https://beam.apache.org/documentation/runners/flink/" target="_blank" rel="noopener noreferrer">Flink Runner<i class="fas fa-external-link-square-alt ms-1"></i></a> in this series. This post begins with demonstrating the <em>portability layer</em> of Apache Beam as it helps understand (1) how a pipeline developed by the Python SDK can be executed in the Flink Runner that only understands Java JAR and (2) how multiple SDKs can be used in a single pipeline. Then we discuss how to start up/tear down local Flink and Kafka clusters using bash scripts. Finally, we end up demonstrating a simple streaming pipeline, which reads and writes website visit logs from and to Kafka topics.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2024-04-18-beam-local-dev-3/featured.png" length="262307" type="image/png"/></item><item><title>Apache Beam Local Development with Python - Part 2 Batch Pipelines</title><link>https://jaehyeon.me/blog/2024-04-04-beam-local-dev-2/</link><pubDate>Thu, 04 Apr 2024 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2024-04-04-beam-local-dev-2/</guid><description><![CDATA[<p>In this series, we discuss local development of <a href="https://beam.apache.org/" target="_blank" rel="noopener noreferrer">Apache Beam<i class="fas fa-external-link-square-alt ms-1"></i></a> pipelines using Python. A basic Beam pipeline was introduced in <a href="/blog/2024-03-28-beam-local-dev-1">Part 1</a>, followed by demonstrating how to utilise Jupyter notebooks, <a href="https://beam.apache.org/documentation/dsls/sql/overview/" target="_blank" rel="noopener noreferrer">Beam SQL<i class="fas fa-external-link-square-alt ms-1"></i></a> and <a href="https://beam.apache.org/documentation/dsls/dataframes/overview/" target="_blank" rel="noopener noreferrer">Beam DataFrames<i class="fas fa-external-link-square-alt ms-1"></i></a>. In this post, we discuss Batch pipelines that aggregate website visit log by user and time. The pipelines are developed with and without <em>Beam SQL</em>. Additionally, each pipeline is implemented on a Jupyter notebook for demonstration.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2024-04-04-beam-local-dev-2/featured.png" length="55405" type="image/png"/></item><item><title>Apache Beam Local Development with Python - Part 1 Pipeline, Notebook, SQL and DataFrame</title><link>https://jaehyeon.me/blog/2024-03-28-beam-local-dev-1/</link><pubDate>Thu, 28 Mar 2024 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2024-03-28-beam-local-dev-1/</guid><description><![CDATA[<p><a href="https://beam.apache.org/" target="_blank" rel="noopener noreferrer">Apache Beam<i class="fas fa-external-link-square-alt ms-1"></i></a> and <a href="https://flink.apache.org/" target="_blank" rel="noopener noreferrer">Apache Flink<i class="fas fa-external-link-square-alt ms-1"></i></a> are open-source frameworks for parallel, distributed data processing at scale. Flink has DataStream and Table/SQL APIs and the former has more capacity to develop sophisticated data streaming applications. The DataStream API of PyFlink, Flink&rsquo;s Python API, however, is not as complete as its Java counterpart, and it doesn&rsquo;t provide enough capability to extend when there are missing features in Python. Recently I had a chance to look through Apache Beam and found it supports more possibility to extend and/or customise its features.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2024-03-28-beam-local-dev-1/featured.png" length="88260" type="image/png"/></item><item><title>Data Build Tool (dbt) Pizza Shop Demo - Part 6 ETL on Amazon Athena via Airflow</title><link>https://jaehyeon.me/blog/2024-03-14-dbt-pizza-shop-6/</link><pubDate>Thu, 14 Mar 2024 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2024-03-14-dbt-pizza-shop-6/</guid><description><![CDATA[<p>In <a href="/blog/2024-03-07-dbt-pizza-shop-5">Part 5</a>, we developed a <a href="https://docs.getdbt.com/docs/introduction" target="_blank" rel="noopener noreferrer">dbt<i class="fas fa-external-link-square-alt ms-1"></i></a> project that that targets <a href="https://iceberg.apache.org/" target="_blank" rel="noopener noreferrer">Apache Iceberg<i class="fas fa-external-link-square-alt ms-1"></i></a> where transformations are performed on <a href="https://aws.amazon.com/athena/" target="_blank" rel="noopener noreferrer">Amazon Athena<i class="fas fa-external-link-square-alt ms-1"></i></a>. Two dimension tables that keep product and user records are created as <a href="https://en.wikipedia.org/wiki/Slowly_changing_dimension" target="_blank" rel="noopener noreferrer">Type 2 slowly changing dimension (SCD Type 2)<i class="fas fa-external-link-square-alt ms-1"></i></a> tables, and one transactional fact table is built to keep pizza orders. To improve query performance, the fact table is denormalized to pre-join records from the dimension tables using the array and struct data types. In this post, we discuss how to set up an ETL process on the project using Apache Airflow.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2024-03-14-dbt-pizza-shop-6/featured.png" length="82921" type="image/png"/></item><item><title>Data Build Tool (dbt) Pizza Shop Demo - Part 5 Modelling on Amazon Athena</title><link>https://jaehyeon.me/blog/2024-03-07-dbt-pizza-shop-5/</link><pubDate>Thu, 07 Mar 2024 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2024-03-07-dbt-pizza-shop-5/</guid><description><![CDATA[<p>In <a href="%28/blog/2024-01-18-dbt-pizza-shop-1%29">Part 1</a> and <a href="%28/blog/2024-02-08-dbt-pizza-shop-3%29">Part 3</a>, we developed <a href="https://docs.getdbt.com/docs/introduction" target="_blank" rel="noopener noreferrer">data build tool (dbt)<i class="fas fa-external-link-square-alt ms-1"></i></a> projects that target <em>PostgreSQL</em> and <em>BigQuery</em> using fictional pizza shop data. The data is modelled by <a href="https://en.wikipedia.org/wiki/Slowly_changing_dimension" target="_blank" rel="noopener noreferrer">SCD type 2<i class="fas fa-external-link-square-alt ms-1"></i></a> dimension tables and one transactional fact table. While the order records should be joined with dimension tables to get complete details for <em>PostgreSQL</em>, the fact table is denormalized using <a href="https://cloud.google.com/bigquery/docs/best-practices-performance-nested" target="_blank" rel="noopener noreferrer">nested and repeated fields<i class="fas fa-external-link-square-alt ms-1"></i></a> to improve query performance for <em>BigQuery</em>.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2024-03-07-dbt-pizza-shop-5/featured.png" length="61499" type="image/png"/></item><item><title>Data Build Tool (dbt) Pizza Shop Demo - Part 4 ETL on BigQuery via Airflow</title><link>https://jaehyeon.me/blog/2024-02-22-dbt-pizza-shop-4/</link><pubDate>Thu, 22 Feb 2024 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2024-02-22-dbt-pizza-shop-4/</guid><description><![CDATA[<p>In <a href="/blog/2024-02-08-dbt-pizza-shop-3">Part 3</a>, we developed a <a href="https://docs.getdbt.com/docs/introduction" target="_blank" rel="noopener noreferrer">dbt<i class="fas fa-external-link-square-alt ms-1"></i></a> project that targets Google BigQuery with fictional pizza shop data. Two dimension tables that keep product and user records are created as <a href="https://en.wikipedia.org/wiki/Slowly_changing_dimension" target="_blank" rel="noopener noreferrer">Type 2 slowly changing dimension (SCD Type 2)<i class="fas fa-external-link-square-alt ms-1"></i></a> tables, and one transactional fact table is built to keep pizza orders. The fact table is denormalized using <a href="https://cloud.google.com/bigquery/docs/best-practices-performance-nested" target="_blank" rel="noopener noreferrer">nested and repeated fields<i class="fas fa-external-link-square-alt ms-1"></i></a> for improving query performance. In this post, we discuss how to set up an ETL process on the project using Apache Airflow.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2024-02-22-dbt-pizza-shop-4/featured.png" length="89588" type="image/png"/></item><item><title>Data Build Tool (dbt) Pizza Shop Demo - Part 3 Modelling on BigQuery</title><link>https://jaehyeon.me/blog/2024-02-08-dbt-pizza-shop-3/</link><pubDate>Thu, 08 Feb 2024 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2024-02-08-dbt-pizza-shop-3/</guid><description><![CDATA[<p>In this series, we discuss practical examples of data warehouse and lakehouse development where data transformation is performed by the <a href="https://docs.getdbt.com/docs/introduction" target="_blank" rel="noopener noreferrer">data build tool (dbt)<i class="fas fa-external-link-square-alt ms-1"></i></a> and ETL is managed by <a href="https://airflow.apache.org/" target="_blank" rel="noopener noreferrer">Apache Airflow<i class="fas fa-external-link-square-alt ms-1"></i></a>. In <a href="/blog/2024-01-18-dbt-pizza-shop-1">Part 1</a>, we developed a <em>dbt</em> project on PostgreSQL using fictional pizza shop data. At the end, the data sets are modelled by two <a href="https://en.wikipedia.org/wiki/Slowly_changing_dimension" target="_blank" rel="noopener noreferrer">SCD type 2<i class="fas fa-external-link-square-alt ms-1"></i></a> dimension tables and one transactional fact table. In this post, we create a new <em>dbt</em> project that targets <a href="https://cloud.google.com/bigquery" target="_blank" rel="noopener noreferrer">Google BigQuery<i class="fas fa-external-link-square-alt ms-1"></i></a>. While the dimension tables are kept by the same SCD type 2 approach, the fact table is denormalized using <a href="https://cloud.google.com/bigquery/docs/best-practices-performance-nested" target="_blank" rel="noopener noreferrer">nested and repeated fields<i class="fas fa-external-link-square-alt ms-1"></i></a>, which potentially can improve query performance by pre-joining corresponding dimension records.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2024-02-08-dbt-pizza-shop-3/featured.png" length="70297" type="image/png"/></item><item><title>Data Build Tool (dbt) Pizza Shop Demo - Part 2 ETL on PostgreSQL via Airflow</title><link>https://jaehyeon.me/blog/2024-01-25-dbt-pizza-shop-2/</link><pubDate>Thu, 25 Jan 2024 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2024-01-25-dbt-pizza-shop-2/</guid><description><![CDATA[<p>In this series of posts, we discuss data warehouse/lakehouse examples using <a href="https://docs.getdbt.com/docs/introduction" target="_blank" rel="noopener noreferrer">data build tool (dbt)<i class="fas fa-external-link-square-alt ms-1"></i></a> including ETL orchestration with Apache Airflow. In Part 1, we developed a <em>dbt</em> project on PostgreSQL with fictional pizza shop data. Two dimension tables that keep product and user records are created as <a href="https://en.wikipedia.org/wiki/Slowly_changing_dimension" target="_blank" rel="noopener noreferrer">Type 2 slowly changing dimension (SCD Type 2)<i class="fas fa-external-link-square-alt ms-1"></i></a> tables, and one transactional fact table is built to keep pizza orders. In this post, we discuss how to set up an ETL process on the project using Apache Airflow.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2024-01-25-dbt-pizza-shop-2/featured.png" length="77355" type="image/png"/></item><item><title>Data Build Tool (dbt) Pizza Shop Demo - Part 1 Modelling on PostgreSQL</title><link>https://jaehyeon.me/blog/2024-01-18-dbt-pizza-shop-1/</link><pubDate>Thu, 18 Jan 2024 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2024-01-18-dbt-pizza-shop-1/</guid><description><![CDATA[<p>The <a href="https://docs.getdbt.com/docs/introduction" target="_blank" rel="noopener noreferrer">data build tool (dbt)<i class="fas fa-external-link-square-alt ms-1"></i></a> is a popular data transformation tool for data warehouse development. Moreover, it can be used for <a href="https://www.databricks.com/glossary/data-lakehouse" target="_blank" rel="noopener noreferrer">data lakehouse<i class="fas fa-external-link-square-alt ms-1"></i></a> development thanks to open table formats such as Apache Iceberg, Apache Hudi and Delta Lake. <em>dbt</em> supports key AWS analytics services and I wrote a series of posts that discuss how to utilise <em>dbt</em> with <a href="/blog/2022-09-28-dbt-on-aws-part-1-redshift">Redshift</a>, <a href="/blog/2022-10-09-dbt-on-aws-part-2-glue">Glue</a>, <a href="/blog/2022-10-19-dbt-on-aws-part-3-emr-ec2">EMR on EC2</a>, <a href="/blog/2022-11-01-dbt-on-aws-part-4-emr-eks">EMR on EKS</a>, and <a href="/blog/2023-04-12-integrate-glue-schema-registry">Athena</a>. Those posts focus on platform integration, however, they do not show realistic ETL scenarios.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2024-01-18-dbt-pizza-shop-1/featured.png" length="85093" type="image/png"/></item><item><title>Kafka Development on Kubernetes - Part 3 Kafka Connect</title><link>https://jaehyeon.me/blog/2024-01-11-kafka-development-on-k8s-part-3/</link><pubDate>Thu, 11 Jan 2024 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2024-01-11-kafka-development-on-k8s-part-3/</guid><description><![CDATA[<p><a href="https://kafka.apache.org/documentation/#connect" target="_blank" rel="noopener noreferrer">Kafka Connect<i class="fas fa-external-link-square-alt ms-1"></i></a> is a tool for scalably and reliably streaming data between Apache Kafka and other systems. It makes it simple to quickly define connectors that move large collections of data into and out of Kafka. In this post, we discuss how to set up a data ingestion pipeline using Kafka connectors. Fake customer and order data is ingested into Kafka topics using the <a href="https://github.com/awslabs/amazon-msk-data-generator" target="_blank" rel="noopener noreferrer">MSK Data Generator<i class="fas fa-external-link-square-alt ms-1"></i></a>. Also, we use the <a href="https://www.confluent.io/hub/confluentinc/kafka-connect-s3" target="_blank" rel="noopener noreferrer">Confluent S3<i class="fas fa-external-link-square-alt ms-1"></i></a> sink connector to save the messages of the topics into a S3 bucket. The Kafka Connect servers and individual connectors are deployed using the custom resources of <a href="https://strimzi.io/" target="_blank" rel="noopener noreferrer">Strimzi<i class="fas fa-external-link-square-alt ms-1"></i></a> on Kubernetes.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2024-01-11-kafka-development-on-k8s-part-3/featured.png" length="97270" type="image/png"/></item><item><title>Kafka Development on Kubernetes - Part 2 Producer and Consumer</title><link>https://jaehyeon.me/blog/2024-01-04-kafka-development-on-k8s-part-2/</link><pubDate>Thu, 04 Jan 2024 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2024-01-04-kafka-development-on-k8s-part-2/</guid><description><![CDATA[<p>Apache Kafka has five <a href="https://kafka.apache.org/documentation/#api" target="_blank" rel="noopener noreferrer">core APIs<i class="fas fa-external-link-square-alt ms-1"></i></a>, and we can develop applications to send/read streams of data to/from topics in a Kafka cluster using the producer and consumer APIs. While the main Kafka project maintains only the Java APIs, there are several <a href="https://cwiki.apache.org/confluence/display/KAFKA/Clients#Clients-Python" target="_blank" rel="noopener noreferrer">open source projects<i class="fas fa-external-link-square-alt ms-1"></i></a> that provide the Kafka client APIs in Python. In this post, we discuss how to develop Kafka client applications using the <a href="https://kafka-python.readthedocs.io/en/master/index.html" target="_blank" rel="noopener noreferrer">kafka-python<i class="fas fa-external-link-square-alt ms-1"></i></a> package on Kubernetes.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2024-01-04-kafka-development-on-k8s-part-2/featured.png" length="75889" type="image/png"/></item><item><title>Kafka Development on Kubernetes - Part 1 Cluster Setup</title><link>https://jaehyeon.me/blog/2023-12-21-kafka-development-on-k8s-part-1/</link><pubDate>Thu, 21 Dec 2023 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2023-12-21-kafka-development-on-k8s-part-1/</guid><description><![CDATA[<p><a href="https://kafka.apache.org/" target="_blank" rel="noopener noreferrer">Apache Kafka<i class="fas fa-external-link-square-alt ms-1"></i></a> is one of the key technologies for implementing data streaming architectures. <a href="https://strimzi.io/" target="_blank" rel="noopener noreferrer">Strimzi<i class="fas fa-external-link-square-alt ms-1"></i></a> provides a way to run an Apache Kafka cluster and related resources on Kubernetes in various deployment configurations. In this series of posts, we will discuss how to create a Kafka cluster, to develop Kafka client applications in Python and to build a data pipeline using Kafka connectors on Kubernetes.</p>
<ul>
<li><a href="/blog/2023-12-21-kafka-development-on-k8s-part-1/#">Part 1 Cluster Setup</a> (this post)</li>
<li><a href="/blog/2024-01-04-kafka-development-on-k8s-part-2">Part 2 Producer and Consumer</a></li>
<li><a href="/blog/2024-01-11-kafka-development-on-k8s-part-3">Part 3 Kafka Connect</a></li>
</ul>

<h2 id="setup-kafka-cluster" data-numberify>Setup Kafka Cluster<a class="anchor ms-1" href="#setup-kafka-cluster"></a></h2>
<p>The Kafka cluster is deployed using the <a href="https://strimzi.io/" target="_blank" rel="noopener noreferrer">Strimzi Operator<i class="fas fa-external-link-square-alt ms-1"></i></a> on a <a href="https://minikube.sigs.k8s.io/docs/" target="_blank" rel="noopener noreferrer">Minikube<i class="fas fa-external-link-square-alt ms-1"></i></a> cluster. We install Strimzi version 0.27.1 and Kubernetes version 1.24.7 as we use Kafka version 2.8.1 - see <a href="https://strimzi.io/downloads/" target="_blank" rel="noopener noreferrer">this page<i class="fas fa-external-link-square-alt ms-1"></i></a> for details about Kafka version compatibility. Once the <a href="https://minikube.sigs.k8s.io/docs/start/" target="_blank" rel="noopener noreferrer">Minikube CLI<i class="fas fa-external-link-square-alt ms-1"></i></a> and <a href="https://www.docker.com/" target="_blank" rel="noopener noreferrer">Docker<i class="fas fa-external-link-square-alt ms-1"></i></a> are installed, a Minikube cluster can be created by specifying the desired Kubernetes version as shown below.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2023-12-21-kafka-development-on-k8s-part-1/featured.png" length="108975" type="image/png"/></item><item><title>Real Time Streaming with Kafka and Flink - Lab 6 Consume data from Kafka using Lambda</title><link>https://jaehyeon.me/blog/2023-12-14-real-time-streaming-with-kafka-and-flink-7/</link><pubDate>Thu, 14 Dec 2023 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2023-12-14-real-time-streaming-with-kafka-and-flink-7/</guid><description><![CDATA[<p>Amazon MSK can be configured as an <a href="https://docs.aws.amazon.com/lambda/latest/dg/with-msk.html" target="_blank" rel="noopener noreferrer">event source<i class="fas fa-external-link-square-alt ms-1"></i></a> of a Lambda function. Lambda internally polls for new messages from the event source and then synchronously invokes the target Lambda function. With this feature, we can develop a Kafka consumer application in serverless environment where developers can focus on application logic. In this lab, we will discuss how to create a Kafka consumer using a Lambda function.</p>
<ul>
<li><a href="/blog/2023-10-05-real-time-streaming-with-kafka-and-flink-1">Introduction</a></li>
<li><a href="/blog/2023-10-26-real-time-streaming-with-kafka-and-flink-2">Lab 1 Produce data to Kafka using Lambda</a></li>
<li><a href="/blog/2023-11-09-real-time-streaming-with-kafka-and-flink-3">Lab 2 Write data to Kafka from S3 using Flink</a></li>
<li><a href="/blog/2023-11-16-real-time-streaming-with-kafka-and-flink-4">Lab 3 Transform and write data to S3 from Kafka using Flink</a></li>
<li><a href="/blog/2023-11-23-real-time-streaming-with-kafka-and-flink-5">Lab 4 Clean, Aggregate, and Enrich Events with Flink</a></li>
<li><a href="/blog/2023-11-30-real-time-streaming-with-kafka-and-flink-6">Lab 5 Write data to DynamoDB using Kafka Connect</a></li>
<li><a href="/blog/2023-12-14-real-time-streaming-with-kafka-and-flink-7/#">Lab 6 Consume data from Kafka using Lambda</a> (this post)</li>
</ul>

<h2 id="architecture" data-numberify>Architecture<a class="anchor ms-1" href="#architecture"></a></h2>
<p>Fake taxi ride data is sent to a Kafka topic by the Kafka producer application that is discussed in <a href="/blog/2023-10-26-real-time-streaming-with-kafka-and-flink-2">Lab 1</a>. The messages of the <em>taxi-rides</em> topic are consumed by a Lambda function where the MSK cluster is configured as an event source of the function.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2023-12-14-real-time-streaming-with-kafka-and-flink-7/featured.png" length="138986" type="image/png"/></item><item><title>Setup Local Development Environment for Apache Flink and Spark Using EMR Container Images</title><link>https://jaehyeon.me/blog/2023-12-07-flink-spark-local-dev/</link><pubDate>Thu, 07 Dec 2023 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2023-12-07-flink-spark-local-dev/</guid><description><![CDATA[<p><a href="https://aws.amazon.com/about-aws/whats-new/2023/11/apache-flink-available-amazon-emr-eks/" target="_blank" rel="noopener noreferrer">Apache Flink became generally available<i class="fas fa-external-link-square-alt ms-1"></i></a> for <a href="https://aws.amazon.com/emr/features/eks/" target="_blank" rel="noopener noreferrer">Amazon EMR on EKS<i class="fas fa-external-link-square-alt ms-1"></i></a> from the <a href="https://docs.aws.amazon.com/emr/latest/EMR-on-EKS-DevelopmentGuide/emr-eks-6.15.0.html" target="_blank" rel="noopener noreferrer">EMR 6.15.0 releases<i class="fas fa-external-link-square-alt ms-1"></i></a>, and we are able to pull the Flink (as well as Spark) container images from the <a href="https://gallery.ecr.aws/emr-on-eks" target="_blank" rel="noopener noreferrer">ECR Public Gallery<i class="fas fa-external-link-square-alt ms-1"></i></a>. As both of them can be integrated with the <em>Glue Data Catalog</em>, it can be particularly useful if we develop real time data ingestion/processing via Flink and build analytical queries using Spark (or any other tools or services that can access to the Glue Data Catalog).</p>]]></description><enclosure url="https://jaehyeon.me/blog/2023-12-07-flink-spark-local-dev/featured.png" length="133053" type="image/png"/></item><item><title>Real Time Streaming with Kafka and Flink - Lab 5 Write data to DynamoDB using Kafka Connect</title><link>https://jaehyeon.me/blog/2023-11-30-real-time-streaming-with-kafka-and-flink-6/</link><pubDate>Thu, 30 Nov 2023 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2023-11-30-real-time-streaming-with-kafka-and-flink-6/</guid><description><![CDATA[<p><a href="https://kafka.apache.org/documentation/#connect" target="_blank" rel="noopener noreferrer">Kafka Connect<i class="fas fa-external-link-square-alt ms-1"></i></a> is a tool for scalably and reliably streaming data between Apache Kafka and other systems. It makes it simple to quickly define connectors that move large collections of data into and out of Kafka. In this lab, we will discuss how to create a data pipeline that ingests data from a Kafka topic into a DynamoDB table using the <a href="https://camel.apache.org/camel-kafka-connector/3.20.x/reference/connectors/camel-aws-ddb-sink-kafka-sink-connector.html" target="_blank" rel="noopener noreferrer">Camel DynamoDB sink connector<i class="fas fa-external-link-square-alt ms-1"></i></a>.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2023-11-30-real-time-streaming-with-kafka-and-flink-6/featured.png" length="113252" type="image/png"/></item><item><title>Real Time Streaming with Kafka and Flink - Lab 4 Clean, Aggregate, and Enrich Events with Flink</title><link>https://jaehyeon.me/blog/2023-11-23-real-time-streaming-with-kafka-and-flink-5/</link><pubDate>Thu, 23 Nov 2023 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2023-11-23-real-time-streaming-with-kafka-and-flink-5/</guid><description>&lt;p>The value of data can be maximised when it is used without delay. With Apache Flink, we can build streaming analytics applications that incorporate the latest events with low latency. In this lab, we will create a Pyflink application that writes accumulated taxi rides data into an OpenSearch cluster. It aggregates the number of trips/passengers and trip durations by vendor ID for a window of 5 seconds. The data is then used to create a chart that monitors the status of taxi rides in the OpenSearch Dashboard.&lt;/p></description><enclosure url="https://jaehyeon.me/blog/2023-11-23-real-time-streaming-with-kafka-and-flink-5/featured.png" length="112340" type="image/png"/></item><item><title>Real Time Streaming with Kafka and Flink - Lab 3 Transform and write data to S3 from Kafka using Flink</title><link>https://jaehyeon.me/blog/2023-11-16-real-time-streaming-with-kafka-and-flink-4/</link><pubDate>Thu, 16 Nov 2023 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2023-11-16-real-time-streaming-with-kafka-and-flink-4/</guid><description><![CDATA[<p>In this lab, we will create a Pyflink application that exports Kafka topic messages into a S3 bucket. The app enriches the records by adding a new column using a user defined function and writes them via the <a href="https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/connectors/table/filesystem/" target="_blank" rel="noopener noreferrer">FileSystem SQL connector<i class="fas fa-external-link-square-alt ms-1"></i></a>. This allows us to achieve a simpler architecture compared to the <a href="https://catalog.us-east-1.prod.workshops.aws/workshops/2300137e-f2ac-4eb9-a4ac-3d25026b235f/en-US/lab-3-kdf" target="_blank" rel="noopener noreferrer">original lab<i class="fas fa-external-link-square-alt ms-1"></i></a> where the records are sent into Amazon Kinesis Data Firehose, enriched by a separate Lambda function and written to a S3 bucket afterwards. While the records are being written to the S3 bucket, a Glue table will be created to query them on Amazon Athena.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2023-11-16-real-time-streaming-with-kafka-and-flink-4/featured.png" length="160359" type="image/png"/></item><item><title>Real Time Streaming with Kafka and Flink - Lab 2 Write data to Kafka from S3 using Flink</title><link>https://jaehyeon.me/blog/2023-11-09-real-time-streaming-with-kafka-and-flink-3/</link><pubDate>Thu, 09 Nov 2023 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2023-11-09-real-time-streaming-with-kafka-and-flink-3/</guid><description>&lt;p>In this lab, we will create a Pyflink application that reads records from S3 and sends them into a Kafka topic. A custom pipeline Jar file will be created as the Kafka cluster is authenticated by IAM, and it will be demonstrated how to execute the app in a Flink cluster deployed on Docker as well as locally as a typical Python app. We can assume the S3 data is static metadata that needs to be joined into another stream, and this exercise can be useful for data enrichment.&lt;/p></description><enclosure url="https://jaehyeon.me/blog/2023-11-09-real-time-streaming-with-kafka-and-flink-3/featured.png" length="139114" type="image/png"/></item><item><title>Benefits and Opportunities of Stateful Stream Processing</title><link>https://jaehyeon.me/blog/2023-11-02-stateful-stream-processing/</link><pubDate>Thu, 02 Nov 2023 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2023-11-02-stateful-stream-processing/</guid><description>&lt;p>Stream processing technology is becoming more and more popular with companies big and small because it provides superior solutions for many established use cases such as data analytics, ETL, and transactional applications, but also facilitates novel applications, software architectures, and business opportunities. Beginning with traditional data infrastructures and application/data development patterns, this post introduces stateful stream processing and demonstrates to what extent it can improve the traditional development patterns. A consulting company can partner with her clients on their journeys of adopting stateful stream processing, and it can bring huge opportunities. Those opportunities are summarised at the end.&lt;/p></description><enclosure url="https://jaehyeon.me/blog/2023-11-02-stateful-stream-processing/featured.png" length="244920" type="image/png"/></item><item><title>Kafka Connect for AWS Services Integration - Part 5 Deploy Aiven OpenSearch Sink Connector</title><link>https://jaehyeon.me/blog/2023-10-30-kafka-connect-for-aws-part-5/</link><pubDate>Mon, 30 Oct 2023 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2023-10-30-kafka-connect-for-aws-part-5/</guid><description><![CDATA[<p>In the previous post, we discussed how to develop a data pipeline from Apache Kafka into OpenSearch locally using Docker. The pipeline will be deployed on AWS using <a href="https://aws.amazon.com/msk/" target="_blank" rel="noopener noreferrer">Amazon MSK<i class="fas fa-external-link-square-alt ms-1"></i></a>, <a href="https://aws.amazon.com/msk/features/msk-connect/" target="_blank" rel="noopener noreferrer">Amazon MSK Connect<i class="fas fa-external-link-square-alt ms-1"></i></a> and <a href="https://aws.amazon.com/opensearch-service/" target="_blank" rel="noopener noreferrer">Amazon OpenSearch Service<i class="fas fa-external-link-square-alt ms-1"></i></a> using <a href="https://www.terraform.io/" target="_blank" rel="noopener noreferrer">Terraform<i class="fas fa-external-link-square-alt ms-1"></i></a> in this post. First the infrastructure will be deployed that covers a Virtual Private Cloud (VPC), Virtual Private Network (VPN) server, MSK Cluster and OpenSearch domain. Then Kafka source and sink connectors will be deployed on MSK Connect, followed by performing quick data analysis.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2023-10-30-kafka-connect-for-aws-part-5/featured.png" length="85575" type="image/png"/></item><item><title>Real Time Streaming with Kafka and Flink - Lab 1 Produce data to Kafka using Lambda</title><link>https://jaehyeon.me/blog/2023-10-26-real-time-streaming-with-kafka-and-flink-2/</link><pubDate>Thu, 26 Oct 2023 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2023-10-26-real-time-streaming-with-kafka-and-flink-2/</guid><description><![CDATA[<p>In this lab, we will create a Kafka producer application using <a href="https://aws.amazon.com/lambda/" target="_blank" rel="noopener noreferrer">AWS Lambda<i class="fas fa-external-link-square-alt ms-1"></i></a>, which sends fake taxi ride data into a Kafka topic on <a href="https://aws.amazon.com/msk/" target="_blank" rel="noopener noreferrer">Amazon MSK<i class="fas fa-external-link-square-alt ms-1"></i></a>. A configurable number of the producer Lambda function will be invoked by an <a href="https://aws.amazon.com/eventbridge/" target="_blank" rel="noopener noreferrer">Amazon EventBridge<i class="fas fa-external-link-square-alt ms-1"></i></a> schedule rule. In this way we are able to generate test data concurrently based on the desired volume of messages.</p>
<ul>
<li><a href="/blog/2023-10-05-real-time-streaming-with-kafka-and-flink-1">Introduction</a></li>
<li><a href="/blog/2023-10-26-real-time-streaming-with-kafka-and-flink-2/#">Lab 1 Produce data to Kafka using Lambda</a> (this post)</li>
<li><a href="/blog/2023-11-09-real-time-streaming-with-kafka-and-flink-3">Lab 2 Write data to Kafka from S3 using Flink</a></li>
<li><a href="/blog/2023-11-16-real-time-streaming-with-kafka-and-flink-4">Lab 3 Transform and write data to S3 from Kafka using Flink</a></li>
<li><a href="/blog/2023-11-23-real-time-streaming-with-kafka-and-flink-5">Lab 4 Clean, Aggregate, and Enrich Events with Flink</a></li>
<li><a href="/blog/2023-11-30-real-time-streaming-with-kafka-and-flink-6">Lab 5 Write data to DynamoDB using Kafka Connect</a></li>
<li><a href="/blog/2023-12-14-real-time-streaming-with-kafka-and-flink-7">Lab 6 Consume data from Kafka using Lambda</a></li>
</ul>
<p>[<strong>Update 2023-11-06</strong>] Initially I planned to deploy Pyflink applications on <a href="https://aws.amazon.com/managed-service-apache-flink/" target="_blank" rel="noopener noreferrer">Amazon Managed Service for Apache Flink<i class="fas fa-external-link-square-alt ms-1"></i></a>, but I changed the plan to use a local Flink cluster deployed on Docker. The main reasons are</p>]]></description><enclosure url="https://jaehyeon.me/blog/2023-10-26-real-time-streaming-with-kafka-and-flink-2/featured.png" length="138560" type="image/png"/></item><item><title>Kafka Connect for AWS Services Integration - Part 4 Develop Aiven OpenSearch Sink Connector</title><link>https://jaehyeon.me/blog/2023-10-23-kafka-connect-for-aws-part-4/</link><pubDate>Mon, 23 Oct 2023 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2023-10-23-kafka-connect-for-aws-part-4/</guid><description><![CDATA[<p><a href="https://opensearch.org/" target="_blank" rel="noopener noreferrer">OpenSearch<i class="fas fa-external-link-square-alt ms-1"></i></a> is a popular search and analytics engine and its use cases cover log analytics, real-time application monitoring, and clickstream analysis. OpenSearch can be deployed on its own or via <a href="https://aws.amazon.com/opensearch-service/" target="_blank" rel="noopener noreferrer">Amazon OpenSearch Service<i class="fas fa-external-link-square-alt ms-1"></i></a>. <a href="https://kafka.apache.org/" target="_blank" rel="noopener noreferrer">Apache Kafka<i class="fas fa-external-link-square-alt ms-1"></i></a> is a distributed event store and stream-processing platform, and it aims to provide a unified, high-throughput, low-latency platform for handling real-time data feeds. On AWS, Apache Kafka can be deployed via <a href="https://aws.amazon.com/msk/" target="_blank" rel="noopener noreferrer">Amazon Managed Streaming for Apache Kafka (MSK)<i class="fas fa-external-link-square-alt ms-1"></i></a>.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2023-10-23-kafka-connect-for-aws-part-4/featured.png" length="61820" type="image/png"/></item><item><title>Building Apache Flink Applications in Python</title><link>https://jaehyeon.me/blog/2023-10-19-build-pyflink-apps/</link><pubDate>Thu, 19 Oct 2023 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2023-10-19-build-pyflink-apps/</guid><description><![CDATA[<p><a href="https://developer.confluent.io/courses/flink-java/overview/" target="_blank" rel="noopener noreferrer">Building Apache Flink Applications in Java<i class="fas fa-external-link-square-alt ms-1"></i></a> is a course to introduce <a href="https://flink.apache.org/" target="_blank" rel="noopener noreferrer">Apache Flink<i class="fas fa-external-link-square-alt ms-1"></i></a> through a series of hands-on exercises, and it is provided by <a href="https://www.confluent.io/" target="_blank" rel="noopener noreferrer">Confluent<i class="fas fa-external-link-square-alt ms-1"></i></a>. Utilising the <a href="https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/dev/datastream/overview/" target="_blank" rel="noopener noreferrer">Flink DataStream API<i class="fas fa-external-link-square-alt ms-1"></i></a>, the course develops three Flink applications that populate multiple source data sets, collect them into a standardised data set, and aggregate it to produce usage statistics. As part of learning the Flink DataStream API in Pyflink, I converted the Java apps into Python equivalent while performing the course exercises in Pyflink. This post summarises the progress of the conversion and shows the final output.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2023-10-19-build-pyflink-apps/featured.png" length="154736" type="image/png"/></item><item><title>How I Prepared for Certified Kubernetes Application Developer (CKAD)</title><link>https://jaehyeon.me/blog/2023-10-12-how-i-prepared-for-ckad/</link><pubDate>Thu, 12 Oct 2023 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2023-10-12-how-i-prepared-for-ckad/</guid><description><![CDATA[<p>I recently obtained the <a href="https://training.linuxfoundation.org/certification/certified-kubernetes-application-developer-ckad/" target="_blank" rel="noopener noreferrer">Certified Kubernetes Application Developer (CKAD)<i class="fas fa-external-link-square-alt ms-1"></i></a> certification. CKAD has been developed by <a href="https://www.linuxfoundation.org/" target="_blank" rel="noopener noreferrer">The Linux Foundation<i class="fas fa-external-link-square-alt ms-1"></i></a> and the <a href="https://www.cncf.io/" target="_blank" rel="noopener noreferrer">Cloud Native Computing Foundation (CNCF)<i class="fas fa-external-link-square-alt ms-1"></i></a>, to help expand the Kubernetes ecosystem through standardized training and certification. Specifically this certification is for Kubernetes engineers, cloud engineers and other IT professionals responsible for building, deploying, and configuring cloud native applications with Kubernetes. In this post, I will summarise how I prepared for the exam by reviewing three online courses and two practice tests that I went through.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2023-10-12-how-i-prepared-for-ckad/featured.png" length="5945" type="image/png"/></item><item><title>Real Time Streaming with Kafka and Flink - Introduction</title><link>https://jaehyeon.me/blog/2023-10-05-real-time-streaming-with-kafka-and-flink-1/</link><pubDate>Thu, 05 Oct 2023 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2023-10-05-real-time-streaming-with-kafka-and-flink-1/</guid><description><![CDATA[<p><a href="https://catalog.us-east-1.prod.workshops.aws/workshops/2300137e-f2ac-4eb9-a4ac-3d25026b235f/en-US" target="_blank" rel="noopener noreferrer">Real Time Streaming with Amazon Kinesis<i class="fas fa-external-link-square-alt ms-1"></i></a> is an AWS workshop that helps users build a streaming analytics application on AWS. Incoming events are stored in a number of streams of the <a href="https://aws.amazon.com/kinesis/data-streams/" target="_blank" rel="noopener noreferrer">Amazon Kinesis Data Streams<i class="fas fa-external-link-square-alt ms-1"></i></a> service, and various other AWS services and tools are used to process and analyse data.</p>
<p><a href="https://kafka.apache.org/" target="_blank" rel="noopener noreferrer">Apache Kafka<i class="fas fa-external-link-square-alt ms-1"></i></a> is a popular distributed event store and stream processing platform, and it stores incoming events in topics. As part of learning real time streaming analytics on AWS, we can rebuild the analytics applications by replacing the Kinesis streams with Kafka topics. As an introduction, this post compares the workshop architecture with the updated architecture of this series. The labs of the updated architecture will be implemented in subsequent posts.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2023-10-05-real-time-streaming-with-kafka-and-flink-1/featured.png" length="138141" type="image/png"/></item><item><title>Kafka, Flink and DynamoDB for Real Time Fraud Detection - Part 2 Deployment via AWS Managed Flink</title><link>https://jaehyeon.me/blog/2023-09-14-fraud-detection-part-2/</link><pubDate>Thu, 14 Sep 2023 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2023-09-14-fraud-detection-part-2/</guid><description><![CDATA[<p>This series aims to help those who are new to <a href="https://flink.apache.org/" target="_blank" rel="noopener noreferrer">Apache Flink<i class="fas fa-external-link-square-alt ms-1"></i></a> and <a href="https://aws.amazon.com/about-aws/whats-new/2023/08/amazon-managed-service-apache-flink/" target="_blank" rel="noopener noreferrer">Amazon Managed Service for Apache Flink<i class="fas fa-external-link-square-alt ms-1"></i></a> by re-implementing a simple fraud detection application that is discussed in an AWS workshop titled <a href="https://catalog.us-east-1.prod.workshops.aws/workshops/ad026e95-37fd-4605-a327-b585a53b1300/en-US" target="_blank" rel="noopener noreferrer">AWS Kafka and DynamoDB for real time fraud detection<i class="fas fa-external-link-square-alt ms-1"></i></a>. In part 1, I demonstrated how to develop the application locally, and the app will be deployed via <em>Amazon Managed Service for Apache Flink</em> in this post.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2023-09-14-fraud-detection-part-2/featured.png" length="66221" type="image/png"/></item><item><title>Getting Started with Pyflink on AWS - Part 3 AWS Managed Flink and MSK</title><link>https://jaehyeon.me/blog/2023-09-04-getting-started-with-pyflink-on-aws-part-3/</link><pubDate>Mon, 04 Sep 2023 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2023-09-04-getting-started-with-pyflink-on-aws-part-3/</guid><description><![CDATA[<p>In this series of posts, we discuss a Flink (Pyflink) application that reads/writes from/to Kafka topics. In the previous posts, I demonstrated a Pyflink app that targets a local Kafka cluster as well as a Kafka cluster on Amazon MSK. The app was executed in a virtual environment as well as in a local Flink cluster for improved monitoring. In this post, the app will be deployed via <a href="https://aws.amazon.com/about-aws/whats-new/2023/08/amazon-managed-service-apache-flink/" target="_blank" rel="noopener noreferrer">Amazon Managed Service for Apache Flink<i class="fas fa-external-link-square-alt ms-1"></i></a>, which is the easiest option to run Flink applications on AWS.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2023-09-04-getting-started-with-pyflink-on-aws-part-3/featured.png" length="74618" type="image/png"/></item><item><title>Getting Started with Pyflink on AWS - Part 2 Local Flink and MSK</title><link>https://jaehyeon.me/blog/2023-08-28-getting-started-with-pyflink-on-aws-part-2/</link><pubDate>Mon, 28 Aug 2023 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2023-08-28-getting-started-with-pyflink-on-aws-part-2/</guid><description><![CDATA[<p>In this series of posts, we discuss a Flink (Pyflink) application that reads/writes from/to Kafka topics. In part 1, an app that targets a local Kafka cluster was created. In this post, we will update the app by connecting a Kafka cluster on Amazon MSK. The Kafka cluster is authenticated by IAM and the app has additional jar dependency. As <a href="https://aws.amazon.com/about-aws/whats-new/2023/08/amazon-managed-service-apache-flink/" target="_blank" rel="noopener noreferrer">Amazon Managed Service for Apache Flink<i class="fas fa-external-link-square-alt ms-1"></i></a> does not allow you to specify multiple pipeline jar files, we have to build a custom Uber Jar that combines multiple jar files. Same as part 1, the app will be executed in a virtual environment as well as in a local Flink cluster for improved monitoring with the updated pipeline jar file.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2023-08-28-getting-started-with-pyflink-on-aws-part-2/featured.png" length="64005" type="image/png"/></item><item><title>Getting Started with Pyflink on AWS - Part 1 Local Flink and Local Kafka</title><link>https://jaehyeon.me/blog/2023-08-17-getting-started-with-pyflink-on-aws-part-1/</link><pubDate>Thu, 17 Aug 2023 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2023-08-17-getting-started-with-pyflink-on-aws-part-1/</guid><description><![CDATA[<p><a href="https://flink.apache.org/" target="_blank" rel="noopener noreferrer">Apache Flink<i class="fas fa-external-link-square-alt ms-1"></i></a> is an open-source, unified stream-processing and batch-processing framework. Its core is a distributed streaming data-flow engine that you can use to run real-time stream processing on high-throughput data sources. Currently, it is widely used to build applications for fraud/anomaly detection, rule-based alerting, business process monitoring, and continuous ETL to name a few. On AWS, we can deploy a Flink application via <a href="https://aws.amazon.com/kinesis/data-analytics/" target="_blank" rel="noopener noreferrer">Amazon Kinesis Data Analytics (KDA)<i class="fas fa-external-link-square-alt ms-1"></i></a>, <a href="https://aws.amazon.com/emr/" target="_blank" rel="noopener noreferrer">Amazon EMR<i class="fas fa-external-link-square-alt ms-1"></i></a> and <a href="https://aws.amazon.com/eks/" target="_blank" rel="noopener noreferrer">Amazon EKS<i class="fas fa-external-link-square-alt ms-1"></i></a>. Among those, KDA is the easiest option as it provides the underlying infrastructure for your Apache Flink applications.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2023-08-17-getting-started-with-pyflink-on-aws-part-1/featured.png" length="55960" type="image/png"/></item><item><title>Kafka, Flink and DynamoDB for Real Time Fraud Detection - Part 1 Local Development</title><link>https://jaehyeon.me/blog/2023-08-10-fraud-detection-part-1/</link><pubDate>Thu, 10 Aug 2023 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2023-08-10-fraud-detection-part-1/</guid><description><![CDATA[<p><a href="https://flink.apache.org/" target="_blank" rel="noopener noreferrer">Apache Flink<i class="fas fa-external-link-square-alt ms-1"></i></a> is an open-source, unified stream-processing and batch-processing framework. Its core is a distributed streaming data-flow engine that you can use to run real-time stream processing on high-throughput data sources. Currently, it is widely used to build applications for fraud/anomaly detection, rule-based alerting, business process monitoring, and continuous ETL to name a few. On AWS, we can deploy a Flink application via <a href="https://aws.amazon.com/kinesis/data-analytics/" target="_blank" rel="noopener noreferrer">Amazon Kinesis Data Analytics (KDA)<i class="fas fa-external-link-square-alt ms-1"></i></a>, <a href="https://aws.amazon.com/emr/" target="_blank" rel="noopener noreferrer">Amazon EMR<i class="fas fa-external-link-square-alt ms-1"></i></a> and <a href="https://aws.amazon.com/eks/" target="_blank" rel="noopener noreferrer">Amazon EKS<i class="fas fa-external-link-square-alt ms-1"></i></a>. Among those, KDA is the easiest option as it provides the underlying infrastructure for your Apache Flink applications.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2023-08-10-fraud-detection-part-1/featured.png" length="72929" type="image/png"/></item><item><title>Kafka Development with Docker - Part 11 Kafka Authorization</title><link>https://jaehyeon.me/blog/2023-07-20-kafka-development-with-docker-part-11/</link><pubDate>Thu, 20 Jul 2023 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2023-07-20-kafka-development-with-docker-part-11/</guid><description>&lt;p>In the previous posts, we discussed how to implement client authentication by TLS (SSL or TLS/SSL) and SASL authentication. One of the key benefits of client authentication is achieving user access control. Kafka ships with a pluggable, out-of-the box authorization framework, which is configured with the &lt;em>authorizer.class.name&lt;/em> property in the server configuration and stores Access Control Lists (ACLs) in the cluster metadata (either Zookeeper or the KRaft metadata log). In this post, we will discuss how to configure Kafka authorization with Java and Python client examples while SASL is kept for client authentication.&lt;/p></description><enclosure url="https://jaehyeon.me/blog/2023-07-20-kafka-development-with-docker-part-11/featured.png" length="458848" type="image/png"/></item><item><title>Kafka Development with Docker - Part 10 SASL Authentication</title><link>https://jaehyeon.me/blog/2023-07-13-kafka-development-with-docker-part-10/</link><pubDate>Thu, 13 Jul 2023 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2023-07-13-kafka-development-with-docker-part-10/</guid><description><![CDATA[<p>In the previous post, we discussed TLS (SSL or TLS/SSL) authentication to improve security. It enforces two-way verification where a client certificate is verified by Kafka brokers. Client authentication can also be enabled by <a href="https://en.wikipedia.org/wiki/Simple_Authentication_and_Security_Layer" target="_blank" rel="noopener noreferrer">Simple Authentication and Security Layer (SASL)<i class="fas fa-external-link-square-alt ms-1"></i></a>, and we will discuss how to implement SASL authentication with Java and Python client examples in this post.</p>
<ul>
<li><a href="/blog/2023-05-04-kafka-development-with-docker-part-1">Part 1 Cluster Setup</a></li>
<li><a href="/blog/2023-05-18-kafka-development-with-docker-part-2">Part 2 Management App</a></li>
<li><a href="/blog/2023-05-25-kafka-development-with-docker-part-3">Part 3 Kafka Connect</a></li>
<li><a href="/blog/2023-06-01-kafka-development-with-docker-part-4">Part 4 Producer and Consumer</a></li>
<li><a href="/blog/2023-06-08-kafka-development-with-docker-part-5">Part 5 Glue Schema Registry</a></li>
<li><a href="/blog/2023-06-15-kafka-development-with-docker-part-6">Part 6 Kafka Connect with Glue Schema Registry</a></li>
<li><a href="/blog/2023-06-22-kafka-development-with-docker-part-7">Part 7 Producer and Consumer with Glue Schema Registry</a></li>
<li><a href="/blog/2023-06-29-kafka-development-with-docker-part-8">Part 8 SSL Encryption</a></li>
<li><a href="/blog/2023-07-06-kafka-development-with-docker-part-9">Part 9 SSL Authentication</a></li>
<li><a href="/blog/2023-07-13-kafka-development-with-docker-part-10/#">Part 10 SASL Authentication</a> (this post)</li>
<li><a href="/blog/2023-07-20-kafka-development-with-docker-part-11">Part 11 Kafka Authorization</a></li>
</ul>

<h2 id="certificate-setup" data-numberify>Certificate Setup<a class="anchor ms-1" href="#certificate-setup"></a></h2>
<p>As we will leave Kafka communication to remain encrypted, we need to keep the components for SSL encryption. The details can be found in <a href="/blog/2023-06-29-kafka-development-with-docker-part-8">Part 8</a>, and those components can be generated by <a href="https://github.com/jaehyeon-kim/kafka-pocs/blob/main/kafka-dev-with-docker/part-10/generate.sh" target="_blank" rel="noopener noreferrer"><em>generate.sh</em><i class="fas fa-external-link-square-alt ms-1"></i></a>. Once we execute the script, the following files are created.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2023-07-13-kafka-development-with-docker-part-10/featured.png" length="471947" type="image/png"/></item><item><title>Kafka Development with Docker - Part 9 SSL Authentication</title><link>https://jaehyeon.me/blog/2023-07-06-kafka-development-with-docker-part-9/</link><pubDate>Thu, 06 Jul 2023 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2023-07-06-kafka-development-with-docker-part-9/</guid><description><![CDATA[<p>In the previous post, we discussed how to configure TLS (SSL or TLS/SSL) encryption with Java and Python client examples. SSL encryption is a one-way verification process where a server certificate is verified by a client via <a href="https://en.wikipedia.org/wiki/Transport_Layer_Security#TLS_handshake" target="_blank" rel="noopener noreferrer">SSL Handshake<i class="fas fa-external-link-square-alt ms-1"></i></a>. To improve security, we can add client authentication either by enforcing two-way verification where a client certificate is verified by Kafka brokers (SSL authentication). Or we can choose a separate authentication mechanism, which is typically <a href="https://en.wikipedia.org/wiki/Simple_Authentication_and_Security_Layer" target="_blank" rel="noopener noreferrer">Simple Authentication and Security Layer (SASL)<i class="fas fa-external-link-square-alt ms-1"></i></a>. In this post, we will discuss how to implement SSL authentication with Java and Python client examples while SASL authentication is covered in the next post.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2023-07-06-kafka-development-with-docker-part-9/featured.png" length="471471" type="image/png"/></item><item><title>Kafka Connect for AWS Services Integration - Part 3 Deploy Camel DynamoDB Sink Connector</title><link>https://jaehyeon.me/blog/2023-07-03-kafka-connect-for-aws-part-3/</link><pubDate>Mon, 03 Jul 2023 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2023-07-03-kafka-connect-for-aws-part-3/</guid><description><![CDATA[<p>As part of investigating how to utilize Kafka Connect effectively for AWS services integration, I demonstrated how to develop the <a href="https://camel.apache.org/camel-kafka-connector/latest/index.html" target="_blank" rel="noopener noreferrer">Camel DynamoDB sink connector<i class="fas fa-external-link-square-alt ms-1"></i></a> using Docker in <a href="/blog/2023-06-04-kafka-connect-for-aws-part-2">Part 2</a>. Fake order data was generated using the <a href="https://github.com/awslabs/amazon-msk-data-generator" target="_blank" rel="noopener noreferrer">MSK Data Generator<i class="fas fa-external-link-square-alt ms-1"></i></a> source connector, and the sink connector was configured to consume the topic messages to ingest them into a DynamoDB table. In this post, I will illustrate how to deploy the data ingestion applications using <a href="https://aws.amazon.com/msk/" target="_blank" rel="noopener noreferrer">Amazon MSK<i class="fas fa-external-link-square-alt ms-1"></i></a> and <a href="https://aws.amazon.com/msk/features/msk-connect/" target="_blank" rel="noopener noreferrer">MSK Connect<i class="fas fa-external-link-square-alt ms-1"></i></a>.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2023-07-03-kafka-connect-for-aws-part-3/featured.png" length="76240" type="image/png"/></item><item><title>Kafka Development with Docker - Part 8 SSL Encryption</title><link>https://jaehyeon.me/blog/2023-06-29-kafka-development-with-docker-part-8/</link><pubDate>Thu, 29 Jun 2023 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2023-06-29-kafka-development-with-docker-part-8/</guid><description><![CDATA[<p>By default, Apache Kafka communicates in <em>PLAINTEXT</em>, which means that all data is sent without being encrypted. To secure communication, we can configure Kafka clients and other components to use <a href="https://en.wikipedia.org/wiki/Transport_Layer_Security" target="_blank" rel="noopener noreferrer">Transport Layer Security (TLS)<i class="fas fa-external-link-square-alt ms-1"></i></a> encryption. Note that TLS is also referred to <a href="https://en.wikipedia.org/wiki/Transport_Layer_Security#SSL_1.0,_2.0,_and_3.0" target="_blank" rel="noopener noreferrer">Secure Sockets Layer (SSL)<i class="fas fa-external-link-square-alt ms-1"></i></a> or TLS/SSL. SSL is the predecessor of TLS, and has been deprecated since June 2015. However, it is used in configuration and code instead of TLS for historical reasons. In this post, SSL, TLS and TLS/SSL will be used interchangeably. SSL encryption is a one-way verification process where a server certificate is verified by a client via <a href="https://en.wikipedia.org/wiki/Transport_Layer_Security#TLS_handshake" target="_blank" rel="noopener noreferrer">SSL Handshake<i class="fas fa-external-link-square-alt ms-1"></i></a>. Moreover, we can improve security by adding client authentication. For example, we can enforce two-way verification so that a client certificate is verified by Kafka brokers as well (<em>SSL Authentication</em>). Alternatively we can choose a separate authentication mechanism and typically <a href="https://en.wikipedia.org/wiki/Simple_Authentication_and_Security_Layer" target="_blank" rel="noopener noreferrer">Simple Authentication and Security Layer (SASL)<i class="fas fa-external-link-square-alt ms-1"></i></a> is used (<em>SASL Authentication</em>). In this post, we will discuss how to configure SSL encryption with Java and Python client examples while SSL and SASL client authentication will be covered in later posts.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2023-06-29-kafka-development-with-docker-part-8/featured.png" length="469311" type="image/png"/></item><item><title>Kafka Development with Docker - Part 7 Producer and Consumer with Glue Schema Registry</title><link>https://jaehyeon.me/blog/2023-06-22-kafka-development-with-docker-part-7/</link><pubDate>Thu, 22 Jun 2023 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2023-06-22-kafka-development-with-docker-part-7/</guid><description><![CDATA[<p>In <a href="/blog/2023-06-01-kafka-development-with-docker-part-4">Part 4</a>, we developed Kafka producer and consumer applications using the <a href="https://kafka-python.readthedocs.io/en/master/index.html" target="_blank" rel="noopener noreferrer">kafka-python<i class="fas fa-external-link-square-alt ms-1"></i></a> package. The Kafka messages are serialized as Json, but are not associated with a schema as there was not an integrated schema registry. Later we discussed how producers and consumers to Kafka topics can use schemas to ensure data consistency and compatibility as schemas evolve in <a href="/blog/2023-06-08-kafka-development-with-docker-part-5">Part 5</a>. In this post, I&rsquo;ll demonstrate how to enhance the existing applications by integrating <a href="https://docs.aws.amazon.com/glue/latest/dg/schema-registry.html" target="_blank" rel="noopener noreferrer"><em>AWS Glue Schema Registry</em><i class="fas fa-external-link-square-alt ms-1"></i></a>.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2023-06-22-kafka-development-with-docker-part-7/featured.png" length="57175" type="image/png"/></item><item><title>Kafka Development with Docker - Part 6 Kafka Connect with Glue Schema Registry</title><link>https://jaehyeon.me/blog/2023-06-15-kafka-development-with-docker-part-6/</link><pubDate>Thu, 15 Jun 2023 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2023-06-15-kafka-development-with-docker-part-6/</guid><description><![CDATA[<p>In <a href="/blog/2023-05-25-kafka-development-with-docker-part-3">Part 3</a>, we developed a data ingestion pipeline with fake online order data using Kafka Connect source and sink connectors. Schemas are not enabled on both of them as there was not an integrated schema registry. Later we discussed how producers and consumers to Kafka topics can use schemas to ensure data consistency and compatibility as schemas evolve in <a href="/blog/2023-06-08-kafka-development-with-docker-part-5">Part 5</a>. In this post, I&rsquo;ll demonstrate how to enhance the existing data ingestion pipeline by integrating <a href="https://docs.aws.amazon.com/glue/latest/dg/schema-registry.html" target="_blank" rel="noopener noreferrer"><em>AWS Glue Schema Registry</em><i class="fas fa-external-link-square-alt ms-1"></i></a>.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2023-06-15-kafka-development-with-docker-part-6/featured.png" length="60354" type="image/png"/></item><item><title>Kafka Development with Docker - Part 5 Glue Schema Registry</title><link>https://jaehyeon.me/blog/2023-06-08-kafka-development-with-docker-part-5/</link><pubDate>Thu, 08 Jun 2023 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2023-06-08-kafka-development-with-docker-part-5/</guid><description><![CDATA[<p>As described in the <a href="https://docs.confluent.io/platform/current/schema-registry/index.html#sr-overview" target="_blank" rel="noopener noreferrer">Confluent document<i class="fas fa-external-link-square-alt ms-1"></i></a>, <em>Schema Registry</em> provides a centralized repository for managing and validating schemas for topic message data, and for serialization and deserialization of the data over the network. Producers and consumers to Kafka topics can use schemas to ensure data consistency and compatibility as schemas evolve. In AWS, the <a href="https://docs.aws.amazon.com/glue/latest/dg/schema-registry.html" target="_blank" rel="noopener noreferrer">Glue Schema Registry<i class="fas fa-external-link-square-alt ms-1"></i></a> supports features to manage and enforce schemas on data streaming applications using convenient integrations with Apache Kafka, <a href="https://aws.amazon.com/msk/" target="_blank" rel="noopener noreferrer">Amazon Managed Streaming for Apache Kafka<i class="fas fa-external-link-square-alt ms-1"></i></a>, <a href="https://aws.amazon.com/kinesis/data-streams/" target="_blank" rel="noopener noreferrer">Amazon Kinesis Data Streams<i class="fas fa-external-link-square-alt ms-1"></i></a>, <a href="https://aws.amazon.com/kinesis/data-analytics/" target="_blank" rel="noopener noreferrer">Amazon Kinesis Data Analytics for Apache Flink<i class="fas fa-external-link-square-alt ms-1"></i></a>, and <a href="https://aws.amazon.com/lambda/" target="_blank" rel="noopener noreferrer">AWS Lambda<i class="fas fa-external-link-square-alt ms-1"></i></a>.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2023-06-08-kafka-development-with-docker-part-5/featured.png" length="51170" type="image/png"/></item><item><title>Kafka Connect for AWS Services Integration - Part 2 Develop Camel DynamoDB Sink Connector</title><link>https://jaehyeon.me/blog/2023-06-04-kafka-connect-for-aws-part-2/</link><pubDate>Sun, 04 Jun 2023 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2023-06-04-kafka-connect-for-aws-part-2/</guid><description><![CDATA[<p>In <a href="/blog/2023-05-03-kafka-connect-for-aws-part-1">Part 1</a>, we reviewed Kafka connectors focusing on AWS services integration. Among the available connectors, the suite of <a href="https://camel.apache.org/camel-kafka-connector/latest/index.html" target="_blank" rel="noopener noreferrer">Apache Camel Kafka connectors<i class="fas fa-external-link-square-alt ms-1"></i></a> and the <a href="https://github.com/awslabs/kinesis-kafka-connector" target="_blank" rel="noopener noreferrer">Kinesis Kafka connector<i class="fas fa-external-link-square-alt ms-1"></i></a> from the AWS Labs can be effective for building data ingestion pipelines on AWS. In this post, I will illustrate how to develop the Camel DynamoDB sink connector using Docker. Fake order data will be generated using the <a href="https://github.com/awslabs/amazon-msk-data-generator" target="_blank" rel="noopener noreferrer">MSK Data Generator<i class="fas fa-external-link-square-alt ms-1"></i></a> source connector, and the sink connector will be configured to consume the topic messages to ingest them into a DynamoDB table.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2023-06-04-kafka-connect-for-aws-part-2/featured.png" length="87044" type="image/png"/></item><item><title>Kafka Development with Docker - Part 4 Producer and Consumer</title><link>https://jaehyeon.me/blog/2023-06-01-kafka-development-with-docker-part-4/</link><pubDate>Thu, 01 Jun 2023 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2023-06-01-kafka-development-with-docker-part-4/</guid><description><![CDATA[<p>In the previous post, we discussed <a href="https://kafka.apache.org/documentation/#connect" target="_blank" rel="noopener noreferrer">Kafka Connect<i class="fas fa-external-link-square-alt ms-1"></i></a> to stream data to/from a Kafka cluster. Kafka also includes the <a href="https://kafka.apache.org/documentation/#api" target="_blank" rel="noopener noreferrer">Producer/Consumer APIs<i class="fas fa-external-link-square-alt ms-1"></i></a> that allow client applications to send/read streams of data to/from topics in a Kafka cluster. While the main Kafka project maintains only the Java clients, there are several <a href="https://cwiki.apache.org/confluence/display/KAFKA/Clients#Clients-Python" target="_blank" rel="noopener noreferrer">open source projects<i class="fas fa-external-link-square-alt ms-1"></i></a> that provide the Kafka client APIs in Python. In this post, I&rsquo;ll demonstrate how to develop producer/consumer applications using the <a href="https://kafka-python.readthedocs.io/en/master/index.html" target="_blank" rel="noopener noreferrer">kafka-python<i class="fas fa-external-link-square-alt ms-1"></i></a> package.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2023-06-01-kafka-development-with-docker-part-4/featured.png" length="75255" type="image/png"/></item><item><title>Kafka Development with Docker - Part 3 Kafka Connect</title><link>https://jaehyeon.me/blog/2023-05-25-kafka-development-with-docker-part-3/</link><pubDate>Thu, 25 May 2023 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2023-05-25-kafka-development-with-docker-part-3/</guid><description><![CDATA[<p>According to the documentation of <a href="https://kafka.apache.org/documentation/#connect" target="_blank" rel="noopener noreferrer">Apache Kafka<i class="fas fa-external-link-square-alt ms-1"></i></a>, <em>Kafka Connect is a tool for scalably and reliably streaming data between Apache Kafka and other systems. It makes it simple to quickly define connectors that move large collections of data into and out of Kafka</em>. Kafka Connect supports two types of connectors - source and sink. Source connectors are used to ingest messages from external systems into Kafka topics while messages are ingested into external systems form Kafka topics with sink connectors. In this post, I will illustrate how to set up a data ingestion pipeline using Kafka connectors. Fake customer and order data will be ingested into the corresponding topics using the <a href="https://github.com/awslabs/amazon-msk-data-generator" target="_blank" rel="noopener noreferrer">MSK Data Generator<i class="fas fa-external-link-square-alt ms-1"></i></a> source connector. The topic messages will then be saved into a S3 bucket using the <a href="https://www.confluent.io/hub/confluentinc/kafka-connect-s3" target="_blank" rel="noopener noreferrer">Confluent S3<i class="fas fa-external-link-square-alt ms-1"></i></a> sink connector.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2023-05-25-kafka-development-with-docker-part-3/featured.png" length="69998" type="image/png"/></item><item><title>Kafka Development with Docker - Part 2 Management App</title><link>https://jaehyeon.me/blog/2023-05-18-kafka-development-with-docker-part-2/</link><pubDate>Thu, 18 May 2023 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2023-05-18-kafka-development-with-docker-part-2/</guid><description><![CDATA[<p>In the previous post, I illustrated how to create a topic and to produce/consume messages using the command utilities provided by Apache Kafka. It is not convenient, however, for example, when you consume serialised messages where their schemas are stored in a schema registry. Also, the utilities don&rsquo;t support to browse or manage related resources such as connectors and schemas. Therefore, a Kafka management app can be a good companion for development, which helps monitor and manage resources on an easy-to-use user interface. An app can be more useful if it supports features that are desirable for Kafka development on AWS. Those features cover <a href="https://docs.aws.amazon.com/msk/latest/developerguide/iam-access-control.html" target="_blank" rel="noopener noreferrer">IAM access control<i class="fas fa-external-link-square-alt ms-1"></i></a> of <a href="https://aws.amazon.com/msk/" target="_blank" rel="noopener noreferrer">Amazon MSK<i class="fas fa-external-link-square-alt ms-1"></i></a> and integration with <a href="https://aws.amazon.com/msk/features/msk-connect/" target="_blank" rel="noopener noreferrer">Amazon MSK Connect<i class="fas fa-external-link-square-alt ms-1"></i></a> and <a href="https://docs.aws.amazon.com/glue/latest/dg/schema-registry.html" target="_blank" rel="noopener noreferrer">AWS Glue Schema Registry<i class="fas fa-external-link-square-alt ms-1"></i></a>. In this post, I&rsquo;ll introduce several management apps that meet those requirements.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2023-05-18-kafka-development-with-docker-part-2/featured.png" length="59675" type="image/png"/></item><item><title>How I Prepared for Confluent Certified Developer for Apache Kafka as a Non-Java Developer</title><link>https://jaehyeon.me/blog/2023-05-11-how-i-prepared-for-ccdak/</link><pubDate>Thu, 11 May 2023 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2023-05-11-how-i-prepared-for-ccdak/</guid><description><![CDATA[<p>I recently obtained the <a href="https://www.confluent.io/certification/" target="_blank" rel="noopener noreferrer">Confluent Certified Developer for Apache Kafka (CCDAK)<i class="fas fa-external-link-square-alt ms-1"></i></a> certification. It focuses on knowledge of developing applications that work with Kafka, and is targeted to developers and solutions architects. As it assumes Java APIs for development and testing, I am contacted to share how I prepared for it as a non-Java developer from time to time. I thought it would be better to write a post to summarise how I did it rather than answering to them individually.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2023-05-11-how-i-prepared-for-ccdak/featured.png" length="215227" type="image/png"/></item><item><title>Kafka Development with Docker - Part 1 Cluster Setup</title><link>https://jaehyeon.me/blog/2023-05-04-kafka-development-with-docker-part-1/</link><pubDate>Thu, 04 May 2023 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2023-05-04-kafka-development-with-docker-part-1/</guid><description><![CDATA[<p>I&rsquo;m teaching myself <a href="https://docs.aws.amazon.com/whitepapers/latest/build-modern-data-streaming-analytics-architectures/build-modern-data-streaming-analytics-architectures.html" target="_blank" rel="noopener noreferrer">modern data streaming architectures<i class="fas fa-external-link-square-alt ms-1"></i></a> on AWS, and <a href="https://kafka.apache.org/" target="_blank" rel="noopener noreferrer">Apache Kafka<i class="fas fa-external-link-square-alt ms-1"></i></a> is one of the key technologies, which can be used for messaging, activity tracking, stream processing and so on. While applications tend to be deployed to cloud, it can be much easier if we develop and test those with <a href="https://www.docker.com/" target="_blank" rel="noopener noreferrer">Docker<i class="fas fa-external-link-square-alt ms-1"></i></a> and <a href="https://docs.docker.com/compose/" target="_blank" rel="noopener noreferrer">Docker Compose<i class="fas fa-external-link-square-alt ms-1"></i></a> locally. As the series title indicates, I plan to publish articles that demonstrate Kafka and related tools in <em>Dockerized</em> environments. Although I covered some of them in previous posts, they are implemented differently in terms of the Kafka Docker image, the number of brokers, Docker volume mapping etc. It can be confusing, and one of the purposes of this series is to illustrate reference implementations that can be applied to future development projects. Also, I can extend my knowledge while preparing for this series. In fact Kafka security is one of the areas that I expect to learn further. Below shows a list of posts that I plan for now.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2023-05-04-kafka-development-with-docker-part-1/featured.png" length="98355" type="image/png"/></item><item><title>Kafka Connect for AWS Services Integration - Part 1 Introduction</title><link>https://jaehyeon.me/blog/2023-05-03-kafka-connect-for-aws-part-1/</link><pubDate>Wed, 03 May 2023 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2023-05-03-kafka-connect-for-aws-part-1/</guid><description><![CDATA[<p><a href="https://aws.amazon.com/kinesis/data-streams/" target="_blank" rel="noopener noreferrer">Amazon Kinesis Data Streams<i class="fas fa-external-link-square-alt ms-1"></i></a> and <a href="https://aws.amazon.com/msk/" target="_blank" rel="noopener noreferrer">Amazon Managed Streaming for Apache Kafka (MSK)<i class="fas fa-external-link-square-alt ms-1"></i></a> are two managed streaming services offered by AWS. Many resources on the web indicate Kinesis Data Streams is better when it comes to integrating with AWS services. However, it is not necessarily the case with the help of Kafka Connect. According to the documentation of <a href="https://kafka.apache.org/documentation/#connect" target="_blank" rel="noopener noreferrer">Apache Kafka<i class="fas fa-external-link-square-alt ms-1"></i></a>, <em>Kafka Connect is a tool for scalably and reliably streaming data between Apache Kafka and other systems. It makes it simple to quickly define connectors that move large collections of data into and out of Kafka</em>. Kafka Connect supports two types of connectors - source and sink. Source connectors are used to ingest messages from external systems into Kafka topics while messages are ingested into external systems form Kafka topics with sink connectors. In this post, I will introduce available Kafka connectors mainly for AWS services integration. Also, developing and deploying some of them will be covered in later posts.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2023-05-03-kafka-connect-for-aws-part-1/featured.png" length="22272" type="image/png"/></item><item><title>Self-managed Blog with Hugo and GitHub Pages</title><link>https://jaehyeon.me/blog/2023-04-24-self-hosted-blog/</link><pubDate>Mon, 24 Apr 2023 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2023-04-24-self-hosted-blog/</guid><description><![CDATA[<p>I started blogging in 2014. At first, it was based on a simple Jekyll theme that supports posting <a href="https://en.wikipedia.org/wiki/Markdown" target="_blank" rel="noopener noreferrer">Markdown<i class="fas fa-external-link-square-alt ms-1"></i></a> files, which are converted from <a href="https://github.com/rstudio/rmarkdown" target="_blank" rel="noopener noreferrer">R Markdown<i class="fas fa-external-link-square-alt ms-1"></i></a> files. Most of my work was in <a href="https://www.r-project.org/about.html" target="_blank" rel="noopener noreferrer">R<i class="fas fa-external-link-square-alt ms-1"></i></a> at that time and the simple theme was good enough, and it was hosted via <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages<i class="fas fa-external-link-square-alt ms-1"></i></a>. It was around 2018 when I changed my blog with a single page application, built by <a href="https://vuejs.org/" target="_blank" rel="noopener noreferrer">Vue.js<i class="fas fa-external-link-square-alt ms-1"></i></a> and hosted on AWS. It was fun as I was teaching myself web development while building an analytics portal at work. In 2020, I paused blogging for some time while expecting a baby and restarted publishing posts to my <a href="https://cevo.com.au/author/jaehyeon-kim/" target="_blank" rel="noopener noreferrer">company&rsquo;s blog page<i class="fas fa-external-link-square-alt ms-1"></i></a> from mid-2021. It is good as I can have peer-reviews and the company provides an incentive for each post published. However, it is not the right place to publish all the posts that I plan. For example, I am recommended to keep in mind e.g. <em>how an article translates into better customer outcomes</em>. That&rsquo;s understandable but not all posts can fit into it. Currently, I am teaching myself <a href="https://docs.aws.amazon.com/whitepapers/latest/build-modern-data-streaming-analytics-architectures/build-modern-data-streaming-analytics-architectures.html" target="_blank" rel="noopener noreferrer">modern data streaming architectures<i class="fas fa-external-link-square-alt ms-1"></i></a>, and some articles could be inadequate for customers until I gain competency. Therefore, I thought I need another place that I can publish posts without worrying about undermining my company&rsquo;s reputation. I&rsquo;d keep publishing to the company site, and I probably repost some of them to this new blog with delay.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2023-04-24-self-hosted-blog/featured.png" length="41850" type="image/png"/></item><item><title>Integrate Glue Schema Registry with Your Python Kafka App</title><link>https://jaehyeon.me/blog/2023-04-12-integrate-glue-schema-registry/</link><pubDate>Wed, 12 Apr 2023 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2023-04-12-integrate-glue-schema-registry/</guid><description><![CDATA[<p>As Kafka producer and consumer apps are decoupled, they operate on Kafka topics rather than communicating with each other directly. As described in the <a href="https://docs.confluent.io/platform/current/schema-registry/index.html#sr-overview" target="_blank" rel="noopener noreferrer">Confluent document<i class="fas fa-external-link-square-alt ms-1"></i></a>, <em>Schema Registry</em> provides a centralized repository for managing and validating schemas for topic message data, and for serialization and deserialization of the data over the network. Producers and consumers to Kafka topics can use schemas to ensure data consistency and compatibility as schemas evolve. In AWS, the <a href="https://docs.aws.amazon.com/glue/latest/dg/schema-registry.html" target="_blank" rel="noopener noreferrer">Glue Schema Registry<i class="fas fa-external-link-square-alt ms-1"></i></a> supports features to manage and enforce schemas on data streaming applications using convenient integrations with Apache Kafka, <a href="https://aws.amazon.com/msk/" target="_blank" rel="noopener noreferrer">Amazon Managed Streaming for Apache Kafka<i class="fas fa-external-link-square-alt ms-1"></i></a>, <a href="https://aws.amazon.com/kinesis/data-streams/" target="_blank" rel="noopener noreferrer">Amazon Kinesis Data Streams<i class="fas fa-external-link-square-alt ms-1"></i></a>, <a href="https://aws.amazon.com/kinesis/data-analytics/" target="_blank" rel="noopener noreferrer">Amazon Kinesis Data Analytics for Apache Flink<i class="fas fa-external-link-square-alt ms-1"></i></a>, and <a href="https://aws.amazon.com/lambda/" target="_blank" rel="noopener noreferrer">AWS Lambda<i class="fas fa-external-link-square-alt ms-1"></i></a>. In this post, we will discuss how to integrate Python Kafka producer and consumer apps In AWS Lambda with the Glue Schema Registry.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2023-04-12-integrate-glue-schema-registry/featured.png" length="46040" type="image/png"/></item><item><title>Simplify Streaming Ingestion on AWS – Part 2 MSK and Athena</title><link>https://jaehyeon.me/blog/2023-03-14-simplify-streaming-ingestion-athena/</link><pubDate>Tue, 14 Mar 2023 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2023-03-14-simplify-streaming-ingestion-athena/</guid><description><![CDATA[<p>In Part 1, we discussed a streaming ingestion solution using <a href="https://aws.amazon.com/eventbridge/" target="_blank" rel="noopener noreferrer">EventBridge<i class="fas fa-external-link-square-alt ms-1"></i></a>, <a href="https://aws.amazon.com/lambda/" target="_blank" rel="noopener noreferrer">Lambda<i class="fas fa-external-link-square-alt ms-1"></i></a>, <a href="https://aws.amazon.com/msk/" target="_blank" rel="noopener noreferrer">MSK<i class="fas fa-external-link-square-alt ms-1"></i></a> and <a href="https://aws.amazon.com/redshift/redshift-serverless/" target="_blank" rel="noopener noreferrer">Redshift Serverless<i class="fas fa-external-link-square-alt ms-1"></i></a>. Athena provides the <a href="https://docs.aws.amazon.com/athena/latest/ug/connectors-msk.html" target="_blank" rel="noopener noreferrer">MSK connector<i class="fas fa-external-link-square-alt ms-1"></i></a> to enable SQL queries on Apache Kafka topics directly, and it can also facilitate the extraction of insights without setting up an additional pipeline to store data into S3. In this post, we discuss how to update the streaming ingestion solution so that data in the Kafka topic can be queried by Athena instead of Redshift.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2023-03-14-simplify-streaming-ingestion-athena/featured.png" length="43403" type="image/png"/></item><item><title>Simplify Streaming Ingestion on AWS – Part 1 MSK and Redshift</title><link>https://jaehyeon.me/blog/2023-02-08-simplify-streaming-ingestion-redshift/</link><pubDate>Wed, 08 Feb 2023 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2023-02-08-simplify-streaming-ingestion-redshift/</guid><description><![CDATA[<p><a href="https://kafka.apache.org/" target="_blank" rel="noopener noreferrer">Apache Kafka<i class="fas fa-external-link-square-alt ms-1"></i></a> is a popular distributed event store and stream processing platform. Previously loading data from Kafka into Redshift and Athena usually required Kafka connectors (e.g. <a href="https://www.confluent.io/hub/confluentinc/kafka-connect-aws-redshift" target="_blank" rel="noopener noreferrer">Amazon Redshift Sink Connector<i class="fas fa-external-link-square-alt ms-1"></i></a> and <a href="https://www.confluent.io/hub/confluentinc/kafka-connect-s3" target="_blank" rel="noopener noreferrer">Amazon S3 Sink Connector<i class="fas fa-external-link-square-alt ms-1"></i></a>). Recently these AWS services provide features to ingest data from Kafka directly, which facilitates a simpler architecture that achieves low-latency and high-speed ingestion of streaming data. In part 1 of the <em>simplify streaming ingestion on AWS</em> series, we discuss how to develop an end-to-end streaming ingestion solution using <a href="https://aws.amazon.com/eventbridge/" target="_blank" rel="noopener noreferrer">EventBridge<i class="fas fa-external-link-square-alt ms-1"></i></a>, <a href="https://aws.amazon.com/lambda/" target="_blank" rel="noopener noreferrer">Lambda<i class="fas fa-external-link-square-alt ms-1"></i></a>, <a href="https://aws.amazon.com/msk/" target="_blank" rel="noopener noreferrer">MSK<i class="fas fa-external-link-square-alt ms-1"></i></a> and <a href="https://aws.amazon.com/redshift/redshift-serverless/" target="_blank" rel="noopener noreferrer">Redshift Serverless<i class="fas fa-external-link-square-alt ms-1"></i></a> on AWS.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2023-02-08-simplify-streaming-ingestion-redshift/featured.png" length="32864" type="image/png"/></item><item><title>How to configure Kafka consumers to seek offsets by timestamp</title><link>https://jaehyeon.me/blog/2023-01-10-kafka-consumer-seek-offsets/</link><pubDate>Tue, 10 Jan 2023 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2023-01-10-kafka-consumer-seek-offsets/</guid><description><![CDATA[<p>Normally we consume Kafka messages from the beginning/end of a topic or last committed offsets. For backfilling or troubleshooting, however, we need to consume messages from a certain timestamp occasionally. If we know which topic partition to choose e.g. by <a href="https://kafka-python.readthedocs.io/en/master/apidoc/KafkaConsumer.html#kafka.KafkaConsumer.assign" target="_blank" rel="noopener noreferrer">assigning a topic partition<i class="fas fa-external-link-square-alt ms-1"></i></a>, we can easily override the fetch offset to a specific timestamp. When we deploy multiple consumer instances together, however, we make them <a href="https://kafka-python.readthedocs.io/en/master/apidoc/KafkaConsumer.html#kafka.KafkaConsumer.subscribe" target="_blank" rel="noopener noreferrer">subscribe to a topic<i class="fas fa-external-link-square-alt ms-1"></i></a> and topic partitions are dynamically assigned, which means we cannot determine which fetch offset to use for an instance. In this post, we develop Kafka producer and consumer applications using the <a href="https://kafka-python.readthedocs.io/en/master/" target="_blank" rel="noopener noreferrer">kafka-python<i class="fas fa-external-link-square-alt ms-1"></i></a> package and discuss how to configure the consumer instances to seek offsets to a specific timestamp where topic partitions are dynamically assigned by subscription.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2023-01-10-kafka-consumer-seek-offsets/featured.png" length="47217" type="image/png"/></item><item><title>Data Build Tool (dbt) for Effective Data Transformation on AWS – Part 5 Athena</title><link>https://jaehyeon.me/blog/2022-12-06-dbt-on-aws-part-5-athena/</link><pubDate>Tue, 06 Dec 2022 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2022-12-06-dbt-on-aws-part-5-athena/</guid><description><![CDATA[<p>The <a href="https://docs.getdbt.com/docs/introduction" target="_blank" rel="noopener noreferrer">data build tool (dbt)<i class="fas fa-external-link-square-alt ms-1"></i></a> is an effective data transformation tool and it supports key AWS analytics services - Redshift, Glue, EMR and Athena. In the previous posts, we discussed benefits of a common data transformation tool and the potential of dbt to cover a wide range of data projects from data warehousing to data lake to data lakehouse. Demo data projects that target Redshift Serverless, Glue, EMR on EC2 and EMR on EKS are illustrated as well. In the last part of the dbt on AWS series, we discuss data transformation pipelines using dbt on <a href="https://aws.amazon.com/athena" target="_blank" rel="noopener noreferrer">Amazon Athena<i class="fas fa-external-link-square-alt ms-1"></i></a>. <a href="https://www.imdb.com/interfaces/" target="_blank" rel="noopener noreferrer">Subsets of IMDb data<i class="fas fa-external-link-square-alt ms-1"></i></a> are used as source and data models are developed in multiple layers according to the <a href="https://docs.getdbt.com/guides/best-practices/how-we-structure/1-guide-overview" target="_blank" rel="noopener noreferrer">dbt best practices<i class="fas fa-external-link-square-alt ms-1"></i></a>. A list of posts of this series can be found below.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2022-12-06-dbt-on-aws-part-5-athena/featured.png" length="91796" type="image/png"/></item><item><title>Data Build Tool (dbt) for Effective Data Transformation on AWS – Part 4 EMR on EKS</title><link>https://jaehyeon.me/blog/2022-11-01-dbt-on-aws-part-4-emr-eks/</link><pubDate>Tue, 01 Nov 2022 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2022-11-01-dbt-on-aws-part-4-emr-eks/</guid><description><![CDATA[<p>The <a href="https://docs.getdbt.com/docs/introduction" target="_blank" rel="noopener noreferrer">data build tool (dbt)<i class="fas fa-external-link-square-alt ms-1"></i></a> is an effective data transformation tool and it supports key AWS analytics services - Redshift, Glue, EMR and Athena. In the previous posts, we discussed benefits of a common data transformation tool and the potential of dbt to cover a wide range of data projects from data warehousing to data lake to data lakehouse. Demo data projects that target Redshift Serverless, Glue and EMR on EC2 are illustrated as well. In part 4 of the dbt on AWS series, we discuss data transformation pipelines using dbt on <a href="https://aws.amazon.com/emr/features/eks/" target="_blank" rel="noopener noreferrer">Amazon EMR on EKS<i class="fas fa-external-link-square-alt ms-1"></i></a>. As Spark Submit does not allow the spark thrift server to run in cluster mode on Kubernetes, a simple wrapper class is created to overcome the limitation and it makes the thrift server run indefinitely. <a href="https://www.imdb.com/interfaces/" target="_blank" rel="noopener noreferrer">Subsets of IMDb data<i class="fas fa-external-link-square-alt ms-1"></i></a> are used as source and data models are developed in multiple layers according to the <a href="https://docs.getdbt.com/guides/best-practices/how-we-structure/1-guide-overview" target="_blank" rel="noopener noreferrer">dbt best practices<i class="fas fa-external-link-square-alt ms-1"></i></a>. A list of posts of this series can be found below.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2022-11-01-dbt-on-aws-part-4-emr-eks/featured.png" length="91067" type="image/png"/></item><item><title>Data Build Tool (dbt) for Effective Data Transformation on AWS – Part 3 EMR on EC2</title><link>https://jaehyeon.me/blog/2022-10-19-dbt-on-aws-part-3-emr-ec2/</link><pubDate>Wed, 19 Oct 2022 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2022-10-19-dbt-on-aws-part-3-emr-ec2/</guid><description><![CDATA[<p>The <a href="https://docs.getdbt.com/docs/introduction" target="_blank" rel="noopener noreferrer">data build tool (dbt)<i class="fas fa-external-link-square-alt ms-1"></i></a> is an effective data transformation tool and it supports key AWS analytics services - Redshift, Glue, EMR and Athena. In the previous posts, we discussed benefits of a common data transformation tool and the potential of dbt to cover a wide range of data projects from data warehousing to data lake to data lakehouse. Demo data projects that target Redshift Serverless and Glue are illustrated as well. In part 3 of the dbt on AWS series, we discuss data transformation pipelines using dbt on <a href="https://aws.amazon.com/emr/" target="_blank" rel="noopener noreferrer">Amazon EMR<i class="fas fa-external-link-square-alt ms-1"></i></a>. <a href="https://www.imdb.com/interfaces/" target="_blank" rel="noopener noreferrer">Subsets of IMDb data<i class="fas fa-external-link-square-alt ms-1"></i></a> are used as source and data models are developed in multiple layers according to the <a href="https://docs.getdbt.com/guides/best-practices/how-we-structure/1-guide-overview" target="_blank" rel="noopener noreferrer">dbt best practices<i class="fas fa-external-link-square-alt ms-1"></i></a>. A list of posts of this series can be found below.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2022-10-19-dbt-on-aws-part-3-emr-ec2/featured.png" length="91067" type="image/png"/></item><item><title>Data Build Tool (dbt) for Effective Data Transformation on AWS – Part 2 Glue</title><link>https://jaehyeon.me/blog/2022-10-09-dbt-on-aws-part-2-glue/</link><pubDate>Sun, 09 Oct 2022 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2022-10-09-dbt-on-aws-part-2-glue/</guid><description><![CDATA[<p>The <a href="https://docs.getdbt.com/docs/introduction" target="_blank" rel="noopener noreferrer">data build tool (dbt)<i class="fas fa-external-link-square-alt ms-1"></i></a> is an effective data transformation tool and it supports key AWS analytics services - Redshift, Glue, EMR and Athena. In <a href="/blog/2022-09-28-dbt-on-aws-part-1-redshift">part 1</a>, we discussed benefits of a common data transformation tool and the potential of dbt to cover a wide range of data projects from data warehousing to data lake to data lakehouse. A demo data project that targets Redshift Serverless is illustrated as well. In part 2 of the dbt on AWS series, we discuss data transformation pipelines using dbt on <a href="https://aws.amazon.com/glue/" target="_blank" rel="noopener noreferrer">AWS Glue<i class="fas fa-external-link-square-alt ms-1"></i></a>. <a href="https://www.imdb.com/interfaces/" target="_blank" rel="noopener noreferrer">Subsets of IMDb data<i class="fas fa-external-link-square-alt ms-1"></i></a> are used as source and data models are developed in multiple layers according to the <a href="https://docs.getdbt.com/guides/best-practices/how-we-structure/1-guide-overview" target="_blank" rel="noopener noreferrer">dbt best practices<i class="fas fa-external-link-square-alt ms-1"></i></a>. A list of posts of this series can be found below.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2022-10-09-dbt-on-aws-part-2-glue/featured.png" length="90647" type="image/png"/></item><item><title>Data Build Tool (dbt) for Effective Data Transformation on AWS – Part 1 Redshift</title><link>https://jaehyeon.me/blog/2022-09-28-dbt-on-aws-part-1-redshift/</link><pubDate>Wed, 28 Sep 2022 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2022-09-28-dbt-on-aws-part-1-redshift/</guid><description><![CDATA[<p>The <a href="https://docs.getdbt.com/docs/introduction" target="_blank" rel="noopener noreferrer">data build tool (dbt)<i class="fas fa-external-link-square-alt ms-1"></i></a> is an effective data transformation tool and it supports key AWS analytics services - Redshift, Glue, EMR and Athena. In part 1 of the dbt on AWS series, we discuss data transformation pipelines using dbt on <a href="https://aws.amazon.com/redshift/redshift-serverless/" target="_blank" rel="noopener noreferrer">Redshift Serverless<i class="fas fa-external-link-square-alt ms-1"></i></a>. <a href="https://www.imdb.com/interfaces/" target="_blank" rel="noopener noreferrer">Subsets of IMDb data<i class="fas fa-external-link-square-alt ms-1"></i></a> are used as source and data models are developed in multiple layers according to the <a href="https://docs.getdbt.com/guides/best-practices/how-we-structure/1-guide-overview" target="_blank" rel="noopener noreferrer">dbt best practices<i class="fas fa-external-link-square-alt ms-1"></i></a>.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2022-09-28-dbt-on-aws-part-1-redshift/featured.png" length="97234" type="image/png"/></item><item><title>Develop and Test Apache Spark Apps for EMR Remotely Using Visual Studio Code</title><link>https://jaehyeon.me/blog/2022-09-07-emr-remote-dev/</link><pubDate>Wed, 07 Sep 2022 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2022-09-07-emr-remote-dev/</guid><description><![CDATA[<p>When we develop a Spark application on EMR, we can use <a href="/blog/2022-05-08-emr-local-dev">docker for local development</a> or notebooks via <a href="https://aws.amazon.com/emr/features/studio/" target="_blank" rel="noopener noreferrer">EMR Studio<i class="fas fa-external-link-square-alt ms-1"></i></a> (or <a href="https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-managed-notebooks.html" target="_blank" rel="noopener noreferrer">EMR Notebooks<i class="fas fa-external-link-square-alt ms-1"></i></a>). However, the local development option is not viable if the size of data is large. Also, I am not a fan of notebooks as it is not possible to utilise the features my editor supports such as syntax highlighting, autocomplete and code formatting. Moreover, it is not possible to organise code into modules and to perform unit testing properly with that option. In this post, We will discuss how to set up a remote development environment on an EMR cluster deployed in a private subnet with VPN and the <a href="https://code.visualstudio.com/docs/remote/ssh" target="_blank" rel="noopener noreferrer">VS Code remote SSH extension<i class="fas fa-external-link-square-alt ms-1"></i></a>. Typical Spark development examples will be illustrated while sharing the cluster with multiple users. Overall it brings another effective way of developing Spark apps on EMR, which improves developer experience significantly.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2022-09-07-emr-remote-dev/featured.png" length="72448" type="image/png"/></item><item><title>Manage EMR on EKS with Terraform</title><link>https://jaehyeon.me/blog/2022-08-26-emr-on-eks-with-terraform/</link><pubDate>Fri, 26 Aug 2022 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2022-08-26-emr-on-eks-with-terraform/</guid><description><![CDATA[<p><a href="https://aws.amazon.com/emr/features/eks/" target="_blank" rel="noopener noreferrer">Amazon EMR on EKS<i class="fas fa-external-link-square-alt ms-1"></i></a> is a deployment option for Amazon EMR that allows you to automate the provisioning and management of open-source big data frameworks on EKS. While <a href="https://eksctl.io/" target="_blank" rel="noopener noreferrer">eksctl<i class="fas fa-external-link-square-alt ms-1"></i></a> is popular for working with <a href="https://aws.amazon.com/eks/" target="_blank" rel="noopener noreferrer">Amazon EKS<i class="fas fa-external-link-square-alt ms-1"></i></a> clusters, it has limitations when it comes to building infrastructure that integrates multiple AWS services. Also, it is not straightforward to update EKS cluster resources incrementally with it. On the other hand <a href="https://www.terraform.io/" target="_blank" rel="noopener noreferrer">Terraform<i class="fas fa-external-link-square-alt ms-1"></i></a> can be an effective tool for managing infrastructure that includes not only EKS and EMR virtual clusters but also other AWS resources. Moreover, Terraform has a wide range of <a href="https://www.terraform.io/language/modules" target="_blank" rel="noopener noreferrer">modules<i class="fas fa-external-link-square-alt ms-1"></i></a>, and it can even be simpler to build and manage infrastructure using those compared to the CLI tool. In this post, we’ll discuss how to provision and manage Spark jobs on EMR on EKS with Terraform. <a href="https://aws-ia.github.io/terraform-aws-eks-blueprints/" target="_blank" rel="noopener noreferrer">Amazon EKS Blueprints for Terraform<i class="fas fa-external-link-square-alt ms-1"></i></a> will be used for provisioning EKS, EMR virtual cluster and related resources. Also, Spark job autoscaling will be managed by <a href="https://karpenter.sh/" target="_blank" rel="noopener noreferrer">Karpenter<i class="fas fa-external-link-square-alt ms-1"></i></a> where two Spark jobs with and without <a href="https://spark.apache.org/docs/latest/job-scheduling.html#dynamic-resource-allocation" target="_blank" rel="noopener noreferrer">Dynamic Resource Allocation (DRA)<i class="fas fa-external-link-square-alt ms-1"></i></a> will be compared.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2022-08-26-emr-on-eks-with-terraform/featured.png" length="67936" type="image/png"/></item><item><title>Revisit AWS Lambda Invoke Function Operator of Apache Airflow</title><link>https://jaehyeon.me/blog/2022-08-06-revisit-lambda-operator/</link><pubDate>Sat, 06 Aug 2022 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2022-08-06-revisit-lambda-operator/</guid><description><![CDATA[<p><a href="https://airflow.apache.org/" target="_blank" rel="noopener noreferrer">Apache Airflow<i class="fas fa-external-link-square-alt ms-1"></i></a> is a popular workflow management platform. A wide range of AWS services are integrated with the platform by <a href="https://airflow.apache.org/docs/apache-airflow-providers-amazon/stable/operators/index.html" target="_blank" rel="noopener noreferrer">Amazon AWS Operators<i class="fas fa-external-link-square-alt ms-1"></i></a>. AWS Lambda is one of the integrated services, and it can be used to develop workflows efficiently. The current <a href="https://airflow.apache.org/docs/apache-airflow-providers-amazon/stable/operators/lambda.html" target="_blank" rel="noopener noreferrer">Lambda Operator<i class="fas fa-external-link-square-alt ms-1"></i></a>, however, just invokes a Lambda function, and it can fail to report the invocation result of a function correctly and to record the exact error message from failure. In this post, we’ll discuss a custom Lambda operator that handles those limitations.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2022-08-06-revisit-lambda-operator/featured.png" length="24814" type="image/png"/></item><item><title>Serverless Application Model (SAM) for Data Professionals</title><link>https://jaehyeon.me/blog/2022-07-18-sam-for-data-professionals/</link><pubDate>Mon, 18 Jul 2022 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2022-07-18-sam-for-data-professionals/</guid><description><![CDATA[<p><a href="https://aws.amazon.com/lambda/" target="_blank" rel="noopener noreferrer">AWS Lambda<i class="fas fa-external-link-square-alt ms-1"></i></a> provides serverless computing capabilities, and it can be used for performing validation or light processing/transformation of data. Moreover, with its integration with more than 140 AWS services, it facilitates building complex systems employing <a href="https://docs.aws.amazon.com/lambda/latest/operatorguide/event-driven-architectures.html" target="_blank" rel="noopener noreferrer">event-driven architectures<i class="fas fa-external-link-square-alt ms-1"></i></a>. There are many ways to build serverless applications and one of the most efficient ways is using specialised frameworks such as the <a href="https://aws.amazon.com/serverless/sam/" target="_blank" rel="noopener noreferrer">AWS Serverless Application Model (SAM)<i class="fas fa-external-link-square-alt ms-1"></i></a> and <a href="https://www.serverless.com/framework/docs" target="_blank" rel="noopener noreferrer">Serverless Framework<i class="fas fa-external-link-square-alt ms-1"></i></a>. In this post, I’ll demonstrate how to build a serverless data processing application using SAM.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2022-07-18-sam-for-data-professionals/featured.png" length="22838" type="image/png"/></item><item><title>Data Warehousing ETL Demo with Apache Iceberg on EMR Local Environment</title><link>https://jaehyeon.me/blog/2022-06-26-iceberg-etl-demo/</link><pubDate>Sun, 26 Jun 2022 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2022-06-26-iceberg-etl-demo/</guid><description><![CDATA[<p>Unlike traditional Data Lake, new table formats (<a href="https://iceberg.apache.org/" target="_blank" rel="noopener noreferrer">Iceberg<i class="fas fa-external-link-square-alt ms-1"></i></a>, <a href="https://hudi.apache.org/" target="_blank" rel="noopener noreferrer">Hudi<i class="fas fa-external-link-square-alt ms-1"></i></a> and <a href="https://delta.io/" target="_blank" rel="noopener noreferrer">Delta Lake<i class="fas fa-external-link-square-alt ms-1"></i></a>) support <a href="https://iceberg.apache.org/docs/latest/spark-writes/" target="_blank" rel="noopener noreferrer">features<i class="fas fa-external-link-square-alt ms-1"></i></a> that can be used to apply data warehousing patterns, which can bring a way to be rescued from <a href="https://www.gartner.com/en/newsroom/press-releases/2014-07-28-gartner-says-beware-of-the-data-lake-fallacy" target="_blank" rel="noopener noreferrer">Data Swamp<i class="fas fa-external-link-square-alt ms-1"></i></a>. In this post, we&rsquo;ll discuss how to implement ETL using retail analytics data. It has two dimension data (user and product) and a single fact data (order). The dimension data sets have different ETL strategies depending on whether to track historical changes. For the fact data, the primary keys of the dimension data are added to facilitate later queries. We&rsquo;ll use Iceberg for data storage/management and Spark for data processing. Instead of provisioning an EMR cluster, a local development environment will be used. Finally, the ETL results will be queried by Athena for verification.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2022-06-26-iceberg-etl-demo/featured.png" length="43604" type="image/png"/></item><item><title>Develop and Test Apache Spark Apps for EMR Locally Using Docker</title><link>https://jaehyeon.me/blog/2022-05-08-emr-local-dev/</link><pubDate>Sun, 08 May 2022 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2022-05-08-emr-local-dev/</guid><description><![CDATA[<p>[<strong>UPDATE 2023-12-07</strong>]</p>
<ul>
<li>I wrote a <a href="/blog/2023-12-07-flink-spark-local-dev">new post</a> that simplifies the Spark configuration dramatically. Besides, the log configuration is based on Log4J2, which applies to newer Spark versions. Moreover, the container is configured to run the Spark History Server, and it allows us to debug and diagnose completed and running Spark applications. I recommend referring to the new post.</li>
</ul>
<p><a href="https://aws.amazon.com/emr/" target="_blank" rel="noopener noreferrer">Amazon EMR<i class="fas fa-external-link-square-alt ms-1"></i></a> is a managed service that simplifies running Apache Spark on AWS. It has multiple deployment options that cover EC2, <a href="https://aws.amazon.com/emr/features/eks/" target="_blank" rel="noopener noreferrer">EKS<i class="fas fa-external-link-square-alt ms-1"></i></a>, <a href="https://aws.amazon.com/emr/features/outposts/" target="_blank" rel="noopener noreferrer">Outposts<i class="fas fa-external-link-square-alt ms-1"></i></a> and <a href="https://aws.amazon.com/emr/serverless/" target="_blank" rel="noopener noreferrer">Serverless<i class="fas fa-external-link-square-alt ms-1"></i></a>. For development and testing, <a href="https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-managed-notebooks.html" target="_blank" rel="noopener noreferrer">EMR Notebooks<i class="fas fa-external-link-square-alt ms-1"></i></a> or <a href="https://aws.amazon.com/emr/features/studio/" target="_blank" rel="noopener noreferrer">EMR Studio<i class="fas fa-external-link-square-alt ms-1"></i></a> can be an option. Both provide a Jupyter Notebook environment and the former is only available for EMR on EC2. There are cases, however, that development (and learning) is performed in a local environment more efficiently. The AWS Glue team understands this demand, and they illustrate how to make use of a custom Docker image for Glue in a <a href="https://aws.amazon.com/blogs/big-data/develop-and-test-aws-glue-version-3-0-jobs-locally-using-a-docker-container/" target="_blank" rel="noopener noreferrer">recent blog post<i class="fas fa-external-link-square-alt ms-1"></i></a>. However, we don’t hear similar news from the EMR team. In order to fill the gap, we’ll discuss how to create a Spark local development environment for EMR using Docker and/or VSCode. Typical Spark development examples will be demonstrated, which covers Spark Submit, Pytest, PySpark shell, Jupyter Notebook and Spark Structured Streaming. For the Spark Submit and Jupyter Notebook examples, <a href="https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-spark-glue.html" target="_blank" rel="noopener noreferrer">Glue Catalog integration<i class="fas fa-external-link-square-alt ms-1"></i></a> will be illustrated as well. And both the cases of utilising <a href="https://code.visualstudio.com/docs/remote/containers" target="_blank" rel="noopener noreferrer">Visual Studio Code Remote - Containers<i class="fas fa-external-link-square-alt ms-1"></i></a> extension and running as an isolated container will be covered in some key examples.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2022-05-08-emr-local-dev/featured.png" length="25693" type="image/png"/></item><item><title>Use External Schema Registry with MSK Connect – Part 2 MSK Deployment</title><link>https://jaehyeon.me/blog/2022-04-03-schema-registry-part2/</link><pubDate>Sun, 03 Apr 2022 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2022-04-03-schema-registry-part2/</guid><description><![CDATA[<p>In the <a href="/blog/2022-03-07-schema-registry-part1">previous post</a>, we discussed a Change Data Capture (CDC) solution with a schema registry. A local development environment is set up using Docker Compose. The Debezium and Confluent S3 connectors are deployed with the Confluent Avro converter and the Apicurio registry is used as the schema registry service. A quick example is shown to illustrate how schema evolution can be managed by the schema registry. In this post, we&rsquo;ll build the solution on AWS using <a href="https://aws.amazon.com/msk/" target="_blank" rel="noopener noreferrer">MSK<i class="fas fa-external-link-square-alt ms-1"></i></a>, <a href="https://aws.amazon.com/msk/features/msk-connect/" target="_blank" rel="noopener noreferrer">MSK Connect<i class="fas fa-external-link-square-alt ms-1"></i></a>, <a href="https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.AuroraPostgreSQL.html" target="_blank" rel="noopener noreferrer">Aurora PostgreSQL<i class="fas fa-external-link-square-alt ms-1"></i></a> and <a href="https://aws.amazon.com/ecs/" target="_blank" rel="noopener noreferrer">ECS<i class="fas fa-external-link-square-alt ms-1"></i></a>.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2022-04-03-schema-registry-part2/featured.png" length="59689" type="image/png"/></item><item><title>Use External Schema Registry with MSK Connect – Part 1 Local Development</title><link>https://jaehyeon.me/blog/2022-03-07-schema-registry-part1/</link><pubDate>Mon, 07 Mar 2022 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2022-03-07-schema-registry-part1/</guid><description><![CDATA[<p>When we discussed a Change Data Capture (CDC) solution in <a href="/blog/2021-12-12-datalake-demo-part2">one of the earlier posts</a>, we used the JSON converter that comes with Kafka Connect. We optionally enabled the key and value schemas and the topic messages include those schemas together with payload. It seems to be convenient at first as the messages are saved into S3 on their own. However, it became cumbersome when we tried to use the <a href="https://hudi.apache.org/docs/hoodie_streaming_ingestion" target="_blank" rel="noopener noreferrer">DeltaStreamer<i class="fas fa-external-link-square-alt ms-1"></i></a> utility. Specifically it requires the scheme of the files, but unfortunately we cannot use the schema that is generated by the default JSON converter - it returns the <a href="https://www.confluent.io/blog/kafka-connect-deep-dive-converters-serialization-explained/#json-schemas" target="_blank" rel="noopener noreferrer">struct type<i class="fas fa-external-link-square-alt ms-1"></i></a>, which is not supported by the Hudi utility. In order to handle this issue, we created a schema with the <a href="https://avro.apache.org/docs/current/spec.html#schema_record" target="_blank" rel="noopener noreferrer">record type<i class="fas fa-external-link-square-alt ms-1"></i></a> using the Confluent Avro converter and used it after saving on S3. However, as we aimed to manage a long-running process, generating a schema manually was not an optimal solution because, for example, we&rsquo;re not able to handle schema evolution effectively. In this post, we&rsquo;ll discuss an improved architecture that makes use of a schema registry that resides outside the Kafka cluster and allows the producers and consumers to reference the schemas externally.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2022-03-07-schema-registry-part1/featured.png" length="59689" type="image/png"/></item><item><title>Simplify Your Development on AWS with Terraform</title><link>https://jaehyeon.me/blog/2022-02-06-dev-infra-terraform/</link><pubDate>Sun, 06 Feb 2022 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2022-02-06-dev-infra-terraform/</guid><description><![CDATA[<p>When I wrote my data lake demo series (<a href="/blog/2021-12-05-datalake-demo-part1">part 1</a>, <a href="/blog/2021-12-12-datalake-demo-part2">part 2</a> and <a href="/blog/2021-12-19-datalake-demo-part3">part 3</a>) recently, I used an Aurora PostgreSQL, MSK and EMR cluster. All of them were deployed to private subnets and dedicated infrastructure was created using CloudFormation. Using the infrastructure as code (IaC) tool helped a lot, but it resulted in creating 7 CloudFormation stacks, which was a bit harder to manage in the end. Then I looked into how to simplify building infrastructure and managing resources on AWS and decided to use Terraform instead. I find it has useful constructs (e.g. <a href="https://blog.knoldus.com/meta-arguments-in-terraform/" target="_blank" rel="noopener noreferrer">meta-arguments<i class="fas fa-external-link-square-alt ms-1"></i></a>) to make it simpler to create and manage resources. It also has a wide range of useful <a href="https://registry.terraform.io/namespaces/terraform-aws-modules" target="_blank" rel="noopener noreferrer">modules<i class="fas fa-external-link-square-alt ms-1"></i></a> that facilitate development significantly. In this post, we’ll build an infrastructure for development on AWS with Terraform. A VPN server will also be included in order to improve developer experience by accessing resources in private subnets from developer machines.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2022-02-06-dev-infra-terraform/featured.png" length="46253" type="image/png"/></item><item><title>EMR on EKS by Example</title><link>https://jaehyeon.me/blog/2022-01-17-emr-on-eks-by-example/</link><pubDate>Mon, 17 Jan 2022 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2022-01-17-emr-on-eks-by-example/</guid><description><![CDATA[<p><a href="https://aws.amazon.com/emr/features/eks/" target="_blank" rel="noopener noreferrer">EMR on EKS<i class="fas fa-external-link-square-alt ms-1"></i></a> provides a deployment option for <a href="https://aws.amazon.com/emr/" target="_blank" rel="noopener noreferrer">Amazon EMR<i class="fas fa-external-link-square-alt ms-1"></i></a> that allows you to automate the provisioning and management of open-source big data frameworks on <a href="https://aws.amazon.com/eks/" target="_blank" rel="noopener noreferrer">Amazon EKS<i class="fas fa-external-link-square-alt ms-1"></i></a>. While a wide range of open source big data components are available in EMR on EC2, only Apache Spark is available in EMR on EKS. It is more flexible, however, that applications of different EMR versions can be run in multiple availability zones on either EC2 or Fargate. Also, other types of containerized applications can be deployed on the same EKS cluster. Therefore, if you have or plan to have, for example, <a href="https://airflow.apache.org/" target="_blank" rel="noopener noreferrer">Apache Airflow<i class="fas fa-external-link-square-alt ms-1"></i></a>, <a href="https://superset.apache.org/" target="_blank" rel="noopener noreferrer">Apache Superset<i class="fas fa-external-link-square-alt ms-1"></i></a> or <a href="https://www.kubeflow.org/" target="_blank" rel="noopener noreferrer">Kubeflow<i class="fas fa-external-link-square-alt ms-1"></i></a> as your analytics toolkits, it can be an effective way to manage big data (as well as non-big data) workloads. While Glue is more for ETL, EMR on EKS can also be used for other types of tasks such as machine learning. Moreover, it allows you to build a Spark application, not a <em>Gluish</em> Spark application. For example, while you have to use custom connectors for <a href="https://aws.amazon.com/marketplace/pp/prodview-zv3vmwbkuat2e" target="_blank" rel="noopener noreferrer">Hudi<i class="fas fa-external-link-square-alt ms-1"></i></a> or <a href="https://aws.amazon.com/marketplace/pp/prodview-iicxofvpqvsio" target="_blank" rel="noopener noreferrer">Iceberg<i class="fas fa-external-link-square-alt ms-1"></i></a> for Glue, you can use their native libraries with EMR on EKS. In this post, we&rsquo;ll discuss EMR on EKS with simple and elaborated examples.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2022-01-17-emr-on-eks-by-example/featured.png" length="76740" type="image/png"/></item><item><title>Data Lake Demo using Change Data Capture (CDC) on AWS – Part 3 Implement Data Lake</title><link>https://jaehyeon.me/blog/2021-12-19-datalake-demo-part3/</link><pubDate>Sun, 19 Dec 2021 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2021-12-19-datalake-demo-part3/</guid><description><![CDATA[<p>In the <a href="/blog/2021-12-12-datalake-demo-part2">previous post</a>, we created a VPC that has private and public subnets in 2 availability zones in order to build and deploy the data lake solution on AWS. NAT instances are created to forward outbound traffic to the internet and a VPN bastion host is set up to facilitate deployment. An <a href="https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.AuroraPostgreSQL.html" target="_blank" rel="noopener noreferrer">Aurora PostgreSQL<i class="fas fa-external-link-square-alt ms-1"></i></a> cluster is deployed to host the source database and a Python command line app is used to create the database. To develop data ingestion using CDC, an <a href="https://aws.amazon.com/msk/" target="_blank" rel="noopener noreferrer">Amazon MSK<i class="fas fa-external-link-square-alt ms-1"></i></a> cluster is deployed and the <a href="https://debezium.io/documentation/reference/stable/connectors/postgresql.html" target="_blank" rel="noopener noreferrer">Debezium source<i class="fas fa-external-link-square-alt ms-1"></i></a> and <a href="https://lenses.io/blog/2020/11/new-kafka-to-S3-connector/" target="_blank" rel="noopener noreferrer">Lenses S3 sink<i class="fas fa-external-link-square-alt ms-1"></i></a> connectors are created on <a href="https://aws.amazon.com/msk/features/msk-connect/" target="_blank" rel="noopener noreferrer">MSK Connect<i class="fas fa-external-link-square-alt ms-1"></i></a>. We also confirmed the order creation and update events are captured as expected. As the last part of this series, we&rsquo;ll build an <a href="https://hudi.apache.org/docs/writing_data/#deltastreamer" target="_blank" rel="noopener noreferrer">Apache Hudi DeltaStreamer<i class="fas fa-external-link-square-alt ms-1"></i></a> app on <a href="https://aws.amazon.com/emr/" target="_blank" rel="noopener noreferrer">Amazon EMR<i class="fas fa-external-link-square-alt ms-1"></i></a> and use the resulting Hudi table with <a href="https://aws.amazon.com/athena/" target="_blank" rel="noopener noreferrer">Amazon Athena<i class="fas fa-external-link-square-alt ms-1"></i></a> and <a href="https://aws.amazon.com/quicksight/" target="_blank" rel="noopener noreferrer">Amazon Quicksight<i class="fas fa-external-link-square-alt ms-1"></i></a> to build a dashboard.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2021-12-19-datalake-demo-part3/featured.png" length="164526" type="image/png"/></item><item><title>Data Lake Demo using Change Data Capture (CDC) on AWS – Part 2 Implement CDC</title><link>https://jaehyeon.me/blog/2021-12-12-datalake-demo-part2/</link><pubDate>Sun, 12 Dec 2021 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2021-12-12-datalake-demo-part2/</guid><description><![CDATA[<p>In the <a href="/blog/2021-12-05-datalake-demo-part1">previous post</a>, we discussed a data lake solution where data ingestion is performed using <a href="https://www.redhat.com/en/topics/integration/what-is-change-data-capture#what-is-cdc" target="_blank" rel="noopener noreferrer">change data capture (CDC)<i class="fas fa-external-link-square-alt ms-1"></i></a> and the output files are <em>upserted</em> to an <a href="https://hudi.apache.org/" target="_blank" rel="noopener noreferrer">Apache Hudi<i class="fas fa-external-link-square-alt ms-1"></i></a> table. Being registered to <a href="https://docs.aws.amazon.com/glue/latest/dg/populate-data-catalog.html" target="_blank" rel="noopener noreferrer">Glue Data Catalog<i class="fas fa-external-link-square-alt ms-1"></i></a>, it can be used for ad-hoc queries and report/dashboard creation. The <a href="https://docs.yugabyte.com/latest/sample-data/northwind/" target="_blank" rel="noopener noreferrer">Northwind database<i class="fas fa-external-link-square-alt ms-1"></i></a> is used as the source database and, following the <a href="https://microservices.io/patterns/data/transactional-outbox.html" target="_blank" rel="noopener noreferrer">transactional outbox pattern<i class="fas fa-external-link-square-alt ms-1"></i></a>, order-related changes are _upserted _to an outbox table by triggers. The data ingestion is developed using Kafka connectors in the <a href="https://docs.confluent.io/platform/current/quickstart/ce-docker-quickstart.html" target="_blank" rel="noopener noreferrer">local Confluent platform<i class="fas fa-external-link-square-alt ms-1"></i></a> where the <a href="https://debezium.io/documentation/reference/stable/connectors/postgresql.html" target="_blank" rel="noopener noreferrer">Debezium for PostgreSQL<i class="fas fa-external-link-square-alt ms-1"></i></a> is used as the source connector and the <a href="https://lenses.io/blog/2020/11/new-kafka-to-S3-connector/" target="_blank" rel="noopener noreferrer">Lenses S3 sink connector<i class="fas fa-external-link-square-alt ms-1"></i></a> is used as the sink connector. We confirmed the order creation and update events are captured as expected, and it is ready for production deployment. In this post, we&rsquo;ll build the CDC part of the solution on AWS using <a href="https://aws.amazon.com/msk/" target="_blank" rel="noopener noreferrer">Amazon MSK<i class="fas fa-external-link-square-alt ms-1"></i></a> and <a href="https://aws.amazon.com/msk/features/msk-connect/" target="_blank" rel="noopener noreferrer">MSK Connect<i class="fas fa-external-link-square-alt ms-1"></i></a>.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2021-12-12-datalake-demo-part2/featured.png" length="164526" type="image/png"/></item><item><title>Data Lake Demo using Change Data Capture (CDC) on AWS – Part 1 Local Development</title><link>https://jaehyeon.me/blog/2021-12-05-datalake-demo-part1/</link><pubDate>Sun, 05 Dec 2021 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2021-12-05-datalake-demo-part1/</guid><description><![CDATA[<p><a href="https://www.redhat.com/en/topics/integration/what-is-change-data-capture#what-is-cdc" target="_blank" rel="noopener noreferrer">Change data capture (CDC)<i class="fas fa-external-link-square-alt ms-1"></i></a> is a proven data integration pattern that has a wide range of applications. Among those, data replication to data lakes is a good use case in data engineering. Coupled with <a href="https://lakefs.io/hudi-iceberg-and-delta-lake-data-lake-table-formats-compared/" target="_blank" rel="noopener noreferrer">best-in-breed data lake formats<i class="fas fa-external-link-square-alt ms-1"></i></a> such as <a href="https://hudi.apache.org/" target="_blank" rel="noopener noreferrer">Apache Hudi<i class="fas fa-external-link-square-alt ms-1"></i></a>, we can build an efficient data replication solution. This is the first post of the data lake demo series. Over time, we&rsquo;ll build a data lake that uses CDC. As a starting point, we&rsquo;ll discuss the source database and CDC streaming infrastructure in the local environment.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2021-12-05-datalake-demo-part1/featured.png" length="164526" type="image/png"/></item><item><title>Local Development of AWS Glue 3.0 and Later</title><link>https://jaehyeon.me/blog/2021-11-14-glue-3-local-development/</link><pubDate>Sun, 14 Nov 2021 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2021-11-14-glue-3-local-development/</guid><description><![CDATA[<p>In an <a href="/blog/2021-08-20-glue-local-development">earlier post</a>, I demonstrated how to set up a local development environment for AWS Glue 1.0 and 2.0 using a <a href="https://aws.amazon.com/blogs/big-data/developing-aws-glue-etl-jobs-locally-using-a-container/" target="_blank" rel="noopener noreferrer">docker image that is published by the AWS Glue team<i class="fas fa-external-link-square-alt ms-1"></i></a> and the <a href="https://code.visualstudio.com/docs/remote/containers" target="_blank" rel="noopener noreferrer">Visual Studio Code Remote – Containers<i class="fas fa-external-link-square-alt ms-1"></i></a> extension. Recently <a href="https://aws.amazon.com/about-aws/whats-new/2021/08/spark-3-1-runtime-aws-glue-3-0/" target="_blank" rel="noopener noreferrer">AWS Glue 3.0 was released<i class="fas fa-external-link-square-alt ms-1"></i></a>, but a docker image for this version is not published. In this post, I&rsquo;ll illustrate how to create a development environment for AWS Glue 3.0 (and later versions) by building a custom docker image.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2021-11-14-glue-3-local-development/featured.png" length="30923" type="image/png"/></item><item><title>Yet another serverless solution for invoking AWS Lambda at a sub-minute frequency</title><link>https://jaehyeon.me/blog/2021-10-13-lambda-schedule/</link><pubDate>Wed, 13 Oct 2021 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2021-10-13-lambda-schedule/</guid><description><![CDATA[<p><a href="https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-run-lambda-schedule.html" target="_blank" rel="noopener noreferrer">Triggering a Lambda function by an EventBridge Events rule<i class="fas fa-external-link-square-alt ms-1"></i></a> can be used as a _serverless _replacement of <a href="https://en.wikipedia.org/wiki/Cron" target="_blank" rel="noopener noreferrer">cron job<i class="fas fa-external-link-square-alt ms-1"></i></a>. The highest frequency of it is one invocation per minute so that it cannot be used directly if you need to schedule a Lambda function more frequently. For example, it may be refreshing an application with real time metrics from an Amazon Connect instance where <a href="https://docs.aws.amazon.com/connect/latest/adminguide/real-time-metrics-reports.html" target="_blank" rel="noopener noreferrer">some metrics are updated every 15 seconds<i class="fas fa-external-link-square-alt ms-1"></i></a>. There is a <a href="https://aws.amazon.com/blogs/architecture/a-serverless-solution-for-invoking-aws-lambda-at-a-sub-minute-frequency/" target="_blank" rel="noopener noreferrer">post in the AWS Architecture Blog<i class="fas fa-external-link-square-alt ms-1"></i></a>, and it suggests using <a href="https://aws.amazon.com/step-functions/" target="_blank" rel="noopener noreferrer">AWS Step Functions<i class="fas fa-external-link-square-alt ms-1"></i></a>. Or a usual recommendation is using <a href="https://stackoverflow.com/questions/35878619/scheduled-aws-lambda-task-at-less-than-1-minute-frequency" target="_blank" rel="noopener noreferrer">Amazon EC2<i class="fas fa-external-link-square-alt ms-1"></i></a>. Albeit being <em>serverless</em>, the former gets a bit complicated especially in order to <a href="https://docs.aws.amazon.com/step-functions/latest/dg/tutorial-continue-new.html" target="_blank" rel="noopener noreferrer">handle the hard quota of 25,000 entries in the execution history<i class="fas fa-external-link-square-alt ms-1"></i></a>. And the latter is not an option if you look for a <em>serverless</em> solution. In this post, I’ll demonstrate another <em>serverless</em> solution of scheduling a Lambda function at a sub-minute frequency using <a href="https://aws.amazon.com/sqs/" target="_blank" rel="noopener noreferrer">Amazon SQS<i class="fas fa-external-link-square-alt ms-1"></i></a>.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2021-10-13-lambda-schedule/featured.png" length="46921" type="image/png"/></item><item><title>AWS Glue Local Development with Docker and Visual Studio Code</title><link>https://jaehyeon.me/blog/2021-08-20-glue-local-development/</link><pubDate>Fri, 20 Aug 2021 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2021-08-20-glue-local-development/</guid><description><![CDATA[<p>As described in the product page, <a href="https://aws.amazon.com/glue" target="_blank" rel="noopener noreferrer">AWS Glue<i class="fas fa-external-link-square-alt ms-1"></i></a> is a <em>serverless</em> data integration service that makes it easy to discover, prepare, and combine data for analytics, machine learning, and application development. For development, a development endpoint is recommended, but it can be costly, inconvenient or <a href="https://docs.aws.amazon.com/glue/latest/dg/reduced-start-times-spark-etl-jobs.html" target="_blank" rel="noopener noreferrer">unavailable (for Glue 2.0)<i class="fas fa-external-link-square-alt ms-1"></i></a>. The <a href="https://aws.amazon.com/blogs/big-data/developing-aws-glue-etl-jobs-locally-using-a-container/" target="_blank" rel="noopener noreferrer">AWS Glue team published a Docker image<i class="fas fa-external-link-square-alt ms-1"></i></a> that includes the AWS Glue binaries and all the dependencies packaged together. After inspecting it, I find some modifications are necessary in order to build a development environment on it. In this post, I&rsquo;ll demonstrate how to build development environments for AWS Glue 1.0 and 2.0 using the Docker image and the <a href="https://code.visualstudio.com/docs/remote/containers" target="_blank" rel="noopener noreferrer">Visual Studio Code Remote - Containers<i class="fas fa-external-link-square-alt ms-1"></i></a> extension.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2021-08-20-glue-local-development/featured.png" length="19535" type="image/png"/></item><item><title>Adding Authorization to a Graphql API</title><link>https://jaehyeon.me/blog/2021-07-20-graphql-api-authorization/</link><pubDate>Tue, 20 Jul 2021 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2021-07-20-graphql-api-authorization/</guid><description><![CDATA[<p>Authorization is the mechanism that controls who can do what on which resource in an application. Although it is a critical part of an application, there are limited resources available on how to build authorization into an app effectively. In this post, I&rsquo;ll be illustrating how to set up authorization in a GraphQL API using a custom <a href="https://www.apollographql.com/docs/apollo-server/schema/directives/" target="_blank" rel="noopener noreferrer">directive<i class="fas fa-external-link-square-alt ms-1"></i></a> and <a href="https://www.osohq.com/" target="_blank" rel="noopener noreferrer">Oso<i class="fas fa-external-link-square-alt ms-1"></i></a>, an open-source authorization library. This tutorial covers the NodeJS variant of Oso, but it also supports Python and other languages.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2021-07-20-graphql-api-authorization/featured.png" length="52143" type="image/png"/></item><item><title>Thoughts on Apache Airflow AWS Lambda Operator</title><link>https://jaehyeon.me/blog/2020-04-13-airflow-lambda-operator/</link><pubDate>Mon, 13 Apr 2020 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2020-04-13-airflow-lambda-operator/</guid><description><![CDATA[<p><a href="https://airflow.apache.org/" target="_blank" rel="noopener noreferrer">Apache Airflow<i class="fas fa-external-link-square-alt ms-1"></i></a> is a popular open-source workflow management platform. Typically tasks run remotely by <a href="http://www.celeryproject.org/" target="_blank" rel="noopener noreferrer">Celery<i class="fas fa-external-link-square-alt ms-1"></i></a> workers for scalability. In AWS, however, scalability can also be achieved using serverless computing services in a simpler way. For example, the <a href="https://airflow.apache.org/docs/apache-airflow-providers-amazon/stable/operators/ecs.html" target="_blank" rel="noopener noreferrer">ECS Operator<i class="fas fa-external-link-square-alt ms-1"></i></a> allows to run <em>dockerized</em> tasks and, with the <em>Fargate</em> launch type, they can run in a serverless environment.</p>
<p>The ECS Operator alone is not sufficient because it can take up to several minutes to pull a Docker image and to set up network interface (for the case of <em>Fargate</em> launch type). Due to its latency, it is not suitable for frequently-running tasks. On the other hand, the latency of a Lambda function is negligible so that it&rsquo;s more suitable for managing such tasks.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2020-04-13-airflow-lambda-operator/featured.png" length="44994" type="image/png"/></item><item><title>Dynamic Routing and Centralized Auth with Traefik, Python and R Example</title><link>https://jaehyeon.me/blog/2019-11-29-traefik-example/</link><pubDate>Fri, 29 Nov 2019 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2019-11-29-traefik-example/</guid><description><![CDATA[<p><a href="https://kubernetes.io/docs/concepts/services-networking/ingress/" target="_blank" rel="noopener noreferrer">Ingress<i class="fas fa-external-link-square-alt ms-1"></i></a> in <a href="https://kubernetes.io/" target="_blank" rel="noopener noreferrer">Kubernetes<i class="fas fa-external-link-square-alt ms-1"></i></a> exposes HTTP and HTTPS routes from outside the cluster to services within the cluster. By setting rules, it routes requests to appropriate services (precisely requests are sent to individual <a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-overview/" target="_blank" rel="noopener noreferrer">Pods<i class="fas fa-external-link-square-alt ms-1"></i></a> by <a href="https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/" target="_blank" rel="noopener noreferrer">Ingress Controller<i class="fas fa-external-link-square-alt ms-1"></i></a>). Rules can be set up dynamically and I find it&rsquo;s more efficient compared to traditional <a href="https://en.wikipedia.org/wiki/Reverse_proxy" target="_blank" rel="noopener noreferrer">reverse proxy<i class="fas fa-external-link-square-alt ms-1"></i></a>.</p>
<p><a href="https://docs.traefik.io/v1.7/" target="_blank" rel="noopener noreferrer">Traefik<i class="fas fa-external-link-square-alt ms-1"></i></a> is a modern HTTP reverse proxy and load balancer and it can be used as a <em>Kubernetes</em> <em>Ingress Controller</em>. Moreover it supports other <a href="https://docs.traefik.io/providers/overview/" target="_blank" rel="noopener noreferrer">providers<i class="fas fa-external-link-square-alt ms-1"></i></a>, which are existing infrastructure components such as orchestrators, container engines, cloud providers, or key-value stores. To name a few, Docker, Kubernetes, AWS ECS, AWS DynamoDB and Consul are <a href="https://docs.traefik.io/v1.7/" target="_blank" rel="noopener noreferrer">supported providers<i class="fas fa-external-link-square-alt ms-1"></i></a>. With <em>Traefik</em>, it is possible to configure routing dynamically. Another interesting feature is <a href="https://docs.traefik.io/v1.7/configuration/entrypoints/#forward-authentication" target="_blank" rel="noopener noreferrer">Forward Authentication<i class="fas fa-external-link-square-alt ms-1"></i></a> where authentication can be handled by an external service. In this post, it&rsquo;ll be demonstrated how <em>path-based</em> routing can be set up by <em>Traefik with Docker</em>. Also a centralized authentication will be illustrated with the <em>Forward Authentication</em> feature of <em>Traefik</em>.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2019-11-29-traefik-example/featured.png" length="139790" type="image/png"/></item><item><title>Distributed Task Queue with Python and R Example</title><link>https://jaehyeon.me/blog/2019-11-15-task-queue/</link><pubDate>Fri, 15 Nov 2019 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2019-11-15-task-queue/</guid><description><![CDATA[<p>While I&rsquo;m looking into <a href="https://airflow.apache.org/" target="_blank" rel="noopener noreferrer">Apache Airflow<i class="fas fa-external-link-square-alt ms-1"></i></a>, a workflow management tool, I thought it would be beneficial to get some understanding of how <a href="http://www.celeryproject.org/" target="_blank" rel="noopener noreferrer">Celery<i class="fas fa-external-link-square-alt ms-1"></i></a> works. To do so, I built a simple web service that sends tasks to Celery workers and collects the results from them. <a href="https://fastapi.tiangolo.com/" target="_blank" rel="noopener noreferrer">FastAPI<i class="fas fa-external-link-square-alt ms-1"></i></a> is used for developing the web service and <a href="https://redis.io/" target="_blank" rel="noopener noreferrer">Redis<i class="fas fa-external-link-square-alt ms-1"></i></a> is used for the message broker and result backend. During the development, I thought it would be possible to implement similar functionality in R with <a href="https://www.rforge.net/Rserve/" target="_blank" rel="noopener noreferrer">Rserve<i class="fas fa-external-link-square-alt ms-1"></i></a>. Therefore a Rserve worker is added as an example as well. Coupling a web service with distributed task queue is beneficial on its own as it helps the service be more responsive by offloading heavyweight and long running processes to task workers.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2019-11-15-task-queue/featured.png" length="51615" type="image/png"/></item><item><title>Linux Dev Environment on Windows</title><link>https://jaehyeon.me/blog/2019-11-01-linux-on-windows/</link><pubDate>Fri, 01 Nov 2019 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2019-11-01-linux-on-windows/</guid><description><![CDATA[<p>I use Linux containers a lot for development. Having Windows computers at home and work, I used to use Linux VMs on VirtualBox or VMWare Workstation. It&rsquo;s not a bad option but it requires a lot of resources. Recently, after my home computer was updated, I was not able to start my hypervisor anymore. Also I didn&rsquo;t like huge resource consumption of it so that I began to look for a different development environment. A while ago, I played with <a href="https://docs.microsoft.com/en-us/windows/wsl/about" target="_blank" rel="noopener noreferrer">Windows Subsystem for Linux (WSL)<i class="fas fa-external-link-square-alt ms-1"></i></a> and it was alright. Also <a href="https://code.visualstudio.com/" target="_blank" rel="noopener noreferrer">Visual Studio Code (VSCode)<i class="fas fa-external-link-square-alt ms-1"></i></a>, <em>my favourite editor</em>, now supports <a href="https://code.visualstudio.com/docs/remote/remote-overview" target="_blank" rel="noopener noreferrer">remote development<i class="fas fa-external-link-square-alt ms-1"></i></a>. Initially I thought I would be able to create a new development environment with WSL and <a href="https://docs.docker.com/docker-for-windows/install/" target="_blank" rel="noopener noreferrer">Docker for Windows<i class="fas fa-external-link-square-alt ms-1"></i></a>. However it was until I tried a bigger app with <a href="https://docs.docker.com/compose/" target="_blank" rel="noopener noreferrer">Docker Compose<i class="fas fa-external-link-square-alt ms-1"></i></a> that Docker for Windows has a number of issues especially when containers are started by Docker Compose in WSL. I didn&rsquo;t like to spend too much time on fixing those issues as I concerned those might not be the only ones. Then I decided to install a Linux VM on <a href="https://docs.microsoft.com/en-us/virtualization/hyper-v-on-windows/about/" target="_blank" rel="noopener noreferrer">Hyper-V<i class="fas fa-external-link-square-alt ms-1"></i></a>. Luckly VSCode also supports a remote VM via SSH.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2019-11-01-linux-on-windows/featured.png" length="187978" type="image/png"/></item><item><title>AWS Local Development with LocalStack</title><link>https://jaehyeon.me/blog/2019-07-20-aws-localstack/</link><pubDate>Sat, 20 Jul 2019 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2019-07-20-aws-localstack/</guid><description><![CDATA[<p><a href="https://github.com/localstack/localstack" target="_blank" rel="noopener noreferrer">LocalStack<i class="fas fa-external-link-square-alt ms-1"></i></a> provides an easy-to-use test/mocking framework for developing AWS applications. In this post, I&rsquo;ll demonstrate how to utilize LocalStack for development using a web service.</p>
<p>Specifically a simple web service built with <a href="https://flask-restplus.readthedocs.io/en/stable/" target="_blank" rel="noopener noreferrer">Flask-RestPlus<i class="fas fa-external-link-square-alt ms-1"></i></a> is used. It supports simple CRUD operations against a database table. It is set that SQS and Lambda are used for creating and updating a record. When a <em>POST</em> or <em>PUT</em> request is made, the service sends a message to a SQS queue and directly returns <em>204</em> reponse. Once a message is received, a Lambda function is invoked and a relevant database operation is performed.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2019-07-20-aws-localstack/featured.png" length="164886" type="image/png"/></item><item><title>Cronicle Multi Server Setup</title><link>https://jaehyeon.me/blog/2019-07-19-cronicle-multi-server-setup/</link><pubDate>Fri, 19 Jul 2019 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2019-07-19-cronicle-multi-server-setup/</guid><description><![CDATA[<p>Accroding to the <a href="https://github.com/jhuckaby/Cronicle" target="_blank" rel="noopener noreferrer">project GitHub repository<i class="fas fa-external-link-square-alt ms-1"></i></a>,</p>
<blockquote>
<p>Cronicle is a multi-server task scheduler and runner, with a web based front-end UI. It handles both scheduled, repeating and on-demand jobs, targeting any number of slave servers, with real-time stats and live log viewer.</p>
</blockquote>
<p>By default, Cronicle is configured to launch a single master server - task scheduling is controlled by the master server. For high availability, it is important that another server takes the role of master when the existing master server fails.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2019-07-19-cronicle-multi-server-setup/featured.png" length="64396" type="image/png"/></item><item><title>Shiny to Vue.js</title><link>https://jaehyeon.me/blog/2018-05-26-shiny-to-vue.js/</link><pubDate>Sat, 26 May 2018 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2018-05-26-shiny-to-vue.js/</guid><description>&lt;p>In the &lt;a href="/blog/2018-05-19-asyn-shiny-and-its-limitation">last post&lt;/a>, the async feature of Shiny was discussed. Although it is a remarkable step forward to web development in R, it is not to the full extent that a Javascript application can bring. In fact, (long running) requests of a user (or session) are not impacted by those of other users (or sessions) but, for a given user, all requests are handled sequentially. On the other hand, it is not the case for a Javascript-backed app where all requests are processed asynchronously.&lt;/p></description><enclosure url="https://jaehyeon.me/blog/2018-05-26-shiny-to-vue.js/featured.png" length="247205" type="image/png"/></item><item><title>Async Shiny and Its Limitation</title><link>https://jaehyeon.me/blog/2018-05-19-asyn-shiny-and-its-limitation/</link><pubDate>Sat, 19 May 2018 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2018-05-19-asyn-shiny-and-its-limitation/</guid><description><![CDATA[<p>A Shiny app is served by one (<em>single-threaded blocking</em>) process by <a href="https://www.rstudio.com/products/shiny/download-server/" target="_blank" rel="noopener noreferrer">Open Source Shiny Server<i class="fas fa-external-link-square-alt ms-1"></i></a>. This causes a scalability issue because all requests are handled one by one in a queue. Recently the creator of <em>Shiny</em> introduced the <a href="https://rstudio.github.io/promises/" target="_blank" rel="noopener noreferrer">promises<i class="fas fa-external-link-square-alt ms-1"></i></a> package, which brings <em>asynchronous programming capabilities to R</em>. This is a remarkable step forward to web development in R.</p>
<p>In this post, it&rsquo;ll be demonstrated how to implement the async feature of Shiny. Then its limitation will be discussed with an alternative app, which is built by <em>JavaScript</em> for the frontend and <em>RServe</em> for the backend.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2018-05-19-asyn-shiny-and-its-limitation/featured.png" length="247205" type="image/png"/></item><item><title>API Development with R Part II</title><link>https://jaehyeon.me/blog/2017-11-19-api-development-with-r-2/</link><pubDate>Sun, 19 Nov 2017 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2017-11-19-api-development-with-r-2/</guid><description><![CDATA[<p>In <a href="/blog/2017-11-18-api-development-with-r-1">Part I</a>, it is discussed how to serve an R function with <em>plumber</em>, <em>Rserve</em> and <em>rApache</em>. In this post, the APIs are deployed in a Docker container and, after showing example requests, their performance is compared. The <a href="https://hub.docker.com/r/rocker/r-ver/" target="_blank" rel="noopener noreferrer">rocker/r-ver:3.4<i class="fas fa-external-link-square-alt ms-1"></i></a> is used as the base image and each of the APIs is added to it. For simplicity, the APIs are served by <a href="http://supervisord.org/" target="_blank" rel="noopener noreferrer">Supervisor<i class="fas fa-external-link-square-alt ms-1"></i></a>. For performance testing, <a href="https://locust.io/" target="_blank" rel="noopener noreferrer">Locust<i class="fas fa-external-link-square-alt ms-1"></i></a> is used. The source of this post can be found in this <a href="https://github.com/jaehyeon-kim/r-api-demo" target="_blank" rel="noopener noreferrer"><strong>GitHub repository</strong><i class="fas fa-external-link-square-alt ms-1"></i></a>.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2017-11-19-api-development-with-r-2/featured.png" length="367256" type="image/png"/></item><item><title>API Development with R Part I</title><link>https://jaehyeon.me/blog/2017-11-18-api-development-with-r-1/</link><pubDate>Sat, 18 Nov 2017 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2017-11-18-api-development-with-r-1/</guid><description><![CDATA[<p>API is an effective way of distributing analysis outputs to external clients. When it comes to API development with R, however, there are not many choices. Probably development would be made with <a href="https://github.com/trestletech/plumber" target="_blank" rel="noopener noreferrer">plumber<i class="fas fa-external-link-square-alt ms-1"></i></a>, <a href="https://www.rforge.net/Rserve/" target="_blank" rel="noopener noreferrer">Rserve<i class="fas fa-external-link-square-alt ms-1"></i></a>, <a href="http://rapache.net/" target="_blank" rel="noopener noreferrer">rApache<i class="fas fa-external-link-square-alt ms-1"></i></a> or <a href="https://www.opencpu.org/" target="_blank" rel="noopener noreferrer">OpenCPU<i class="fas fa-external-link-square-alt ms-1"></i></a> if a client or bridge layer to R is not considered.</p>
<p>This is 2 part series in relation to <em>API development with R</em>. In this post, serving an R function with <em>plumber</em>, <em>Rserve</em> and <em>rApache</em> is discussed. <em>OpenCPU</em> is not discussed partly because it could be overkill for API. Also its performance may be similar to <em>rApache</em> with <a href="http://httpd.apache.org/docs/2.2/mod/prefork.html" target="_blank" rel="noopener noreferrer">Prefork Multi-Processing Module<i class="fas fa-external-link-square-alt ms-1"></i></a> enabled. Then deploying the APIs in a Docker container, making example HTTP requests and their performance will be discussed in <a href="/blog/2015-02-08-tree-based-methods-2">Part II</a>.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2017-11-18-api-development-with-r-1/featured.png" length="367256" type="image/png"/></item><item><title>Serverless Data Product POC Backend Part IV - Serving R ML Model via S3</title><link>https://jaehyeon.me/blog/2017-04-17-serverless-data-product-4/</link><pubDate>Mon, 17 Apr 2017 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2017-04-17-serverless-data-product-4/</guid><description><![CDATA[<p>In the previous posts, it is discussed how to package/deploy an <a href="https://www.r-project.org/about.html" target="_blank" rel="noopener noreferrer">R<i class="fas fa-external-link-square-alt ms-1"></i></a> model with <a href="https://aws.amazon.com/lambda/details/" target="_blank" rel="noopener noreferrer">AWS Lambda<i class="fas fa-external-link-square-alt ms-1"></i></a> and to expose the Lambda function via <a href="https://aws.amazon.com/api-gateway/" target="_blank" rel="noopener noreferrer">Amazon API Gateway<i class="fas fa-external-link-square-alt ms-1"></i></a>. Main benefits of <strong>serverless architecture</strong> is cost-effectiveness and being hassle-free from provisioning/managing servers. While the API returns a predicted admission status value given <em>GRE</em>, <em>GPA</em> and <em>Rank</em>, there is an issue if it is served within a web application: <em>Cross-Origin Resource Sharing (CORS)</em>. This post discusses how to resolve this issue by updating API configuration and the Lambda function handler with a simple web application. Also it is illustrated how to host the application in a serverless environment.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2017-04-17-serverless-data-product-4/featured.png" length="225463" type="image/png"/></item><item><title>Serverless Data Product POC Backend Part III - Exposing R ML Model via APIG</title><link>https://jaehyeon.me/blog/2017-04-13-serverless-data-product-3/</link><pubDate>Thu, 13 Apr 2017 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2017-04-13-serverless-data-product-3/</guid><description><![CDATA[<p>In <a href="/blog/2017-04-08-serverless-data-product-1">Part I</a> of this series, R and necessary libraries/packages together with a Lambda function handler are packaged and saved to <a href="https://aws.amazon.com/s3/" target="_blank" rel="noopener noreferrer">Amazon S3<i class="fas fa-external-link-square-alt ms-1"></i></a>. Then, in <a href="/blog/2017-04-11-serverless-data-product-2">Part II</a>, the package is deployed at <a href="https://aws.amazon.com/lambda/" target="_blank" rel="noopener noreferrer">AWS Lambda<i class="fas fa-external-link-square-alt ms-1"></i></a> after creating and assigning a role to the Lambda function. Although the Lambda function can be called via the Invoke API, it&rsquo;ll be much more useful if the function can be called as a web service (or API). In this post, it is discussed how to expose the Lambda function via <a href="https://aws.amazon.com/api-gateway/" target="_blank" rel="noopener noreferrer">Amazon API Gateway<i class="fas fa-external-link-square-alt ms-1"></i></a>. After creating an API by integrating the Lambda function, it is protected with an API key. Finally a custom domain name is used as an alternative URL of the API.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2017-04-13-serverless-data-product-3/featured.png" length="173293" type="image/png"/></item><item><title>Serverless Data Product POC Backend Part II - Deploying R ML Model via Lambda</title><link>https://jaehyeon.me/blog/2017-04-11-serverless-data-product-2/</link><pubDate>Tue, 11 Apr 2017 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2017-04-11-serverless-data-product-2/</guid><description><![CDATA[<p>In the <a href="/blog/2017-04-08-serverless-data-product-1">previous post</a>, <strong>serverless</strong> <strong>event-driven</strong> application development is introduced. Also how to package R, necessary libraries/packages and a Lambda function handler is discussed. No need of provisioning/managing servers is one of the key benefits of the architecture. It is also a cost-effective way of delivering a data product as functions are executed <em>on-demand</em> rather than in servers that run 24/7. Furthermore <a href="https://aws.amazon.com/lambda/pricing/" target="_blank" rel="noopener noreferrer">AWS Lambda free tier<i class="fas fa-external-link-square-alt ms-1"></i></a> includes 1M free requests per month and 400,000 GB-seconds of compute time per month, which is available to both existing and new AWS customers indefinitely. (GB-seconds is applicable when execution is made with 1 GB of memory.) Lowering the size of memory increases the execution time and thus 3.2M seconds or about 37 days are free with 128 MB of memory (1 GB divided by 8) - note that CPU power is proportional to allocated memory.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2017-04-11-serverless-data-product-2/featured.png" length="139725" type="image/png"/></item><item><title>Serverless Data Product POC Backend Part I - Packaging R ML Model for Lambda</title><link>https://jaehyeon.me/blog/2017-04-08-serverless-data-product-1/</link><pubDate>Sat, 08 Apr 2017 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2017-04-08-serverless-data-product-1/</guid><description><![CDATA[<p>Let say you&rsquo;ve got a prediction model built in R and you&rsquo;d like to <em>productionize</em> it, for example, by serving it in a web application. One way is exposing the model through an API that returns the predicted result as a web service. However there are many issues. Firstly R is not a language for API development although there may be some ways - eg the <a href="https://github.com/trestletech/plumber" target="_blank" rel="noopener noreferrer">plumber<i class="fas fa-external-link-square-alt ms-1"></i></a> package. More importantly developing an API is not the end of the story as the API can&rsquo;t be served in a production system if it is not <em>deployed/managed/upgraded/patched/&hellip;</em> appropriately in a server or if it is not <em>scalable</em>, <em>protected via authentication/authorization</em> and so on. Therefore it requires quite a vast range of skill sets that cover both development and DevOps (engineering).</p>]]></description><enclosure url="https://jaehyeon.me/blog/2017-04-08-serverless-data-product-1/featured.png" length="139725" type="image/png"/></item><item><title>Some Thoughts on Shiny Open Source - Render Multiple Pages</title><link>https://jaehyeon.me/blog/2016-06-27-shiny-open-source-render-multiple-pages/</link><pubDate>Mon, 27 Jun 2016 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2016-06-27-shiny-open-source-render-multiple-pages/</guid><description><![CDATA[<p>R Shiny applications are served as a single page application and it is not built to render multiple pages. There are benefits of rendering multiple pages such as code management and implement authentication. In this page, we discuss how to implement multi-page rendering in a Shiny app.</p>
<p>As indicated above, Shiny is not designed to render multiple pages and, in general, the UI is rendered on the fly as defined in <em>ui.R</em> or <em>app.R</em>. However this is not the only way as the UI can be rendered as a html output using <code>htmlOutput()</code> in <em>ui.R</em> and <code>renderUI()</code> in <em>server.R</em>. In this post, rendering multiple pages will be illustrated using an <a href="https://github.com/jaehyeon-kim/shiny-multipage" target="_blank" rel="noopener noreferrer"><strong>example application</strong><i class="fas fa-external-link-square-alt ms-1"></i></a>.</p>]]></description></item><item><title>Some Thoughts on Shiny Open Source - Internal Load Balancing</title><link>https://jaehyeon.me/blog/2016-05-23-shiny-open-source-internal-load-balancing/</link><pubDate>Mon, 23 May 2016 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2016-05-23-shiny-open-source-internal-load-balancing/</guid><description>&lt;p>Shiny is an interesting web framework that helps create a web application quickly. If it targets a large number of users, however, there are several limitations and it is so true when the open source version of Shiny is in use. It would be possible to tackle down some of the limitations with the enterprise version but it is not easy to see enough examples of Shiny applications in production environment. While whether Shiny can be used in production environment is a controversial issue, this series of posts illustrate some ways to use &lt;strong>open source Shiny&lt;/strong> a bit more wisely. Specifically the following topics are going to be covered.&lt;/p></description></item><item><title>Asynchronous Processing Using Job Queue</title><link>https://jaehyeon.me/blog/2016-05-12-asynchronous-processing-using-job-queue/</link><pubDate>Thu, 12 May 2016 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2016-05-12-asynchronous-processing-using-job-queue/</guid><description><![CDATA[<p>In this post, a way to overcome one of R&rsquo;s limitations (<strong>lack of multi-threading</strong>) is discussed by job queuing using the <a href="http://jobqueue.r-forge.r-project.org/" target="_blank" rel="noopener noreferrer">jobqueue package<i class="fas fa-external-link-square-alt ms-1"></i></a> - a generic asynchronous job queue implementation for R. See the package description below.</p>
<blockquote>
<p>The jobqueue package is meant to provide an easy-to-use interface that allows to queue computations for background evaluation while the calling R session remains responsive. It is based on a <em>1-node socket cluster from the parallel package</em>. The package provides a way to do basic threading in R. The main focus of the package is on an intuitive and easy-to-use interface for the job queue programming construct. &hellip; Typical applications include: <strong>background computation of lengthy tasks (such as data sourcing, model fitting, bootstrapping), simple/interactive parallelization (if you have 5 different jobs, move them to up to 5 different job queues), and concurrent task scheduling in more complicated R programs.</strong> &hellip;</p>]]></description></item><item><title>Boost SparkR with Hive</title><link>https://jaehyeon.me/blog/2016-04-30-boost-sparkr-with-hive/</link><pubDate>Sat, 30 Apr 2016 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2016-04-30-boost-sparkr-with-hive/</guid><description><![CDATA[<p>In the <a href="/blog/2016-03-02-quick-start-sparkr-in-local-and-cluster-mode">previous post</a>, it is demonstrated how to start SparkR in local and cluster mode. While SparkR is in active development, it is yet to fully support Spark&rsquo;s key libraries such as MLlib and Spark Streaming. Even, as a data processing engine, this R API is still limited as it is not possible to manipulate RDDs directly but only via Spark SQL/DataFrame API. As can be checked in the <a href="http://spark.apache.org/docs/latest/api/R/index.html" target="_blank" rel="noopener noreferrer">API doc<i class="fas fa-external-link-square-alt ms-1"></i></a>, SparkR rebuilds many existing R functions to work with Spark DataFrame and notably it borrows some functions from the dplyr package. Also there are some alien functions (eg <code>from_utc_timestamp()</code>) and many of them are from <a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual" target="_blank" rel="noopener noreferrer">Hive Query Language (HiveQL)<i class="fas fa-external-link-square-alt ms-1"></i></a>. In relation to those functions from HiveQL, although some Hive user defined functions (UDFs) are ported, still many useful <a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual&#43;UDF" target="_blank" rel="noopener noreferrer">UDFs<i class="fas fa-external-link-square-alt ms-1"></i></a> and <a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual&#43;WindowingAndAnalytics" target="_blank" rel="noopener noreferrer">Window functions<i class="fas fa-external-link-square-alt ms-1"></i></a> don&rsquo;t exist.</p>]]></description></item><item><title>Quick Start SparkR in Local and Cluster Mode</title><link>https://jaehyeon.me/blog/2016-03-02-quick-start-sparkr-in-local-and-cluster-mode/</link><pubDate>Wed, 02 Mar 2016 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2016-03-02-quick-start-sparkr-in-local-and-cluster-mode/</guid><description><![CDATA[<p>In the <a href="/blog/2016-02-22-spark-cluster-setup-on-virtualbox">previous post</a>, a Spark cluster is set up using 2 VirtualBox Ubuntu guests. While this is a viable option for many, it is not always for others. For those who find setting-up such a cluster is not convenient, there&rsquo;s still another option, which is relying on the local mode of Spark. In this post, a <a href="https://bitbucket.org/jaehyeon/sparkr-test" target="_blank" rel="noopener noreferrer"><strong>BitBucket repository</strong><i class="fas fa-external-link-square-alt ms-1"></i></a> is introduced, which is a R project that includes <em>Spark 1.6.0 Pre-built for Hadoop 2.0 and later</em> and <em>hadoop-common 2.2.0</em> - the latter is necessary if it is tested on Windows. Then several initialization steps are discussed such as setting-up environment variables and library path as well as including the <a href="https://github.com/databricks/spark-csv" target="_blank" rel="noopener noreferrer">spark-csv package<i class="fas fa-external-link-square-alt ms-1"></i></a> and a JDBC driver. Finally it shows some examples of reading JSON and CSV files in the cluster mode.</p>]]></description></item><item><title>Spark Cluster Setup on VirtualBox</title><link>https://jaehyeon.me/blog/2016-02-22-spark-cluster-setup-on-virtualbox/</link><pubDate>Mon, 22 Feb 2016 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2016-02-22-spark-cluster-setup-on-virtualbox/</guid><description><![CDATA[<p>We discuss how to set up a Spark cluser between 2 Ubuntu guests. Firstly it begins with machine preparation. Once a machine is baked, its image file (<em>VDI</em>) is be copied for the second one. Then how to launch a cluster by <a href="http://spark.apache.org/docs/latest/spark-standalone.html" target="_blank" rel="noopener noreferrer">standalone mode<i class="fas fa-external-link-square-alt ms-1"></i></a> is discussed. Let&rsquo;s get started.</p>

<h2 id="machine-preparation" data-numberify>Machine preparation<a class="anchor ms-1" href="#machine-preparation"></a></h2>
<p>If you haven&rsquo;t read the previous post, I recommend reading as it introduces <a href="http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html" target="_blank" rel="noopener noreferrer">Putty<i class="fas fa-external-link-square-alt ms-1"></i></a> as well. Also, as Spark need Java Development Kit (JDK), you may need to <em>apt-get</em> it first - see <a href="https://www.digitalocean.com/community/tutorials/how-to-install-java-on-ubuntu-with-apt-get" target="_blank" rel="noopener noreferrer">this tutorial<i class="fas fa-external-link-square-alt ms-1"></i></a> for further details.</p>]]></description></item><item><title>Quick Test to Wrap Python in R</title><link>https://jaehyeon.me/blog/2015-11-21-quick-test-to-wrap-python-in-r/</link><pubDate>Sat, 21 Nov 2015 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2015-11-21-quick-test-to-wrap-python-in-r/</guid><description><![CDATA[<p>As mentioned in an <a href="/blog/2015-08-09-some-thoughts-on-python-for-r-users">earlier post</a>, things that are not easy in R can be relatively simple in other languages. Another example would be connecting to Amazon Web Services. In relation to s3, although there are a number of existing packages, many of them seem to be deprecated, premature or platform-dependent. (I consider the <a href="https://cloudyr.github.io/" target="_blank" rel="noopener noreferrer">cloudyr<i class="fas fa-external-link-square-alt ms-1"></i></a> project looks promising though.)</p>
<p>If there isn&rsquo;t a comprehensive <em>R-way</em> of doing something yet, it may be necessary to create it from scratch. Actually there are some options to do so by using <a href="https://aws.amazon.com/cli/" target="_blank" rel="noopener noreferrer">AWS Command Line Interface<i class="fas fa-external-link-square-alt ms-1"></i></a>, <a href="http://docs.aws.amazon.com/AmazonS3/latest/API/APIRest.html" target="_blank" rel="noopener noreferrer">AWS REST API<i class="fas fa-external-link-square-alt ms-1"></i></a> or wrapping functionality of another language.</p>]]></description></item><item><title>Some Thoughts on Python for R Users</title><link>https://jaehyeon.me/blog/2015-08-09-some-thoughts-on-python-for-r-users/</link><pubDate>Sun, 09 Aug 2015 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2015-08-09-some-thoughts-on-python-for-r-users/</guid><description><![CDATA[<p>There seem to be growing interest in Python in the R cummunity. While there can be a range of opinions about using R over Python (or vice versa) for exploratory data analysis, fitting statistical/machine learning algorithms and so on, I consider one of the strongest attractions of using Python comes from the fact that <em>Python is a general purpose programming language</em>. As more developers are involved in, it can provide a way to get jobs done easily, which can be tricky in R. In this article, an example is introduced by illustrating how to connect to <a href="https://en.wikipedia.org/wiki/SOAP" target="_blank" rel="noopener noreferrer">SOAP (Simple Object Access Protocol)<i class="fas fa-external-link-square-alt ms-1"></i></a> web services.</p>]]></description></item><item><title>Some Thoughts on Python</title><link>https://jaehyeon.me/blog/2015-08-08-some-thoughts-on-python/</link><pubDate>Sat, 08 Aug 2015 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2015-08-08-some-thoughts-on-python/</guid><description><![CDATA[<p>When I bagan to teach myself C# 3 years ago, I only had some experience in interactive analysis tools such as MATLAB and R - I didn&rsquo;t consider R as a programming language at that time. The general purpose programming language shares some common features (data type, loop, if&hellip;) but it is rather different in the way how code is written/organized, which is object oriented. Therefore, while it was not a problem to grap the common features, it took quite some time to understand and keep my code in an object oriented way.</p>]]></description></item><item><title>Setup Random Seeds on Caret Package</title><link>https://jaehyeon.me/blog/2015-05-30-setup-random-seeds-on-caret-package/</link><pubDate>Sat, 30 May 2015 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2015-05-30-setup-random-seeds-on-caret-package/</guid><description><![CDATA[<p>A short while ago I had a chance to perform analysis using the <strong>caret</strong> package. One of the requirements is to run it parallelly and to work in both Windows and Linux. The requirement can be met by using the <strong>parallel</strong> and <strong>doParallel</strong> packages as the <strong>caret</strong> package trains a model using the <strong>foreach</strong> package if clusters are registered by the <strong>doParallel</strong> package - further details about how to implement parallel processing on a single machine can be found in earlier posts (<a href="/blog/2015-02-01-tree-based-methods-1">Link 1</a>, <a href="/blog/2015-02-08-tree-based-methods-2">Link 2</a> and <a href="/blog/2015-02-14-tree-based-methods-3">Link 3</a>). While it is relatively straightforward to train a model across multiple clusters using the <strong>caret</strong> package, setting up random seeds may be a bit tricky. As analysis can be more reproducible by random seeds, a way of setting them up is illustrated using a simple function in this post.</p>]]></description></item><item><title>Packaging Analysis</title><link>https://jaehyeon.me/blog/2015-03-24-packaging-analysis/</link><pubDate>Tue, 24 Mar 2015 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2015-03-24-packaging-analysis/</guid><description><![CDATA[<p>When I imagine a workflow, it is performing the same or similar tasks regularly (daily or weekly) in an automated way. Although those tasks can be executed in a script or a *source()*d script, it may not be easy to maintain separate scripts while the size of tasks gets bigger or if they have to be executed in different machines. In academia, reproducible research shares similar ideas but the level of reproducibility introduced in <a href="https://christophergandrud.github.io/RepResR-RStudio/" target="_blank" rel="noopener noreferrer">Gandrud, 2013<i class="fas fa-external-link-square-alt ms-1"></i></a> may not suffice in a business environment as the focus is documenting in a reproducible way. A R package, however, can be an effective tool and it can be considered like a portable class library in C# or Java. Like a class library, it can include a set of necessary tasks (usually using functions) and, being portable, its dependency can be managed well - for example, it is possible to set so that dependent packages can also be installed if some of them are not installed already. Moreover the benefit of creating a R package would be significant if it has to be deployed in a production server as it&rsquo;d be a lot easier to convince system admin with the built-in unit tests, object documents and package vignettes. In this article an example of creating a R package is illustrated.</p>]]></description></item><item><title>Parallel Processing on Single Machine - Part III</title><link>https://jaehyeon.me/blog/2015-03-19-parallel-processing-on-single-machine-3/</link><pubDate>Thu, 19 Mar 2015 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2015-03-19-parallel-processing-on-single-machine-3/</guid><description><![CDATA[<p>In the <a href="/blog/2015-03-17-parallel-processing-on-single-machine-2">previous posts</a>, two groups of ways to implement parallel processing on a single machine are introduced. The first group is provided by the <strong>snow</strong> or <strong>parallel</strong> package and the functions are an extension of <code>lapply()</code> (<a href="/blog/2015-03-14-parallel-processing-on-single-machine-1">LINK</a>). The second group is based on an extension of the <em>for</em> construct (<em>foreach</em>, <em>%dopar%</em> and <em>%:%</em>). The <em>foreach</em> construct is provided by the <em>foreach</em> package while clusters are made and registered by the <strong>parallel</strong> and <strong>doParallel</strong> packages respectively (<a href="/blog/2015-03-17-parallel-processing-on-single-machine-2">LINK</a>). To conclude this series, three practical examples are discussed for comparison in this article.</p>]]></description></item><item><title>Parallel Processing on Single Machine - Part II</title><link>https://jaehyeon.me/blog/2015-03-17-parallel-processing-on-single-machine-2/</link><pubDate>Tue, 17 Mar 2015 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2015-03-17-parallel-processing-on-single-machine-2/</guid><description><![CDATA[<p>In the <a href="/blog/2015-03-14-parallel-processing-on-single-machine-1">previous article</a>, parallel processing on a single machine using the <strong>snow</strong> and <strong>parallel</strong> packages are introduced. The four functions are an extension of <code>lapply()</code> with an additional argument that specifies a cluster object. In spite of their effectiveness and ease of use, there may be cases where creating a function that can be sent into clusters is not easy or looping may be more natural. In this article, another way of implementing parallel processing on a single machine is introduced using the <strong>foreach</strong> and <strong>doParallel</strong> packages where clusters are created by the <strong>parallel</strong> package. Finally the <strong>iterators</strong> package is briefly covered as it can facilitate writing a loop. The examples here are largely based on the individual packages&rsquo; vignettes and further details can be found there.</p>]]></description></item><item><title>Parallel Processing on Single Machine - Part I</title><link>https://jaehyeon.me/blog/2015-03-14-parallel-processing-on-single-machine-1/</link><pubDate>Sat, 14 Mar 2015 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2015-03-14-parallel-processing-on-single-machine-1/</guid><description><![CDATA[<p>Lack of multi-threading and memory limitation are two outstanding weaknesses of base R. In fact, however, if the size of data is not so large that it can be read in RAM, the former would be relatively easily handled by parallel processing, provided that multiple processors are equipped. This article introduces to a way of implementing parallel processing on a single machine using the <strong>snow</strong> and <strong>parallel</strong> packages - the examples are largely based on <a href="http://shop.oreilly.com/product/0636920021421.do" target="_blank" rel="noopener noreferrer">McCallum and Weston (2012)<i class="fas fa-external-link-square-alt ms-1"></i></a>.</p>]]></description></item><item><title>Tree Based Methods in R - Part VI</title><link>https://jaehyeon.me/blog/2015-03-07-tree-based-methods-6/</link><pubDate>Sat, 07 Mar 2015 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2015-03-07-tree-based-methods-6/</guid><description><![CDATA[<ul>
<li><a href="/blog/2015-02-01-tree-based-methods-1">Part I</a></li>
<li><a href="/blog/2015-02-08-tree-based-methods-2">Part II</a></li>
<li><a href="/blog/2015-02-14-tree-based-methods-3">Part III</a></li>
<li><a href="/blog/2015-02-15-tree-based-methods-4">Part IV</a></li>
<li><a href="/blog/2015-03-05-tree-based-methods-5">Part V</a></li>
<li><a href="/blog/2015-03-07-tree-based-methods-6/#">Part VI</a> (this post)</li>
</ul>
<p>A regression tree is evaluated using bagged trees in the <a href="/blog/2015-03-05-tree-based-methods-5">previous article</a>. In this article, the response variable of the same data set is converted into a binary factor variable and a classification tree is evaluated by comparing to bagged trees&rsquo; individual oob/test errors, cumulative oob/test errors and variable importance measures.</p>
<p>Before getting started, note that the source of the classes can be found in <a href="https://gist.github.com/jaehyeon-kim/b89dcbd2fb0b84fd236e" target="_blank" rel="noopener noreferrer">this gist<i class="fas fa-external-link-square-alt ms-1"></i></a> and, together with the relevant packages (see <em>tags</em>), it requires a utility function (<code>bestParam()</code>) that can be found <a href="https://gist.github.com/jaehyeon-kim/5622ae9fa982e0b46550" target="_blank" rel="noopener noreferrer">here<i class="fas fa-external-link-square-alt ms-1"></i></a>.</p>]]></description></item><item><title>Tree Based Methods in R - Part V</title><link>https://jaehyeon.me/blog/2015-03-05-tree-based-methods-5/</link><pubDate>Thu, 05 Mar 2015 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2015-03-05-tree-based-methods-5/</guid><description><![CDATA[<ul>
<li><a href="/blog/2015-02-01-tree-based-methods-1">Part I</a></li>
<li><a href="/blog/2015-02-08-tree-based-methods-2">Part II</a></li>
<li><a href="/blog/2015-02-14-tree-based-methods-3">Part III</a></li>
<li><a href="/blog/2015-02-15-tree-based-methods-4">Part IV</a></li>
<li><a href="/blog/2015-03-05-tree-based-methods-5/#">Part V</a> (this post)</li>
<li><a href="/blog/2015-03-07-tree-based-methods-6">Part VI</a></li>
</ul>
<p>This article evaluates a single regression tree&rsquo;s performance by comparing to bagged trees&rsquo; individual oob/test errors, cumulative oob/test errors and variable importance measures.</p>
<p>Before getting started, note that the source of the classes can be found in <a href="https://gist.github.com/jaehyeon-kim/b89dcbd2fb0b84fd236e" target="_blank" rel="noopener noreferrer">this gist<i class="fas fa-external-link-square-alt ms-1"></i></a> and, together with the relevant packages (see <em>tags</em>), it requires a utility function (<code>bestParam()</code>) that can be found <a href="https://gist.github.com/jaehyeon-kim/5622ae9fa982e0b46550" target="_blank" rel="noopener noreferrer">here<i class="fas fa-external-link-square-alt ms-1"></i></a>.</p>]]></description></item><item><title>Tree Based Methods in R - Part IV</title><link>https://jaehyeon.me/blog/2015-02-15-tree-based-methods-4/</link><pubDate>Sun, 15 Feb 2015 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2015-02-15-tree-based-methods-4/</guid><description><![CDATA[<ul>
<li><a href="/blog/2015-02-01-tree-based-methods-1">Part I</a></li>
<li><a href="/blog/2015-02-08-tree-based-methods-2">Part II</a></li>
<li><a href="/blog/2015-02-14-tree-based-methods-3">Part III</a></li>
<li><a href="/blog/2015-02-15-tree-based-methods-4/#">Part IV</a> (this post)</li>
<li><a href="/blog/2015-03-05-tree-based-methods-5">Part V</a></li>
<li><a href="/blog/2015-03-07-tree-based-methods-6">Part VI</a></li>
</ul>
<p>While the last three articles illustrated the CART model for both classification (with equal/unequal costs) and regression tasks, this article is rather technical as it compares three packages: <strong>rpart</strong>, <strong>caret</strong> and <strong>mlr</strong>. For those who are not familiar with the last two packages, they are wrappers (or frameworks) that implement a range of models (or algorithms) in a unified way. They are useful because inconsistent API could be a drawback of R (like other open source tools) and it would be quite beneficial if there is a way to implement different models in a standardized way. In line with the earlier articles, the <em>Carseats</em> data is used for a classification task.</p>]]></description></item><item><title>Tree Based Methods in R - Part III</title><link>https://jaehyeon.me/blog/2015-02-14-tree-based-methods-3/</link><pubDate>Sat, 14 Feb 2015 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2015-02-14-tree-based-methods-3/</guid><description><![CDATA[<ul>
<li><a href="/blog/2015-02-01-tree-based-methods-1">Part I</a></li>
<li><a href="/blog/2015-02-08-tree-based-methods-2">Part II</a></li>
<li><a href="/blog/2015-02-14-tree-based-methods-3/#">Part III</a> (this post)</li>
<li><a href="/blog/2015-02-15-tree-based-methods-4">Part IV</a></li>
<li><a href="/blog/2015-03-05-tree-based-methods-5">Part V</a></li>
<li><a href="/blog/2015-03-07-tree-based-methods-6">Part VI</a></li>
</ul>
<p>While classification tasks are implemented in the last two articles (<a href="/blog/2015-02-01-tree-based-methods-1">Part I</a> and <a href="/blog/2015-02-08-tree-based-methods-2">Part II</a>), a regression task is the topic of this article. While the <strong>caret</strong> package selects the tuning parameter (<em>cp</em>) that minimizes the error (<em>RMSE</em>), the <strong>rpart</strong> packages recommends the <em>1-SE rule</em>, which selects the smallest tree within 1 standard error of the minimum cross validation error (<em>xerror</em>). The models with 2 complexity parameters that are suggested by the packages are compared.</p>]]></description></item><item><title>Tree Based Methods in R - Part II</title><link>https://jaehyeon.me/blog/2015-02-08-tree-based-methods-2/</link><pubDate>Sun, 08 Feb 2015 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2015-02-08-tree-based-methods-2/</guid><description><![CDATA[<ul>
<li><a href="/blog/2015-02-01-tree-based-methods-1">Part I</a></li>
<li><a href="/blog/2015-02-08-tree-based-methods-2/#">Part II</a> (this post)</li>
<li><a href="/blog/2015-02-14-tree-based-methods-3">Part III</a></li>
<li><a href="/blog/2015-02-15-tree-based-methods-4">Part IV</a></li>
<li><a href="/blog/2015-03-05-tree-based-methods-5">Part V</a></li>
<li><a href="/blog/2015-03-07-tree-based-methods-6">Part VI</a></li>
</ul>
<p>In the previous article (<a href="/blog/2015-02-01-tree-based-methods-1">Tree Based Methods in R - Part I</a>), a decision tree is created on the <em>Carseats</em> data which is in the chapter 8 lab of <a href="https://www.statlearning.com/" target="_blank" rel="noopener noreferrer">ISLR<i class="fas fa-external-link-square-alt ms-1"></i></a>. In that article, potentially asymetric costs due to misclassification are not taken into account. When unbalance between false positive and false negative can have a significant impact, it can be explicitly adjusted either by altering prior (or empirical) probabilities or by adding a loss matrix.</p>]]></description></item><item><title>Tree Based Methods in R - Part I</title><link>https://jaehyeon.me/blog/2015-02-01-tree-based-methods-1/</link><pubDate>Sun, 01 Feb 2015 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2015-02-01-tree-based-methods-1/</guid><description><![CDATA[<ul>
<li><a href="/blog/2015-02-01-tree-based-methods-1/#">Part I</a> (this post)</li>
<li><a href="/blog/2015-02-08-tree-based-methods-2">Part II</a></li>
<li><a href="/blog/2015-02-14-tree-based-methods-3">Part III</a></li>
<li><a href="/blog/2015-02-15-tree-based-methods-4">Part IV</a></li>
<li><a href="/blog/2015-03-05-tree-based-methods-5">Part V</a></li>
<li><a href="/blog/2015-03-07-tree-based-methods-6">Part VI</a></li>
</ul>
<p>This is the first article about tree based methods using R. <em>Carseats</em> data in the chapter 8 lab of <a href="https://www.statlearning.com/" target="_blank" rel="noopener noreferrer">ISLR<i class="fas fa-external-link-square-alt ms-1"></i></a> is used to perform classification analysis. Unlike the lab example, the <strong>rpart</strong> package is used to fit the CART model on the data and the <strong>caret</strong> package is used for tuning the pruning parameter (<code>cp</code>).</p>]]></description></item><item><title>Quick Trial of Adding Column</title><link>https://jaehyeon.me/blog/2015-01-14-quick-trial-of-adding-column/</link><pubDate>Wed, 14 Jan 2015 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2015-01-14-quick-trial-of-adding-column/</guid><description><![CDATA[<p>This is a quick trial of adding overall and conditional (by user) average columns in a data frame. <code>base</code>,<code>plyr</code>,<code>dplyr</code>,<code>data.table</code>,<code>dplyr + data.table</code> packages are used. Personally I perfer <code>dplyr + data.table</code> - <code>dplyr</code> for comperhensive syntax and <code>data.table</code> for speed.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-r" data-lang="r"><span class="line"><span class="ln"> 1</span><span class="cl"><span class="c1">## set up variables</span>
</span></span><span class="line"><span class="ln"> 2</span><span class="cl"><span class="n">size</span> <span class="o">&lt;-</span> <span class="m">36000</span>
</span></span><span class="line"><span class="ln"> 3</span><span class="cl"><span class="n">numUsers</span> <span class="o">&lt;-</span> <span class="m">4900</span>
</span></span><span class="line"><span class="ln"> 4</span><span class="cl"><span class="c1"># roughly each user has 7 sessions</span>
</span></span><span class="line"><span class="ln"> 5</span><span class="cl"><span class="n">numSessions</span> <span class="o">&lt;-</span> <span class="p">(</span><span class="n">numUsers</span> <span class="o">/</span> <span class="m">7</span><span class="p">)</span> <span class="o">-</span> <span class="p">((</span><span class="n">numUsers</span> <span class="o">/</span> <span class="m">7</span><span class="p">)</span> <span class="o">%%</span> <span class="m">1</span><span class="p">)</span>
</span></span><span class="line"><span class="ln"> 6</span><span class="cl"> 
</span></span><span class="line"><span class="ln"> 7</span><span class="cl"><span class="c1">## create data frame</span>
</span></span><span class="line"><span class="ln"> 8</span><span class="cl"><span class="nf">set.seed</span><span class="p">(</span><span class="m">123457</span><span class="p">)</span>
</span></span><span class="line"><span class="ln"> 9</span><span class="cl"><span class="n">userIds</span> <span class="o">&lt;-</span> <span class="nf">sample.int</span><span class="p">(</span><span class="n">numUsers</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">size</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">TRUE</span><span class="p">)</span>
</span></span><span class="line"><span class="ln">10</span><span class="cl"><span class="n">ssIds</span> <span class="o">&lt;-</span> <span class="nf">sample.int</span><span class="p">(</span><span class="n">numSessions</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">size</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">TRUE</span><span class="p">)</span>
</span></span><span class="line"><span class="ln">11</span><span class="cl"><span class="n">scores</span> <span class="o">&lt;-</span> <span class="nf">sample.int</span><span class="p">(</span><span class="m">10</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">size</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">TRUE</span><span class="p">)</span> 
</span></span><span class="line"><span class="ln">12</span><span class="cl">
</span></span><span class="line"><span class="ln">13</span><span class="cl"><span class="n">preDf</span> <span class="o">&lt;-</span> <span class="nf">data.frame</span><span class="p">(</span><span class="n">User</span><span class="o">=</span><span class="n">userIds</span><span class="p">,</span> <span class="n">Session</span><span class="o">=</span><span class="n">ssIds</span><span class="p">,</span> <span class="n">Score</span><span class="o">=</span><span class="n">scores</span><span class="p">)</span>
</span></span><span class="line"><span class="ln">14</span><span class="cl"><span class="n">preDf</span><span class="o">$</span><span class="n">User</span> <span class="o">&lt;-</span> <span class="nf">as.factor</span><span class="p">(</span><span class="n">preDf</span><span class="o">$</span><span class="n">User</span><span class="p">)</span> 
</span></span></code></pre></div>
<h2 id="adding-overall-average" data-numberify>Adding overall average<a class="anchor ms-1" href="#adding-overall-average"></a></h2>
<p>As calculating overall average is not complicated, I don&rsquo;t find a big difference among packages.</p>]]></description></item><item><title>Looping without for</title><link>https://jaehyeon.me/blog/2014-12-17-looping-without-for/</link><pubDate>Wed, 17 Dec 2014 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2014-12-17-looping-without-for/</guid><description><![CDATA[<p>Purely programming point of view, I consider <strong>for-loops</strong> would be better to be avoided in R as</p>
<ul>
<li>the script can be more readable</li>
<li>it is easier to handle errors</li>
</ul>
<p>Some articles on the web indicate that looping functions (or apply family of functions) don&rsquo;t guarantee faster execution and sometimes even slower. Although, assuming that the experiments are correct, in my opinion, code readability itself is beneficial enough to avoid for-loops. Even worse, R&rsquo;s dynamic typing system coupled with poor readability can result in a frustrating consequence as the code grows.</p>]]></description></item><item><title>Short R Examples</title><link>https://jaehyeon.me/blog/2014-12-03-short-r-examples/</link><pubDate>Wed, 03 Dec 2014 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2014-12-03-short-r-examples/</guid><description><![CDATA[<p>As I don&rsquo;t use R at work yet, I haven&rsquo;t got enough chances to learn R by doing. Together with teaching myself machine learning with R, I consider it would be a good idea to collect examples on the web. Recently I have been visiting a Linkedin group called <a href="http://www.linkedin.com/groups/R-Project-Statistical-Computing-77616?home=&amp;gid=77616&amp;trk=anet_ug_hm" target="_blank" rel="noopener noreferrer">The R Project for Statistical Computing<i class="fas fa-external-link-square-alt ms-1"></i></a> and suggesting scripts on data manipulation or programming topics. Actually I expected some of the topics may also be helpful to learn machine learning/statistics but, possibly due to lack of understaning of the topics in context, I&rsquo;ve found only a few - it may be necessary to look for another source. Anyway below is a summary of two examples.</p>]]></description></item><item><title>Summarise Stock Returns from Multiple Files</title><link>https://jaehyeon.me/blog/2014-11-27-summarise-stock-returns-from-multiple-files/</link><pubDate>Thu, 27 Nov 2014 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2014-11-27-summarise-stock-returns-from-multiple-files/</guid><description><![CDATA[<p>This post is a slight extension of the previous two articles (<a href="/blog/2014-11-20-download-stock-data-1">Download Stock Data - Part I</a>, <a href="/blog/2014-11-21-download-stock-data-2">Download Stock Data - Part II</a>) and we discuss how to produce gross returns, standard deviation and correlation of multiple shares.</p>
<p>The following packages are used.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-r" data-lang="r"><span class="line"><span class="ln">1</span><span class="cl"><span class="nf">library</span><span class="p">(</span><span class="n">knitr</span><span class="p">)</span>
</span></span><span class="line"><span class="ln">2</span><span class="cl"><span class="nf">library</span><span class="p">(</span><span class="n">lubridate</span><span class="p">)</span>
</span></span><span class="line"><span class="ln">3</span><span class="cl"><span class="nf">library</span><span class="p">(</span><span class="n">stringr</span><span class="p">)</span>
</span></span><span class="line"><span class="ln">4</span><span class="cl"><span class="nf">library</span><span class="p">(</span><span class="n">reshape2</span><span class="p">)</span>
</span></span><span class="line"><span class="ln">5</span><span class="cl"><span class="nf">library</span><span class="p">(</span><span class="n">plyr</span><span class="p">)</span>
</span></span><span class="line"><span class="ln">6</span><span class="cl"><span class="nf">library</span><span class="p">(</span><span class="n">dplyr</span><span class="p">)</span>
</span></span></code></pre></div><p>The script begins with creating a data folder in the format of <em>data_YYYY-MM-DD</em>.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-r" data-lang="r"><span class="line"><span class="ln">1</span><span class="cl"><span class="c1"># create data folder</span>
</span></span><span class="line"><span class="ln">2</span><span class="cl"><span class="n">dataDir</span> <span class="o">&lt;-</span> <span class="nf">paste0</span><span class="p">(</span><span class="s">&#34;data&#34;</span><span class="p">,</span><span class="s">&#34;_&#34;</span><span class="p">,</span><span class="nf">format</span><span class="p">(</span><span class="nf">Sys.Date</span><span class="p">(),</span><span class="s">&#34;%Y-%m-%d&#34;</span><span class="p">))</span>
</span></span><span class="line"><span class="ln">3</span><span class="cl"><span class="kr">if</span><span class="p">(</span><span class="nf">file.exists</span><span class="p">(</span><span class="n">dataDir</span><span class="p">))</span> <span class="p">{</span>
</span></span><span class="line"><span class="ln">4</span><span class="cl">  <span class="nf">unlink</span><span class="p">(</span><span class="n">dataDir</span><span class="p">,</span> <span class="n">recursive</span> <span class="o">=</span> <span class="kc">TRUE</span><span class="p">)</span>
</span></span><span class="line"><span class="ln">5</span><span class="cl">  <span class="nf">dir.create</span><span class="p">(</span><span class="n">dataDir</span><span class="p">)</span>
</span></span><span class="line"><span class="ln">6</span><span class="cl"><span class="p">}</span> <span class="kr">else</span> <span class="p">{</span>
</span></span><span class="line"><span class="ln">7</span><span class="cl">  <span class="nf">dir.create</span><span class="p">(</span><span class="n">dataDir</span><span class="p">)</span>
</span></span><span class="line"><span class="ln">8</span><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><p>Given company codes, URLs and file paths are created. Then data files are downloaded by <code>Map</code>, which is a wrapper of <code>mapply</code>. Note that R&rsquo;s <code>download.file</code> function is wrapped by <code>downloadFile</code> so that the function does not break when an error occurs.</p>]]></description></item><item><title>Download Stock Data - Part II</title><link>https://jaehyeon.me/blog/2014-11-21-download-stock-data-2/</link><pubDate>Fri, 21 Nov 2014 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2014-11-21-download-stock-data-2/</guid><description><![CDATA[<p>In an <a href="/blog/2014-11-20-download-stock-data-1">earlier article</a>, a way to download stock price data files from Google, save it into a local drive and merge them into a single data frame. If files are not large, however, it wouldn&rsquo;t be effective and, in this article, files are downloaded and merged internally.</p>
<p>The following packages are used.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-r" data-lang="r"><span class="line"><span class="ln">1</span><span class="cl"><span class="nf">library</span><span class="p">(</span><span class="n">knitr</span><span class="p">)</span>
</span></span><span class="line"><span class="ln">2</span><span class="cl"><span class="nf">library</span><span class="p">(</span><span class="n">lubridate</span><span class="p">)</span>
</span></span><span class="line"><span class="ln">3</span><span class="cl"><span class="nf">library</span><span class="p">(</span><span class="n">stringr</span><span class="p">)</span>
</span></span><span class="line"><span class="ln">4</span><span class="cl"><span class="nf">library</span><span class="p">(</span><span class="n">plyr</span><span class="p">)</span>
</span></span><span class="line"><span class="ln">5</span><span class="cl"><span class="nf">library</span><span class="p">(</span><span class="n">dplyr</span><span class="p">)</span>
</span></span></code></pre></div><p>Taking urls as file locations, files are directly read using <code>llply</code> and they are combined using <code>rbind_all</code>. As the merged data has multiple stocks&rsquo; records, <code>Code</code> column is created. Note that, when an error occurrs, the function returns a dummy data frame in order not to break the loop - values of the dummy data frame(s) are filtered out at the end.</p>]]></description></item><item><title>Download Stock Data - Part I</title><link>https://jaehyeon.me/blog/2014-11-20-download-stock-data-1/</link><pubDate>Thu, 20 Nov 2014 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2014-11-20-download-stock-data-1/</guid><description><![CDATA[<p>This article illustrates how to download stock price data files from Google, save it into a local drive and merge them into a single data frame. This script is slightly modified from a script which downloads RStudio package download log data. The original source can be found <a href="https://github.com/hadley/cran-logs-dplyr/blob/master/1-download.r" target="_blank" rel="noopener noreferrer">here<i class="fas fa-external-link-square-alt ms-1"></i></a>.</p>
<p>First of all, the following three packages are used.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-r" data-lang="r"><span class="line"><span class="ln">1</span><span class="cl"><span class="nf">library</span><span class="p">(</span><span class="n">knitr</span><span class="p">)</span>
</span></span><span class="line"><span class="ln">2</span><span class="cl"><span class="nf">library</span><span class="p">(</span><span class="n">lubridate</span><span class="p">)</span>
</span></span><span class="line"><span class="ln">3</span><span class="cl"><span class="nf">library</span><span class="p">(</span><span class="n">stringr</span><span class="p">)</span>
</span></span><span class="line"><span class="ln">4</span><span class="cl"><span class="nf">library</span><span class="p">(</span><span class="n">plyr</span><span class="p">)</span>
</span></span><span class="line"><span class="ln">5</span><span class="cl"><span class="nf">library</span><span class="p">(</span><span class="n">dplyr</span><span class="p">)</span>
</span></span></code></pre></div><p>The script begins with creating a folder to save data files.</p>]]></description></item></channel></rss>