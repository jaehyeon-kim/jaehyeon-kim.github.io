<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Apache Iceberg on Jaehyeon Kim</title><link>https://jaehyeon.me/tags/apache-iceberg/</link><description>Recent content in Apache Iceberg on Jaehyeon Kim</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>Copyright Â© 2023-2024 Jaehyeon Kim. All Rights Reserved.</copyright><lastBuildDate>Thu, 14 Mar 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://jaehyeon.me/tags/apache-iceberg/index.xml" rel="self" type="application/rss+xml"/><item><title>Data Build Tool (dbt) Pizza Shop Demo - Part 6 ETL on Amazon Athena via Airflow</title><link>https://jaehyeon.me/blog/2024-03-14-dbt-pizza-shop-6/</link><pubDate>Thu, 14 Mar 2024 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2024-03-14-dbt-pizza-shop-6/</guid><description><![CDATA[<p>In <a href="/blog/2024-03-07-dbt-pizza-shop-5">Part 5</a>, we developed a <a href="https://docs.getdbt.com/docs/introduction" target="_blank" rel="noopener noreferrer">dbt<i class="fas fa-external-link-square-alt ms-1"></i></a> project that that targets <a href="https://iceberg.apache.org/" target="_blank" rel="noopener noreferrer">Apache Iceberg<i class="fas fa-external-link-square-alt ms-1"></i></a> where transformations are performed on <a href="https://aws.amazon.com/athena/" target="_blank" rel="noopener noreferrer">Amazon Athena<i class="fas fa-external-link-square-alt ms-1"></i></a>. Two dimension tables that keep product and user records are created as <a href="https://en.wikipedia.org/wiki/Slowly_changing_dimension" target="_blank" rel="noopener noreferrer">Type 2 slowly changing dimension (SCD Type 2)<i class="fas fa-external-link-square-alt ms-1"></i></a> tables, and one transactional fact table is built to keep pizza orders. To improve query performance, the fact table is denormalized to pre-join records from the dimension tables using the array and struct data types. In this post, we discuss how to set up an ETL process on the project using Apache Airflow.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2024-03-14-dbt-pizza-shop-6/featured.png" length="82921" type="image/png"/></item><item><title>Data Build Tool (dbt) Pizza Shop Demo - Part 5 Modelling on Amazon Athena</title><link>https://jaehyeon.me/blog/2024-03-07-dbt-pizza-shop-5/</link><pubDate>Thu, 07 Mar 2024 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2024-03-07-dbt-pizza-shop-5/</guid><description><![CDATA[<p>In <a href="%28/blog/2024-01-18-dbt-pizza-shop-1%29">Part 1</a> and <a href="%28/blog/2024-02-08-dbt-pizza-shop-3%29">Part 3</a>, we developed <a href="https://docs.getdbt.com/docs/introduction" target="_blank" rel="noopener noreferrer">data build tool (dbt)<i class="fas fa-external-link-square-alt ms-1"></i></a> projects that target <em>PostgreSQL</em> and <em>BigQuery</em> using fictional pizza shop data. The data is modelled by <a href="https://en.wikipedia.org/wiki/Slowly_changing_dimension" target="_blank" rel="noopener noreferrer">SCD type 2<i class="fas fa-external-link-square-alt ms-1"></i></a> dimension tables and one transactional fact table. While the order records should be joined with dimension tables to get complete details for <em>PostgreSQL</em>, the fact table is denormalized using <a href="https://cloud.google.com/bigquery/docs/best-practices-performance-nested" target="_blank" rel="noopener noreferrer">nested and repeated fields<i class="fas fa-external-link-square-alt ms-1"></i></a> to improve query performance for <em>BigQuery</em>.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2024-03-07-dbt-pizza-shop-5/featured.png" length="61499" type="image/png"/></item><item><title>Data Warehousing ETL Demo with Apache Iceberg on EMR Local Environment</title><link>https://jaehyeon.me/blog/2022-06-26-iceberg-etl-demo/</link><pubDate>Sun, 26 Jun 2022 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2022-06-26-iceberg-etl-demo/</guid><description><![CDATA[<p>Unlike traditional Data Lake, new table formats (<a href="https://iceberg.apache.org/" target="_blank" rel="noopener noreferrer">Iceberg<i class="fas fa-external-link-square-alt ms-1"></i></a>, <a href="https://hudi.apache.org/" target="_blank" rel="noopener noreferrer">Hudi<i class="fas fa-external-link-square-alt ms-1"></i></a> and <a href="https://delta.io/" target="_blank" rel="noopener noreferrer">Delta Lake<i class="fas fa-external-link-square-alt ms-1"></i></a>) support <a href="https://iceberg.apache.org/docs/latest/spark-writes/" target="_blank" rel="noopener noreferrer">features<i class="fas fa-external-link-square-alt ms-1"></i></a> that can be used to apply data warehousing patterns, which can bring a way to be rescued from <a href="https://www.gartner.com/en/newsroom/press-releases/2014-07-28-gartner-says-beware-of-the-data-lake-fallacy" target="_blank" rel="noopener noreferrer">Data Swamp<i class="fas fa-external-link-square-alt ms-1"></i></a>. In this post, we&rsquo;ll discuss how to implement ETL using retail analytics data. It has two dimension data (user and product) and a single fact data (order). The dimension data sets have different ETL strategies depending on whether to track historical changes. For the fact data, the primary keys of the dimension data are added to facilitate later queries. We&rsquo;ll use Iceberg for data storage/management and Spark for data processing. Instead of provisioning an EMR cluster, a local development environment will be used. Finally, the ETL results will be queried by Athena for verification.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2022-06-26-iceberg-etl-demo/featured.png" length="43604" type="image/png"/></item></channel></rss>