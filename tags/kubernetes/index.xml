<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Kubernetes on Jaehyeon Kim</title><link>https://jaehyeon.me/tags/kubernetes/</link><description>Recent content in Kubernetes on Jaehyeon Kim</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>Copyright Â© 2023-2024 Jaehyeon Kim. All Rights Reserved.</copyright><lastBuildDate>Thu, 06 Jun 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://jaehyeon.me/tags/kubernetes/index.xml" rel="self" type="application/rss+xml"/><item><title>Deploy Python Stream Processing App on Kubernetes - Part 2 Beam Pipeline on Flink Runner</title><link>https://jaehyeon.me/blog/2024-06-06-beam-deploy-2/</link><pubDate>Thu, 06 Jun 2024 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2024-06-06-beam-deploy-2/</guid><description><![CDATA[<p>In this post, we develop an <a href="https://beam.apache.org/" target="_blank" rel="noopener noreferrer">Apache Beam<i class="fas fa-external-link-square-alt ms-1"></i></a> pipeline using the <a href="https://beam.apache.org/documentation/sdks/python/" target="_blank" rel="noopener noreferrer">Python SDK<i class="fas fa-external-link-square-alt ms-1"></i></a> and deploy it on an <a href="https://flink.apache.org/" target="_blank" rel="noopener noreferrer">Apache Flink<i class="fas fa-external-link-square-alt ms-1"></i></a> cluster via the <a href="https://beam.apache.org/documentation/runners/flink/" target="_blank" rel="noopener noreferrer">Apache Flink Runner<i class="fas fa-external-link-square-alt ms-1"></i></a>. Same as <a href="/blog/2024-05-30-beam-deploy-1">Part I</a>, we deploy a Kafka cluster using the <a href="https://strimzi.io/" target="_blank" rel="noopener noreferrer">Strimzi Operator<i class="fas fa-external-link-square-alt ms-1"></i></a> on a <a href="https://minikube.sigs.k8s.io/docs/" target="_blank" rel="noopener noreferrer">minikube<i class="fas fa-external-link-square-alt ms-1"></i></a> cluster as the pipeline uses <a href="https://kafka.apache.org/" target="_blank" rel="noopener noreferrer">Apache Kafka<i class="fas fa-external-link-square-alt ms-1"></i></a> topics for its data source and sink. Then, we develop the pipeline as a Python package and add the package to a custom Docker image so that Python user code can be executed externally. For deployment, we create a Flink session cluster via the <a href="https://nightlies.apache.org/flink/flink-kubernetes-operator-docs-main/" target="_blank" rel="noopener noreferrer">Flink Kubernetes Operator<i class="fas fa-external-link-square-alt ms-1"></i></a>, and deploy the pipeline using a Kubernetes job. Finally, we check the output of the application by sending messages to the input Kafka topic using a Python producer application.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2024-06-06-beam-deploy-2/featured.png" length="58020" type="image/png"/></item><item><title>Deploy Python Stream Processing App on Kubernetes - Part 1 PyFlink Application</title><link>https://jaehyeon.me/blog/2024-05-30-beam-deploy-1/</link><pubDate>Thu, 30 May 2024 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2024-05-30-beam-deploy-1/</guid><description><![CDATA[<p><a href="https://nightlies.apache.org/flink/flink-kubernetes-operator-docs-main/docs/concepts/overview/" target="_blank" rel="noopener noreferrer">Flink Kubernetes Operator<i class="fas fa-external-link-square-alt ms-1"></i></a> acts as a control plane to manage the complete deployment lifecycle of Apache Flink applications. With the operator, we can simplify deployment and management of Python stream processing applications. In this series, we discuss how to deploy a PyFlink application and Python Apache Beam pipeline on the <a href="https://beam.apache.org/documentation/runners/flink/" target="_blank" rel="noopener noreferrer">Flink Runner<i class="fas fa-external-link-square-alt ms-1"></i></a> on Kubernetes. In Part 1, we first deploy a Kafka cluster on a <a href="https://minikube.sigs.k8s.io/docs/" target="_blank" rel="noopener noreferrer">minikube<i class="fas fa-external-link-square-alt ms-1"></i></a> cluster as the source and sink of the PyFlink application are Kafka topics. Then, the application source is packaged in a custom Docker image and deployed on the minikube cluster using the Flink Kubernetes Operator. Finally, the output of the application is checked by sending messages to the input Kafka topic using a Python producer application.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2024-05-30-beam-deploy-1/featured.png" length="64457" type="image/png"/></item><item><title>Kafka Development on Kubernetes - Part 3 Kafka Connect</title><link>https://jaehyeon.me/blog/2024-01-11-kafka-development-on-k8s-part-3/</link><pubDate>Thu, 11 Jan 2024 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2024-01-11-kafka-development-on-k8s-part-3/</guid><description><![CDATA[<p><a href="https://kafka.apache.org/documentation/#connect" target="_blank" rel="noopener noreferrer">Kafka Connect<i class="fas fa-external-link-square-alt ms-1"></i></a> is a tool for scalably and reliably streaming data between Apache Kafka and other systems. It makes it simple to quickly define connectors that move large collections of data into and out of Kafka. In this post, we discuss how to set up a data ingestion pipeline using Kafka connectors. Fake customer and order data is ingested into Kafka topics using the <a href="https://github.com/awslabs/amazon-msk-data-generator" target="_blank" rel="noopener noreferrer">MSK Data Generator<i class="fas fa-external-link-square-alt ms-1"></i></a>. Also, we use the <a href="https://www.confluent.io/hub/confluentinc/kafka-connect-s3" target="_blank" rel="noopener noreferrer">Confluent S3<i class="fas fa-external-link-square-alt ms-1"></i></a> sink connector to save the messages of the topics into a S3 bucket. The Kafka Connect servers and individual connectors are deployed using the custom resources of <a href="https://strimzi.io/" target="_blank" rel="noopener noreferrer">Strimzi<i class="fas fa-external-link-square-alt ms-1"></i></a> on Kubernetes.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2024-01-11-kafka-development-on-k8s-part-3/featured.png" length="97270" type="image/png"/></item><item><title>Kafka Development on Kubernetes - Part 2 Producer and Consumer</title><link>https://jaehyeon.me/blog/2024-01-04-kafka-development-on-k8s-part-2/</link><pubDate>Thu, 04 Jan 2024 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2024-01-04-kafka-development-on-k8s-part-2/</guid><description><![CDATA[<p>Apache Kafka has five <a href="https://kafka.apache.org/documentation/#api" target="_blank" rel="noopener noreferrer">core APIs<i class="fas fa-external-link-square-alt ms-1"></i></a>, and we can develop applications to send/read streams of data to/from topics in a Kafka cluster using the producer and consumer APIs. While the main Kafka project maintains only the Java APIs, there are several <a href="https://cwiki.apache.org/confluence/display/KAFKA/Clients#Clients-Python" target="_blank" rel="noopener noreferrer">open source projects<i class="fas fa-external-link-square-alt ms-1"></i></a> that provide the Kafka client APIs in Python. In this post, we discuss how to develop Kafka client applications using the <a href="https://kafka-python.readthedocs.io/en/master/index.html" target="_blank" rel="noopener noreferrer">kafka-python<i class="fas fa-external-link-square-alt ms-1"></i></a> package on Kubernetes.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2024-01-04-kafka-development-on-k8s-part-2/featured.png" length="75889" type="image/png"/></item><item><title>Kafka Development on Kubernetes - Part 1 Cluster Setup</title><link>https://jaehyeon.me/blog/2023-12-21-kafka-development-on-k8s-part-1/</link><pubDate>Thu, 21 Dec 2023 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2023-12-21-kafka-development-on-k8s-part-1/</guid><description><![CDATA[<p><a href="https://kafka.apache.org/" target="_blank" rel="noopener noreferrer">Apache Kafka<i class="fas fa-external-link-square-alt ms-1"></i></a> is one of the key technologies for implementing data streaming architectures. <a href="https://strimzi.io/" target="_blank" rel="noopener noreferrer">Strimzi<i class="fas fa-external-link-square-alt ms-1"></i></a> provides a way to run an Apache Kafka cluster and related resources on Kubernetes in various deployment configurations. In this series of posts, we will discuss how to create a Kafka cluster, to develop Kafka client applications in Python and to build a data pipeline using Kafka connectors on Kubernetes.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2023-12-21-kafka-development-on-k8s-part-1/featured.png" length="108975" type="image/png"/></item><item><title>How I Prepared for Certified Kubernetes Application Developer (CKAD)</title><link>https://jaehyeon.me/blog/2023-10-12-how-i-prepared-for-ckad/</link><pubDate>Thu, 12 Oct 2023 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2023-10-12-how-i-prepared-for-ckad/</guid><description><![CDATA[<p>I recently obtained the <a href="https://training.linuxfoundation.org/certification/certified-kubernetes-application-developer-ckad/" target="_blank" rel="noopener noreferrer">Certified Kubernetes Application Developer (CKAD)<i class="fas fa-external-link-square-alt ms-1"></i></a> certification. CKAD has been developed by <a href="https://www.linuxfoundation.org/" target="_blank" rel="noopener noreferrer">The Linux Foundation<i class="fas fa-external-link-square-alt ms-1"></i></a> and the <a href="https://www.cncf.io/" target="_blank" rel="noopener noreferrer">Cloud Native Computing Foundation (CNCF)<i class="fas fa-external-link-square-alt ms-1"></i></a>, to help expand the Kubernetes ecosystem through standardized training and certification. Specifically this certification is for Kubernetes engineers, cloud engineers and other IT professionals responsible for building, deploying, and configuring cloud native applications with Kubernetes. In this post, I will summarise how I prepared for the exam by reviewing three online courses and two practice tests that I went through.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2023-10-12-how-i-prepared-for-ckad/featured.png" length="5945" type="image/png"/></item><item><title>Manage EMR on EKS with Terraform</title><link>https://jaehyeon.me/blog/2022-08-26-emr-on-eks-with-terraform/</link><pubDate>Fri, 26 Aug 2022 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2022-08-26-emr-on-eks-with-terraform/</guid><description><![CDATA[<p><a href="https://aws.amazon.com/emr/features/eks/" target="_blank" rel="noopener noreferrer">Amazon EMR on EKS<i class="fas fa-external-link-square-alt ms-1"></i></a> is a deployment option for Amazon EMR that allows you to automate the provisioning and management of open-source big data frameworks on EKS. While <a href="https://eksctl.io/" target="_blank" rel="noopener noreferrer">eksctl<i class="fas fa-external-link-square-alt ms-1"></i></a> is popular for working with <a href="https://aws.amazon.com/eks/" target="_blank" rel="noopener noreferrer">Amazon EKS<i class="fas fa-external-link-square-alt ms-1"></i></a> clusters, it has limitations when it comes to building infrastructure that integrates multiple AWS services. Also, it is not straightforward to update EKS cluster resources incrementally with it. On the other hand <a href="https://www.terraform.io/" target="_blank" rel="noopener noreferrer">Terraform<i class="fas fa-external-link-square-alt ms-1"></i></a> can be an effective tool for managing infrastructure that includes not only EKS and EMR virtual clusters but also other AWS resources. Moreover, Terraform has a wide range of <a href="https://www.terraform.io/language/modules" target="_blank" rel="noopener noreferrer">modules<i class="fas fa-external-link-square-alt ms-1"></i></a>, and it can even be simpler to build and manage infrastructure using those compared to the CLI tool. In this post, weâll discuss how to provision and manage Spark jobs on EMR on EKS with Terraform. <a href="https://aws-ia.github.io/terraform-aws-eks-blueprints/" target="_blank" rel="noopener noreferrer">Amazon EKS Blueprints for Terraform<i class="fas fa-external-link-square-alt ms-1"></i></a> will be used for provisioning EKS, EMR virtual cluster and related resources. Also, Spark job autoscaling will be managed by <a href="https://karpenter.sh/" target="_blank" rel="noopener noreferrer">Karpenter<i class="fas fa-external-link-square-alt ms-1"></i></a> where two Spark jobs with and without <a href="https://spark.apache.org/docs/latest/job-scheduling.html#dynamic-resource-allocation" target="_blank" rel="noopener noreferrer">Dynamic Resource Allocation (DRA)<i class="fas fa-external-link-square-alt ms-1"></i></a> will be compared.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2022-08-26-emr-on-eks-with-terraform/featured.png" length="67936" type="image/png"/></item><item><title>EMR on EKS by Example</title><link>https://jaehyeon.me/blog/2022-01-17-emr-on-eks-by-example/</link><pubDate>Mon, 17 Jan 2022 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2022-01-17-emr-on-eks-by-example/</guid><description><![CDATA[<p><a href="https://aws.amazon.com/emr/features/eks/" target="_blank" rel="noopener noreferrer">EMR on EKS<i class="fas fa-external-link-square-alt ms-1"></i></a> provides a deployment option for <a href="https://aws.amazon.com/emr/" target="_blank" rel="noopener noreferrer">Amazon EMR<i class="fas fa-external-link-square-alt ms-1"></i></a> that allows you to automate the provisioning and management of open-source big data frameworks on <a href="https://aws.amazon.com/eks/" target="_blank" rel="noopener noreferrer">Amazon EKS<i class="fas fa-external-link-square-alt ms-1"></i></a>. While a wide range of open source big data components are available in EMR on EC2, only Apache Spark is available in EMR on EKS. It is more flexible, however, that applications of different EMR versions can be run in multiple availability zones on either EC2 or Fargate. Also, other types of containerized applications can be deployed on the same EKS cluster. Therefore, if you have or plan to have, for example, <a href="https://airflow.apache.org/" target="_blank" rel="noopener noreferrer">Apache Airflow<i class="fas fa-external-link-square-alt ms-1"></i></a>, <a href="https://superset.apache.org/" target="_blank" rel="noopener noreferrer">Apache Superset<i class="fas fa-external-link-square-alt ms-1"></i></a> or <a href="https://www.kubeflow.org/" target="_blank" rel="noopener noreferrer">Kubeflow<i class="fas fa-external-link-square-alt ms-1"></i></a> as your analytics toolkits, it can be an effective way to manage big data (as well as non-big data) workloads. While Glue is more for ETL, EMR on EKS can also be used for other types of tasks such as machine learning. Moreover, it allows you to build a Spark application, not a <em>Gluish</em> Spark application. For example, while you have to use custom connectors for <a href="https://aws.amazon.com/marketplace/pp/prodview-zv3vmwbkuat2e" target="_blank" rel="noopener noreferrer">Hudi<i class="fas fa-external-link-square-alt ms-1"></i></a> or <a href="https://aws.amazon.com/marketplace/pp/prodview-iicxofvpqvsio" target="_blank" rel="noopener noreferrer">Iceberg<i class="fas fa-external-link-square-alt ms-1"></i></a> for Glue, you can use their native libraries with EMR on EKS. In this post, we&rsquo;ll discuss EMR on EKS with simple and elaborated examples.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2022-01-17-emr-on-eks-by-example/featured.png" length="76740" type="image/png"/></item><item><title>Distributed Task Queue with Python and R Example</title><link>https://jaehyeon.me/blog/2019-11-15-task-queue/</link><pubDate>Fri, 15 Nov 2019 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2019-11-15-task-queue/</guid><description><![CDATA[<p>While I&rsquo;m looking into <a href="https://airflow.apache.org/" target="_blank" rel="noopener noreferrer">Apache Airflow<i class="fas fa-external-link-square-alt ms-1"></i></a>, a workflow management tool, I thought it would be beneficial to get some understanding of how <a href="http://www.celeryproject.org/" target="_blank" rel="noopener noreferrer">Celery<i class="fas fa-external-link-square-alt ms-1"></i></a> works. To do so, I built a simple web service that sends tasks to Celery workers and collects the results from them. <a href="https://fastapi.tiangolo.com/" target="_blank" rel="noopener noreferrer">FastAPI<i class="fas fa-external-link-square-alt ms-1"></i></a> is used for developing the web service and <a href="https://redis.io/" target="_blank" rel="noopener noreferrer">Redis<i class="fas fa-external-link-square-alt ms-1"></i></a> is used for the message broker and result backend. During the development, I thought it would be possible to implement similar functionality in R with <a href="https://www.rforge.net/Rserve/" target="_blank" rel="noopener noreferrer">Rserve<i class="fas fa-external-link-square-alt ms-1"></i></a>. Therefore a Rserve worker is added as an example as well. Coupling a web service with distributed task queue is beneficial on its own as it helps the service be more responsive by offloading heavyweight and long running processes to task workers.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2019-11-15-task-queue/featured.png" length="51615" type="image/png"/></item><item><title>Linux Dev Environment on Windows</title><link>https://jaehyeon.me/blog/2019-11-01-linux-on-windows/</link><pubDate>Fri, 01 Nov 2019 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2019-11-01-linux-on-windows/</guid><description><![CDATA[<p>I use Linux containers a lot for development. Having Windows computers at home and work, I used to use Linux VMs on VirtualBox or VMWare Workstation. It&rsquo;s not a bad option but it requires a lot of resources. Recently, after my home computer was updated, I was not able to start my hypervisor anymore. Also I didn&rsquo;t like huge resource consumption of it so that I began to look for a different development environment. A while ago, I played with <a href="https://docs.microsoft.com/en-us/windows/wsl/about" target="_blank" rel="noopener noreferrer">Windows Subsystem for Linux (WSL)<i class="fas fa-external-link-square-alt ms-1"></i></a> and it was alright. Also <a href="https://code.visualstudio.com/" target="_blank" rel="noopener noreferrer">Visual Studio Code (VSCode)<i class="fas fa-external-link-square-alt ms-1"></i></a>, <em>my favourite editor</em>, now supports <a href="https://code.visualstudio.com/docs/remote/remote-overview" target="_blank" rel="noopener noreferrer">remote development<i class="fas fa-external-link-square-alt ms-1"></i></a>. Initially I thought I would be able to create a new development environment with WSL and <a href="https://docs.docker.com/docker-for-windows/install/" target="_blank" rel="noopener noreferrer">Docker for Windows<i class="fas fa-external-link-square-alt ms-1"></i></a>. However it was until I tried a bigger app with <a href="https://docs.docker.com/compose/" target="_blank" rel="noopener noreferrer">Docker Compose<i class="fas fa-external-link-square-alt ms-1"></i></a> that Docker for Windows has a number of issues especially when containers are started by Docker Compose in WSL. I didn&rsquo;t like to spend too much time on fixing those issues as I concerned those might not be the only ones. Then I decided to install a Linux VM on <a href="https://docs.microsoft.com/en-us/virtualization/hyper-v-on-windows/about/" target="_blank" rel="noopener noreferrer">Hyper-V<i class="fas fa-external-link-square-alt ms-1"></i></a>. Luckly VSCode also supports a remote VM via SSH.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2019-11-01-linux-on-windows/featured.png" length="187978" type="image/png"/></item></channel></rss>