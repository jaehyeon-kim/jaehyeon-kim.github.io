<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>GCP on Jaehyeon Kim</title><link>https://jaehyeon.me/tags/gcp/</link><description>Recent content in GCP on Jaehyeon Kim</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>Copyright Â© 2023-2024 Jaehyeon Kim. All Rights Reserved.</copyright><lastBuildDate>Fri, 13 Sep 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://jaehyeon.me/tags/gcp/index.xml" rel="self" type="application/rss+xml"/><item><title>Guide to Running DBT in Production</title><link>https://jaehyeon.me/blog/2024-09-13-dbt-guide/</link><pubDate>Fri, 13 Sep 2024 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2024-09-13-dbt-guide/</guid><description><![CDATA[<p>In the <a href="/blog/2024-09-05-dbt-cicd-demo">previous post</a>, we started discussing a <em>continuous integration/continuous delivery (CI/CD)</em> process of a <em>dbt</em> project by introducing two GitHub Actions workflows - <code>slim-ci</code> and <code>deploy</code>. The former is triggered when a pull request is created to the main branch, and it builds only modified models and its first-order children in a <em>ci</em> dataset, followed by performing tests on them. The second workflow gets triggered once a pull request is merged. Beginning with running unit tests, it packages the <em>dbt</em> project as a Docker container and publishes to <em>Artifact Registry</em>. In this post, we focus on how to deploy a <em>dbt</em> project in multiple environments while walking through the entire CI/CD process step-by-step.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2024-09-13-dbt-guide/featured.png" length="261700" type="image/png"/></item><item><title>DBT CI/CD Demo with BigQuery and GitHub Actions</title><link>https://jaehyeon.me/blog/2024-09-05-dbt-cicd-demo/</link><pubDate>Thu, 05 Sep 2024 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2024-09-05-dbt-cicd-demo/</guid><description><![CDATA[<p>Continuous integration (CI) is the process of ensuring new code integrates with the larger code base, and it puts a great emphasis on testing automation to check that the application is not broken whenever new commits are integrated into the main branch. Continuous delivery (CD) is an extension of continuous integration since it automatically deploys all code changes to a testing and/or production environment after the build stage. CI/CD helps development teams avoid bugs and code failures while maintaining a continuous cycle of software development and updates. In this post, we discuss how to set up a CI/CD pipeline for a <a href="https://www.getdbt.com/" target="_blank" rel="noopener noreferrer">data build tool (<em>dbt</em>)<i class="fas fa-external-link-square-alt ms-1"></i></a> project using <a href="https://github.com/features/actions" target="_blank" rel="noopener noreferrer">GitHub Actions<i class="fas fa-external-link-square-alt ms-1"></i></a> where <a href="https://cloud.google.com/bigquery?hl=en" target="_blank" rel="noopener noreferrer">BigQuery<i class="fas fa-external-link-square-alt ms-1"></i></a> is used as the target data warehouse.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2024-09-05-dbt-cicd-demo/featured.png" length="60835" type="image/png"/></item><item><title>Data Build Tool (dbt) Pizza Shop Demo - Part 4 ETL on BigQuery via Airflow</title><link>https://jaehyeon.me/blog/2024-02-22-dbt-pizza-shop-4/</link><pubDate>Thu, 22 Feb 2024 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2024-02-22-dbt-pizza-shop-4/</guid><description><![CDATA[<p>In <a href="/blog/2024-02-08-dbt-pizza-shop-3">Part 3</a>, we developed a <a href="https://docs.getdbt.com/docs/introduction" target="_blank" rel="noopener noreferrer">dbt<i class="fas fa-external-link-square-alt ms-1"></i></a> project that targets Google BigQuery with fictional pizza shop data. Two dimension tables that keep product and user records are created as <a href="https://en.wikipedia.org/wiki/Slowly_changing_dimension" target="_blank" rel="noopener noreferrer">Type 2 slowly changing dimension (SCD Type 2)<i class="fas fa-external-link-square-alt ms-1"></i></a> tables, and one transactional fact table is built to keep pizza orders. The fact table is denormalized using <a href="https://cloud.google.com/bigquery/docs/best-practices-performance-nested" target="_blank" rel="noopener noreferrer">nested and repeated fields<i class="fas fa-external-link-square-alt ms-1"></i></a> for improving query performance. In this post, we discuss how to set up an ETL process on the project using Apache Airflow.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2024-02-22-dbt-pizza-shop-4/featured.png" length="89588" type="image/png"/></item><item><title>Data Build Tool (dbt) Pizza Shop Demo - Part 3 Modelling on BigQuery</title><link>https://jaehyeon.me/blog/2024-02-08-dbt-pizza-shop-3/</link><pubDate>Thu, 08 Feb 2024 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2024-02-08-dbt-pizza-shop-3/</guid><description><![CDATA[<p>In this series, we discuss practical examples of data warehouse and lakehouse development where data transformation is performed by the <a href="https://docs.getdbt.com/docs/introduction" target="_blank" rel="noopener noreferrer">data build tool (dbt)<i class="fas fa-external-link-square-alt ms-1"></i></a> and ETL is managed by <a href="https://airflow.apache.org/" target="_blank" rel="noopener noreferrer">Apache Airflow<i class="fas fa-external-link-square-alt ms-1"></i></a>. In <a href="/blog/2024-01-18-dbt-pizza-shop-1">Part 1</a>, we developed a <em>dbt</em> project on PostgreSQL using fictional pizza shop data. At the end, the data sets are modelled by two <a href="https://en.wikipedia.org/wiki/Slowly_changing_dimension" target="_blank" rel="noopener noreferrer">SCD type 2<i class="fas fa-external-link-square-alt ms-1"></i></a> dimension tables and one transactional fact table. In this post, we create a new <em>dbt</em> project that targets <a href="https://cloud.google.com/bigquery" target="_blank" rel="noopener noreferrer">Google BigQuery<i class="fas fa-external-link-square-alt ms-1"></i></a>. While the dimension tables are kept by the same SCD type 2 approach, the fact table is denormalized using <a href="https://cloud.google.com/bigquery/docs/best-practices-performance-nested" target="_blank" rel="noopener noreferrer">nested and repeated fields<i class="fas fa-external-link-square-alt ms-1"></i></a>, which potentially can improve query performance by pre-joining corresponding dimension records.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2024-02-08-dbt-pizza-shop-3/featured.png" length="70297" type="image/png"/></item></channel></rss>