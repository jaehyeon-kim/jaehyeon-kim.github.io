<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Docker Compose on Jaehyeon Kim</title><link>https://jaehyeon.me/tags/docker-compose/</link><description>Recent content in Docker Compose on Jaehyeon Kim</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>Copyright © 2023-2024 Jaehyeon Kim. All Rights Reserved.</copyright><lastBuildDate>Thu, 15 Aug 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://jaehyeon.me/tags/docker-compose/index.xml" rel="self" type="application/rss+xml"/><item><title>Apache Beam Python Examples - Part 4 Call RPC Service for Data Augmentation</title><link>https://jaehyeon.me/blog/2024-08-15-beam-examples-4/</link><pubDate>Thu, 15 Aug 2024 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2024-08-15-beam-examples-4/</guid><description>&lt;p>In this post, we develop an Apache Beam pipeline where the input data is augmented by an &lt;strong>Remote Procedure Call (RPC)&lt;/strong>. Each of the input elements performs an RPC call and the output is enriched by the response. This is not an efficient way of accessing an external service provided that the service can accept more than one element. In the subsequent two posts, we will discuss updated pipelines that make RPC calls more efficiently. We begin with illustrating how to manage development resources followed by demonstrating the RPC service that we use in this series. Finally, we develop a Beam pipeline that accesses the external service to augment the input elements.&lt;/p></description><enclosure url="https://jaehyeon.me/blog/2024-08-15-beam-examples-4/featured.png" length="93408" type="image/png"/></item><item><title>Apache Beam Python Examples - Part 3 Build Sport Activity Tracker with/without SQL</title><link>https://jaehyeon.me/blog/2024-08-01-beam-examples-3/</link><pubDate>Thu, 01 Aug 2024 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2024-08-01-beam-examples-3/</guid><description><![CDATA[<p>In this post, we develop two Apache Beam pipelines that track sport activities of users and output their speed periodically. The first pipeline uses native transforms and <a href="https://beam.apache.org/documentation/dsls/sql/overview/" target="_blank" rel="noopener noreferrer">Beam SQL<i class="fas fa-external-link-square-alt ms-1"></i></a> is used for the latter. While <em>Beam SQL</em> can be useful in some situations, its features in the Python SDK are not complete compared to the Java SDK. Therefore, we are not able to build the required tracking pipeline using it. We end up discussing potential improvements of <em>Beam SQL</em> so that it can be used for building competitive applications with the Python SDK.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2024-08-01-beam-examples-3/featured.png" length="94507" type="image/png"/></item><item><title>Apache Beam Python Examples - Part 2 Calculate Average Word Length with/without Fixed Look back</title><link>https://jaehyeon.me/blog/2024-07-18-beam-examples-2/</link><pubDate>Thu, 18 Jul 2024 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2024-07-18-beam-examples-2/</guid><description><![CDATA[<p>In this post, we develop two Apache Beam pipelines that calculate average word lengths from input texts that are ingested by a Kafka topic. They obtain the statistics in different angles. The first pipeline emits the global average lengths whenever a new input text arrives while the latter triggers those values in a sliding time window.</p>
<ul>
<li><a href="/blog/2024-07-04-beam-examples-1">Part 1 Calculate K Most Frequent Words and Max Word Length</a></li>
<li><a href="/blog/2024-07-18-beam-examples-2/#">Part 2 Calculate Average Word Length with/without Fixed Look back</a> (this post)</li>
<li><a href="/blog/2024-08-01-beam-examples-3">Part 3 Build Sport Activity Tracker with/without SQL</a></li>
<li><a href="/blog/2024-08-15-beam-examples-4">Part 4 Call RPC Service for Data Augmentation</a></li>
<li>Part 5 Call RPC Service in Batch using Stateless DoFn</li>
<li>Part 6 Call RPC Service in Batch with Defined Batch Size using Stateful DoFn</li>
<li>Part 7 Separate Droppable Data into Side Output</li>
<li>Part 8 Enhance Sport Activity Tracker with Runner Motivation</li>
<li>Part 9 Develop Batch File Reader and PiSampler using Splittable DoFn</li>
<li>Part 10 Develop Streaming File Reader using Splittable DoFn</li>
</ul>

<h2 id="development-environment" data-numberify>Development Environment<a class="anchor ms-1" href="#development-environment"></a></h2>
<p>The development environment has an Apache Flink cluster and Apache Kafka cluster and <a href="https://grpc.io/" target="_blank" rel="noopener noreferrer">gRPC<i class="fas fa-external-link-square-alt ms-1"></i></a> server - gRPC server will be used in later posts. For Flink, we can use either an embedded cluster or a local cluster while <a href="https://docs.docker.com/compose/" target="_blank" rel="noopener noreferrer">Docker Compose<i class="fas fa-external-link-square-alt ms-1"></i></a> is used for the rest. See <a href="/blog/2024-07-04-beam-examples-1">Part 1</a> for details about how to set up the development environment. The source of this post can be found in this <a href="https://github.com/jaehyeon-kim/beam-demos/tree/master/beam-pipelines" target="_blank" rel="noopener noreferrer"><strong>GitHub repository</strong><i class="fas fa-external-link-square-alt ms-1"></i></a>.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2024-07-18-beam-examples-2/featured.png" length="96924" type="image/png"/></item><item><title>Apache Beam Python Examples - Part 1 Calculate K Most Frequent Words and Max Word Length</title><link>https://jaehyeon.me/blog/2024-07-04-beam-examples-1/</link><pubDate>Thu, 04 Jul 2024 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2024-07-04-beam-examples-1/</guid><description><![CDATA[<p>In this series, we develop <a href="https://beam.apache.org/" target="_blank" rel="noopener noreferrer">Apache Beam<i class="fas fa-external-link-square-alt ms-1"></i></a> Python pipelines. The majority of them are from <a href="https://www.packtpub.com/en-us/product/building-big-data-pipelines-with-apache-beam-9781800564930" target="_blank" rel="noopener noreferrer">Building Big Data Pipelines with Apache Beam by Jan Lukavský<i class="fas fa-external-link-square-alt ms-1"></i></a>. Mainly relying on the Java SDK, the book teaches fundamentals of Apache Beam using hands-on tasks, and we convert those tasks using the Python SDK. We focus on streaming pipelines, and they are deployed on a local (or embedded) <a href="https://flink.apache.org/" target="_blank" rel="noopener noreferrer">Apache Flink<i class="fas fa-external-link-square-alt ms-1"></i></a> cluster using the <a href="https://beam.apache.org/documentation/runners/flink/" target="_blank" rel="noopener noreferrer">Apache Flink Runner<i class="fas fa-external-link-square-alt ms-1"></i></a>. Beginning with setting up the development environment, we build two pipelines that obtain top K most frequent words and the word that has the longest word length in this post.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2024-07-04-beam-examples-1/featured.png" length="96881" type="image/png"/></item><item><title>Data Build Tool (dbt) Pizza Shop Demo - Part 6 ETL on Amazon Athena via Airflow</title><link>https://jaehyeon.me/blog/2024-03-14-dbt-pizza-shop-6/</link><pubDate>Thu, 14 Mar 2024 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2024-03-14-dbt-pizza-shop-6/</guid><description><![CDATA[<p>In <a href="/blog/2024-03-07-dbt-pizza-shop-5">Part 5</a>, we developed a <a href="https://docs.getdbt.com/docs/introduction" target="_blank" rel="noopener noreferrer">dbt<i class="fas fa-external-link-square-alt ms-1"></i></a> project that that targets <a href="https://iceberg.apache.org/" target="_blank" rel="noopener noreferrer">Apache Iceberg<i class="fas fa-external-link-square-alt ms-1"></i></a> where transformations are performed on <a href="https://aws.amazon.com/athena/" target="_blank" rel="noopener noreferrer">Amazon Athena<i class="fas fa-external-link-square-alt ms-1"></i></a>. Two dimension tables that keep product and user records are created as <a href="https://en.wikipedia.org/wiki/Slowly_changing_dimension" target="_blank" rel="noopener noreferrer">Type 2 slowly changing dimension (SCD Type 2)<i class="fas fa-external-link-square-alt ms-1"></i></a> tables, and one transactional fact table is built to keep pizza orders. To improve query performance, the fact table is denormalized to pre-join records from the dimension tables using the array and struct data types. In this post, we discuss how to set up an ETL process on the project using Apache Airflow.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2024-03-14-dbt-pizza-shop-6/featured.png" length="82921" type="image/png"/></item><item><title>Data Build Tool (dbt) Pizza Shop Demo - Part 4 ETL on BigQuery via Airflow</title><link>https://jaehyeon.me/blog/2024-02-22-dbt-pizza-shop-4/</link><pubDate>Thu, 22 Feb 2024 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2024-02-22-dbt-pizza-shop-4/</guid><description><![CDATA[<p>In <a href="/blog/2024-02-08-dbt-pizza-shop-3">Part 3</a>, we developed a <a href="https://docs.getdbt.com/docs/introduction" target="_blank" rel="noopener noreferrer">dbt<i class="fas fa-external-link-square-alt ms-1"></i></a> project that targets Google BigQuery with fictional pizza shop data. Two dimension tables that keep product and user records are created as <a href="https://en.wikipedia.org/wiki/Slowly_changing_dimension" target="_blank" rel="noopener noreferrer">Type 2 slowly changing dimension (SCD Type 2)<i class="fas fa-external-link-square-alt ms-1"></i></a> tables, and one transactional fact table is built to keep pizza orders. The fact table is denormalized using <a href="https://cloud.google.com/bigquery/docs/best-practices-performance-nested" target="_blank" rel="noopener noreferrer">nested and repeated fields<i class="fas fa-external-link-square-alt ms-1"></i></a> for improving query performance. In this post, we discuss how to set up an ETL process on the project using Apache Airflow.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2024-02-22-dbt-pizza-shop-4/featured.png" length="89588" type="image/png"/></item><item><title>Data Build Tool (dbt) Pizza Shop Demo - Part 2 ETL on PostgreSQL via Airflow</title><link>https://jaehyeon.me/blog/2024-01-25-dbt-pizza-shop-2/</link><pubDate>Thu, 25 Jan 2024 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2024-01-25-dbt-pizza-shop-2/</guid><description><![CDATA[<p>In this series of posts, we discuss data warehouse/lakehouse examples using <a href="https://docs.getdbt.com/docs/introduction" target="_blank" rel="noopener noreferrer">data build tool (dbt)<i class="fas fa-external-link-square-alt ms-1"></i></a> including ETL orchestration with Apache Airflow. In Part 1, we developed a <em>dbt</em> project on PostgreSQL with fictional pizza shop data. Two dimension tables that keep product and user records are created as <a href="https://en.wikipedia.org/wiki/Slowly_changing_dimension" target="_blank" rel="noopener noreferrer">Type 2 slowly changing dimension (SCD Type 2)<i class="fas fa-external-link-square-alt ms-1"></i></a> tables, and one transactional fact table is built to keep pizza orders. In this post, we discuss how to set up an ETL process on the project using Apache Airflow.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2024-01-25-dbt-pizza-shop-2/featured.png" length="77355" type="image/png"/></item><item><title>Data Build Tool (dbt) Pizza Shop Demo - Part 1 Modelling on PostgreSQL</title><link>https://jaehyeon.me/blog/2024-01-18-dbt-pizza-shop-1/</link><pubDate>Thu, 18 Jan 2024 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2024-01-18-dbt-pizza-shop-1/</guid><description><![CDATA[<p>The <a href="https://docs.getdbt.com/docs/introduction" target="_blank" rel="noopener noreferrer">data build tool (dbt)<i class="fas fa-external-link-square-alt ms-1"></i></a> is a popular data transformation tool for data warehouse development. Moreover, it can be used for <a href="https://www.databricks.com/glossary/data-lakehouse" target="_blank" rel="noopener noreferrer">data lakehouse<i class="fas fa-external-link-square-alt ms-1"></i></a> development thanks to open table formats such as Apache Iceberg, Apache Hudi and Delta Lake. <em>dbt</em> supports key AWS analytics services and I wrote a series of posts that discuss how to utilise <em>dbt</em> with <a href="/blog/2022-09-28-dbt-on-aws-part-1-redshift">Redshift</a>, <a href="/blog/2022-10-09-dbt-on-aws-part-2-glue">Glue</a>, <a href="/blog/2022-10-19-dbt-on-aws-part-3-emr-ec2">EMR on EC2</a>, <a href="/blog/2022-11-01-dbt-on-aws-part-4-emr-eks">EMR on EKS</a>, and <a href="/blog/2023-04-12-integrate-glue-schema-registry">Athena</a>. Those posts focus on platform integration, however, they do not show realistic ETL scenarios.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2024-01-18-dbt-pizza-shop-1/featured.png" length="85093" type="image/png"/></item><item><title>Real Time Streaming with Kafka and Flink - Lab 6 Consume data from Kafka using Lambda</title><link>https://jaehyeon.me/blog/2023-12-14-real-time-streaming-with-kafka-and-flink-7/</link><pubDate>Thu, 14 Dec 2023 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2023-12-14-real-time-streaming-with-kafka-and-flink-7/</guid><description><![CDATA[<p>Amazon MSK can be configured as an <a href="https://docs.aws.amazon.com/lambda/latest/dg/with-msk.html" target="_blank" rel="noopener noreferrer">event source<i class="fas fa-external-link-square-alt ms-1"></i></a> of a Lambda function. Lambda internally polls for new messages from the event source and then synchronously invokes the target Lambda function. With this feature, we can develop a Kafka consumer application in serverless environment where developers can focus on application logic. In this lab, we will discuss how to create a Kafka consumer using a Lambda function.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2023-12-14-real-time-streaming-with-kafka-and-flink-7/featured.png" length="138986" type="image/png"/></item><item><title>Setup Local Development Environment for Apache Flink and Spark Using EMR Container Images</title><link>https://jaehyeon.me/blog/2023-12-07-flink-spark-local-dev/</link><pubDate>Thu, 07 Dec 2023 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2023-12-07-flink-spark-local-dev/</guid><description><![CDATA[<p><a href="https://aws.amazon.com/about-aws/whats-new/2023/11/apache-flink-available-amazon-emr-eks/" target="_blank" rel="noopener noreferrer">Apache Flink became generally available<i class="fas fa-external-link-square-alt ms-1"></i></a> for <a href="https://aws.amazon.com/emr/features/eks/" target="_blank" rel="noopener noreferrer">Amazon EMR on EKS<i class="fas fa-external-link-square-alt ms-1"></i></a> from the <a href="https://docs.aws.amazon.com/emr/latest/EMR-on-EKS-DevelopmentGuide/emr-eks-6.15.0.html" target="_blank" rel="noopener noreferrer">EMR 6.15.0 releases<i class="fas fa-external-link-square-alt ms-1"></i></a>, and we are able to pull the Flink (as well as Spark) container images from the <a href="https://gallery.ecr.aws/emr-on-eks" target="_blank" rel="noopener noreferrer">ECR Public Gallery<i class="fas fa-external-link-square-alt ms-1"></i></a>. As both of them can be integrated with the <em>Glue Data Catalog</em>, it can be particularly useful if we develop real time data ingestion/processing via Flink and build analytical queries using Spark (or any other tools or services that can access to the Glue Data Catalog).</p>]]></description><enclosure url="https://jaehyeon.me/blog/2023-12-07-flink-spark-local-dev/featured.png" length="133053" type="image/png"/></item><item><title>Real Time Streaming with Kafka and Flink - Lab 5 Write data to DynamoDB using Kafka Connect</title><link>https://jaehyeon.me/blog/2023-11-30-real-time-streaming-with-kafka-and-flink-6/</link><pubDate>Thu, 30 Nov 2023 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2023-11-30-real-time-streaming-with-kafka-and-flink-6/</guid><description><![CDATA[<p><a href="https://kafka.apache.org/documentation/#connect" target="_blank" rel="noopener noreferrer">Kafka Connect<i class="fas fa-external-link-square-alt ms-1"></i></a> is a tool for scalably and reliably streaming data between Apache Kafka and other systems. It makes it simple to quickly define connectors that move large collections of data into and out of Kafka. In this lab, we will discuss how to create a data pipeline that ingests data from a Kafka topic into a DynamoDB table using the <a href="https://camel.apache.org/camel-kafka-connector/3.20.x/reference/connectors/camel-aws-ddb-sink-kafka-sink-connector.html" target="_blank" rel="noopener noreferrer">Camel DynamoDB sink connector<i class="fas fa-external-link-square-alt ms-1"></i></a>.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2023-11-30-real-time-streaming-with-kafka-and-flink-6/featured.png" length="113252" type="image/png"/></item><item><title>Real Time Streaming with Kafka and Flink - Lab 4 Clean, Aggregate, and Enrich Events with Flink</title><link>https://jaehyeon.me/blog/2023-11-23-real-time-streaming-with-kafka-and-flink-5/</link><pubDate>Thu, 23 Nov 2023 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2023-11-23-real-time-streaming-with-kafka-and-flink-5/</guid><description>&lt;p>The value of data can be maximised when it is used without delay. With Apache Flink, we can build streaming analytics applications that incorporate the latest events with low latency. In this lab, we will create a Pyflink application that writes accumulated taxi rides data into an OpenSearch cluster. It aggregates the number of trips/passengers and trip durations by vendor ID for a window of 5 seconds. The data is then used to create a chart that monitors the status of taxi rides in the OpenSearch Dashboard.&lt;/p></description><enclosure url="https://jaehyeon.me/blog/2023-11-23-real-time-streaming-with-kafka-and-flink-5/featured.png" length="112340" type="image/png"/></item><item><title>Real Time Streaming with Kafka and Flink - Lab 3 Transform and write data to S3 from Kafka using Flink</title><link>https://jaehyeon.me/blog/2023-11-16-real-time-streaming-with-kafka-and-flink-4/</link><pubDate>Thu, 16 Nov 2023 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2023-11-16-real-time-streaming-with-kafka-and-flink-4/</guid><description><![CDATA[<p>In this lab, we will create a Pyflink application that exports Kafka topic messages into a S3 bucket. The app enriches the records by adding a new column using a user defined function and writes them via the <a href="https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/connectors/table/filesystem/" target="_blank" rel="noopener noreferrer">FileSystem SQL connector<i class="fas fa-external-link-square-alt ms-1"></i></a>. This allows us to achieve a simpler architecture compared to the <a href="https://catalog.us-east-1.prod.workshops.aws/workshops/2300137e-f2ac-4eb9-a4ac-3d25026b235f/en-US/lab-3-kdf" target="_blank" rel="noopener noreferrer">original lab<i class="fas fa-external-link-square-alt ms-1"></i></a> where the records are sent into Amazon Kinesis Data Firehose, enriched by a separate Lambda function and written to a S3 bucket afterwards. While the records are being written to the S3 bucket, a Glue table will be created to query them on Amazon Athena.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2023-11-16-real-time-streaming-with-kafka-and-flink-4/featured.png" length="160359" type="image/png"/></item><item><title>Real Time Streaming with Kafka and Flink - Lab 2 Write data to Kafka from S3 using Flink</title><link>https://jaehyeon.me/blog/2023-11-09-real-time-streaming-with-kafka-and-flink-3/</link><pubDate>Thu, 09 Nov 2023 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2023-11-09-real-time-streaming-with-kafka-and-flink-3/</guid><description>&lt;p>In this lab, we will create a Pyflink application that reads records from S3 and sends them into a Kafka topic. A custom pipeline Jar file will be created as the Kafka cluster is authenticated by IAM, and it will be demonstrated how to execute the app in a Flink cluster deployed on Docker as well as locally as a typical Python app. We can assume the S3 data is static metadata that needs to be joined into another stream, and this exercise can be useful for data enrichment.&lt;/p></description><enclosure url="https://jaehyeon.me/blog/2023-11-09-real-time-streaming-with-kafka-and-flink-3/featured.png" length="139114" type="image/png"/></item><item><title>Kafka Connect for AWS Services Integration - Part 5 Deploy Aiven OpenSearch Sink Connector</title><link>https://jaehyeon.me/blog/2023-10-30-kafka-connect-for-aws-part-5/</link><pubDate>Mon, 30 Oct 2023 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2023-10-30-kafka-connect-for-aws-part-5/</guid><description><![CDATA[<p>In the previous post, we discussed how to develop a data pipeline from Apache Kafka into OpenSearch locally using Docker. The pipeline will be deployed on AWS using <a href="https://aws.amazon.com/msk/" target="_blank" rel="noopener noreferrer">Amazon MSK<i class="fas fa-external-link-square-alt ms-1"></i></a>, <a href="https://aws.amazon.com/msk/features/msk-connect/" target="_blank" rel="noopener noreferrer">Amazon MSK Connect<i class="fas fa-external-link-square-alt ms-1"></i></a> and <a href="https://aws.amazon.com/opensearch-service/" target="_blank" rel="noopener noreferrer">Amazon OpenSearch Service<i class="fas fa-external-link-square-alt ms-1"></i></a> using <a href="https://www.terraform.io/" target="_blank" rel="noopener noreferrer">Terraform<i class="fas fa-external-link-square-alt ms-1"></i></a> in this post. First the infrastructure will be deployed that covers a Virtual Private Cloud (VPC), Virtual Private Network (VPN) server, MSK Cluster and OpenSearch domain. Then Kafka source and sink connectors will be deployed on MSK Connect, followed by performing quick data analysis.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2023-10-30-kafka-connect-for-aws-part-5/featured.png" length="85575" type="image/png"/></item><item><title>Real Time Streaming with Kafka and Flink - Lab 1 Produce data to Kafka using Lambda</title><link>https://jaehyeon.me/blog/2023-10-26-real-time-streaming-with-kafka-and-flink-2/</link><pubDate>Thu, 26 Oct 2023 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2023-10-26-real-time-streaming-with-kafka-and-flink-2/</guid><description><![CDATA[<p>In this lab, we will create a Kafka producer application using <a href="https://aws.amazon.com/lambda/" target="_blank" rel="noopener noreferrer">AWS Lambda<i class="fas fa-external-link-square-alt ms-1"></i></a>, which sends fake taxi ride data into a Kafka topic on <a href="https://aws.amazon.com/msk/" target="_blank" rel="noopener noreferrer">Amazon MSK<i class="fas fa-external-link-square-alt ms-1"></i></a>. A configurable number of the producer Lambda function will be invoked by an <a href="https://aws.amazon.com/eventbridge/" target="_blank" rel="noopener noreferrer">Amazon EventBridge<i class="fas fa-external-link-square-alt ms-1"></i></a> schedule rule. In this way we are able to generate test data concurrently based on the desired volume of messages.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2023-10-26-real-time-streaming-with-kafka-and-flink-2/featured.png" length="138560" type="image/png"/></item><item><title>Kafka Connect for AWS Services Integration - Part 4 Develop Aiven OpenSearch Sink Connector</title><link>https://jaehyeon.me/blog/2023-10-23-kafka-connect-for-aws-part-4/</link><pubDate>Mon, 23 Oct 2023 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2023-10-23-kafka-connect-for-aws-part-4/</guid><description><![CDATA[<p><a href="https://opensearch.org/" target="_blank" rel="noopener noreferrer">OpenSearch<i class="fas fa-external-link-square-alt ms-1"></i></a> is a popular search and analytics engine and its use cases cover log analytics, real-time application monitoring, and clickstream analysis. OpenSearch can be deployed on its own or via <a href="https://aws.amazon.com/opensearch-service/" target="_blank" rel="noopener noreferrer">Amazon OpenSearch Service<i class="fas fa-external-link-square-alt ms-1"></i></a>. <a href="https://kafka.apache.org/" target="_blank" rel="noopener noreferrer">Apache Kafka<i class="fas fa-external-link-square-alt ms-1"></i></a> is a distributed event store and stream-processing platform, and it aims to provide a unified, high-throughput, low-latency platform for handling real-time data feeds. On AWS, Apache Kafka can be deployed via <a href="https://aws.amazon.com/msk/" target="_blank" rel="noopener noreferrer">Amazon Managed Streaming for Apache Kafka (MSK)<i class="fas fa-external-link-square-alt ms-1"></i></a>.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2023-10-23-kafka-connect-for-aws-part-4/featured.png" length="61820" type="image/png"/></item><item><title>Building Apache Flink Applications in Python</title><link>https://jaehyeon.me/blog/2023-10-19-build-pyflink-apps/</link><pubDate>Thu, 19 Oct 2023 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2023-10-19-build-pyflink-apps/</guid><description><![CDATA[<p><a href="https://developer.confluent.io/courses/flink-java/overview/" target="_blank" rel="noopener noreferrer">Building Apache Flink Applications in Java<i class="fas fa-external-link-square-alt ms-1"></i></a> is a course to introduce <a href="https://flink.apache.org/" target="_blank" rel="noopener noreferrer">Apache Flink<i class="fas fa-external-link-square-alt ms-1"></i></a> through a series of hands-on exercises, and it is provided by <a href="https://www.confluent.io/" target="_blank" rel="noopener noreferrer">Confluent<i class="fas fa-external-link-square-alt ms-1"></i></a>. Utilising the <a href="https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/dev/datastream/overview/" target="_blank" rel="noopener noreferrer">Flink DataStream API<i class="fas fa-external-link-square-alt ms-1"></i></a>, the course develops three Flink applications that populate multiple source data sets, collect them into a standardised data set, and aggregate it to produce usage statistics. As part of learning the Flink DataStream API in Pyflink, I converted the Java apps into Python equivalent while performing the course exercises in Pyflink. This post summarises the progress of the conversion and shows the final output.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2023-10-19-build-pyflink-apps/featured.png" length="154736" type="image/png"/></item><item><title>Real Time Streaming with Kafka and Flink - Introduction</title><link>https://jaehyeon.me/blog/2023-10-05-real-time-streaming-with-kafka-and-flink-1/</link><pubDate>Thu, 05 Oct 2023 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2023-10-05-real-time-streaming-with-kafka-and-flink-1/</guid><description><![CDATA[<p><a href="https://catalog.us-east-1.prod.workshops.aws/workshops/2300137e-f2ac-4eb9-a4ac-3d25026b235f/en-US" target="_blank" rel="noopener noreferrer">Real Time Streaming with Amazon Kinesis<i class="fas fa-external-link-square-alt ms-1"></i></a> is an AWS workshop that helps users build a streaming analytics application on AWS. Incoming events are stored in a number of streams of the <a href="https://aws.amazon.com/kinesis/data-streams/" target="_blank" rel="noopener noreferrer">Amazon Kinesis Data Streams<i class="fas fa-external-link-square-alt ms-1"></i></a> service, and various other AWS services and tools are used to process and analyse data.</p>
<p><a href="https://kafka.apache.org/" target="_blank" rel="noopener noreferrer">Apache Kafka<i class="fas fa-external-link-square-alt ms-1"></i></a> is a popular distributed event store and stream processing platform, and it stores incoming events in topics. As part of learning real time streaming analytics on AWS, we can rebuild the analytics applications by replacing the Kinesis streams with Kafka topics. As an introduction, this post compares the workshop architecture with the updated architecture of this series. The labs of the updated architecture will be implemented in subsequent posts.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2023-10-05-real-time-streaming-with-kafka-and-flink-1/featured.png" length="138141" type="image/png"/></item><item><title>Getting Started with Pyflink on AWS - Part 3 AWS Managed Flink and MSK</title><link>https://jaehyeon.me/blog/2023-09-04-getting-started-with-pyflink-on-aws-part-3/</link><pubDate>Mon, 04 Sep 2023 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2023-09-04-getting-started-with-pyflink-on-aws-part-3/</guid><description><![CDATA[<p>In this series of posts, we discuss a Flink (Pyflink) application that reads/writes from/to Kafka topics. In the previous posts, I demonstrated a Pyflink app that targets a local Kafka cluster as well as a Kafka cluster on Amazon MSK. The app was executed in a virtual environment as well as in a local Flink cluster for improved monitoring. In this post, the app will be deployed via <a href="https://aws.amazon.com/about-aws/whats-new/2023/08/amazon-managed-service-apache-flink/" target="_blank" rel="noopener noreferrer">Amazon Managed Service for Apache Flink<i class="fas fa-external-link-square-alt ms-1"></i></a>, which is the easiest option to run Flink applications on AWS.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2023-09-04-getting-started-with-pyflink-on-aws-part-3/featured.png" length="74618" type="image/png"/></item><item><title>Getting Started with Pyflink on AWS - Part 2 Local Flink and MSK</title><link>https://jaehyeon.me/blog/2023-08-28-getting-started-with-pyflink-on-aws-part-2/</link><pubDate>Mon, 28 Aug 2023 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2023-08-28-getting-started-with-pyflink-on-aws-part-2/</guid><description><![CDATA[<p>In this series of posts, we discuss a Flink (Pyflink) application that reads/writes from/to Kafka topics. In part 1, an app that targets a local Kafka cluster was created. In this post, we will update the app by connecting a Kafka cluster on Amazon MSK. The Kafka cluster is authenticated by IAM and the app has additional jar dependency. As <a href="https://aws.amazon.com/about-aws/whats-new/2023/08/amazon-managed-service-apache-flink/" target="_blank" rel="noopener noreferrer">Amazon Managed Service for Apache Flink<i class="fas fa-external-link-square-alt ms-1"></i></a> does not allow you to specify multiple pipeline jar files, we have to build a custom Uber Jar that combines multiple jar files. Same as part 1, the app will be executed in a virtual environment as well as in a local Flink cluster for improved monitoring with the updated pipeline jar file.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2023-08-28-getting-started-with-pyflink-on-aws-part-2/featured.png" length="64005" type="image/png"/></item><item><title>Getting Started with Pyflink on AWS - Part 1 Local Flink and Local Kafka</title><link>https://jaehyeon.me/blog/2023-08-17-getting-started-with-pyflink-on-aws-part-1/</link><pubDate>Thu, 17 Aug 2023 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2023-08-17-getting-started-with-pyflink-on-aws-part-1/</guid><description><![CDATA[<p><a href="https://flink.apache.org/" target="_blank" rel="noopener noreferrer">Apache Flink<i class="fas fa-external-link-square-alt ms-1"></i></a> is an open-source, unified stream-processing and batch-processing framework. Its core is a distributed streaming data-flow engine that you can use to run real-time stream processing on high-throughput data sources. Currently, it is widely used to build applications for fraud/anomaly detection, rule-based alerting, business process monitoring, and continuous ETL to name a few. On AWS, we can deploy a Flink application via <a href="https://aws.amazon.com/kinesis/data-analytics/" target="_blank" rel="noopener noreferrer">Amazon Kinesis Data Analytics (KDA)<i class="fas fa-external-link-square-alt ms-1"></i></a>, <a href="https://aws.amazon.com/emr/" target="_blank" rel="noopener noreferrer">Amazon EMR<i class="fas fa-external-link-square-alt ms-1"></i></a> and <a href="https://aws.amazon.com/eks/" target="_blank" rel="noopener noreferrer">Amazon EKS<i class="fas fa-external-link-square-alt ms-1"></i></a>. Among those, KDA is the easiest option as it provides the underlying infrastructure for your Apache Flink applications.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2023-08-17-getting-started-with-pyflink-on-aws-part-1/featured.png" length="55960" type="image/png"/></item><item><title>Kafka, Flink and DynamoDB for Real Time Fraud Detection - Part 1 Local Development</title><link>https://jaehyeon.me/blog/2023-08-10-fraud-detection-part-1/</link><pubDate>Thu, 10 Aug 2023 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2023-08-10-fraud-detection-part-1/</guid><description><![CDATA[<p><a href="https://flink.apache.org/" target="_blank" rel="noopener noreferrer">Apache Flink<i class="fas fa-external-link-square-alt ms-1"></i></a> is an open-source, unified stream-processing and batch-processing framework. Its core is a distributed streaming data-flow engine that you can use to run real-time stream processing on high-throughput data sources. Currently, it is widely used to build applications for fraud/anomaly detection, rule-based alerting, business process monitoring, and continuous ETL to name a few. On AWS, we can deploy a Flink application via <a href="https://aws.amazon.com/kinesis/data-analytics/" target="_blank" rel="noopener noreferrer">Amazon Kinesis Data Analytics (KDA)<i class="fas fa-external-link-square-alt ms-1"></i></a>, <a href="https://aws.amazon.com/emr/" target="_blank" rel="noopener noreferrer">Amazon EMR<i class="fas fa-external-link-square-alt ms-1"></i></a> and <a href="https://aws.amazon.com/eks/" target="_blank" rel="noopener noreferrer">Amazon EKS<i class="fas fa-external-link-square-alt ms-1"></i></a>. Among those, KDA is the easiest option as it provides the underlying infrastructure for your Apache Flink applications.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2023-08-10-fraud-detection-part-1/featured.png" length="72929" type="image/png"/></item><item><title>Kafka Development with Docker - Part 11 Kafka Authorization</title><link>https://jaehyeon.me/blog/2023-07-20-kafka-development-with-docker-part-11/</link><pubDate>Thu, 20 Jul 2023 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2023-07-20-kafka-development-with-docker-part-11/</guid><description>&lt;p>In the previous posts, we discussed how to implement client authentication by TLS (SSL or TLS/SSL) and SASL authentication. One of the key benefits of client authentication is achieving user access control. Kafka ships with a pluggable, out-of-the box authorization framework, which is configured with the &lt;em>authorizer.class.name&lt;/em> property in the server configuration and stores Access Control Lists (ACLs) in the cluster metadata (either Zookeeper or the KRaft metadata log). In this post, we will discuss how to configure Kafka authorization with Java and Python client examples while SASL is kept for client authentication.&lt;/p></description><enclosure url="https://jaehyeon.me/blog/2023-07-20-kafka-development-with-docker-part-11/featured.png" length="458848" type="image/png"/></item><item><title>Kafka Development with Docker - Part 10 SASL Authentication</title><link>https://jaehyeon.me/blog/2023-07-13-kafka-development-with-docker-part-10/</link><pubDate>Thu, 13 Jul 2023 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2023-07-13-kafka-development-with-docker-part-10/</guid><description><![CDATA[<p>In the previous post, we discussed TLS (SSL or TLS/SSL) authentication to improve security. It enforces two-way verification where a client certificate is verified by Kafka brokers. Client authentication can also be enabled by <a href="https://en.wikipedia.org/wiki/Simple_Authentication_and_Security_Layer" target="_blank" rel="noopener noreferrer">Simple Authentication and Security Layer (SASL)<i class="fas fa-external-link-square-alt ms-1"></i></a>, and we will discuss how to implement SASL authentication with Java and Python client examples in this post.</p>
<ul>
<li><a href="/blog/2023-05-04-kafka-development-with-docker-part-1">Part 1 Cluster Setup</a></li>
<li><a href="/blog/2023-05-18-kafka-development-with-docker-part-2">Part 2 Management App</a></li>
<li><a href="/blog/2023-05-25-kafka-development-with-docker-part-3">Part 3 Kafka Connect</a></li>
<li><a href="/blog/2023-06-01-kafka-development-with-docker-part-4">Part 4 Producer and Consumer</a></li>
<li><a href="/blog/2023-06-08-kafka-development-with-docker-part-5">Part 5 Glue Schema Registry</a></li>
<li><a href="/blog/2023-06-15-kafka-development-with-docker-part-6">Part 6 Kafka Connect with Glue Schema Registry</a></li>
<li><a href="/blog/2023-06-22-kafka-development-with-docker-part-7">Part 7 Producer and Consumer with Glue Schema Registry</a></li>
<li><a href="/blog/2023-06-29-kafka-development-with-docker-part-8">Part 8 SSL Encryption</a></li>
<li><a href="/blog/2023-07-06-kafka-development-with-docker-part-9">Part 9 SSL Authentication</a></li>
<li><a href="/blog/2023-07-13-kafka-development-with-docker-part-10/#">Part 10 SASL Authentication</a> (this post)</li>
<li><a href="/blog/2023-07-20-kafka-development-with-docker-part-11">Part 11 Kafka Authorization</a></li>
</ul>

<h2 id="certificate-setup" data-numberify>Certificate Setup<a class="anchor ms-1" href="#certificate-setup"></a></h2>
<p>As we will leave Kafka communication to remain encrypted, we need to keep the components for SSL encryption. The details can be found in <a href="/blog/2023-06-29-kafka-development-with-docker-part-8">Part 8</a>, and those components can be generated by <a href="https://github.com/jaehyeon-kim/kafka-pocs/blob/main/kafka-dev-with-docker/part-10/generate.sh" target="_blank" rel="noopener noreferrer"><em>generate.sh</em><i class="fas fa-external-link-square-alt ms-1"></i></a>. Once we execute the script, the following files are created.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2023-07-13-kafka-development-with-docker-part-10/featured.png" length="471947" type="image/png"/></item><item><title>Kafka Development with Docker - Part 9 SSL Authentication</title><link>https://jaehyeon.me/blog/2023-07-06-kafka-development-with-docker-part-9/</link><pubDate>Thu, 06 Jul 2023 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2023-07-06-kafka-development-with-docker-part-9/</guid><description><![CDATA[<p>In the previous post, we discussed how to configure TLS (SSL or TLS/SSL) encryption with Java and Python client examples. SSL encryption is a one-way verification process where a server certificate is verified by a client via <a href="https://en.wikipedia.org/wiki/Transport_Layer_Security#TLS_handshake" target="_blank" rel="noopener noreferrer">SSL Handshake<i class="fas fa-external-link-square-alt ms-1"></i></a>. To improve security, we can add client authentication either by enforcing two-way verification where a client certificate is verified by Kafka brokers (SSL authentication). Or we can choose a separate authentication mechanism, which is typically <a href="https://en.wikipedia.org/wiki/Simple_Authentication_and_Security_Layer" target="_blank" rel="noopener noreferrer">Simple Authentication and Security Layer (SASL)<i class="fas fa-external-link-square-alt ms-1"></i></a>. In this post, we will discuss how to implement SSL authentication with Java and Python client examples while SASL authentication is covered in the next post.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2023-07-06-kafka-development-with-docker-part-9/featured.png" length="471471" type="image/png"/></item><item><title>Kafka Development with Docker - Part 8 SSL Encryption</title><link>https://jaehyeon.me/blog/2023-06-29-kafka-development-with-docker-part-8/</link><pubDate>Thu, 29 Jun 2023 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2023-06-29-kafka-development-with-docker-part-8/</guid><description><![CDATA[<p>By default, Apache Kafka communicates in <em>PLAINTEXT</em>, which means that all data is sent without being encrypted. To secure communication, we can configure Kafka clients and other components to use <a href="https://en.wikipedia.org/wiki/Transport_Layer_Security" target="_blank" rel="noopener noreferrer">Transport Layer Security (TLS)<i class="fas fa-external-link-square-alt ms-1"></i></a> encryption. Note that TLS is also referred to <a href="https://en.wikipedia.org/wiki/Transport_Layer_Security#SSL_1.0,_2.0,_and_3.0" target="_blank" rel="noopener noreferrer">Secure Sockets Layer (SSL)<i class="fas fa-external-link-square-alt ms-1"></i></a> or TLS/SSL. SSL is the predecessor of TLS, and has been deprecated since June 2015. However, it is used in configuration and code instead of TLS for historical reasons. In this post, SSL, TLS and TLS/SSL will be used interchangeably. SSL encryption is a one-way verification process where a server certificate is verified by a client via <a href="https://en.wikipedia.org/wiki/Transport_Layer_Security#TLS_handshake" target="_blank" rel="noopener noreferrer">SSL Handshake<i class="fas fa-external-link-square-alt ms-1"></i></a>. Moreover, we can improve security by adding client authentication. For example, we can enforce two-way verification so that a client certificate is verified by Kafka brokers as well (<em>SSL Authentication</em>). Alternatively we can choose a separate authentication mechanism and typically <a href="https://en.wikipedia.org/wiki/Simple_Authentication_and_Security_Layer" target="_blank" rel="noopener noreferrer">Simple Authentication and Security Layer (SASL)<i class="fas fa-external-link-square-alt ms-1"></i></a> is used (<em>SASL Authentication</em>). In this post, we will discuss how to configure SSL encryption with Java and Python client examples while SSL and SASL client authentication will be covered in later posts.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2023-06-29-kafka-development-with-docker-part-8/featured.png" length="469311" type="image/png"/></item><item><title>Kafka Development with Docker - Part 7 Producer and Consumer with Glue Schema Registry</title><link>https://jaehyeon.me/blog/2023-06-22-kafka-development-with-docker-part-7/</link><pubDate>Thu, 22 Jun 2023 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2023-06-22-kafka-development-with-docker-part-7/</guid><description><![CDATA[<p>In <a href="/blog/2023-06-01-kafka-development-with-docker-part-4">Part 4</a>, we developed Kafka producer and consumer applications using the <a href="https://kafka-python.readthedocs.io/en/master/index.html" target="_blank" rel="noopener noreferrer">kafka-python<i class="fas fa-external-link-square-alt ms-1"></i></a> package. The Kafka messages are serialized as Json, but are not associated with a schema as there was not an integrated schema registry. Later we discussed how producers and consumers to Kafka topics can use schemas to ensure data consistency and compatibility as schemas evolve in <a href="/blog/2023-06-08-kafka-development-with-docker-part-5">Part 5</a>. In this post, I&rsquo;ll demonstrate how to enhance the existing applications by integrating <a href="https://docs.aws.amazon.com/glue/latest/dg/schema-registry.html" target="_blank" rel="noopener noreferrer"><em>AWS Glue Schema Registry</em><i class="fas fa-external-link-square-alt ms-1"></i></a>.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2023-06-22-kafka-development-with-docker-part-7/featured.png" length="57175" type="image/png"/></item><item><title>Kafka Development with Docker - Part 6 Kafka Connect with Glue Schema Registry</title><link>https://jaehyeon.me/blog/2023-06-15-kafka-development-with-docker-part-6/</link><pubDate>Thu, 15 Jun 2023 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2023-06-15-kafka-development-with-docker-part-6/</guid><description><![CDATA[<p>In <a href="/blog/2023-05-25-kafka-development-with-docker-part-3">Part 3</a>, we developed a data ingestion pipeline with fake online order data using Kafka Connect source and sink connectors. Schemas are not enabled on both of them as there was not an integrated schema registry. Later we discussed how producers and consumers to Kafka topics can use schemas to ensure data consistency and compatibility as schemas evolve in <a href="/blog/2023-06-08-kafka-development-with-docker-part-5">Part 5</a>. In this post, I&rsquo;ll demonstrate how to enhance the existing data ingestion pipeline by integrating <a href="https://docs.aws.amazon.com/glue/latest/dg/schema-registry.html" target="_blank" rel="noopener noreferrer"><em>AWS Glue Schema Registry</em><i class="fas fa-external-link-square-alt ms-1"></i></a>.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2023-06-15-kafka-development-with-docker-part-6/featured.png" length="60354" type="image/png"/></item><item><title>Kafka Connect for AWS Services Integration - Part 2 Develop Camel DynamoDB Sink Connector</title><link>https://jaehyeon.me/blog/2023-06-04-kafka-connect-for-aws-part-2/</link><pubDate>Sun, 04 Jun 2023 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2023-06-04-kafka-connect-for-aws-part-2/</guid><description><![CDATA[<p>In <a href="/blog/2023-05-03-kafka-connect-for-aws-part-1">Part 1</a>, we reviewed Kafka connectors focusing on AWS services integration. Among the available connectors, the suite of <a href="https://camel.apache.org/camel-kafka-connector/latest/index.html" target="_blank" rel="noopener noreferrer">Apache Camel Kafka connectors<i class="fas fa-external-link-square-alt ms-1"></i></a> and the <a href="https://github.com/awslabs/kinesis-kafka-connector" target="_blank" rel="noopener noreferrer">Kinesis Kafka connector<i class="fas fa-external-link-square-alt ms-1"></i></a> from the AWS Labs can be effective for building data ingestion pipelines on AWS. In this post, I will illustrate how to develop the Camel DynamoDB sink connector using Docker. Fake order data will be generated using the <a href="https://github.com/awslabs/amazon-msk-data-generator" target="_blank" rel="noopener noreferrer">MSK Data Generator<i class="fas fa-external-link-square-alt ms-1"></i></a> source connector, and the sink connector will be configured to consume the topic messages to ingest them into a DynamoDB table.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2023-06-04-kafka-connect-for-aws-part-2/featured.png" length="87044" type="image/png"/></item><item><title>Kafka Development with Docker - Part 4 Producer and Consumer</title><link>https://jaehyeon.me/blog/2023-06-01-kafka-development-with-docker-part-4/</link><pubDate>Thu, 01 Jun 2023 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2023-06-01-kafka-development-with-docker-part-4/</guid><description><![CDATA[<p>In the previous post, we discussed <a href="https://kafka.apache.org/documentation/#connect" target="_blank" rel="noopener noreferrer">Kafka Connect<i class="fas fa-external-link-square-alt ms-1"></i></a> to stream data to/from a Kafka cluster. Kafka also includes the <a href="https://kafka.apache.org/documentation/#api" target="_blank" rel="noopener noreferrer">Producer/Consumer APIs<i class="fas fa-external-link-square-alt ms-1"></i></a> that allow client applications to send/read streams of data to/from topics in a Kafka cluster. While the main Kafka project maintains only the Java clients, there are several <a href="https://cwiki.apache.org/confluence/display/KAFKA/Clients#Clients-Python" target="_blank" rel="noopener noreferrer">open source projects<i class="fas fa-external-link-square-alt ms-1"></i></a> that provide the Kafka client APIs in Python. In this post, I&rsquo;ll demonstrate how to develop producer/consumer applications using the <a href="https://kafka-python.readthedocs.io/en/master/index.html" target="_blank" rel="noopener noreferrer">kafka-python<i class="fas fa-external-link-square-alt ms-1"></i></a> package.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2023-06-01-kafka-development-with-docker-part-4/featured.png" length="75255" type="image/png"/></item><item><title>Kafka Development with Docker - Part 3 Kafka Connect</title><link>https://jaehyeon.me/blog/2023-05-25-kafka-development-with-docker-part-3/</link><pubDate>Thu, 25 May 2023 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2023-05-25-kafka-development-with-docker-part-3/</guid><description><![CDATA[<p>According to the documentation of <a href="https://kafka.apache.org/documentation/#connect" target="_blank" rel="noopener noreferrer">Apache Kafka<i class="fas fa-external-link-square-alt ms-1"></i></a>, <em>Kafka Connect is a tool for scalably and reliably streaming data between Apache Kafka and other systems. It makes it simple to quickly define connectors that move large collections of data into and out of Kafka</em>. Kafka Connect supports two types of connectors - source and sink. Source connectors are used to ingest messages from external systems into Kafka topics while messages are ingested into external systems form Kafka topics with sink connectors. In this post, I will illustrate how to set up a data ingestion pipeline using Kafka connectors. Fake customer and order data will be ingested into the corresponding topics using the <a href="https://github.com/awslabs/amazon-msk-data-generator" target="_blank" rel="noopener noreferrer">MSK Data Generator<i class="fas fa-external-link-square-alt ms-1"></i></a> source connector. The topic messages will then be saved into a S3 bucket using the <a href="https://www.confluent.io/hub/confluentinc/kafka-connect-s3" target="_blank" rel="noopener noreferrer">Confluent S3<i class="fas fa-external-link-square-alt ms-1"></i></a> sink connector.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2023-05-25-kafka-development-with-docker-part-3/featured.png" length="69998" type="image/png"/></item><item><title>Kafka Development with Docker - Part 2 Management App</title><link>https://jaehyeon.me/blog/2023-05-18-kafka-development-with-docker-part-2/</link><pubDate>Thu, 18 May 2023 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2023-05-18-kafka-development-with-docker-part-2/</guid><description><![CDATA[<p>In the previous post, I illustrated how to create a topic and to produce/consume messages using the command utilities provided by Apache Kafka. It is not convenient, however, for example, when you consume serialised messages where their schemas are stored in a schema registry. Also, the utilities don&rsquo;t support to browse or manage related resources such as connectors and schemas. Therefore, a Kafka management app can be a good companion for development, which helps monitor and manage resources on an easy-to-use user interface. An app can be more useful if it supports features that are desirable for Kafka development on AWS. Those features cover <a href="https://docs.aws.amazon.com/msk/latest/developerguide/iam-access-control.html" target="_blank" rel="noopener noreferrer">IAM access control<i class="fas fa-external-link-square-alt ms-1"></i></a> of <a href="https://aws.amazon.com/msk/" target="_blank" rel="noopener noreferrer">Amazon MSK<i class="fas fa-external-link-square-alt ms-1"></i></a> and integration with <a href="https://aws.amazon.com/msk/features/msk-connect/" target="_blank" rel="noopener noreferrer">Amazon MSK Connect<i class="fas fa-external-link-square-alt ms-1"></i></a> and <a href="https://docs.aws.amazon.com/glue/latest/dg/schema-registry.html" target="_blank" rel="noopener noreferrer">AWS Glue Schema Registry<i class="fas fa-external-link-square-alt ms-1"></i></a>. In this post, I&rsquo;ll introduce several management apps that meet those requirements.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2023-05-18-kafka-development-with-docker-part-2/featured.png" length="59675" type="image/png"/></item><item><title>Kafka Development with Docker - Part 1 Cluster Setup</title><link>https://jaehyeon.me/blog/2023-05-04-kafka-development-with-docker-part-1/</link><pubDate>Thu, 04 May 2023 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2023-05-04-kafka-development-with-docker-part-1/</guid><description><![CDATA[<p>I&rsquo;m teaching myself <a href="https://docs.aws.amazon.com/whitepapers/latest/build-modern-data-streaming-analytics-architectures/build-modern-data-streaming-analytics-architectures.html" target="_blank" rel="noopener noreferrer">modern data streaming architectures<i class="fas fa-external-link-square-alt ms-1"></i></a> on AWS, and <a href="https://kafka.apache.org/" target="_blank" rel="noopener noreferrer">Apache Kafka<i class="fas fa-external-link-square-alt ms-1"></i></a> is one of the key technologies, which can be used for messaging, activity tracking, stream processing and so on. While applications tend to be deployed to cloud, it can be much easier if we develop and test those with <a href="https://www.docker.com/" target="_blank" rel="noopener noreferrer">Docker<i class="fas fa-external-link-square-alt ms-1"></i></a> and <a href="https://docs.docker.com/compose/" target="_blank" rel="noopener noreferrer">Docker Compose<i class="fas fa-external-link-square-alt ms-1"></i></a> locally. As the series title indicates, I plan to publish articles that demonstrate Kafka and related tools in <em>Dockerized</em> environments. Although I covered some of them in previous posts, they are implemented differently in terms of the Kafka Docker image, the number of brokers, Docker volume mapping etc. It can be confusing, and one of the purposes of this series is to illustrate reference implementations that can be applied to future development projects. Also, I can extend my knowledge while preparing for this series. In fact Kafka security is one of the areas that I expect to learn further. Below shows a list of posts that I plan for now.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2023-05-04-kafka-development-with-docker-part-1/featured.png" length="98355" type="image/png"/></item><item><title>Integrate Glue Schema Registry with Your Python Kafka App</title><link>https://jaehyeon.me/blog/2023-04-12-integrate-glue-schema-registry/</link><pubDate>Wed, 12 Apr 2023 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2023-04-12-integrate-glue-schema-registry/</guid><description><![CDATA[<p>As Kafka producer and consumer apps are decoupled, they operate on Kafka topics rather than communicating with each other directly. As described in the <a href="https://docs.confluent.io/platform/current/schema-registry/index.html#sr-overview" target="_blank" rel="noopener noreferrer">Confluent document<i class="fas fa-external-link-square-alt ms-1"></i></a>, <em>Schema Registry</em> provides a centralized repository for managing and validating schemas for topic message data, and for serialization and deserialization of the data over the network. Producers and consumers to Kafka topics can use schemas to ensure data consistency and compatibility as schemas evolve. In AWS, the <a href="https://docs.aws.amazon.com/glue/latest/dg/schema-registry.html" target="_blank" rel="noopener noreferrer">Glue Schema Registry<i class="fas fa-external-link-square-alt ms-1"></i></a> supports features to manage and enforce schemas on data streaming applications using convenient integrations with Apache Kafka, <a href="https://aws.amazon.com/msk/" target="_blank" rel="noopener noreferrer">Amazon Managed Streaming for Apache Kafka<i class="fas fa-external-link-square-alt ms-1"></i></a>, <a href="https://aws.amazon.com/kinesis/data-streams/" target="_blank" rel="noopener noreferrer">Amazon Kinesis Data Streams<i class="fas fa-external-link-square-alt ms-1"></i></a>, <a href="https://aws.amazon.com/kinesis/data-analytics/" target="_blank" rel="noopener noreferrer">Amazon Kinesis Data Analytics for Apache Flink<i class="fas fa-external-link-square-alt ms-1"></i></a>, and <a href="https://aws.amazon.com/lambda/" target="_blank" rel="noopener noreferrer">AWS Lambda<i class="fas fa-external-link-square-alt ms-1"></i></a>. In this post, we will discuss how to integrate Python Kafka producer and consumer apps In AWS Lambda with the Glue Schema Registry.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2023-04-12-integrate-glue-schema-registry/featured.png" length="46040" type="image/png"/></item><item><title>How to configure Kafka consumers to seek offsets by timestamp</title><link>https://jaehyeon.me/blog/2023-01-10-kafka-consumer-seek-offsets/</link><pubDate>Tue, 10 Jan 2023 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2023-01-10-kafka-consumer-seek-offsets/</guid><description><![CDATA[<p>Normally we consume Kafka messages from the beginning/end of a topic or last committed offsets. For backfilling or troubleshooting, however, we need to consume messages from a certain timestamp occasionally. If we know which topic partition to choose e.g. by <a href="https://kafka-python.readthedocs.io/en/master/apidoc/KafkaConsumer.html#kafka.KafkaConsumer.assign" target="_blank" rel="noopener noreferrer">assigning a topic partition<i class="fas fa-external-link-square-alt ms-1"></i></a>, we can easily override the fetch offset to a specific timestamp. When we deploy multiple consumer instances together, however, we make them <a href="https://kafka-python.readthedocs.io/en/master/apidoc/KafkaConsumer.html#kafka.KafkaConsumer.subscribe" target="_blank" rel="noopener noreferrer">subscribe to a topic<i class="fas fa-external-link-square-alt ms-1"></i></a> and topic partitions are dynamically assigned, which means we cannot determine which fetch offset to use for an instance. In this post, we develop Kafka producer and consumer applications using the <a href="https://kafka-python.readthedocs.io/en/master/" target="_blank" rel="noopener noreferrer">kafka-python<i class="fas fa-external-link-square-alt ms-1"></i></a> package and discuss how to configure the consumer instances to seek offsets to a specific timestamp where topic partitions are dynamically assigned by subscription.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2023-01-10-kafka-consumer-seek-offsets/featured.png" length="47217" type="image/png"/></item><item><title>Revisit AWS Lambda Invoke Function Operator of Apache Airflow</title><link>https://jaehyeon.me/blog/2022-08-06-revisit-lambda-operator/</link><pubDate>Sat, 06 Aug 2022 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2022-08-06-revisit-lambda-operator/</guid><description><![CDATA[<p><a href="https://airflow.apache.org/" target="_blank" rel="noopener noreferrer">Apache Airflow<i class="fas fa-external-link-square-alt ms-1"></i></a> is a popular workflow management platform. A wide range of AWS services are integrated with the platform by <a href="https://airflow.apache.org/docs/apache-airflow-providers-amazon/stable/operators/index.html" target="_blank" rel="noopener noreferrer">Amazon AWS Operators<i class="fas fa-external-link-square-alt ms-1"></i></a>. AWS Lambda is one of the integrated services, and it can be used to develop workflows efficiently. The current <a href="https://airflow.apache.org/docs/apache-airflow-providers-amazon/stable/operators/lambda.html" target="_blank" rel="noopener noreferrer">Lambda Operator<i class="fas fa-external-link-square-alt ms-1"></i></a>, however, just invokes a Lambda function, and it can fail to report the invocation result of a function correctly and to record the exact error message from failure. In this post, we’ll discuss a custom Lambda operator that handles those limitations.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2022-08-06-revisit-lambda-operator/featured.png" length="24814" type="image/png"/></item><item><title>Data Warehousing ETL Demo with Apache Iceberg on EMR Local Environment</title><link>https://jaehyeon.me/blog/2022-06-26-iceberg-etl-demo/</link><pubDate>Sun, 26 Jun 2022 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2022-06-26-iceberg-etl-demo/</guid><description><![CDATA[<p>Unlike traditional Data Lake, new table formats (<a href="https://iceberg.apache.org/" target="_blank" rel="noopener noreferrer">Iceberg<i class="fas fa-external-link-square-alt ms-1"></i></a>, <a href="https://hudi.apache.org/" target="_blank" rel="noopener noreferrer">Hudi<i class="fas fa-external-link-square-alt ms-1"></i></a> and <a href="https://delta.io/" target="_blank" rel="noopener noreferrer">Delta Lake<i class="fas fa-external-link-square-alt ms-1"></i></a>) support <a href="https://iceberg.apache.org/docs/latest/spark-writes/" target="_blank" rel="noopener noreferrer">features<i class="fas fa-external-link-square-alt ms-1"></i></a> that can be used to apply data warehousing patterns, which can bring a way to be rescued from <a href="https://www.gartner.com/en/newsroom/press-releases/2014-07-28-gartner-says-beware-of-the-data-lake-fallacy" target="_blank" rel="noopener noreferrer">Data Swamp<i class="fas fa-external-link-square-alt ms-1"></i></a>. In this post, we&rsquo;ll discuss how to implement ETL using retail analytics data. It has two dimension data (user and product) and a single fact data (order). The dimension data sets have different ETL strategies depending on whether to track historical changes. For the fact data, the primary keys of the dimension data are added to facilitate later queries. We&rsquo;ll use Iceberg for data storage/management and Spark for data processing. Instead of provisioning an EMR cluster, a local development environment will be used. Finally, the ETL results will be queried by Athena for verification.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2022-06-26-iceberg-etl-demo/featured.png" length="43604" type="image/png"/></item><item><title>Develop and Test Apache Spark Apps for EMR Locally Using Docker</title><link>https://jaehyeon.me/blog/2022-05-08-emr-local-dev/</link><pubDate>Sun, 08 May 2022 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2022-05-08-emr-local-dev/</guid><description><![CDATA[<p>[<strong>UPDATE 2023-12-07</strong>]</p>
<ul>
<li>I wrote a <a href="/blog/2023-12-07-flink-spark-local-dev">new post</a> that simplifies the Spark configuration dramatically. Besides, the log configuration is based on Log4J2, which applies to newer Spark versions. Moreover, the container is configured to run the Spark History Server, and it allows us to debug and diagnose completed and running Spark applications. I recommend referring to the new post.</li>
</ul>
<p><a href="https://aws.amazon.com/emr/" target="_blank" rel="noopener noreferrer">Amazon EMR<i class="fas fa-external-link-square-alt ms-1"></i></a> is a managed service that simplifies running Apache Spark on AWS. It has multiple deployment options that cover EC2, <a href="https://aws.amazon.com/emr/features/eks/" target="_blank" rel="noopener noreferrer">EKS<i class="fas fa-external-link-square-alt ms-1"></i></a>, <a href="https://aws.amazon.com/emr/features/outposts/" target="_blank" rel="noopener noreferrer">Outposts<i class="fas fa-external-link-square-alt ms-1"></i></a> and <a href="https://aws.amazon.com/emr/serverless/" target="_blank" rel="noopener noreferrer">Serverless<i class="fas fa-external-link-square-alt ms-1"></i></a>. For development and testing, <a href="https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-managed-notebooks.html" target="_blank" rel="noopener noreferrer">EMR Notebooks<i class="fas fa-external-link-square-alt ms-1"></i></a> or <a href="https://aws.amazon.com/emr/features/studio/" target="_blank" rel="noopener noreferrer">EMR Studio<i class="fas fa-external-link-square-alt ms-1"></i></a> can be an option. Both provide a Jupyter Notebook environment and the former is only available for EMR on EC2. There are cases, however, that development (and learning) is performed in a local environment more efficiently. The AWS Glue team understands this demand, and they illustrate how to make use of a custom Docker image for Glue in a <a href="https://aws.amazon.com/blogs/big-data/develop-and-test-aws-glue-version-3-0-jobs-locally-using-a-docker-container/" target="_blank" rel="noopener noreferrer">recent blog post<i class="fas fa-external-link-square-alt ms-1"></i></a>. However, we don’t hear similar news from the EMR team. In order to fill the gap, we’ll discuss how to create a Spark local development environment for EMR using Docker and/or VSCode. Typical Spark development examples will be demonstrated, which covers Spark Submit, Pytest, PySpark shell, Jupyter Notebook and Spark Structured Streaming. For the Spark Submit and Jupyter Notebook examples, <a href="https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-spark-glue.html" target="_blank" rel="noopener noreferrer">Glue Catalog integration<i class="fas fa-external-link-square-alt ms-1"></i></a> will be illustrated as well. And both the cases of utilising <a href="https://code.visualstudio.com/docs/remote/containers" target="_blank" rel="noopener noreferrer">Visual Studio Code Remote - Containers<i class="fas fa-external-link-square-alt ms-1"></i></a> extension and running as an isolated container will be covered in some key examples.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2022-05-08-emr-local-dev/featured.png" length="25693" type="image/png"/></item><item><title>Use External Schema Registry with MSK Connect – Part 2 MSK Deployment</title><link>https://jaehyeon.me/blog/2022-04-03-schema-registry-part2/</link><pubDate>Sun, 03 Apr 2022 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2022-04-03-schema-registry-part2/</guid><description><![CDATA[<p>In the <a href="/blog/2022-03-07-schema-registry-part1">previous post</a>, we discussed a Change Data Capture (CDC) solution with a schema registry. A local development environment is set up using Docker Compose. The Debezium and Confluent S3 connectors are deployed with the Confluent Avro converter and the Apicurio registry is used as the schema registry service. A quick example is shown to illustrate how schema evolution can be managed by the schema registry. In this post, we&rsquo;ll build the solution on AWS using <a href="https://aws.amazon.com/msk/" target="_blank" rel="noopener noreferrer">MSK<i class="fas fa-external-link-square-alt ms-1"></i></a>, <a href="https://aws.amazon.com/msk/features/msk-connect/" target="_blank" rel="noopener noreferrer">MSK Connect<i class="fas fa-external-link-square-alt ms-1"></i></a>, <a href="https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.AuroraPostgreSQL.html" target="_blank" rel="noopener noreferrer">Aurora PostgreSQL<i class="fas fa-external-link-square-alt ms-1"></i></a> and <a href="https://aws.amazon.com/ecs/" target="_blank" rel="noopener noreferrer">ECS<i class="fas fa-external-link-square-alt ms-1"></i></a>.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2022-04-03-schema-registry-part2/featured.png" length="59689" type="image/png"/></item><item><title>Use External Schema Registry with MSK Connect – Part 1 Local Development</title><link>https://jaehyeon.me/blog/2022-03-07-schema-registry-part1/</link><pubDate>Mon, 07 Mar 2022 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2022-03-07-schema-registry-part1/</guid><description><![CDATA[<p>When we discussed a Change Data Capture (CDC) solution in <a href="/blog/2021-12-12-datalake-demo-part2">one of the earlier posts</a>, we used the JSON converter that comes with Kafka Connect. We optionally enabled the key and value schemas and the topic messages include those schemas together with payload. It seems to be convenient at first as the messages are saved into S3 on their own. However, it became cumbersome when we tried to use the <a href="https://hudi.apache.org/docs/hoodie_streaming_ingestion" target="_blank" rel="noopener noreferrer">DeltaStreamer<i class="fas fa-external-link-square-alt ms-1"></i></a> utility. Specifically it requires the scheme of the files, but unfortunately we cannot use the schema that is generated by the default JSON converter - it returns the <a href="https://www.confluent.io/blog/kafka-connect-deep-dive-converters-serialization-explained/#json-schemas" target="_blank" rel="noopener noreferrer">struct type<i class="fas fa-external-link-square-alt ms-1"></i></a>, which is not supported by the Hudi utility. In order to handle this issue, we created a schema with the <a href="https://avro.apache.org/docs/current/spec.html#schema_record" target="_blank" rel="noopener noreferrer">record type<i class="fas fa-external-link-square-alt ms-1"></i></a> using the Confluent Avro converter and used it after saving on S3. However, as we aimed to manage a long-running process, generating a schema manually was not an optimal solution because, for example, we&rsquo;re not able to handle schema evolution effectively. In this post, we&rsquo;ll discuss an improved architecture that makes use of a schema registry that resides outside the Kafka cluster and allows the producers and consumers to reference the schemas externally.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2022-03-07-schema-registry-part1/featured.png" length="59689" type="image/png"/></item><item><title>Data Lake Demo using Change Data Capture (CDC) on AWS – Part 1 Local Development</title><link>https://jaehyeon.me/blog/2021-12-05-datalake-demo-part1/</link><pubDate>Sun, 05 Dec 2021 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2021-12-05-datalake-demo-part1/</guid><description><![CDATA[<p><a href="https://www.redhat.com/en/topics/integration/what-is-change-data-capture#what-is-cdc" target="_blank" rel="noopener noreferrer">Change data capture (CDC)<i class="fas fa-external-link-square-alt ms-1"></i></a> is a proven data integration pattern that has a wide range of applications. Among those, data replication to data lakes is a good use case in data engineering. Coupled with <a href="https://lakefs.io/hudi-iceberg-and-delta-lake-data-lake-table-formats-compared/" target="_blank" rel="noopener noreferrer">best-in-breed data lake formats<i class="fas fa-external-link-square-alt ms-1"></i></a> such as <a href="https://hudi.apache.org/" target="_blank" rel="noopener noreferrer">Apache Hudi<i class="fas fa-external-link-square-alt ms-1"></i></a>, we can build an efficient data replication solution. This is the first post of the data lake demo series. Over time, we&rsquo;ll build a data lake that uses CDC. As a starting point, we&rsquo;ll discuss the source database and CDC streaming infrastructure in the local environment.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2021-12-05-datalake-demo-part1/featured.png" length="164526" type="image/png"/></item><item><title>Thoughts on Apache Airflow AWS Lambda Operator</title><link>https://jaehyeon.me/blog/2020-04-13-airflow-lambda-operator/</link><pubDate>Mon, 13 Apr 2020 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2020-04-13-airflow-lambda-operator/</guid><description><![CDATA[<p><a href="https://airflow.apache.org/" target="_blank" rel="noopener noreferrer">Apache Airflow<i class="fas fa-external-link-square-alt ms-1"></i></a> is a popular open-source workflow management platform. Typically tasks run remotely by <a href="http://www.celeryproject.org/" target="_blank" rel="noopener noreferrer">Celery<i class="fas fa-external-link-square-alt ms-1"></i></a> workers for scalability. In AWS, however, scalability can also be achieved using serverless computing services in a simpler way. For example, the <a href="https://airflow.apache.org/docs/apache-airflow-providers-amazon/stable/operators/ecs.html" target="_blank" rel="noopener noreferrer">ECS Operator<i class="fas fa-external-link-square-alt ms-1"></i></a> allows to run <em>dockerized</em> tasks and, with the <em>Fargate</em> launch type, they can run in a serverless environment.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2020-04-13-airflow-lambda-operator/featured.png" length="44994" type="image/png"/></item><item><title>Dynamic Routing and Centralized Auth with Traefik, Python and R Example</title><link>https://jaehyeon.me/blog/2019-11-29-traefik-example/</link><pubDate>Fri, 29 Nov 2019 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2019-11-29-traefik-example/</guid><description><![CDATA[<p><a href="https://kubernetes.io/docs/concepts/services-networking/ingress/" target="_blank" rel="noopener noreferrer">Ingress<i class="fas fa-external-link-square-alt ms-1"></i></a> in <a href="https://kubernetes.io/" target="_blank" rel="noopener noreferrer">Kubernetes<i class="fas fa-external-link-square-alt ms-1"></i></a> exposes HTTP and HTTPS routes from outside the cluster to services within the cluster. By setting rules, it routes requests to appropriate services (precisely requests are sent to individual <a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-overview/" target="_blank" rel="noopener noreferrer">Pods<i class="fas fa-external-link-square-alt ms-1"></i></a> by <a href="https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/" target="_blank" rel="noopener noreferrer">Ingress Controller<i class="fas fa-external-link-square-alt ms-1"></i></a>). Rules can be set up dynamically and I find it&rsquo;s more efficient compared to traditional <a href="https://en.wikipedia.org/wiki/Reverse_proxy" target="_blank" rel="noopener noreferrer">reverse proxy<i class="fas fa-external-link-square-alt ms-1"></i></a>.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2019-11-29-traefik-example/featured.png" length="139790" type="image/png"/></item><item><title>Distributed Task Queue with Python and R Example</title><link>https://jaehyeon.me/blog/2019-11-15-task-queue/</link><pubDate>Fri, 15 Nov 2019 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2019-11-15-task-queue/</guid><description><![CDATA[<p>While I&rsquo;m looking into <a href="https://airflow.apache.org/" target="_blank" rel="noopener noreferrer">Apache Airflow<i class="fas fa-external-link-square-alt ms-1"></i></a>, a workflow management tool, I thought it would be beneficial to get some understanding of how <a href="http://www.celeryproject.org/" target="_blank" rel="noopener noreferrer">Celery<i class="fas fa-external-link-square-alt ms-1"></i></a> works. To do so, I built a simple web service that sends tasks to Celery workers and collects the results from them. <a href="https://fastapi.tiangolo.com/" target="_blank" rel="noopener noreferrer">FastAPI<i class="fas fa-external-link-square-alt ms-1"></i></a> is used for developing the web service and <a href="https://redis.io/" target="_blank" rel="noopener noreferrer">Redis<i class="fas fa-external-link-square-alt ms-1"></i></a> is used for the message broker and result backend. During the development, I thought it would be possible to implement similar functionality in R with <a href="https://www.rforge.net/Rserve/" target="_blank" rel="noopener noreferrer">Rserve<i class="fas fa-external-link-square-alt ms-1"></i></a>. Therefore a Rserve worker is added as an example as well. Coupling a web service with distributed task queue is beneficial on its own as it helps the service be more responsive by offloading heavyweight and long running processes to task workers.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2019-11-15-task-queue/featured.png" length="51615" type="image/png"/></item><item><title>Linux Dev Environment on Windows</title><link>https://jaehyeon.me/blog/2019-11-01-linux-on-windows/</link><pubDate>Fri, 01 Nov 2019 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2019-11-01-linux-on-windows/</guid><description><![CDATA[<p>I use Linux containers a lot for development. Having Windows computers at home and work, I used to use Linux VMs on VirtualBox or VMWare Workstation. It&rsquo;s not a bad option but it requires a lot of resources. Recently, after my home computer was updated, I was not able to start my hypervisor anymore. Also I didn&rsquo;t like huge resource consumption of it so that I began to look for a different development environment. A while ago, I played with <a href="https://docs.microsoft.com/en-us/windows/wsl/about" target="_blank" rel="noopener noreferrer">Windows Subsystem for Linux (WSL)<i class="fas fa-external-link-square-alt ms-1"></i></a> and it was alright. Also <a href="https://code.visualstudio.com/" target="_blank" rel="noopener noreferrer">Visual Studio Code (VSCode)<i class="fas fa-external-link-square-alt ms-1"></i></a>, <em>my favourite editor</em>, now supports <a href="https://code.visualstudio.com/docs/remote/remote-overview" target="_blank" rel="noopener noreferrer">remote development<i class="fas fa-external-link-square-alt ms-1"></i></a>. Initially I thought I would be able to create a new development environment with WSL and <a href="https://docs.docker.com/docker-for-windows/install/" target="_blank" rel="noopener noreferrer">Docker for Windows<i class="fas fa-external-link-square-alt ms-1"></i></a>. However it was until I tried a bigger app with <a href="https://docs.docker.com/compose/" target="_blank" rel="noopener noreferrer">Docker Compose<i class="fas fa-external-link-square-alt ms-1"></i></a> that Docker for Windows has a number of issues especially when containers are started by Docker Compose in WSL. I didn&rsquo;t like to spend too much time on fixing those issues as I concerned those might not be the only ones. Then I decided to install a Linux VM on <a href="https://docs.microsoft.com/en-us/virtualization/hyper-v-on-windows/about/" target="_blank" rel="noopener noreferrer">Hyper-V<i class="fas fa-external-link-square-alt ms-1"></i></a>. Luckly VSCode also supports a remote VM via SSH.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2019-11-01-linux-on-windows/featured.png" length="187978" type="image/png"/></item><item><title>AWS Local Development with LocalStack</title><link>https://jaehyeon.me/blog/2019-07-20-aws-localstack/</link><pubDate>Sat, 20 Jul 2019 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2019-07-20-aws-localstack/</guid><description><![CDATA[<p><a href="https://github.com/localstack/localstack" target="_blank" rel="noopener noreferrer">LocalStack<i class="fas fa-external-link-square-alt ms-1"></i></a> provides an easy-to-use test/mocking framework for developing AWS applications. In this post, I&rsquo;ll demonstrate how to utilize LocalStack for development using a web service.</p>
<p>Specifically a simple web service built with <a href="https://flask-restplus.readthedocs.io/en/stable/" target="_blank" rel="noopener noreferrer">Flask-RestPlus<i class="fas fa-external-link-square-alt ms-1"></i></a> is used. It supports simple CRUD operations against a database table. It is set that SQS and Lambda are used for creating and updating a record. When a <em>POST</em> or <em>PUT</em> request is made, the service sends a message to a SQS queue and directly returns <em>204</em> reponse. Once a message is received, a Lambda function is invoked and a relevant database operation is performed.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2019-07-20-aws-localstack/featured.png" length="164886" type="image/png"/></item></channel></rss>