<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>AWS Lambda on Jaehyeon Kim</title><link>https://jaehyeon.me/tags/aws-lambda/</link><description>Recent content in AWS Lambda on Jaehyeon Kim</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>Copyright © 2023-2024 Jaehyeon Kim. All Rights Reserved.</copyright><lastBuildDate>Thu, 14 Dec 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://jaehyeon.me/tags/aws-lambda/index.xml" rel="self" type="application/rss+xml"/><item><title>Real Time Streaming with Kafka and Flink - Lab 6 Consume data from Kafka using Lambda</title><link>https://jaehyeon.me/blog/2023-12-14-real-time-streaming-with-kafka-and-flink-7/</link><pubDate>Thu, 14 Dec 2023 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2023-12-14-real-time-streaming-with-kafka-and-flink-7/</guid><description><![CDATA[<p>Amazon MSK can be configured as an <a href="https://docs.aws.amazon.com/lambda/latest/dg/with-msk.html" target="_blank" rel="noopener noreferrer">event source<i class="fas fa-external-link-square-alt ms-1"></i></a> of a Lambda function. Lambda internally polls for new messages from the event source and then synchronously invokes the target Lambda function. With this feature, we can develop a Kafka consumer application in serverless environment where developers can focus on application logic. In this lab, we will discuss how to create a Kafka consumer using a Lambda function.</p>
<ul>
<li><a href="/blog/2023-10-05-real-time-streaming-with-kafka-and-flink-1">Introduction</a></li>
<li><a href="/blog/2023-10-26-real-time-streaming-with-kafka-and-flink-2">Lab 1 Produce data to Kafka using Lambda</a></li>
<li><a href="/blog/2023-11-09-real-time-streaming-with-kafka-and-flink-3">Lab 2 Write data to Kafka from S3 using Flink</a></li>
<li><a href="/blog/2023-11-16-real-time-streaming-with-kafka-and-flink-4">Lab 3 Transform and write data to S3 from Kafka using Flink</a></li>
<li><a href="/blog/2023-11-23-real-time-streaming-with-kafka-and-flink-5">Lab 4 Clean, Aggregate, and Enrich Events with Flink</a></li>
<li><a href="/blog/2023-11-30-real-time-streaming-with-kafka-and-flink-6">Lab 5 Write data to DynamoDB using Kafka Connect</a></li>
<li><a href="/blog/2023-12-14-real-time-streaming-with-kafka-and-flink-7/#">Lab 6 Consume data from Kafka using Lambda</a> (this post)</li>
</ul>

<h2 id="architecture" data-numberify>Architecture<a class="anchor ms-1" href="#architecture"></a></h2>
<p>Fake taxi ride data is sent to a Kafka topic by the Kafka producer application that is discussed in <a href="/blog/2023-10-26-real-time-streaming-with-kafka-and-flink-2">Lab 1</a>. The messages of the <em>taxi-rides</em> topic are consumed by a Lambda function where the MSK cluster is configured as an event source of the function.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2023-12-14-real-time-streaming-with-kafka-and-flink-7/featured.png" length="138986" type="image/png"/></item><item><title>Real Time Streaming with Kafka and Flink - Lab 1 Produce data to Kafka using Lambda</title><link>https://jaehyeon.me/blog/2023-10-26-real-time-streaming-with-kafka-and-flink-2/</link><pubDate>Thu, 26 Oct 2023 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2023-10-26-real-time-streaming-with-kafka-and-flink-2/</guid><description><![CDATA[<p>In this lab, we will create a Kafka producer application using <a href="https://aws.amazon.com/lambda/" target="_blank" rel="noopener noreferrer">AWS Lambda<i class="fas fa-external-link-square-alt ms-1"></i></a>, which sends fake taxi ride data into a Kafka topic on <a href="https://aws.amazon.com/msk/" target="_blank" rel="noopener noreferrer">Amazon MSK<i class="fas fa-external-link-square-alt ms-1"></i></a>. A configurable number of the producer Lambda function will be invoked by an <a href="https://aws.amazon.com/eventbridge/" target="_blank" rel="noopener noreferrer">Amazon EventBridge<i class="fas fa-external-link-square-alt ms-1"></i></a> schedule rule. In this way we are able to generate test data concurrently based on the desired volume of messages.</p>
<ul>
<li><a href="/blog/2023-10-05-real-time-streaming-with-kafka-and-flink-1">Introduction</a></li>
<li><a href="/blog/2023-10-26-real-time-streaming-with-kafka-and-flink-2/#">Lab 1 Produce data to Kafka using Lambda</a> (this post)</li>
<li><a href="/blog/2023-11-09-real-time-streaming-with-kafka-and-flink-3">Lab 2 Write data to Kafka from S3 using Flink</a></li>
<li><a href="/blog/2023-11-16-real-time-streaming-with-kafka-and-flink-4">Lab 3 Transform and write data to S3 from Kafka using Flink</a></li>
<li><a href="/blog/2023-11-23-real-time-streaming-with-kafka-and-flink-5">Lab 4 Clean, Aggregate, and Enrich Events with Flink</a></li>
<li><a href="/blog/2023-11-30-real-time-streaming-with-kafka-and-flink-6">Lab 5 Write data to DynamoDB using Kafka Connect</a></li>
<li><a href="/blog/2023-12-14-real-time-streaming-with-kafka-and-flink-7">Lab 6 Consume data from Kafka using Lambda</a></li>
</ul>
<p>[<strong>Update 2023-11-06</strong>] Initially I planned to deploy Pyflink applications on <a href="https://aws.amazon.com/managed-service-apache-flink/" target="_blank" rel="noopener noreferrer">Amazon Managed Service for Apache Flink<i class="fas fa-external-link-square-alt ms-1"></i></a>, but I changed the plan to use a local Flink cluster deployed on Docker. The main reasons are</p>]]></description><enclosure url="https://jaehyeon.me/blog/2023-10-26-real-time-streaming-with-kafka-and-flink-2/featured.png" length="138560" type="image/png"/></item><item><title>Real Time Streaming with Kafka and Flink - Introduction</title><link>https://jaehyeon.me/blog/2023-10-05-real-time-streaming-with-kafka-and-flink-1/</link><pubDate>Thu, 05 Oct 2023 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2023-10-05-real-time-streaming-with-kafka-and-flink-1/</guid><description><![CDATA[<p><a href="https://catalog.us-east-1.prod.workshops.aws/workshops/2300137e-f2ac-4eb9-a4ac-3d25026b235f/en-US" target="_blank" rel="noopener noreferrer">Real Time Streaming with Amazon Kinesis<i class="fas fa-external-link-square-alt ms-1"></i></a> is an AWS workshop that helps users build a streaming analytics application on AWS. Incoming events are stored in a number of streams of the <a href="https://aws.amazon.com/kinesis/data-streams/" target="_blank" rel="noopener noreferrer">Amazon Kinesis Data Streams<i class="fas fa-external-link-square-alt ms-1"></i></a> service, and various other AWS services and tools are used to process and analyse data.</p>
<p><a href="https://kafka.apache.org/" target="_blank" rel="noopener noreferrer">Apache Kafka<i class="fas fa-external-link-square-alt ms-1"></i></a> is a popular distributed event store and stream processing platform, and it stores incoming events in topics. As part of learning real time streaming analytics on AWS, we can rebuild the analytics applications by replacing the Kinesis streams with Kafka topics. As an introduction, this post compares the workshop architecture with the updated architecture of this series. The labs of the updated architecture will be implemented in subsequent posts.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2023-10-05-real-time-streaming-with-kafka-and-flink-1/featured.png" length="138141" type="image/png"/></item><item><title>Integrate Glue Schema Registry with Your Python Kafka App</title><link>https://jaehyeon.me/blog/2023-04-12-integrate-glue-schema-registry/</link><pubDate>Wed, 12 Apr 2023 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2023-04-12-integrate-glue-schema-registry/</guid><description><![CDATA[<p>As Kafka producer and consumer apps are decoupled, they operate on Kafka topics rather than communicating with each other directly. As described in the <a href="https://docs.confluent.io/platform/current/schema-registry/index.html#sr-overview" target="_blank" rel="noopener noreferrer">Confluent document<i class="fas fa-external-link-square-alt ms-1"></i></a>, <em>Schema Registry</em> provides a centralized repository for managing and validating schemas for topic message data, and for serialization and deserialization of the data over the network. Producers and consumers to Kafka topics can use schemas to ensure data consistency and compatibility as schemas evolve. In AWS, the <a href="https://docs.aws.amazon.com/glue/latest/dg/schema-registry.html" target="_blank" rel="noopener noreferrer">Glue Schema Registry<i class="fas fa-external-link-square-alt ms-1"></i></a> supports features to manage and enforce schemas on data streaming applications using convenient integrations with Apache Kafka, <a href="https://aws.amazon.com/msk/" target="_blank" rel="noopener noreferrer">Amazon Managed Streaming for Apache Kafka<i class="fas fa-external-link-square-alt ms-1"></i></a>, <a href="https://aws.amazon.com/kinesis/data-streams/" target="_blank" rel="noopener noreferrer">Amazon Kinesis Data Streams<i class="fas fa-external-link-square-alt ms-1"></i></a>, <a href="https://aws.amazon.com/kinesis/data-analytics/" target="_blank" rel="noopener noreferrer">Amazon Kinesis Data Analytics for Apache Flink<i class="fas fa-external-link-square-alt ms-1"></i></a>, and <a href="https://aws.amazon.com/lambda/" target="_blank" rel="noopener noreferrer">AWS Lambda<i class="fas fa-external-link-square-alt ms-1"></i></a>. In this post, we will discuss how to integrate Python Kafka producer and consumer apps In AWS Lambda with the Glue Schema Registry.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2023-04-12-integrate-glue-schema-registry/featured.png" length="46040" type="image/png"/></item><item><title>Simplify Streaming Ingestion on AWS – Part 2 MSK and Athena</title><link>https://jaehyeon.me/blog/2023-03-14-simplify-streaming-ingestion-athena/</link><pubDate>Tue, 14 Mar 2023 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2023-03-14-simplify-streaming-ingestion-athena/</guid><description><![CDATA[<p>In Part 1, we discussed a streaming ingestion solution using <a href="https://aws.amazon.com/eventbridge/" target="_blank" rel="noopener noreferrer">EventBridge<i class="fas fa-external-link-square-alt ms-1"></i></a>, <a href="https://aws.amazon.com/lambda/" target="_blank" rel="noopener noreferrer">Lambda<i class="fas fa-external-link-square-alt ms-1"></i></a>, <a href="https://aws.amazon.com/msk/" target="_blank" rel="noopener noreferrer">MSK<i class="fas fa-external-link-square-alt ms-1"></i></a> and <a href="https://aws.amazon.com/redshift/redshift-serverless/" target="_blank" rel="noopener noreferrer">Redshift Serverless<i class="fas fa-external-link-square-alt ms-1"></i></a>. Athena provides the <a href="https://docs.aws.amazon.com/athena/latest/ug/connectors-msk.html" target="_blank" rel="noopener noreferrer">MSK connector<i class="fas fa-external-link-square-alt ms-1"></i></a> to enable SQL queries on Apache Kafka topics directly, and it can also facilitate the extraction of insights without setting up an additional pipeline to store data into S3. In this post, we discuss how to update the streaming ingestion solution so that data in the Kafka topic can be queried by Athena instead of Redshift.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2023-03-14-simplify-streaming-ingestion-athena/featured.png" length="43403" type="image/png"/></item><item><title>Simplify Streaming Ingestion on AWS – Part 1 MSK and Redshift</title><link>https://jaehyeon.me/blog/2023-02-08-simplify-streaming-ingestion-redshift/</link><pubDate>Wed, 08 Feb 2023 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2023-02-08-simplify-streaming-ingestion-redshift/</guid><description><![CDATA[<p><a href="https://kafka.apache.org/" target="_blank" rel="noopener noreferrer">Apache Kafka<i class="fas fa-external-link-square-alt ms-1"></i></a> is a popular distributed event store and stream processing platform. Previously loading data from Kafka into Redshift and Athena usually required Kafka connectors (e.g. <a href="https://www.confluent.io/hub/confluentinc/kafka-connect-aws-redshift" target="_blank" rel="noopener noreferrer">Amazon Redshift Sink Connector<i class="fas fa-external-link-square-alt ms-1"></i></a> and <a href="https://www.confluent.io/hub/confluentinc/kafka-connect-s3" target="_blank" rel="noopener noreferrer">Amazon S3 Sink Connector<i class="fas fa-external-link-square-alt ms-1"></i></a>). Recently these AWS services provide features to ingest data from Kafka directly, which facilitates a simpler architecture that achieves low-latency and high-speed ingestion of streaming data. In part 1 of the <em>simplify streaming ingestion on AWS</em> series, we discuss how to develop an end-to-end streaming ingestion solution using <a href="https://aws.amazon.com/eventbridge/" target="_blank" rel="noopener noreferrer">EventBridge<i class="fas fa-external-link-square-alt ms-1"></i></a>, <a href="https://aws.amazon.com/lambda/" target="_blank" rel="noopener noreferrer">Lambda<i class="fas fa-external-link-square-alt ms-1"></i></a>, <a href="https://aws.amazon.com/msk/" target="_blank" rel="noopener noreferrer">MSK<i class="fas fa-external-link-square-alt ms-1"></i></a> and <a href="https://aws.amazon.com/redshift/redshift-serverless/" target="_blank" rel="noopener noreferrer">Redshift Serverless<i class="fas fa-external-link-square-alt ms-1"></i></a> on AWS.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2023-02-08-simplify-streaming-ingestion-redshift/featured.png" length="32864" type="image/png"/></item><item><title>Revisit AWS Lambda Invoke Function Operator of Apache Airflow</title><link>https://jaehyeon.me/blog/2022-08-06-revisit-lambda-operator/</link><pubDate>Sat, 06 Aug 2022 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2022-08-06-revisit-lambda-operator/</guid><description><![CDATA[<p><a href="https://airflow.apache.org/" target="_blank" rel="noopener noreferrer">Apache Airflow<i class="fas fa-external-link-square-alt ms-1"></i></a> is a popular workflow management platform. A wide range of AWS services are integrated with the platform by <a href="https://airflow.apache.org/docs/apache-airflow-providers-amazon/stable/operators/index.html" target="_blank" rel="noopener noreferrer">Amazon AWS Operators<i class="fas fa-external-link-square-alt ms-1"></i></a>. AWS Lambda is one of the integrated services, and it can be used to develop workflows efficiently. The current <a href="https://airflow.apache.org/docs/apache-airflow-providers-amazon/stable/operators/lambda.html" target="_blank" rel="noopener noreferrer">Lambda Operator<i class="fas fa-external-link-square-alt ms-1"></i></a>, however, just invokes a Lambda function, and it can fail to report the invocation result of a function correctly and to record the exact error message from failure. In this post, we’ll discuss a custom Lambda operator that handles those limitations.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2022-08-06-revisit-lambda-operator/featured.png" length="24814" type="image/png"/></item><item><title>Serverless Application Model (SAM) for Data Professionals</title><link>https://jaehyeon.me/blog/2022-07-18-sam-for-data-professionals/</link><pubDate>Mon, 18 Jul 2022 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2022-07-18-sam-for-data-professionals/</guid><description><![CDATA[<p><a href="https://aws.amazon.com/lambda/" target="_blank" rel="noopener noreferrer">AWS Lambda<i class="fas fa-external-link-square-alt ms-1"></i></a> provides serverless computing capabilities, and it can be used for performing validation or light processing/transformation of data. Moreover, with its integration with more than 140 AWS services, it facilitates building complex systems employing <a href="https://docs.aws.amazon.com/lambda/latest/operatorguide/event-driven-architectures.html" target="_blank" rel="noopener noreferrer">event-driven architectures<i class="fas fa-external-link-square-alt ms-1"></i></a>. There are many ways to build serverless applications and one of the most efficient ways is using specialised frameworks such as the <a href="https://aws.amazon.com/serverless/sam/" target="_blank" rel="noopener noreferrer">AWS Serverless Application Model (SAM)<i class="fas fa-external-link-square-alt ms-1"></i></a> and <a href="https://www.serverless.com/framework/docs" target="_blank" rel="noopener noreferrer">Serverless Framework<i class="fas fa-external-link-square-alt ms-1"></i></a>. In this post, I’ll demonstrate how to build a serverless data processing application using SAM.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2022-07-18-sam-for-data-professionals/featured.png" length="22838" type="image/png"/></item><item><title>Yet another serverless solution for invoking AWS Lambda at a sub-minute frequency</title><link>https://jaehyeon.me/blog/2021-10-13-lambda-schedule/</link><pubDate>Wed, 13 Oct 2021 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2021-10-13-lambda-schedule/</guid><description><![CDATA[<p><a href="https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-run-lambda-schedule.html" target="_blank" rel="noopener noreferrer">Triggering a Lambda function by an EventBridge Events rule<i class="fas fa-external-link-square-alt ms-1"></i></a> can be used as a _serverless _replacement of <a href="https://en.wikipedia.org/wiki/Cron" target="_blank" rel="noopener noreferrer">cron job<i class="fas fa-external-link-square-alt ms-1"></i></a>. The highest frequency of it is one invocation per minute so that it cannot be used directly if you need to schedule a Lambda function more frequently. For example, it may be refreshing an application with real time metrics from an Amazon Connect instance where <a href="https://docs.aws.amazon.com/connect/latest/adminguide/real-time-metrics-reports.html" target="_blank" rel="noopener noreferrer">some metrics are updated every 15 seconds<i class="fas fa-external-link-square-alt ms-1"></i></a>. There is a <a href="https://aws.amazon.com/blogs/architecture/a-serverless-solution-for-invoking-aws-lambda-at-a-sub-minute-frequency/" target="_blank" rel="noopener noreferrer">post in the AWS Architecture Blog<i class="fas fa-external-link-square-alt ms-1"></i></a>, and it suggests using <a href="https://aws.amazon.com/step-functions/" target="_blank" rel="noopener noreferrer">AWS Step Functions<i class="fas fa-external-link-square-alt ms-1"></i></a>. Or a usual recommendation is using <a href="https://stackoverflow.com/questions/35878619/scheduled-aws-lambda-task-at-less-than-1-minute-frequency" target="_blank" rel="noopener noreferrer">Amazon EC2<i class="fas fa-external-link-square-alt ms-1"></i></a>. Albeit being <em>serverless</em>, the former gets a bit complicated especially in order to <a href="https://docs.aws.amazon.com/step-functions/latest/dg/tutorial-continue-new.html" target="_blank" rel="noopener noreferrer">handle the hard quota of 25,000 entries in the execution history<i class="fas fa-external-link-square-alt ms-1"></i></a>. And the latter is not an option if you look for a <em>serverless</em> solution. In this post, I’ll demonstrate another <em>serverless</em> solution of scheduling a Lambda function at a sub-minute frequency using <a href="https://aws.amazon.com/sqs/" target="_blank" rel="noopener noreferrer">Amazon SQS<i class="fas fa-external-link-square-alt ms-1"></i></a>.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2021-10-13-lambda-schedule/featured.png" length="46921" type="image/png"/></item><item><title>Thoughts on Apache Airflow AWS Lambda Operator</title><link>https://jaehyeon.me/blog/2020-04-13-airflow-lambda-operator/</link><pubDate>Mon, 13 Apr 2020 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2020-04-13-airflow-lambda-operator/</guid><description><![CDATA[<p><a href="https://airflow.apache.org/" target="_blank" rel="noopener noreferrer">Apache Airflow<i class="fas fa-external-link-square-alt ms-1"></i></a> is a popular open-source workflow management platform. Typically tasks run remotely by <a href="http://www.celeryproject.org/" target="_blank" rel="noopener noreferrer">Celery<i class="fas fa-external-link-square-alt ms-1"></i></a> workers for scalability. In AWS, however, scalability can also be achieved using serverless computing services in a simpler way. For example, the <a href="https://airflow.apache.org/docs/apache-airflow-providers-amazon/stable/operators/ecs.html" target="_blank" rel="noopener noreferrer">ECS Operator<i class="fas fa-external-link-square-alt ms-1"></i></a> allows to run <em>dockerized</em> tasks and, with the <em>Fargate</em> launch type, they can run in a serverless environment.</p>
<p>The ECS Operator alone is not sufficient because it can take up to several minutes to pull a Docker image and to set up network interface (for the case of <em>Fargate</em> launch type). Due to its latency, it is not suitable for frequently-running tasks. On the other hand, the latency of a Lambda function is negligible so that it&rsquo;s more suitable for managing such tasks.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2020-04-13-airflow-lambda-operator/featured.png" length="44994" type="image/png"/></item><item><title>AWS Local Development with LocalStack</title><link>https://jaehyeon.me/blog/2019-07-20-aws-localstack/</link><pubDate>Sat, 20 Jul 2019 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2019-07-20-aws-localstack/</guid><description><![CDATA[<p><a href="https://github.com/localstack/localstack" target="_blank" rel="noopener noreferrer">LocalStack<i class="fas fa-external-link-square-alt ms-1"></i></a> provides an easy-to-use test/mocking framework for developing AWS applications. In this post, I&rsquo;ll demonstrate how to utilize LocalStack for development using a web service.</p>
<p>Specifically a simple web service built with <a href="https://flask-restplus.readthedocs.io/en/stable/" target="_blank" rel="noopener noreferrer">Flask-RestPlus<i class="fas fa-external-link-square-alt ms-1"></i></a> is used. It supports simple CRUD operations against a database table. It is set that SQS and Lambda are used for creating and updating a record. When a <em>POST</em> or <em>PUT</em> request is made, the service sends a message to a SQS queue and directly returns <em>204</em> reponse. Once a message is received, a Lambda function is invoked and a relevant database operation is performed.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2019-07-20-aws-localstack/featured.png" length="164886" type="image/png"/></item><item><title>Serverless Data Product POC Backend Part IV - Serving R ML Model via S3</title><link>https://jaehyeon.me/blog/2017-04-17-serverless-data-product-4/</link><pubDate>Mon, 17 Apr 2017 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2017-04-17-serverless-data-product-4/</guid><description><![CDATA[<p>In the previous posts, it is discussed how to package/deploy an <a href="https://www.r-project.org/about.html" target="_blank" rel="noopener noreferrer">R<i class="fas fa-external-link-square-alt ms-1"></i></a> model with <a href="https://aws.amazon.com/lambda/details/" target="_blank" rel="noopener noreferrer">AWS Lambda<i class="fas fa-external-link-square-alt ms-1"></i></a> and to expose the Lambda function via <a href="https://aws.amazon.com/api-gateway/" target="_blank" rel="noopener noreferrer">Amazon API Gateway<i class="fas fa-external-link-square-alt ms-1"></i></a>. Main benefits of <strong>serverless architecture</strong> is cost-effectiveness and being hassle-free from provisioning/managing servers. While the API returns a predicted admission status value given <em>GRE</em>, <em>GPA</em> and <em>Rank</em>, there is an issue if it is served within a web application: <em>Cross-Origin Resource Sharing (CORS)</em>. This post discusses how to resolve this issue by updating API configuration and the Lambda function handler with a simple web application. Also it is illustrated how to host the application in a serverless environment.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2017-04-17-serverless-data-product-4/featured.png" length="225463" type="image/png"/></item><item><title>Serverless Data Product POC Backend Part III - Exposing R ML Model via APIG</title><link>https://jaehyeon.me/blog/2017-04-13-serverless-data-product-3/</link><pubDate>Thu, 13 Apr 2017 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2017-04-13-serverless-data-product-3/</guid><description><![CDATA[<p>In <a href="/blog/2017-04-08-serverless-data-product-1">Part I</a> of this series, R and necessary libraries/packages together with a Lambda function handler are packaged and saved to <a href="https://aws.amazon.com/s3/" target="_blank" rel="noopener noreferrer">Amazon S3<i class="fas fa-external-link-square-alt ms-1"></i></a>. Then, in <a href="/blog/2017-04-11-serverless-data-product-2">Part II</a>, the package is deployed at <a href="https://aws.amazon.com/lambda/" target="_blank" rel="noopener noreferrer">AWS Lambda<i class="fas fa-external-link-square-alt ms-1"></i></a> after creating and assigning a role to the Lambda function. Although the Lambda function can be called via the Invoke API, it&rsquo;ll be much more useful if the function can be called as a web service (or API). In this post, it is discussed how to expose the Lambda function via <a href="https://aws.amazon.com/api-gateway/" target="_blank" rel="noopener noreferrer">Amazon API Gateway<i class="fas fa-external-link-square-alt ms-1"></i></a>. After creating an API by integrating the Lambda function, it is protected with an API key. Finally a custom domain name is used as an alternative URL of the API.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2017-04-13-serverless-data-product-3/featured.png" length="173293" type="image/png"/></item><item><title>Serverless Data Product POC Backend Part II - Deploying R ML Model via Lambda</title><link>https://jaehyeon.me/blog/2017-04-11-serverless-data-product-2/</link><pubDate>Tue, 11 Apr 2017 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2017-04-11-serverless-data-product-2/</guid><description><![CDATA[<p>In the <a href="/blog/2017-04-08-serverless-data-product-1">previous post</a>, <strong>serverless</strong> <strong>event-driven</strong> application development is introduced. Also how to package R, necessary libraries/packages and a Lambda function handler is discussed. No need of provisioning/managing servers is one of the key benefits of the architecture. It is also a cost-effective way of delivering a data product as functions are executed <em>on-demand</em> rather than in servers that run 24/7. Furthermore <a href="https://aws.amazon.com/lambda/pricing/" target="_blank" rel="noopener noreferrer">AWS Lambda free tier<i class="fas fa-external-link-square-alt ms-1"></i></a> includes 1M free requests per month and 400,000 GB-seconds of compute time per month, which is available to both existing and new AWS customers indefinitely. (GB-seconds is applicable when execution is made with 1 GB of memory.) Lowering the size of memory increases the execution time and thus 3.2M seconds or about 37 days are free with 128 MB of memory (1 GB divided by 8) - note that CPU power is proportional to allocated memory.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2017-04-11-serverless-data-product-2/featured.png" length="139725" type="image/png"/></item><item><title>Serverless Data Product POC Backend Part I - Packaging R ML Model for Lambda</title><link>https://jaehyeon.me/blog/2017-04-08-serverless-data-product-1/</link><pubDate>Sat, 08 Apr 2017 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2017-04-08-serverless-data-product-1/</guid><description><![CDATA[<p>Let say you&rsquo;ve got a prediction model built in R and you&rsquo;d like to <em>productionize</em> it, for example, by serving it in a web application. One way is exposing the model through an API that returns the predicted result as a web service. However there are many issues. Firstly R is not a language for API development although there may be some ways - eg the <a href="https://github.com/trestletech/plumber" target="_blank" rel="noopener noreferrer">plumber<i class="fas fa-external-link-square-alt ms-1"></i></a> package. More importantly developing an API is not the end of the story as the API can&rsquo;t be served in a production system if it is not <em>deployed/managed/upgraded/patched/&hellip;</em> appropriately in a server or if it is not <em>scalable</em>, <em>protected via authentication/authorization</em> and so on. Therefore it requires quite a vast range of skill sets that cover both development and DevOps (engineering).</p>]]></description><enclosure url="https://jaehyeon.me/blog/2017-04-08-serverless-data-product-1/featured.png" length="139725" type="image/png"/></item></channel></rss>