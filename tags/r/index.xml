<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>R on Jaehyeon's Personal Site</title><link>https://jaehyeon.me/tags/r/</link><description>Recent content in R on Jaehyeon's Personal Site</description><generator>Hugo -- gohugo.io</generator><copyright>Copyright Â© 2023-2023 Jaehyeon Kim. All Rights Reserved.</copyright><lastBuildDate>Fri, 29 Nov 2019 00:00:00 +0000</lastBuildDate><atom:link href="https://jaehyeon.me/tags/r/index.xml" rel="self" type="application/rss+xml"/><item><title>Dynamic Routing and Centralized Auth with Traefik, Python and R Example</title><link>https://jaehyeon.me/blog/2019-11-29-traefik-example/</link><pubDate>Fri, 29 Nov 2019 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2019-11-29-traefik-example/</guid><description>Ingress in Kubernetes exposes HTTP and HTTPS routes from outside the cluster to services within the cluster. By setting rules, it routes requests to appropriate services (precisely requests are sent to individual Pods by Ingress Controller). Rules can be set up dynamically and I find it&amp;rsquo;s more efficient compared to traditional reverse proxy.
Traefik is a modern HTTP reverse proxy and load balancer and it can be used as a Kubernetes Ingress Controller.</description><enclosure url="https://jaehyeon.me/blog/2019-11-29-traefik-example/featured.png" length="139790" type="image/png"/></item><item><title>Distributed Task Queue with Python and R Example</title><link>https://jaehyeon.me/blog/2019-11-15-task-queue/</link><pubDate>Fri, 15 Nov 2019 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2019-11-15-task-queue/</guid><description>While I&amp;rsquo;m looking into Apache Airflow, a workflow management tool, I thought it would be beneficial to get some understanding of how Celery works. To do so, I built a simple web service that sends tasks to Celery workers and collects the results from them. FastAPI is used for developing the web service and Redis is used for the message broker and result backend. During the development, I thought it would be possible to implement similar functionality in R with Rserve.</description><enclosure url="https://jaehyeon.me/blog/2019-11-15-task-queue/featured.png" length="51615" type="image/png"/></item><item><title>Linux Dev Environment on Windows</title><link>https://jaehyeon.me/blog/2019-11-01-linux-on-windows/</link><pubDate>Fri, 01 Nov 2019 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2019-11-01-linux-on-windows/</guid><description>I use Linux containers a lot for development. Having Windows computers at home and work, I used to use Linux VMs on VirtualBox or VMWare Workstation. It&amp;rsquo;s not a bad option but it requires a lot of resources. Recently, after my home computer was updated, I was not able to start my hypervisor anymore. Also I didn&amp;rsquo;t like huge resource consumption of it so that I began to look for a different development environment.</description><enclosure url="https://jaehyeon.me/blog/2019-11-01-linux-on-windows/featured.png" length="187978" type="image/png"/></item><item><title>Shiny to Vue.js</title><link>https://jaehyeon.me/blog/2018-05-26-shiny-to-vue.js/</link><pubDate>Sat, 26 May 2018 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2018-05-26-shiny-to-vue.js/</guid><description>In the last post, the async feature of Shiny was discussed. Although it is a remarkable step forward to web development in R, it is not to the full extent that a Javascript application can bring. In fact, (long running) requests of a user (or session) are not impacted by those of other users (or sessions) but, for a given user, all requests are handled sequentially. On the other hand, it is not the case for a Javascript-backed app where all requests are processed asynchronously.</description><enclosure url="https://jaehyeon.me/blog/2018-05-26-shiny-to-vue.js/featured.png" length="247205" type="image/png"/></item><item><title>Async Shiny and Its Limitation</title><link>https://jaehyeon.me/blog/2018-05-19-asyn-shiny-and-its-limitation/</link><pubDate>Sat, 19 May 2018 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2018-05-19-asyn-shiny-and-its-limitation/</guid><description>A Shiny app is served by one (single-threaded blocking) process by Open Source Shiny Server. This causes a scalability issue because all requests are handled one by one in a queue. Recently the creator of Shiny introduced the promises package, which brings asynchronous programming capabilities to R. This is a remarkable step forward to web development in R.
In this post, it&amp;rsquo;ll be demonstrated how to implement the async feature of Shiny.</description><enclosure url="https://jaehyeon.me/blog/2018-05-19-asyn-shiny-and-its-limitation/featured.png" length="247205" type="image/png"/></item><item><title>API Development with R Part II</title><link>https://jaehyeon.me/blog/2017-11-19-api-development-with-r-2/</link><pubDate>Sun, 19 Nov 2017 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2017-11-19-api-development-with-r-2/</guid><description>In Part I, it is discussed how to serve an R function with plumber, Rserve and rApache. In this post, the APIs are deployed in a Docker container and, after showing example requests, their performance is compared. The rocker/r-ver:3.4 is used as the base image and each of the APIs is added to it. For simplicity, the APIs are served by Supervisor. For performance testing, Locust is used. The source of this post can be found in this GitHub repository.</description><enclosure url="https://jaehyeon.me/blog/2017-11-19-api-development-with-r-2/featured.png" length="367256" type="image/png"/></item><item><title>API Development with R Part I</title><link>https://jaehyeon.me/blog/2017-11-18-api-development-with-r-1/</link><pubDate>Sat, 18 Nov 2017 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2017-11-18-api-development-with-r-1/</guid><description>API is an effective way of distributing analysis outputs to external clients. When it comes to API development with R, however, there are not many choices. Probably development would be made with plumber, Rserve, rApache or OpenCPU if a client or bridge layer to R is not considered.
This is 2 part series in relation to API development with R. In this post, serving an R function with plumber, Rserve and rApache is discussed.</description><enclosure url="https://jaehyeon.me/blog/2017-11-18-api-development-with-r-1/featured.png" length="367256" type="image/png"/></item><item><title>Serverless Data Product POC Backend Part IV - Serving R ML Model via S3</title><link>https://jaehyeon.me/blog/2017-04-17-serverless-data-product-4/</link><pubDate>Mon, 17 Apr 2017 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2017-04-17-serverless-data-product-4/</guid><description>In the previous posts, it is discussed how to package/deploy an R model with AWS Lambda and to expose the Lambda function via Amazon API Gateway. Main benefits of serverless architecture is cost-effectiveness and being hassle-free from provisioning/managing servers. While the API returns a predicted admission status value given GRE, GPA and Rank, there is an issue if it is served within a web application: Cross-Origin Resource Sharing (CORS). This post discusses how to resolve this issue by updating API configuration and the Lambda function handler with a simple web application.</description><enclosure url="https://jaehyeon.me/blog/2017-04-17-serverless-data-product-4/featured.png" length="225463" type="image/png"/></item><item><title>Serverless Data Product POC Backend Part III - Exposing R ML Model via APIG</title><link>https://jaehyeon.me/blog/2017-04-13-serverless-data-product-3/</link><pubDate>Thu, 13 Apr 2017 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2017-04-13-serverless-data-product-3/</guid><description>In Part I of this series, R and necessary libraries/packages together with a Lambda function handler are packaged and saved to Amazon S3. Then, in Part II, the package is deployed at AWS Lambda after creating and assigning a role to the Lambda function. Although the Lambda function can be called via the Invoke API, it&amp;rsquo;ll be much more useful if the function can be called as a web service (or API).</description><enclosure url="https://jaehyeon.me/blog/2017-04-13-serverless-data-product-3/featured.png" length="173293" type="image/png"/></item><item><title>Serverless Data Product POC Backend Part II - Deploying R ML Model via Lambda</title><link>https://jaehyeon.me/blog/2017-04-11-serverless-data-product-2/</link><pubDate>Tue, 11 Apr 2017 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2017-04-11-serverless-data-product-2/</guid><description>In the previous post, serverless event-driven application development is introduced. Also how to package R, necessary libraries/packages and a Lambda function handler is discussed. No need of provisioning/managing servers is one of the key benefits of the architecture. It is also a cost-effective way of delivering a data product as functions are executed on-demand rather than in servers that run 24/7. Furthermore AWS Lambda free tier includes 1M free requests per month and 400,000 GB-seconds of compute time per month, which is available to both existing and new AWS customers indefinitely.</description><enclosure url="https://jaehyeon.me/blog/2017-04-11-serverless-data-product-2/featured.png" length="139725" type="image/png"/></item><item><title>Serverless Data Product POC Backend Part I - Packaging R ML Model for Lambda</title><link>https://jaehyeon.me/blog/2017-04-08-serverless-data-product-1/</link><pubDate>Sat, 08 Apr 2017 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2017-04-08-serverless-data-product-1/</guid><description><![CDATA[Let say you&rsquo;ve got a prediction model built in R and you&rsquo;d like to productionize it, for example, by serving it in a web application. One way is exposing the model through an API that returns the predicted result as a web service. However there are many issues. Firstly R is not a language for API development although there may be some ways - eg the plumber package. More importantly developing an API is not the end of the story as the API can&rsquo;t be served in a production system if it is not deployed/managed/upgraded/patched/&hellip; appropriately in a server or if it is not scalable, protected via authentication/authorization and so on.]]></description><enclosure url="https://jaehyeon.me/blog/2017-04-08-serverless-data-product-1/featured.png" length="139725" type="image/png"/></item><item><title>Some Thoughts on Shiny Open Source - Render Multiple Pages</title><link>https://jaehyeon.me/blog/2016-06-27-shiny-open-source-render-multiple-pages/</link><pubDate>Mon, 27 Jun 2016 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2016-06-27-shiny-open-source-render-multiple-pages/</guid><description>R Shiny applications are served as a single page application and it is not built to render multiple pages. There are benefits of rendering multiple pages such as code management and implement authentication. In this page, we discuss how to implement multi-page rendering in a Shiny app.
As indicated above, Shiny is not designed to render multiple pages and, in general, the UI is rendered on the fly as defined in ui.</description></item><item><title>Some Thoughts on Shiny Open Source - Internal Load Balancing</title><link>https://jaehyeon.me/blog/2016-05-23-shiny-open-source-internal-load-balancing/</link><pubDate>Mon, 23 May 2016 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2016-05-23-shiny-open-source-internal-load-balancing/</guid><description>Shiny is an interesting web framework that helps create a web application quickly. If it targets a large number of users, however, there are several limitations and it is so true when the open source version of Shiny is in use. It would be possible to tackle down some of the limitations with the enterprise version but it is not easy to see enough examples of Shiny applications in production environment.</description></item><item><title>Asynchronous Processing Using Job Queue</title><link>https://jaehyeon.me/blog/2016-05-12-asynchronous-processing-using-job-queue/</link><pubDate>Thu, 12 May 2016 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2016-05-12-asynchronous-processing-using-job-queue/</guid><description>In this post, a way to overcome one of R&amp;rsquo;s limitations (lack of multi-threading) is discussed by job queuing using the jobqueue package - a generic asynchronous job queue implementation for R. See the package description below.
The jobqueue package is meant to provide an easy-to-use interface that allows to queue computations for background evaluation while the calling R session remains responsive. It is based on a 1-node socket cluster from the parallel package.</description></item><item><title>Boost SparkR with Hive</title><link>https://jaehyeon.me/blog/2016-04-30-boost-sparkr-with-hive/</link><pubDate>Sat, 30 Apr 2016 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2016-04-30-boost-sparkr-with-hive/</guid><description>In the previous post, it is demonstrated how to start SparkR in local and cluster mode. While SparkR is in active development, it is yet to fully support Spark&amp;rsquo;s key libraries such as MLlib and Spark Streaming. Even, as a data processing engine, this R API is still limited as it is not possible to manipulate RDDs directly but only via Spark SQL/DataFrame API. As can be checked in the API doc, SparkR rebuilds many existing R functions to work with Spark DataFrame and notably it borrows some functions from the dplyr package.</description></item><item><title>Quick Start SparkR in Local and Cluster Mode</title><link>https://jaehyeon.me/blog/2016-03-02-quick-start-sparkr-in-local-and-cluster-mode/</link><pubDate>Wed, 02 Mar 2016 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2016-03-02-quick-start-sparkr-in-local-and-cluster-mode/</guid><description>In the previous post, a Spark cluster is set up using 2 VirtualBox Ubuntu guests. While this is a viable option for many, it is not always for others. For those who find setting-up such a cluster is not convenient, there&amp;rsquo;s still another option, which is relying on the local mode of Spark. In this post, a BitBucket repository is introduced, which is a R project that includes Spark 1.6.0 Pre-built for Hadoop 2.</description></item><item><title>Spark Cluster Setup on VirtualBox</title><link>https://jaehyeon.me/blog/2016-02-22-spark-cluster-setup-on-virtualbox/</link><pubDate>Mon, 22 Feb 2016 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2016-02-22-spark-cluster-setup-on-virtualbox/</guid><description>We discuss how to set up a Spark cluser between 2 Ubuntu guests. Firstly it begins with machine preparation. Once a machine is baked, its image file (VDI) is be copied for the second one. Then how to launch a cluster by standalone mode is discussed. Let&amp;rsquo;s get started.
Machine preparation If you haven&amp;rsquo;t read the previous post, I recommend reading as it introduces Putty as well. Also, as Spark need Java Development Kit (JDK), you may need to apt-get it first - see this tutorial for further details.</description></item><item><title>Quick Test to Wrap Python in R</title><link>https://jaehyeon.me/blog/2015-11-21-quick-test-to-wrap-python-in-r/</link><pubDate>Sat, 21 Nov 2015 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2015-11-21-quick-test-to-wrap-python-in-r/</guid><description>As mentioned in an earlier post, things that are not easy in R can be relatively simple in other languages. Another example would be connecting to Amazon Web Services. In relation to s3, although there are a number of existing packages, many of them seem to be deprecated, premature or platform-dependent. (I consider the cloudyr project looks promising though.)
If there isn&amp;rsquo;t a comprehensive R-way of doing something yet, it may be necessary to create it from scratch.</description></item><item><title>Some Thoughts on Python for R Users</title><link>https://jaehyeon.me/blog/2015-08-09-some-thoughts-on-python-for-r-users/</link><pubDate>Sun, 09 Aug 2015 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2015-08-09-some-thoughts-on-python-for-r-users/</guid><description>There seem to be growing interest in Python in the R cummunity. While there can be a range of opinions about using R over Python (or vice versa) for exploratory data analysis, fitting statistical/machine learning algorithms and so on, I consider one of the strongest attractions of using Python comes from the fact that Python is a general purpose programming language. As more developers are involved in, it can provide a way to get jobs done easily, which can be tricky in R.</description></item><item><title>Setup Random Seeds on Caret Package</title><link>https://jaehyeon.me/blog/2015-05-30-setup-random-seeds-on-caret-package/</link><pubDate>Sat, 30 May 2015 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2015-05-30-setup-random-seeds-on-caret-package/</guid><description>A short while ago I had a chance to perform analysis using the caret package. One of the requirements is to run it parallelly and to work in both Windows and Linux. The requirement can be met by using the parallel and doParallel packages as the caret package trains a model using the foreach package if clusters are registered by the doParallel package - further details about how to implement parallel processing on a single machine can be found in earlier posts (Link 1, Link 2 and Link 3).</description></item><item><title>Packaging Analysis</title><link>https://jaehyeon.me/blog/2015-03-24-packaging-analysis/</link><pubDate>Tue, 24 Mar 2015 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2015-03-24-packaging-analysis/</guid><description>When I imagine a workflow, it is performing the same or similar tasks regularly (daily or weekly) in an automated way. Although those tasks can be executed in a script or a *source()*d script, it may not be easy to maintain separate scripts while the size of tasks gets bigger or if they have to be executed in different machines. In academia, reproducible research shares similar ideas but the level of reproducibility introduced in Gandrud, 2013 may not suffice in a business environment as the focus is documenting in a reproducible way.</description></item><item><title>Parallel Processing on Single Machine - Part II</title><link>https://jaehyeon.me/blog/2015-03-17-parallel-processing-on-single-machine-2/</link><pubDate>Tue, 17 Mar 2015 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2015-03-17-parallel-processing-on-single-machine-2/</guid><description>In the previous article, parallel processing on a single machine using the snow and parallel packages are introduced. The four functions are an extension of lapply() with an additional argument that specifies a cluster object. In spite of their effectiveness and ease of use, there may be cases where creating a function that can be sent into clusters is not easy or looping may be more natural. In this article, another way of implementing parallel processing on a single machine is introduced using the foreach and doParallel packages where clusters are created by the parallel package.</description></item><item><title>Parallel Processing on Single Machine - Part III</title><link>https://jaehyeon.me/blog/2015-03-19-parallel-processing-on-single-machine-3/</link><pubDate>Tue, 17 Mar 2015 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2015-03-19-parallel-processing-on-single-machine-3/</guid><description>In the previous posts, two groups of ways to implement parallel processing on a single machine are introduced. The first group is provided by the snow or parallel package and the functions are an extension of lapply() (LINK). The second group is based on an extension of the for construct (foreach, %dopar% and %:%). The foreach construct is provided by the foreach package while clusters are made and registered by the parallel and doParallel packages respectively (LINK).</description></item><item><title>Parallel Processing on Single Machine - Part I</title><link>https://jaehyeon.me/blog/2015-03-14-parallel-processing-on-single-machine-1/</link><pubDate>Sat, 14 Mar 2015 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2015-03-14-parallel-processing-on-single-machine-1/</guid><description>Lack of multi-threading and memory limitation are two outstanding weaknesses of base R. In fact, however, if the size of data is not so large that it can be read in RAM, the former would be relatively easily handled by parallel processing, provided that multiple processors are equipped. This article introduces to a way of implementing parallel processing on a single machine using the snow and parallel packages - the examples are largely based on McCallum and Weston (2012).</description></item><item><title>Tree Based Methods in R - Part VI</title><link>https://jaehyeon.me/blog/2015-03-07-tree-based-methods-6/</link><pubDate>Sat, 07 Mar 2015 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2015-03-07-tree-based-methods-6/</guid><description>Part I Part II Part III Part IV Part V Part VI (this post) A regression tree is evaluated using bagged trees in the previous article. In this article, the response variable of the same data set is converted into a binary factor variable and a classification tree is evaluated by comparing to bagged trees&amp;rsquo; individual oob/test errors, cumulative oob/test errors and variable importance measures.
Before getting started, note that the source of the classes can be found in this gist and, together with the relevant packages (see tags), it requires a utility function (bestParam()) that can be found here.</description></item><item><title>Tree Based Methods in R - Part V</title><link>https://jaehyeon.me/blog/2015-03-05-tree-based-methods-5/</link><pubDate>Thu, 05 Mar 2015 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2015-03-05-tree-based-methods-5/</guid><description>Part I Part II Part III Part IV Part V (this post) Part VI This article evaluates a single regression tree&amp;rsquo;s performance by comparing to bagged trees&amp;rsquo; individual oob/test errors, cumulative oob/test errors and variable importance measures.
Before getting started, note that the source of the classes can be found in this gist and, together with the relevant packages (see tags), it requires a utility function (bestParam()) that can be found here.</description></item><item><title>Tree Based Methods in R - Part IV</title><link>https://jaehyeon.me/blog/2015-02-15-tree-based-methods-4/</link><pubDate>Sun, 15 Feb 2015 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2015-02-15-tree-based-methods-4/</guid><description>Part I Part II Part III Part IV (this post) Part V Part VI While the last three articles illustrated the CART model for both classification (with equal/unequal costs) and regression tasks, this article is rather technical as it compares three packages: rpart, caret and mlr. For those who are not familiar with the last two packages, they are wrappers (or frameworks) that implement a range of models (or algorithms) in a unified way.</description></item><item><title>Tree Based Methods in R - Part III</title><link>https://jaehyeon.me/blog/2015-02-14-tree-based-methods-3/</link><pubDate>Sat, 14 Feb 2015 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2015-02-14-tree-based-methods-3/</guid><description>Part I Part II Part III (this post) Part IV Part V Part VI While classification tasks are implemented in the last two articles (Part I and Part II), a regression task is the topic of this article. While the caret package selects the tuning parameter (cp) that minimizes the error (RMSE), the rpart packages recommends the 1-SE rule, which selects the smallest tree within 1 standard error of the minimum cross validation error (xerror).</description></item><item><title>Tree Based Methods in R - Part II</title><link>https://jaehyeon.me/blog/2015-02-08-tree-based-methods-2/</link><pubDate>Sun, 08 Feb 2015 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2015-02-08-tree-based-methods-2/</guid><description>Part I Part II (this post) Part III Part IV Part V Part VI In the previous article (Tree Based Methods in R - Part I), a decision tree is created on the Carseats data which is in the chapter 8 lab of ISLR. In that article, potentially asymetric costs due to misclassification are not taken into account. When unbalance between false positive and false negative can have a significant impact, it can be explicitly adjusted either by altering prior (or empirical) probabilities or by adding a loss matrix.</description></item><item><title>Tree Based Methods in R - Part I</title><link>https://jaehyeon.me/blog/2015-02-01-tree-based-methods-1/</link><pubDate>Sun, 01 Feb 2015 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2015-02-01-tree-based-methods-1/</guid><description>Part I (this post) Part II Part III Part IV Part V Part VI This is the first article about tree based methods using R. Carseats data in the chapter 8 lab of ISLR is used to perform classification analysis. Unlike the lab example, the rpart package is used to fit the CART model on the data and the caret package is used for tuning the pruning parameter (cp).</description></item><item><title>Quick Trial of Adding Column</title><link>https://jaehyeon.me/blog/2015-01-14-quick-trial-of-adding-column/</link><pubDate>Wed, 14 Jan 2015 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2015-01-14-quick-trial-of-adding-column/</guid><description>This is a quick trial of adding overall and conditional (by user) average columns in a data frame. base,plyr,dplyr,data.table,dplyr + data.table packages are used. Personally I perfer dplyr + data.table - dplyr for comperhensive syntax and data.table for speed.
1## set up variables 2size &amp;lt;- 36000 3numUsers &amp;lt;- 4900 4# roughly each user has 7 sessions 5numSessions &amp;lt;- (numUsers / 7) - ((numUsers / 7) %% 1) 6 7## create data frame 8set.</description></item><item><title>Looping without for</title><link>https://jaehyeon.me/blog/2014-12-17-looping-without-for/</link><pubDate>Wed, 17 Dec 2014 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2014-12-17-looping-without-for/</guid><description>Purely programming point of view, I consider for-loops would be better to be avoided in R as
the script can be more readable it is easier to handle errors Some articles on the web indicate that looping functions (or apply family of functions) don&amp;rsquo;t guarantee faster execution and sometimes even slower. Although, assuming that the experiments are correct, in my opinion, code readability itself is beneficial enough to avoid for-loops. Even worse, R&amp;rsquo;s dynamic typing system coupled with poor readability can result in a frustrating consequence as the code grows.</description></item><item><title>Short R Examples</title><link>https://jaehyeon.me/blog/2014-12-03-short-r-examples/</link><pubDate>Wed, 03 Dec 2014 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2014-12-03-short-r-examples/</guid><description>As I don&amp;rsquo;t use R at work yet, I haven&amp;rsquo;t got enough chances to learn R by doing. Together with teaching myself machine learning with R, I consider it would be a good idea to collect examples on the web. Recently I have been visiting a Linkedin group called The R Project for Statistical Computing and suggesting scripts on data manipulation or programming topics. Actually I expected some of the topics may also be helpful to learn machine learning/statistics but, possibly due to lack of understaning of the topics in context, I&amp;rsquo;ve found only a few - it may be necessary to look for another source.</description></item><item><title>Summarise Stock Returns from Multiple Files</title><link>https://jaehyeon.me/blog/2014-11-27-summarise-stock-returns-from-multiple-files/</link><pubDate>Thu, 27 Nov 2014 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2014-11-27-summarise-stock-returns-from-multiple-files/</guid><description><![CDATA[This post is a slight extension of the previous two articles (Download Stock Data - Part I, Download Stock Data - Part II) and we discuss how to produce gross returns, standard deviation and correlation of multiple shares.
The following packages are used.
1library(knitr) 2library(lubridate) 3library(stringr) 4library(reshape2) 5library(plyr) 6library(dplyr) The script begins with creating a data folder in the format of data_YYYY-MM-DD.
1# create data folder 2dataDir &lt;- paste0(&#34;data&#34;,&#34;_&#34;,format(Sys.Date(),&#34;%Y-%m-%d&#34;)) 3if(file.exists(dataDir)) { 4 unlink(dataDir, recursive = TRUE) 5 dir.]]></description></item><item><title>Download Stock Data - Part II</title><link>https://jaehyeon.me/blog/2014-11-21-download-stock-data-2/</link><pubDate>Fri, 21 Nov 2014 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2014-11-21-download-stock-data-2/</guid><description>In an earlier article, a way to download stock price data files from Google, save it into a local drive and merge them into a single data frame. If files are not large, however, it wouldn&amp;rsquo;t be effective and, in this article, files are downloaded and merged internally.
The following packages are used.
1library(knitr) 2library(lubridate) 3library(stringr) 4library(plyr) 5library(dplyr) Taking urls as file locations, files are directly read using llply and they are combined using rbind_all.</description></item><item><title>Download Stock Data - Part I</title><link>https://jaehyeon.me/blog/2014-11-20-download-stock-data-1/</link><pubDate>Thu, 20 Nov 2014 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2014-11-20-download-stock-data-1/</guid><description>This article illustrates how to download stock price data files from Google, save it into a local drive and merge them into a single data frame. This script is slightly modified from a script which downloads RStudio package download log data. The original source can be found here.
First of all, the following three packages are used.
1library(knitr) 2library(lubridate) 3library(stringr) 4library(plyr) 5library(dplyr) The script begins with creating a folder to save data files.</description></item></channel></rss>