<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>R on Jaehyeon Kim</title><link>https://jaehyeon.me/tags/r/</link><description>Recent content in R on Jaehyeon Kim</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>Copyright Â© 2023-2024 Jaehyeon Kim. All Rights Reserved.</copyright><lastBuildDate>Fri, 29 Nov 2019 00:00:00 +0000</lastBuildDate><atom:link href="https://jaehyeon.me/tags/r/index.xml" rel="self" type="application/rss+xml"/><item><title>Dynamic Routing and Centralized Auth with Traefik, Python and R Example</title><link>https://jaehyeon.me/blog/2019-11-29-traefik-example/</link><pubDate>Fri, 29 Nov 2019 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2019-11-29-traefik-example/</guid><description><![CDATA[<p><a href="https://kubernetes.io/docs/concepts/services-networking/ingress/" target="_blank" rel="noopener noreferrer">Ingress<i class="fas fa-external-link-square-alt ms-1"></i></a> in <a href="https://kubernetes.io/" target="_blank" rel="noopener noreferrer">Kubernetes<i class="fas fa-external-link-square-alt ms-1"></i></a> exposes HTTP and HTTPS routes from outside the cluster to services within the cluster. By setting rules, it routes requests to appropriate services (precisely requests are sent to individual <a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-overview/" target="_blank" rel="noopener noreferrer">Pods<i class="fas fa-external-link-square-alt ms-1"></i></a> by <a href="https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/" target="_blank" rel="noopener noreferrer">Ingress Controller<i class="fas fa-external-link-square-alt ms-1"></i></a>). Rules can be set up dynamically and I find it&rsquo;s more efficient compared to traditional <a href="https://en.wikipedia.org/wiki/Reverse_proxy" target="_blank" rel="noopener noreferrer">reverse proxy<i class="fas fa-external-link-square-alt ms-1"></i></a>.</p>
<p><a href="https://docs.traefik.io/v1.7/" target="_blank" rel="noopener noreferrer">Traefik<i class="fas fa-external-link-square-alt ms-1"></i></a> is a modern HTTP reverse proxy and load balancer and it can be used as a <em>Kubernetes</em> <em>Ingress Controller</em>. Moreover it supports other <a href="https://docs.traefik.io/providers/overview/" target="_blank" rel="noopener noreferrer">providers<i class="fas fa-external-link-square-alt ms-1"></i></a>, which are existing infrastructure components such as orchestrators, container engines, cloud providers, or key-value stores. To name a few, Docker, Kubernetes, AWS ECS, AWS DynamoDB and Consul are <a href="https://docs.traefik.io/v1.7/" target="_blank" rel="noopener noreferrer">supported providers<i class="fas fa-external-link-square-alt ms-1"></i></a>. With <em>Traefik</em>, it is possible to configure routing dynamically. Another interesting feature is <a href="https://docs.traefik.io/v1.7/configuration/entrypoints/#forward-authentication" target="_blank" rel="noopener noreferrer">Forward Authentication<i class="fas fa-external-link-square-alt ms-1"></i></a> where authentication can be handled by an external service. In this post, it&rsquo;ll be demonstrated how <em>path-based</em> routing can be set up by <em>Traefik with Docker</em>. Also a centralized authentication will be illustrated with the <em>Forward Authentication</em> feature of <em>Traefik</em>.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2019-11-29-traefik-example/featured.png" length="139790" type="image/png"/></item><item><title>Distributed Task Queue with Python and R Example</title><link>https://jaehyeon.me/blog/2019-11-15-task-queue/</link><pubDate>Fri, 15 Nov 2019 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2019-11-15-task-queue/</guid><description><![CDATA[<p>While I&rsquo;m looking into <a href="https://airflow.apache.org/" target="_blank" rel="noopener noreferrer">Apache Airflow<i class="fas fa-external-link-square-alt ms-1"></i></a>, a workflow management tool, I thought it would be beneficial to get some understanding of how <a href="http://www.celeryproject.org/" target="_blank" rel="noopener noreferrer">Celery<i class="fas fa-external-link-square-alt ms-1"></i></a> works. To do so, I built a simple web service that sends tasks to Celery workers and collects the results from them. <a href="https://fastapi.tiangolo.com/" target="_blank" rel="noopener noreferrer">FastAPI<i class="fas fa-external-link-square-alt ms-1"></i></a> is used for developing the web service and <a href="https://redis.io/" target="_blank" rel="noopener noreferrer">Redis<i class="fas fa-external-link-square-alt ms-1"></i></a> is used for the message broker and result backend. During the development, I thought it would be possible to implement similar functionality in R with <a href="https://www.rforge.net/Rserve/" target="_blank" rel="noopener noreferrer">Rserve<i class="fas fa-external-link-square-alt ms-1"></i></a>. Therefore a Rserve worker is added as an example as well. Coupling a web service with distributed task queue is beneficial on its own as it helps the service be more responsive by offloading heavyweight and long running processes to task workers.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2019-11-15-task-queue/featured.png" length="51615" type="image/png"/></item><item><title>Linux Dev Environment on Windows</title><link>https://jaehyeon.me/blog/2019-11-01-linux-on-windows/</link><pubDate>Fri, 01 Nov 2019 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2019-11-01-linux-on-windows/</guid><description><![CDATA[<p>I use Linux containers a lot for development. Having Windows computers at home and work, I used to use Linux VMs on VirtualBox or VMWare Workstation. It&rsquo;s not a bad option but it requires a lot of resources. Recently, after my home computer was updated, I was not able to start my hypervisor anymore. Also I didn&rsquo;t like huge resource consumption of it so that I began to look for a different development environment. A while ago, I played with <a href="https://docs.microsoft.com/en-us/windows/wsl/about" target="_blank" rel="noopener noreferrer">Windows Subsystem for Linux (WSL)<i class="fas fa-external-link-square-alt ms-1"></i></a> and it was alright. Also <a href="https://code.visualstudio.com/" target="_blank" rel="noopener noreferrer">Visual Studio Code (VSCode)<i class="fas fa-external-link-square-alt ms-1"></i></a>, <em>my favourite editor</em>, now supports <a href="https://code.visualstudio.com/docs/remote/remote-overview" target="_blank" rel="noopener noreferrer">remote development<i class="fas fa-external-link-square-alt ms-1"></i></a>. Initially I thought I would be able to create a new development environment with WSL and <a href="https://docs.docker.com/docker-for-windows/install/" target="_blank" rel="noopener noreferrer">Docker for Windows<i class="fas fa-external-link-square-alt ms-1"></i></a>. However it was until I tried a bigger app with <a href="https://docs.docker.com/compose/" target="_blank" rel="noopener noreferrer">Docker Compose<i class="fas fa-external-link-square-alt ms-1"></i></a> that Docker for Windows has a number of issues especially when containers are started by Docker Compose in WSL. I didn&rsquo;t like to spend too much time on fixing those issues as I concerned those might not be the only ones. Then I decided to install a Linux VM on <a href="https://docs.microsoft.com/en-us/virtualization/hyper-v-on-windows/about/" target="_blank" rel="noopener noreferrer">Hyper-V<i class="fas fa-external-link-square-alt ms-1"></i></a>. Luckly VSCode also supports a remote VM via SSH.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2019-11-01-linux-on-windows/featured.png" length="187978" type="image/png"/></item><item><title>Shiny to Vue.js</title><link>https://jaehyeon.me/blog/2018-05-26-shiny-to-vue.js/</link><pubDate>Sat, 26 May 2018 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2018-05-26-shiny-to-vue.js/</guid><description>&lt;p>In the &lt;a href="/blog/2018-05-19-asyn-shiny-and-its-limitation">last post&lt;/a>, the async feature of Shiny was discussed. Although it is a remarkable step forward to web development in R, it is not to the full extent that a Javascript application can bring. In fact, (long running) requests of a user (or session) are not impacted by those of other users (or sessions) but, for a given user, all requests are handled sequentially. On the other hand, it is not the case for a Javascript-backed app where all requests are processed asynchronously.&lt;/p></description><enclosure url="https://jaehyeon.me/blog/2018-05-26-shiny-to-vue.js/featured.png" length="247205" type="image/png"/></item><item><title>Async Shiny and Its Limitation</title><link>https://jaehyeon.me/blog/2018-05-19-asyn-shiny-and-its-limitation/</link><pubDate>Sat, 19 May 2018 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2018-05-19-asyn-shiny-and-its-limitation/</guid><description><![CDATA[<p>A Shiny app is served by one (<em>single-threaded blocking</em>) process by <a href="https://www.rstudio.com/products/shiny/download-server/" target="_blank" rel="noopener noreferrer">Open Source Shiny Server<i class="fas fa-external-link-square-alt ms-1"></i></a>. This causes a scalability issue because all requests are handled one by one in a queue. Recently the creator of <em>Shiny</em> introduced the <a href="https://rstudio.github.io/promises/" target="_blank" rel="noopener noreferrer">promises<i class="fas fa-external-link-square-alt ms-1"></i></a> package, which brings <em>asynchronous programming capabilities to R</em>. This is a remarkable step forward to web development in R.</p>
<p>In this post, it&rsquo;ll be demonstrated how to implement the async feature of Shiny. Then its limitation will be discussed with an alternative app, which is built by <em>JavaScript</em> for the frontend and <em>RServe</em> for the backend.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2018-05-19-asyn-shiny-and-its-limitation/featured.png" length="247205" type="image/png"/></item><item><title>API Development with R Part II</title><link>https://jaehyeon.me/blog/2017-11-19-api-development-with-r-2/</link><pubDate>Sun, 19 Nov 2017 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2017-11-19-api-development-with-r-2/</guid><description><![CDATA[<p>In <a href="/blog/2017-11-18-api-development-with-r-1">Part I</a>, it is discussed how to serve an R function with <em>plumber</em>, <em>Rserve</em> and <em>rApache</em>. In this post, the APIs are deployed in a Docker container and, after showing example requests, their performance is compared. The <a href="https://hub.docker.com/r/rocker/r-ver/" target="_blank" rel="noopener noreferrer">rocker/r-ver:3.4<i class="fas fa-external-link-square-alt ms-1"></i></a> is used as the base image and each of the APIs is added to it. For simplicity, the APIs are served by <a href="http://supervisord.org/" target="_blank" rel="noopener noreferrer">Supervisor<i class="fas fa-external-link-square-alt ms-1"></i></a>. For performance testing, <a href="https://locust.io/" target="_blank" rel="noopener noreferrer">Locust<i class="fas fa-external-link-square-alt ms-1"></i></a> is used. The source of this post can be found in this <a href="https://github.com/jaehyeon-kim/r-api-demo" target="_blank" rel="noopener noreferrer"><strong>GitHub repository</strong><i class="fas fa-external-link-square-alt ms-1"></i></a>.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2017-11-19-api-development-with-r-2/featured.png" length="367256" type="image/png"/></item><item><title>API Development with R Part I</title><link>https://jaehyeon.me/blog/2017-11-18-api-development-with-r-1/</link><pubDate>Sat, 18 Nov 2017 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2017-11-18-api-development-with-r-1/</guid><description><![CDATA[<p>API is an effective way of distributing analysis outputs to external clients. When it comes to API development with R, however, there are not many choices. Probably development would be made with <a href="https://github.com/trestletech/plumber" target="_blank" rel="noopener noreferrer">plumber<i class="fas fa-external-link-square-alt ms-1"></i></a>, <a href="https://www.rforge.net/Rserve/" target="_blank" rel="noopener noreferrer">Rserve<i class="fas fa-external-link-square-alt ms-1"></i></a>, <a href="http://rapache.net/" target="_blank" rel="noopener noreferrer">rApache<i class="fas fa-external-link-square-alt ms-1"></i></a> or <a href="https://www.opencpu.org/" target="_blank" rel="noopener noreferrer">OpenCPU<i class="fas fa-external-link-square-alt ms-1"></i></a> if a client or bridge layer to R is not considered.</p>
<p>This is 2 part series in relation to <em>API development with R</em>. In this post, serving an R function with <em>plumber</em>, <em>Rserve</em> and <em>rApache</em> is discussed. <em>OpenCPU</em> is not discussed partly because it could be overkill for API. Also its performance may be similar to <em>rApache</em> with <a href="http://httpd.apache.org/docs/2.2/mod/prefork.html" target="_blank" rel="noopener noreferrer">Prefork Multi-Processing Module<i class="fas fa-external-link-square-alt ms-1"></i></a> enabled. Then deploying the APIs in a Docker container, making example HTTP requests and their performance will be discussed in <a href="/blog/2015-02-08-tree-based-methods-2">Part II</a>.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2017-11-18-api-development-with-r-1/featured.png" length="367256" type="image/png"/></item><item><title>Serverless Data Product POC Backend Part IV - Serving R ML Model via S3</title><link>https://jaehyeon.me/blog/2017-04-17-serverless-data-product-4/</link><pubDate>Mon, 17 Apr 2017 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2017-04-17-serverless-data-product-4/</guid><description><![CDATA[<p>In the previous posts, it is discussed how to package/deploy an <a href="https://www.r-project.org/about.html" target="_blank" rel="noopener noreferrer">R<i class="fas fa-external-link-square-alt ms-1"></i></a> model with <a href="https://aws.amazon.com/lambda/details/" target="_blank" rel="noopener noreferrer">AWS Lambda<i class="fas fa-external-link-square-alt ms-1"></i></a> and to expose the Lambda function via <a href="https://aws.amazon.com/api-gateway/" target="_blank" rel="noopener noreferrer">Amazon API Gateway<i class="fas fa-external-link-square-alt ms-1"></i></a>. Main benefits of <strong>serverless architecture</strong> is cost-effectiveness and being hassle-free from provisioning/managing servers. While the API returns a predicted admission status value given <em>GRE</em>, <em>GPA</em> and <em>Rank</em>, there is an issue if it is served within a web application: <em>Cross-Origin Resource Sharing (CORS)</em>. This post discusses how to resolve this issue by updating API configuration and the Lambda function handler with a simple web application. Also it is illustrated how to host the application in a serverless environment.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2017-04-17-serverless-data-product-4/featured.png" length="225463" type="image/png"/></item><item><title>Serverless Data Product POC Backend Part III - Exposing R ML Model via APIG</title><link>https://jaehyeon.me/blog/2017-04-13-serverless-data-product-3/</link><pubDate>Thu, 13 Apr 2017 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2017-04-13-serverless-data-product-3/</guid><description><![CDATA[<p>In <a href="/blog/2017-04-08-serverless-data-product-1">Part I</a> of this series, R and necessary libraries/packages together with a Lambda function handler are packaged and saved to <a href="https://aws.amazon.com/s3/" target="_blank" rel="noopener noreferrer">Amazon S3<i class="fas fa-external-link-square-alt ms-1"></i></a>. Then, in <a href="/blog/2017-04-11-serverless-data-product-2">Part II</a>, the package is deployed at <a href="https://aws.amazon.com/lambda/" target="_blank" rel="noopener noreferrer">AWS Lambda<i class="fas fa-external-link-square-alt ms-1"></i></a> after creating and assigning a role to the Lambda function. Although the Lambda function can be called via the Invoke API, it&rsquo;ll be much more useful if the function can be called as a web service (or API). In this post, it is discussed how to expose the Lambda function via <a href="https://aws.amazon.com/api-gateway/" target="_blank" rel="noopener noreferrer">Amazon API Gateway<i class="fas fa-external-link-square-alt ms-1"></i></a>. After creating an API by integrating the Lambda function, it is protected with an API key. Finally a custom domain name is used as an alternative URL of the API.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2017-04-13-serverless-data-product-3/featured.png" length="173293" type="image/png"/></item><item><title>Serverless Data Product POC Backend Part II - Deploying R ML Model via Lambda</title><link>https://jaehyeon.me/blog/2017-04-11-serverless-data-product-2/</link><pubDate>Tue, 11 Apr 2017 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2017-04-11-serverless-data-product-2/</guid><description><![CDATA[<p>In the <a href="/blog/2017-04-08-serverless-data-product-1">previous post</a>, <strong>serverless</strong> <strong>event-driven</strong> application development is introduced. Also how to package R, necessary libraries/packages and a Lambda function handler is discussed. No need of provisioning/managing servers is one of the key benefits of the architecture. It is also a cost-effective way of delivering a data product as functions are executed <em>on-demand</em> rather than in servers that run 24/7. Furthermore <a href="https://aws.amazon.com/lambda/pricing/" target="_blank" rel="noopener noreferrer">AWS Lambda free tier<i class="fas fa-external-link-square-alt ms-1"></i></a> includes 1M free requests per month and 400,000 GB-seconds of compute time per month, which is available to both existing and new AWS customers indefinitely. (GB-seconds is applicable when execution is made with 1 GB of memory.) Lowering the size of memory increases the execution time and thus 3.2M seconds or about 37 days are free with 128 MB of memory (1 GB divided by 8) - note that CPU power is proportional to allocated memory.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2017-04-11-serverless-data-product-2/featured.png" length="139725" type="image/png"/></item><item><title>Serverless Data Product POC Backend Part I - Packaging R ML Model for Lambda</title><link>https://jaehyeon.me/blog/2017-04-08-serverless-data-product-1/</link><pubDate>Sat, 08 Apr 2017 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2017-04-08-serverless-data-product-1/</guid><description><![CDATA[<p>Let say you&rsquo;ve got a prediction model built in R and you&rsquo;d like to <em>productionize</em> it, for example, by serving it in a web application. One way is exposing the model through an API that returns the predicted result as a web service. However there are many issues. Firstly R is not a language for API development although there may be some ways - eg the <a href="https://github.com/trestletech/plumber" target="_blank" rel="noopener noreferrer">plumber<i class="fas fa-external-link-square-alt ms-1"></i></a> package. More importantly developing an API is not the end of the story as the API can&rsquo;t be served in a production system if it is not <em>deployed/managed/upgraded/patched/&hellip;</em> appropriately in a server or if it is not <em>scalable</em>, <em>protected via authentication/authorization</em> and so on. Therefore it requires quite a vast range of skill sets that cover both development and DevOps (engineering).</p>]]></description><enclosure url="https://jaehyeon.me/blog/2017-04-08-serverless-data-product-1/featured.png" length="139725" type="image/png"/></item><item><title>Some Thoughts on Shiny Open Source - Render Multiple Pages</title><link>https://jaehyeon.me/blog/2016-06-27-shiny-open-source-render-multiple-pages/</link><pubDate>Mon, 27 Jun 2016 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2016-06-27-shiny-open-source-render-multiple-pages/</guid><description><![CDATA[<p>R Shiny applications are served as a single page application and it is not built to render multiple pages. There are benefits of rendering multiple pages such as code management and implement authentication. In this page, we discuss how to implement multi-page rendering in a Shiny app.</p>
<p>As indicated above, Shiny is not designed to render multiple pages and, in general, the UI is rendered on the fly as defined in <em>ui.R</em> or <em>app.R</em>. However this is not the only way as the UI can be rendered as a html output using <code>htmlOutput()</code> in <em>ui.R</em> and <code>renderUI()</code> in <em>server.R</em>. In this post, rendering multiple pages will be illustrated using an <a href="https://github.com/jaehyeon-kim/shiny-multipage" target="_blank" rel="noopener noreferrer"><strong>example application</strong><i class="fas fa-external-link-square-alt ms-1"></i></a>.</p>]]></description></item><item><title>Some Thoughts on Shiny Open Source - Internal Load Balancing</title><link>https://jaehyeon.me/blog/2016-05-23-shiny-open-source-internal-load-balancing/</link><pubDate>Mon, 23 May 2016 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2016-05-23-shiny-open-source-internal-load-balancing/</guid><description>&lt;p>Shiny is an interesting web framework that helps create a web application quickly. If it targets a large number of users, however, there are several limitations and it is so true when the open source version of Shiny is in use. It would be possible to tackle down some of the limitations with the enterprise version but it is not easy to see enough examples of Shiny applications in production environment. While whether Shiny can be used in production environment is a controversial issue, this series of posts illustrate some ways to use &lt;strong>open source Shiny&lt;/strong> a bit more wisely. Specifically the following topics are going to be covered.&lt;/p></description></item><item><title>Asynchronous Processing Using Job Queue</title><link>https://jaehyeon.me/blog/2016-05-12-asynchronous-processing-using-job-queue/</link><pubDate>Thu, 12 May 2016 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2016-05-12-asynchronous-processing-using-job-queue/</guid><description><![CDATA[<p>In this post, a way to overcome one of R&rsquo;s limitations (<strong>lack of multi-threading</strong>) is discussed by job queuing using the <a href="http://jobqueue.r-forge.r-project.org/" target="_blank" rel="noopener noreferrer">jobqueue package<i class="fas fa-external-link-square-alt ms-1"></i></a> - a generic asynchronous job queue implementation for R. See the package description below.</p>
<blockquote>
<p>The jobqueue package is meant to provide an easy-to-use interface that allows to queue computations for background evaluation while the calling R session remains responsive. It is based on a <em>1-node socket cluster from the parallel package</em>. The package provides a way to do basic threading in R. The main focus of the package is on an intuitive and easy-to-use interface for the job queue programming construct. &hellip; Typical applications include: <strong>background computation of lengthy tasks (such as data sourcing, model fitting, bootstrapping), simple/interactive parallelization (if you have 5 different jobs, move them to up to 5 different job queues), and concurrent task scheduling in more complicated R programs.</strong> &hellip;</p>]]></description></item><item><title>Boost SparkR with Hive</title><link>https://jaehyeon.me/blog/2016-04-30-boost-sparkr-with-hive/</link><pubDate>Sat, 30 Apr 2016 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2016-04-30-boost-sparkr-with-hive/</guid><description><![CDATA[<p>In the <a href="/blog/2016-03-02-quick-start-sparkr-in-local-and-cluster-mode">previous post</a>, it is demonstrated how to start SparkR in local and cluster mode. While SparkR is in active development, it is yet to fully support Spark&rsquo;s key libraries such as MLlib and Spark Streaming. Even, as a data processing engine, this R API is still limited as it is not possible to manipulate RDDs directly but only via Spark SQL/DataFrame API. As can be checked in the <a href="http://spark.apache.org/docs/latest/api/R/index.html" target="_blank" rel="noopener noreferrer">API doc<i class="fas fa-external-link-square-alt ms-1"></i></a>, SparkR rebuilds many existing R functions to work with Spark DataFrame and notably it borrows some functions from the dplyr package. Also there are some alien functions (eg <code>from_utc_timestamp()</code>) and many of them are from <a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual" target="_blank" rel="noopener noreferrer">Hive Query Language (HiveQL)<i class="fas fa-external-link-square-alt ms-1"></i></a>. In relation to those functions from HiveQL, although some Hive user defined functions (UDFs) are ported, still many useful <a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual&#43;UDF" target="_blank" rel="noopener noreferrer">UDFs<i class="fas fa-external-link-square-alt ms-1"></i></a> and <a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual&#43;WindowingAndAnalytics" target="_blank" rel="noopener noreferrer">Window functions<i class="fas fa-external-link-square-alt ms-1"></i></a> don&rsquo;t exist.</p>]]></description></item><item><title>Quick Start SparkR in Local and Cluster Mode</title><link>https://jaehyeon.me/blog/2016-03-02-quick-start-sparkr-in-local-and-cluster-mode/</link><pubDate>Wed, 02 Mar 2016 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2016-03-02-quick-start-sparkr-in-local-and-cluster-mode/</guid><description><![CDATA[<p>In the <a href="/blog/2016-02-22-spark-cluster-setup-on-virtualbox">previous post</a>, a Spark cluster is set up using 2 VirtualBox Ubuntu guests. While this is a viable option for many, it is not always for others. For those who find setting-up such a cluster is not convenient, there&rsquo;s still another option, which is relying on the local mode of Spark. In this post, a <a href="https://bitbucket.org/jaehyeon/sparkr-test" target="_blank" rel="noopener noreferrer"><strong>BitBucket repository</strong><i class="fas fa-external-link-square-alt ms-1"></i></a> is introduced, which is a R project that includes <em>Spark 1.6.0 Pre-built for Hadoop 2.0 and later</em> and <em>hadoop-common 2.2.0</em> - the latter is necessary if it is tested on Windows. Then several initialization steps are discussed such as setting-up environment variables and library path as well as including the <a href="https://github.com/databricks/spark-csv" target="_blank" rel="noopener noreferrer">spark-csv package<i class="fas fa-external-link-square-alt ms-1"></i></a> and a JDBC driver. Finally it shows some examples of reading JSON and CSV files in the cluster mode.</p>]]></description></item><item><title>Spark Cluster Setup on VirtualBox</title><link>https://jaehyeon.me/blog/2016-02-22-spark-cluster-setup-on-virtualbox/</link><pubDate>Mon, 22 Feb 2016 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2016-02-22-spark-cluster-setup-on-virtualbox/</guid><description><![CDATA[<p>We discuss how to set up a Spark cluser between 2 Ubuntu guests. Firstly it begins with machine preparation. Once a machine is baked, its image file (<em>VDI</em>) is be copied for the second one. Then how to launch a cluster by <a href="http://spark.apache.org/docs/latest/spark-standalone.html" target="_blank" rel="noopener noreferrer">standalone mode<i class="fas fa-external-link-square-alt ms-1"></i></a> is discussed. Let&rsquo;s get started.</p>

<h2 id="machine-preparation" data-numberify>Machine preparation<a class="anchor ms-1" href="#machine-preparation"></a></h2>
<p>If you haven&rsquo;t read the previous post, I recommend reading as it introduces <a href="http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html" target="_blank" rel="noopener noreferrer">Putty<i class="fas fa-external-link-square-alt ms-1"></i></a> as well. Also, as Spark need Java Development Kit (JDK), you may need to <em>apt-get</em> it first - see <a href="https://www.digitalocean.com/community/tutorials/how-to-install-java-on-ubuntu-with-apt-get" target="_blank" rel="noopener noreferrer">this tutorial<i class="fas fa-external-link-square-alt ms-1"></i></a> for further details.</p>]]></description></item><item><title>Quick Test to Wrap Python in R</title><link>https://jaehyeon.me/blog/2015-11-21-quick-test-to-wrap-python-in-r/</link><pubDate>Sat, 21 Nov 2015 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2015-11-21-quick-test-to-wrap-python-in-r/</guid><description><![CDATA[<p>As mentioned in an <a href="/blog/2015-08-09-some-thoughts-on-python-for-r-users">earlier post</a>, things that are not easy in R can be relatively simple in other languages. Another example would be connecting to Amazon Web Services. In relation to s3, although there are a number of existing packages, many of them seem to be deprecated, premature or platform-dependent. (I consider the <a href="https://cloudyr.github.io/" target="_blank" rel="noopener noreferrer">cloudyr<i class="fas fa-external-link-square-alt ms-1"></i></a> project looks promising though.)</p>
<p>If there isn&rsquo;t a comprehensive <em>R-way</em> of doing something yet, it may be necessary to create it from scratch. Actually there are some options to do so by using <a href="https://aws.amazon.com/cli/" target="_blank" rel="noopener noreferrer">AWS Command Line Interface<i class="fas fa-external-link-square-alt ms-1"></i></a>, <a href="http://docs.aws.amazon.com/AmazonS3/latest/API/APIRest.html" target="_blank" rel="noopener noreferrer">AWS REST API<i class="fas fa-external-link-square-alt ms-1"></i></a> or wrapping functionality of another language.</p>]]></description></item><item><title>Some Thoughts on Python for R Users</title><link>https://jaehyeon.me/blog/2015-08-09-some-thoughts-on-python-for-r-users/</link><pubDate>Sun, 09 Aug 2015 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2015-08-09-some-thoughts-on-python-for-r-users/</guid><description><![CDATA[<p>There seem to be growing interest in Python in the R cummunity. While there can be a range of opinions about using R over Python (or vice versa) for exploratory data analysis, fitting statistical/machine learning algorithms and so on, I consider one of the strongest attractions of using Python comes from the fact that <em>Python is a general purpose programming language</em>. As more developers are involved in, it can provide a way to get jobs done easily, which can be tricky in R. In this article, an example is introduced by illustrating how to connect to <a href="https://en.wikipedia.org/wiki/SOAP" target="_blank" rel="noopener noreferrer">SOAP (Simple Object Access Protocol)<i class="fas fa-external-link-square-alt ms-1"></i></a> web services.</p>]]></description></item><item><title>Setup Random Seeds on Caret Package</title><link>https://jaehyeon.me/blog/2015-05-30-setup-random-seeds-on-caret-package/</link><pubDate>Sat, 30 May 2015 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2015-05-30-setup-random-seeds-on-caret-package/</guid><description><![CDATA[<p>A short while ago I had a chance to perform analysis using the <strong>caret</strong> package. One of the requirements is to run it parallelly and to work in both Windows and Linux. The requirement can be met by using the <strong>parallel</strong> and <strong>doParallel</strong> packages as the <strong>caret</strong> package trains a model using the <strong>foreach</strong> package if clusters are registered by the <strong>doParallel</strong> package - further details about how to implement parallel processing on a single machine can be found in earlier posts (<a href="/blog/2015-02-01-tree-based-methods-1">Link 1</a>, <a href="/blog/2015-02-08-tree-based-methods-2">Link 2</a> and <a href="/blog/2015-02-14-tree-based-methods-3">Link 3</a>). While it is relatively straightforward to train a model across multiple clusters using the <strong>caret</strong> package, setting up random seeds may be a bit tricky. As analysis can be more reproducible by random seeds, a way of setting them up is illustrated using a simple function in this post.</p>]]></description></item><item><title>Packaging Analysis</title><link>https://jaehyeon.me/blog/2015-03-24-packaging-analysis/</link><pubDate>Tue, 24 Mar 2015 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2015-03-24-packaging-analysis/</guid><description><![CDATA[<p>When I imagine a workflow, it is performing the same or similar tasks regularly (daily or weekly) in an automated way. Although those tasks can be executed in a script or a *source()*d script, it may not be easy to maintain separate scripts while the size of tasks gets bigger or if they have to be executed in different machines. In academia, reproducible research shares similar ideas but the level of reproducibility introduced in <a href="https://christophergandrud.github.io/RepResR-RStudio/" target="_blank" rel="noopener noreferrer">Gandrud, 2013<i class="fas fa-external-link-square-alt ms-1"></i></a> may not suffice in a business environment as the focus is documenting in a reproducible way. A R package, however, can be an effective tool and it can be considered like a portable class library in C# or Java. Like a class library, it can include a set of necessary tasks (usually using functions) and, being portable, its dependency can be managed well - for example, it is possible to set so that dependent packages can also be installed if some of them are not installed already. Moreover the benefit of creating a R package would be significant if it has to be deployed in a production server as it&rsquo;d be a lot easier to convince system admin with the built-in unit tests, object documents and package vignettes. In this article an example of creating a R package is illustrated.</p>]]></description></item><item><title>Parallel Processing on Single Machine - Part III</title><link>https://jaehyeon.me/blog/2015-03-19-parallel-processing-on-single-machine-3/</link><pubDate>Thu, 19 Mar 2015 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2015-03-19-parallel-processing-on-single-machine-3/</guid><description><![CDATA[<p>In the <a href="/blog/2015-03-17-parallel-processing-on-single-machine-2">previous posts</a>, two groups of ways to implement parallel processing on a single machine are introduced. The first group is provided by the <strong>snow</strong> or <strong>parallel</strong> package and the functions are an extension of <code>lapply()</code> (<a href="/blog/2015-03-14-parallel-processing-on-single-machine-1">LINK</a>). The second group is based on an extension of the <em>for</em> construct (<em>foreach</em>, <em>%dopar%</em> and <em>%:%</em>). The <em>foreach</em> construct is provided by the <em>foreach</em> package while clusters are made and registered by the <strong>parallel</strong> and <strong>doParallel</strong> packages respectively (<a href="/blog/2015-03-17-parallel-processing-on-single-machine-2">LINK</a>). To conclude this series, three practical examples are discussed for comparison in this article.</p>]]></description></item><item><title>Parallel Processing on Single Machine - Part II</title><link>https://jaehyeon.me/blog/2015-03-17-parallel-processing-on-single-machine-2/</link><pubDate>Tue, 17 Mar 2015 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2015-03-17-parallel-processing-on-single-machine-2/</guid><description><![CDATA[<p>In the <a href="/blog/2015-03-14-parallel-processing-on-single-machine-1">previous article</a>, parallel processing on a single machine using the <strong>snow</strong> and <strong>parallel</strong> packages are introduced. The four functions are an extension of <code>lapply()</code> with an additional argument that specifies a cluster object. In spite of their effectiveness and ease of use, there may be cases where creating a function that can be sent into clusters is not easy or looping may be more natural. In this article, another way of implementing parallel processing on a single machine is introduced using the <strong>foreach</strong> and <strong>doParallel</strong> packages where clusters are created by the <strong>parallel</strong> package. Finally the <strong>iterators</strong> package is briefly covered as it can facilitate writing a loop. The examples here are largely based on the individual packages&rsquo; vignettes and further details can be found there.</p>]]></description></item><item><title>Parallel Processing on Single Machine - Part I</title><link>https://jaehyeon.me/blog/2015-03-14-parallel-processing-on-single-machine-1/</link><pubDate>Sat, 14 Mar 2015 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2015-03-14-parallel-processing-on-single-machine-1/</guid><description><![CDATA[<p>Lack of multi-threading and memory limitation are two outstanding weaknesses of base R. In fact, however, if the size of data is not so large that it can be read in RAM, the former would be relatively easily handled by parallel processing, provided that multiple processors are equipped. This article introduces to a way of implementing parallel processing on a single machine using the <strong>snow</strong> and <strong>parallel</strong> packages - the examples are largely based on <a href="http://shop.oreilly.com/product/0636920021421.do" target="_blank" rel="noopener noreferrer">McCallum and Weston (2012)<i class="fas fa-external-link-square-alt ms-1"></i></a>.</p>]]></description></item><item><title>Tree Based Methods in R - Part VI</title><link>https://jaehyeon.me/blog/2015-03-07-tree-based-methods-6/</link><pubDate>Sat, 07 Mar 2015 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2015-03-07-tree-based-methods-6/</guid><description><![CDATA[<ul>
<li><a href="/blog/2015-02-01-tree-based-methods-1">Part I</a></li>
<li><a href="/blog/2015-02-08-tree-based-methods-2">Part II</a></li>
<li><a href="/blog/2015-02-14-tree-based-methods-3">Part III</a></li>
<li><a href="/blog/2015-02-15-tree-based-methods-4">Part IV</a></li>
<li><a href="/blog/2015-03-05-tree-based-methods-5">Part V</a></li>
<li><a href="/blog/2015-03-07-tree-based-methods-6/#">Part VI</a> (this post)</li>
</ul>
<p>A regression tree is evaluated using bagged trees in the <a href="/blog/2015-03-05-tree-based-methods-5">previous article</a>. In this article, the response variable of the same data set is converted into a binary factor variable and a classification tree is evaluated by comparing to bagged trees&rsquo; individual oob/test errors, cumulative oob/test errors and variable importance measures.</p>
<p>Before getting started, note that the source of the classes can be found in <a href="https://gist.github.com/jaehyeon-kim/b89dcbd2fb0b84fd236e" target="_blank" rel="noopener noreferrer">this gist<i class="fas fa-external-link-square-alt ms-1"></i></a> and, together with the relevant packages (see <em>tags</em>), it requires a utility function (<code>bestParam()</code>) that can be found <a href="https://gist.github.com/jaehyeon-kim/5622ae9fa982e0b46550" target="_blank" rel="noopener noreferrer">here<i class="fas fa-external-link-square-alt ms-1"></i></a>.</p>]]></description></item><item><title>Tree Based Methods in R - Part V</title><link>https://jaehyeon.me/blog/2015-03-05-tree-based-methods-5/</link><pubDate>Thu, 05 Mar 2015 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2015-03-05-tree-based-methods-5/</guid><description><![CDATA[<ul>
<li><a href="/blog/2015-02-01-tree-based-methods-1">Part I</a></li>
<li><a href="/blog/2015-02-08-tree-based-methods-2">Part II</a></li>
<li><a href="/blog/2015-02-14-tree-based-methods-3">Part III</a></li>
<li><a href="/blog/2015-02-15-tree-based-methods-4">Part IV</a></li>
<li><a href="/blog/2015-03-05-tree-based-methods-5/#">Part V</a> (this post)</li>
<li><a href="/blog/2015-03-07-tree-based-methods-6">Part VI</a></li>
</ul>
<p>This article evaluates a single regression tree&rsquo;s performance by comparing to bagged trees&rsquo; individual oob/test errors, cumulative oob/test errors and variable importance measures.</p>
<p>Before getting started, note that the source of the classes can be found in <a href="https://gist.github.com/jaehyeon-kim/b89dcbd2fb0b84fd236e" target="_blank" rel="noopener noreferrer">this gist<i class="fas fa-external-link-square-alt ms-1"></i></a> and, together with the relevant packages (see <em>tags</em>), it requires a utility function (<code>bestParam()</code>) that can be found <a href="https://gist.github.com/jaehyeon-kim/5622ae9fa982e0b46550" target="_blank" rel="noopener noreferrer">here<i class="fas fa-external-link-square-alt ms-1"></i></a>.</p>]]></description></item><item><title>Tree Based Methods in R - Part IV</title><link>https://jaehyeon.me/blog/2015-02-15-tree-based-methods-4/</link><pubDate>Sun, 15 Feb 2015 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2015-02-15-tree-based-methods-4/</guid><description><![CDATA[<ul>
<li><a href="/blog/2015-02-01-tree-based-methods-1">Part I</a></li>
<li><a href="/blog/2015-02-08-tree-based-methods-2">Part II</a></li>
<li><a href="/blog/2015-02-14-tree-based-methods-3">Part III</a></li>
<li><a href="/blog/2015-02-15-tree-based-methods-4/#">Part IV</a> (this post)</li>
<li><a href="/blog/2015-03-05-tree-based-methods-5">Part V</a></li>
<li><a href="/blog/2015-03-07-tree-based-methods-6">Part VI</a></li>
</ul>
<p>While the last three articles illustrated the CART model for both classification (with equal/unequal costs) and regression tasks, this article is rather technical as it compares three packages: <strong>rpart</strong>, <strong>caret</strong> and <strong>mlr</strong>. For those who are not familiar with the last two packages, they are wrappers (or frameworks) that implement a range of models (or algorithms) in a unified way. They are useful because inconsistent API could be a drawback of R (like other open source tools) and it would be quite beneficial if there is a way to implement different models in a standardized way. In line with the earlier articles, the <em>Carseats</em> data is used for a classification task.</p>]]></description></item><item><title>Tree Based Methods in R - Part III</title><link>https://jaehyeon.me/blog/2015-02-14-tree-based-methods-3/</link><pubDate>Sat, 14 Feb 2015 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2015-02-14-tree-based-methods-3/</guid><description><![CDATA[<ul>
<li><a href="/blog/2015-02-01-tree-based-methods-1">Part I</a></li>
<li><a href="/blog/2015-02-08-tree-based-methods-2">Part II</a></li>
<li><a href="/blog/2015-02-14-tree-based-methods-3/#">Part III</a> (this post)</li>
<li><a href="/blog/2015-02-15-tree-based-methods-4">Part IV</a></li>
<li><a href="/blog/2015-03-05-tree-based-methods-5">Part V</a></li>
<li><a href="/blog/2015-03-07-tree-based-methods-6">Part VI</a></li>
</ul>
<p>While classification tasks are implemented in the last two articles (<a href="/blog/2015-02-01-tree-based-methods-1">Part I</a> and <a href="/blog/2015-02-08-tree-based-methods-2">Part II</a>), a regression task is the topic of this article. While the <strong>caret</strong> package selects the tuning parameter (<em>cp</em>) that minimizes the error (<em>RMSE</em>), the <strong>rpart</strong> packages recommends the <em>1-SE rule</em>, which selects the smallest tree within 1 standard error of the minimum cross validation error (<em>xerror</em>). The models with 2 complexity parameters that are suggested by the packages are compared.</p>]]></description></item><item><title>Tree Based Methods in R - Part II</title><link>https://jaehyeon.me/blog/2015-02-08-tree-based-methods-2/</link><pubDate>Sun, 08 Feb 2015 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2015-02-08-tree-based-methods-2/</guid><description><![CDATA[<ul>
<li><a href="/blog/2015-02-01-tree-based-methods-1">Part I</a></li>
<li><a href="/blog/2015-02-08-tree-based-methods-2/#">Part II</a> (this post)</li>
<li><a href="/blog/2015-02-14-tree-based-methods-3">Part III</a></li>
<li><a href="/blog/2015-02-15-tree-based-methods-4">Part IV</a></li>
<li><a href="/blog/2015-03-05-tree-based-methods-5">Part V</a></li>
<li><a href="/blog/2015-03-07-tree-based-methods-6">Part VI</a></li>
</ul>
<p>In the previous article (<a href="/blog/2015-02-01-tree-based-methods-1">Tree Based Methods in R - Part I</a>), a decision tree is created on the <em>Carseats</em> data which is in the chapter 8 lab of <a href="https://www.statlearning.com/" target="_blank" rel="noopener noreferrer">ISLR<i class="fas fa-external-link-square-alt ms-1"></i></a>. In that article, potentially asymetric costs due to misclassification are not taken into account. When unbalance between false positive and false negative can have a significant impact, it can be explicitly adjusted either by altering prior (or empirical) probabilities or by adding a loss matrix.</p>]]></description></item><item><title>Tree Based Methods in R - Part I</title><link>https://jaehyeon.me/blog/2015-02-01-tree-based-methods-1/</link><pubDate>Sun, 01 Feb 2015 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2015-02-01-tree-based-methods-1/</guid><description><![CDATA[<ul>
<li><a href="/blog/2015-02-01-tree-based-methods-1/#">Part I</a> (this post)</li>
<li><a href="/blog/2015-02-08-tree-based-methods-2">Part II</a></li>
<li><a href="/blog/2015-02-14-tree-based-methods-3">Part III</a></li>
<li><a href="/blog/2015-02-15-tree-based-methods-4">Part IV</a></li>
<li><a href="/blog/2015-03-05-tree-based-methods-5">Part V</a></li>
<li><a href="/blog/2015-03-07-tree-based-methods-6">Part VI</a></li>
</ul>
<p>This is the first article about tree based methods using R. <em>Carseats</em> data in the chapter 8 lab of <a href="https://www.statlearning.com/" target="_blank" rel="noopener noreferrer">ISLR<i class="fas fa-external-link-square-alt ms-1"></i></a> is used to perform classification analysis. Unlike the lab example, the <strong>rpart</strong> package is used to fit the CART model on the data and the <strong>caret</strong> package is used for tuning the pruning parameter (<code>cp</code>).</p>]]></description></item><item><title>Quick Trial of Adding Column</title><link>https://jaehyeon.me/blog/2015-01-14-quick-trial-of-adding-column/</link><pubDate>Wed, 14 Jan 2015 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2015-01-14-quick-trial-of-adding-column/</guid><description><![CDATA[<p>This is a quick trial of adding overall and conditional (by user) average columns in a data frame. <code>base</code>,<code>plyr</code>,<code>dplyr</code>,<code>data.table</code>,<code>dplyr + data.table</code> packages are used. Personally I perfer <code>dplyr + data.table</code> - <code>dplyr</code> for comperhensive syntax and <code>data.table</code> for speed.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-r" data-lang="r"><span class="line"><span class="ln"> 1</span><span class="cl"><span class="c1">## set up variables</span>
</span></span><span class="line"><span class="ln"> 2</span><span class="cl"><span class="n">size</span> <span class="o">&lt;-</span> <span class="m">36000</span>
</span></span><span class="line"><span class="ln"> 3</span><span class="cl"><span class="n">numUsers</span> <span class="o">&lt;-</span> <span class="m">4900</span>
</span></span><span class="line"><span class="ln"> 4</span><span class="cl"><span class="c1"># roughly each user has 7 sessions</span>
</span></span><span class="line"><span class="ln"> 5</span><span class="cl"><span class="n">numSessions</span> <span class="o">&lt;-</span> <span class="p">(</span><span class="n">numUsers</span> <span class="o">/</span> <span class="m">7</span><span class="p">)</span> <span class="o">-</span> <span class="p">((</span><span class="n">numUsers</span> <span class="o">/</span> <span class="m">7</span><span class="p">)</span> <span class="o">%%</span> <span class="m">1</span><span class="p">)</span>
</span></span><span class="line"><span class="ln"> 6</span><span class="cl"> 
</span></span><span class="line"><span class="ln"> 7</span><span class="cl"><span class="c1">## create data frame</span>
</span></span><span class="line"><span class="ln"> 8</span><span class="cl"><span class="nf">set.seed</span><span class="p">(</span><span class="m">123457</span><span class="p">)</span>
</span></span><span class="line"><span class="ln"> 9</span><span class="cl"><span class="n">userIds</span> <span class="o">&lt;-</span> <span class="nf">sample.int</span><span class="p">(</span><span class="n">numUsers</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">size</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">TRUE</span><span class="p">)</span>
</span></span><span class="line"><span class="ln">10</span><span class="cl"><span class="n">ssIds</span> <span class="o">&lt;-</span> <span class="nf">sample.int</span><span class="p">(</span><span class="n">numSessions</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">size</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">TRUE</span><span class="p">)</span>
</span></span><span class="line"><span class="ln">11</span><span class="cl"><span class="n">scores</span> <span class="o">&lt;-</span> <span class="nf">sample.int</span><span class="p">(</span><span class="m">10</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">size</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">TRUE</span><span class="p">)</span> 
</span></span><span class="line"><span class="ln">12</span><span class="cl">
</span></span><span class="line"><span class="ln">13</span><span class="cl"><span class="n">preDf</span> <span class="o">&lt;-</span> <span class="nf">data.frame</span><span class="p">(</span><span class="n">User</span><span class="o">=</span><span class="n">userIds</span><span class="p">,</span> <span class="n">Session</span><span class="o">=</span><span class="n">ssIds</span><span class="p">,</span> <span class="n">Score</span><span class="o">=</span><span class="n">scores</span><span class="p">)</span>
</span></span><span class="line"><span class="ln">14</span><span class="cl"><span class="n">preDf</span><span class="o">$</span><span class="n">User</span> <span class="o">&lt;-</span> <span class="nf">as.factor</span><span class="p">(</span><span class="n">preDf</span><span class="o">$</span><span class="n">User</span><span class="p">)</span> 
</span></span></code></pre></div>
<h2 id="adding-overall-average" data-numberify>Adding overall average<a class="anchor ms-1" href="#adding-overall-average"></a></h2>
<p>As calculating overall average is not complicated, I don&rsquo;t find a big difference among packages.</p>]]></description></item><item><title>Looping without for</title><link>https://jaehyeon.me/blog/2014-12-17-looping-without-for/</link><pubDate>Wed, 17 Dec 2014 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2014-12-17-looping-without-for/</guid><description><![CDATA[<p>Purely programming point of view, I consider <strong>for-loops</strong> would be better to be avoided in R as</p>
<ul>
<li>the script can be more readable</li>
<li>it is easier to handle errors</li>
</ul>
<p>Some articles on the web indicate that looping functions (or apply family of functions) don&rsquo;t guarantee faster execution and sometimes even slower. Although, assuming that the experiments are correct, in my opinion, code readability itself is beneficial enough to avoid for-loops. Even worse, R&rsquo;s dynamic typing system coupled with poor readability can result in a frustrating consequence as the code grows.</p>]]></description></item><item><title>Short R Examples</title><link>https://jaehyeon.me/blog/2014-12-03-short-r-examples/</link><pubDate>Wed, 03 Dec 2014 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2014-12-03-short-r-examples/</guid><description><![CDATA[<p>As I don&rsquo;t use R at work yet, I haven&rsquo;t got enough chances to learn R by doing. Together with teaching myself machine learning with R, I consider it would be a good idea to collect examples on the web. Recently I have been visiting a Linkedin group called <a href="http://www.linkedin.com/groups/R-Project-Statistical-Computing-77616?home=&amp;gid=77616&amp;trk=anet_ug_hm" target="_blank" rel="noopener noreferrer">The R Project for Statistical Computing<i class="fas fa-external-link-square-alt ms-1"></i></a> and suggesting scripts on data manipulation or programming topics. Actually I expected some of the topics may also be helpful to learn machine learning/statistics but, possibly due to lack of understaning of the topics in context, I&rsquo;ve found only a few - it may be necessary to look for another source. Anyway below is a summary of two examples.</p>]]></description></item><item><title>Summarise Stock Returns from Multiple Files</title><link>https://jaehyeon.me/blog/2014-11-27-summarise-stock-returns-from-multiple-files/</link><pubDate>Thu, 27 Nov 2014 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2014-11-27-summarise-stock-returns-from-multiple-files/</guid><description><![CDATA[<p>This post is a slight extension of the previous two articles (<a href="/blog/2014-11-20-download-stock-data-1">Download Stock Data - Part I</a>, <a href="/blog/2014-11-21-download-stock-data-2">Download Stock Data - Part II</a>) and we discuss how to produce gross returns, standard deviation and correlation of multiple shares.</p>
<p>The following packages are used.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-r" data-lang="r"><span class="line"><span class="ln">1</span><span class="cl"><span class="nf">library</span><span class="p">(</span><span class="n">knitr</span><span class="p">)</span>
</span></span><span class="line"><span class="ln">2</span><span class="cl"><span class="nf">library</span><span class="p">(</span><span class="n">lubridate</span><span class="p">)</span>
</span></span><span class="line"><span class="ln">3</span><span class="cl"><span class="nf">library</span><span class="p">(</span><span class="n">stringr</span><span class="p">)</span>
</span></span><span class="line"><span class="ln">4</span><span class="cl"><span class="nf">library</span><span class="p">(</span><span class="n">reshape2</span><span class="p">)</span>
</span></span><span class="line"><span class="ln">5</span><span class="cl"><span class="nf">library</span><span class="p">(</span><span class="n">plyr</span><span class="p">)</span>
</span></span><span class="line"><span class="ln">6</span><span class="cl"><span class="nf">library</span><span class="p">(</span><span class="n">dplyr</span><span class="p">)</span>
</span></span></code></pre></div><p>The script begins with creating a data folder in the format of <em>data_YYYY-MM-DD</em>.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-r" data-lang="r"><span class="line"><span class="ln">1</span><span class="cl"><span class="c1"># create data folder</span>
</span></span><span class="line"><span class="ln">2</span><span class="cl"><span class="n">dataDir</span> <span class="o">&lt;-</span> <span class="nf">paste0</span><span class="p">(</span><span class="s">&#34;data&#34;</span><span class="p">,</span><span class="s">&#34;_&#34;</span><span class="p">,</span><span class="nf">format</span><span class="p">(</span><span class="nf">Sys.Date</span><span class="p">(),</span><span class="s">&#34;%Y-%m-%d&#34;</span><span class="p">))</span>
</span></span><span class="line"><span class="ln">3</span><span class="cl"><span class="kr">if</span><span class="p">(</span><span class="nf">file.exists</span><span class="p">(</span><span class="n">dataDir</span><span class="p">))</span> <span class="p">{</span>
</span></span><span class="line"><span class="ln">4</span><span class="cl">  <span class="nf">unlink</span><span class="p">(</span><span class="n">dataDir</span><span class="p">,</span> <span class="n">recursive</span> <span class="o">=</span> <span class="kc">TRUE</span><span class="p">)</span>
</span></span><span class="line"><span class="ln">5</span><span class="cl">  <span class="nf">dir.create</span><span class="p">(</span><span class="n">dataDir</span><span class="p">)</span>
</span></span><span class="line"><span class="ln">6</span><span class="cl"><span class="p">}</span> <span class="kr">else</span> <span class="p">{</span>
</span></span><span class="line"><span class="ln">7</span><span class="cl">  <span class="nf">dir.create</span><span class="p">(</span><span class="n">dataDir</span><span class="p">)</span>
</span></span><span class="line"><span class="ln">8</span><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><p>Given company codes, URLs and file paths are created. Then data files are downloaded by <code>Map</code>, which is a wrapper of <code>mapply</code>. Note that R&rsquo;s <code>download.file</code> function is wrapped by <code>downloadFile</code> so that the function does not break when an error occurs.</p>]]></description></item><item><title>Download Stock Data - Part II</title><link>https://jaehyeon.me/blog/2014-11-21-download-stock-data-2/</link><pubDate>Fri, 21 Nov 2014 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2014-11-21-download-stock-data-2/</guid><description><![CDATA[<p>In an <a href="/blog/2014-11-20-download-stock-data-1">earlier article</a>, a way to download stock price data files from Google, save it into a local drive and merge them into a single data frame. If files are not large, however, it wouldn&rsquo;t be effective and, in this article, files are downloaded and merged internally.</p>
<p>The following packages are used.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-r" data-lang="r"><span class="line"><span class="ln">1</span><span class="cl"><span class="nf">library</span><span class="p">(</span><span class="n">knitr</span><span class="p">)</span>
</span></span><span class="line"><span class="ln">2</span><span class="cl"><span class="nf">library</span><span class="p">(</span><span class="n">lubridate</span><span class="p">)</span>
</span></span><span class="line"><span class="ln">3</span><span class="cl"><span class="nf">library</span><span class="p">(</span><span class="n">stringr</span><span class="p">)</span>
</span></span><span class="line"><span class="ln">4</span><span class="cl"><span class="nf">library</span><span class="p">(</span><span class="n">plyr</span><span class="p">)</span>
</span></span><span class="line"><span class="ln">5</span><span class="cl"><span class="nf">library</span><span class="p">(</span><span class="n">dplyr</span><span class="p">)</span>
</span></span></code></pre></div><p>Taking urls as file locations, files are directly read using <code>llply</code> and they are combined using <code>rbind_all</code>. As the merged data has multiple stocks&rsquo; records, <code>Code</code> column is created. Note that, when an error occurrs, the function returns a dummy data frame in order not to break the loop - values of the dummy data frame(s) are filtered out at the end.</p>]]></description></item><item><title>Download Stock Data - Part I</title><link>https://jaehyeon.me/blog/2014-11-20-download-stock-data-1/</link><pubDate>Thu, 20 Nov 2014 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2014-11-20-download-stock-data-1/</guid><description><![CDATA[<p>This article illustrates how to download stock price data files from Google, save it into a local drive and merge them into a single data frame. This script is slightly modified from a script which downloads RStudio package download log data. The original source can be found <a href="https://github.com/hadley/cran-logs-dplyr/blob/master/1-download.r" target="_blank" rel="noopener noreferrer">here<i class="fas fa-external-link-square-alt ms-1"></i></a>.</p>
<p>First of all, the following three packages are used.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-r" data-lang="r"><span class="line"><span class="ln">1</span><span class="cl"><span class="nf">library</span><span class="p">(</span><span class="n">knitr</span><span class="p">)</span>
</span></span><span class="line"><span class="ln">2</span><span class="cl"><span class="nf">library</span><span class="p">(</span><span class="n">lubridate</span><span class="p">)</span>
</span></span><span class="line"><span class="ln">3</span><span class="cl"><span class="nf">library</span><span class="p">(</span><span class="n">stringr</span><span class="p">)</span>
</span></span><span class="line"><span class="ln">4</span><span class="cl"><span class="nf">library</span><span class="p">(</span><span class="n">plyr</span><span class="p">)</span>
</span></span><span class="line"><span class="ln">5</span><span class="cl"><span class="nf">library</span><span class="p">(</span><span class="n">dplyr</span><span class="p">)</span>
</span></span></code></pre></div><p>The script begins with creating a folder to save data files.</p>]]></description></item></channel></rss>