<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Apache Hudi on Jaehyeon Kim</title><link>https://jaehyeon.me/tags/apache-hudi/</link><description>Recent content in Apache Hudi on Jaehyeon Kim</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>Copyright © 2023-2024 Jaehyeon Kim. All Rights Reserved.</copyright><lastBuildDate>Sun, 19 Dec 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://jaehyeon.me/tags/apache-hudi/index.xml" rel="self" type="application/rss+xml"/><item><title>Data Lake Demo using Change Data Capture (CDC) on AWS – Part 3 Implement Data Lake</title><link>https://jaehyeon.me/blog/2021-12-19-datalake-demo-part3/</link><pubDate>Sun, 19 Dec 2021 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2021-12-19-datalake-demo-part3/</guid><description><![CDATA[<p>In the <a href="/blog/2021-12-12-datalake-demo-part2">previous post</a>, we created a VPC that has private and public subnets in 2 availability zones in order to build and deploy the data lake solution on AWS. NAT instances are created to forward outbound traffic to the internet and a VPN bastion host is set up to facilitate deployment. An <a href="https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.AuroraPostgreSQL.html" target="_blank" rel="noopener noreferrer">Aurora PostgreSQL<i class="fas fa-external-link-square-alt ms-1"></i></a> cluster is deployed to host the source database and a Python command line app is used to create the database. To develop data ingestion using CDC, an <a href="https://aws.amazon.com/msk/" target="_blank" rel="noopener noreferrer">Amazon MSK<i class="fas fa-external-link-square-alt ms-1"></i></a> cluster is deployed and the <a href="https://debezium.io/documentation/reference/stable/connectors/postgresql.html" target="_blank" rel="noopener noreferrer">Debezium source<i class="fas fa-external-link-square-alt ms-1"></i></a> and <a href="https://lenses.io/blog/2020/11/new-kafka-to-S3-connector/" target="_blank" rel="noopener noreferrer">Lenses S3 sink<i class="fas fa-external-link-square-alt ms-1"></i></a> connectors are created on <a href="https://aws.amazon.com/msk/features/msk-connect/" target="_blank" rel="noopener noreferrer">MSK Connect<i class="fas fa-external-link-square-alt ms-1"></i></a>. We also confirmed the order creation and update events are captured as expected. As the last part of this series, we&rsquo;ll build an <a href="https://hudi.apache.org/docs/writing_data/#deltastreamer" target="_blank" rel="noopener noreferrer">Apache Hudi DeltaStreamer<i class="fas fa-external-link-square-alt ms-1"></i></a> app on <a href="https://aws.amazon.com/emr/" target="_blank" rel="noopener noreferrer">Amazon EMR<i class="fas fa-external-link-square-alt ms-1"></i></a> and use the resulting Hudi table with <a href="https://aws.amazon.com/athena/" target="_blank" rel="noopener noreferrer">Amazon Athena<i class="fas fa-external-link-square-alt ms-1"></i></a> and <a href="https://aws.amazon.com/quicksight/" target="_blank" rel="noopener noreferrer">Amazon Quicksight<i class="fas fa-external-link-square-alt ms-1"></i></a> to build a dashboard.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2021-12-19-datalake-demo-part3/featured.png" length="164526" type="image/png"/></item><item><title>Data Lake Demo using Change Data Capture (CDC) on AWS – Part 2 Implement CDC</title><link>https://jaehyeon.me/blog/2021-12-12-datalake-demo-part2/</link><pubDate>Sun, 12 Dec 2021 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2021-12-12-datalake-demo-part2/</guid><description><![CDATA[<p>In the <a href="/blog/2021-12-05-datalake-demo-part1">previous post</a>, we discussed a data lake solution where data ingestion is performed using <a href="https://www.redhat.com/en/topics/integration/what-is-change-data-capture#what-is-cdc" target="_blank" rel="noopener noreferrer">change data capture (CDC)<i class="fas fa-external-link-square-alt ms-1"></i></a> and the output files are <em>upserted</em> to an <a href="https://hudi.apache.org/" target="_blank" rel="noopener noreferrer">Apache Hudi<i class="fas fa-external-link-square-alt ms-1"></i></a> table. Being registered to <a href="https://docs.aws.amazon.com/glue/latest/dg/populate-data-catalog.html" target="_blank" rel="noopener noreferrer">Glue Data Catalog<i class="fas fa-external-link-square-alt ms-1"></i></a>, it can be used for ad-hoc queries and report/dashboard creation. The <a href="https://docs.yugabyte.com/latest/sample-data/northwind/" target="_blank" rel="noopener noreferrer">Northwind database<i class="fas fa-external-link-square-alt ms-1"></i></a> is used as the source database and, following the <a href="https://microservices.io/patterns/data/transactional-outbox.html" target="_blank" rel="noopener noreferrer">transactional outbox pattern<i class="fas fa-external-link-square-alt ms-1"></i></a>, order-related changes are _upserted _to an outbox table by triggers. The data ingestion is developed using Kafka connectors in the <a href="https://docs.confluent.io/platform/current/quickstart/ce-docker-quickstart.html" target="_blank" rel="noopener noreferrer">local Confluent platform<i class="fas fa-external-link-square-alt ms-1"></i></a> where the <a href="https://debezium.io/documentation/reference/stable/connectors/postgresql.html" target="_blank" rel="noopener noreferrer">Debezium for PostgreSQL<i class="fas fa-external-link-square-alt ms-1"></i></a> is used as the source connector and the <a href="https://lenses.io/blog/2020/11/new-kafka-to-S3-connector/" target="_blank" rel="noopener noreferrer">Lenses S3 sink connector<i class="fas fa-external-link-square-alt ms-1"></i></a> is used as the sink connector. We confirmed the order creation and update events are captured as expected, and it is ready for production deployment. In this post, we&rsquo;ll build the CDC part of the solution on AWS using <a href="https://aws.amazon.com/msk/" target="_blank" rel="noopener noreferrer">Amazon MSK<i class="fas fa-external-link-square-alt ms-1"></i></a> and <a href="https://aws.amazon.com/msk/features/msk-connect/" target="_blank" rel="noopener noreferrer">MSK Connect<i class="fas fa-external-link-square-alt ms-1"></i></a>.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2021-12-12-datalake-demo-part2/featured.png" length="164526" type="image/png"/></item><item><title>Data Lake Demo using Change Data Capture (CDC) on AWS – Part 1 Local Development</title><link>https://jaehyeon.me/blog/2021-12-05-datalake-demo-part1/</link><pubDate>Sun, 05 Dec 2021 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2021-12-05-datalake-demo-part1/</guid><description><![CDATA[<p><a href="https://www.redhat.com/en/topics/integration/what-is-change-data-capture#what-is-cdc" target="_blank" rel="noopener noreferrer">Change data capture (CDC)<i class="fas fa-external-link-square-alt ms-1"></i></a> is a proven data integration pattern that has a wide range of applications. Among those, data replication to data lakes is a good use case in data engineering. Coupled with <a href="https://lakefs.io/hudi-iceberg-and-delta-lake-data-lake-table-formats-compared/" target="_blank" rel="noopener noreferrer">best-in-breed data lake formats<i class="fas fa-external-link-square-alt ms-1"></i></a> such as <a href="https://hudi.apache.org/" target="_blank" rel="noopener noreferrer">Apache Hudi<i class="fas fa-external-link-square-alt ms-1"></i></a>, we can build an efficient data replication solution. This is the first post of the data lake demo series. Over time, we&rsquo;ll build a data lake that uses CDC. As a starting point, we&rsquo;ll discuss the source database and CDC streaming infrastructure in the local environment.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2021-12-05-datalake-demo-part1/featured.png" length="164526" type="image/png"/></item></channel></rss>