<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Terraform on Jaehyeon's Personal Site</title><link>https://jaehyeon-kim.github.io/tags/terraform/</link><description>Recent content in Terraform on Jaehyeon's Personal Site</description><generator>Hugo -- gohugo.io</generator><copyright>Copyright © 2023-2023 Jaehyeon Kim. All Rights Reserved.</copyright><lastBuildDate>Tue, 14 Mar 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://jaehyeon-kim.github.io/tags/terraform/index.xml" rel="self" type="application/rss+xml"/><item><title>Simplify Streaming Ingestion on AWS – Part 2 MSK and Athena</title><link>https://jaehyeon-kim.github.io/blog/2023-03-14-simplify-streaming-ingestion-athena/</link><pubDate>Tue, 14 Mar 2023 00:00:00 +0000</pubDate><guid>https://jaehyeon-kim.github.io/blog/2023-03-14-simplify-streaming-ingestion-athena/</guid><description>This article is originally posted in the Tech Insights of Cevo Australia - Link.
In Part 1, we discussed a streaming ingestion solution using EventBridge, Lambda, MSK and Redshift Serverless. Athena provides the MSK connector to enable SQL queries on Apache Kafka topics directly, and it can also facilitate the extraction of insights without setting up an additional pipeline to store data into S3. In this post, we discuss how to update the streaming ingestion solution so that data in the Kafka topic can be queried by Athena instead of Redshift.</description><enclosure url="https://jaehyeon-kim.github.io/blog/2023-03-14-simplify-streaming-ingestion-athena/featured.png" length="43403" type="image/png"/></item><item><title>Simplify Streaming Ingestion on AWS – Part 1 MSK and Redshift</title><link>https://jaehyeon-kim.github.io/blog/2023-02-08-simplify-streaming-ingestion-redshift/</link><pubDate>Wed, 08 Feb 2023 00:00:00 +0000</pubDate><guid>https://jaehyeon-kim.github.io/blog/2023-02-08-simplify-streaming-ingestion-redshift/</guid><description>This article is originally posted in the Tech Insights of Cevo Australia - Link.
Apache Kafka is a popular distributed event store and stream processing platform. Previously loading data from Kafka into Redshift and Athena usually required Kafka connectors (e.g. Amazon Redshift Sink Connector and Amazon S3 Sink Connector). Recently these AWS services provide features to ingest data from Kafka directly, which facilitates a simpler architecture that achieves low-latency and high-speed ingestion of streaming data.</description><enclosure url="https://jaehyeon-kim.github.io/blog/2023-02-08-simplify-streaming-ingestion-redshift/featured.png" length="32864" type="image/png"/></item><item><title>Data Build Tool (dbt) for Effective Data Transformation on AWS – Part 5 Athena</title><link>https://jaehyeon-kim.github.io/blog/2022-12-06-dbt-on-aws-part-5-athena/</link><pubDate>Tue, 06 Dec 2022 00:00:00 +0000</pubDate><guid>https://jaehyeon-kim.github.io/blog/2022-12-06-dbt-on-aws-part-5-athena/</guid><description>This article is originally posted in the Tech Insights of Cevo Australia - Link.
The data build tool (dbt) is an effective data transformation tool and it supports key AWS analytics services - Redshift, Glue, EMR and Athena. In the previous posts, we discussed benefits of a common data transformation tool and the potential of dbt to cover a wide range of data projects from data warehousing to data lake to data lakehouse.</description><enclosure url="https://jaehyeon-kim.github.io/blog/2022-12-06-dbt-on-aws-part-5-athena/featured.png" length="91796" type="image/png"/></item><item><title>Data Build Tool (dbt) for Effective Data Transformation on AWS – Part 4 EMR on EKS</title><link>https://jaehyeon-kim.github.io/blog/2022-11-01-dbt-on-aws-part-4-emr-eks/</link><pubDate>Tue, 01 Nov 2022 00:00:00 +0000</pubDate><guid>https://jaehyeon-kim.github.io/blog/2022-11-01-dbt-on-aws-part-4-emr-eks/</guid><description>This article is originally posted in the Tech Insights of Cevo Australia - Link.
The data build tool (dbt) is an effective data transformation tool and it supports key AWS analytics services - Redshift, Glue, EMR and Athena. In the previous posts, we discussed benefits of a common data transformation tool and the potential of dbt to cover a wide range of data projects from data warehousing to data lake to data lakehouse.</description><enclosure url="https://jaehyeon-kim.github.io/blog/2022-11-01-dbt-on-aws-part-4-emr-eks/featured.png" length="91067" type="image/png"/></item><item><title>Data Build Tool (dbt) for Effective Data Transformation on AWS – Part 3 EMR on EC2</title><link>https://jaehyeon-kim.github.io/blog/2022-10-19-dbt-on-aws-part-3-emr-ec2/</link><pubDate>Wed, 19 Oct 2022 00:00:00 +0000</pubDate><guid>https://jaehyeon-kim.github.io/blog/2022-10-19-dbt-on-aws-part-3-emr-ec2/</guid><description>This article is originally posted in the Tech Insights of Cevo Australia - Link.
The data build tool (dbt) is an effective data transformation tool and it supports key AWS analytics services - Redshift, Glue, EMR and Athena. In the previous posts, we discussed benefits of a common data transformation tool and the potential of dbt to cover a wide range of data projects from data warehousing to data lake to data lakehouse.</description><enclosure url="https://jaehyeon-kim.github.io/blog/2022-10-19-dbt-on-aws-part-3-emr-ec2/featured.png" length="91067" type="image/png"/></item><item><title>Data Build Tool (dbt) for Effective Data Transformation on AWS – Part 2 Glue</title><link>https://jaehyeon-kim.github.io/blog/2022-10-09-dbt-on-aws-part-2-glue/</link><pubDate>Sun, 09 Oct 2022 00:00:00 +0000</pubDate><guid>https://jaehyeon-kim.github.io/blog/2022-10-09-dbt-on-aws-part-2-glue/</guid><description>This article is originally posted in the Tech Insights of Cevo Australia - Link.
The data build tool (dbt) is an effective data transformation tool and it supports key AWS analytics services - Redshift, Glue, EMR and Athena. In part 1, we discussed benefits of a common data transformation tool and the potential of dbt to cover a wide range of data projects from data warehousing to data lake to data lakehouse.</description><enclosure url="https://jaehyeon-kim.github.io/blog/2022-10-09-dbt-on-aws-part-2-glue/featured.png" length="90647" type="image/png"/></item><item><title>Data Build Tool (dbt) for Effective Data Transformation on AWS – Part 1 Redshift</title><link>https://jaehyeon-kim.github.io/blog/2022-09-28-dbt-on-aws-part-1-redshift/</link><pubDate>Wed, 28 Sep 2022 00:00:00 +0000</pubDate><guid>https://jaehyeon-kim.github.io/blog/2022-09-28-dbt-on-aws-part-1-redshift/</guid><description>This article is originally posted in the Tech Insights of Cevo Australia - Link.
The data build tool (dbt) is an effective data transformation tool and it supports key AWS analytics services - Redshift, Glue, EMR and Athena. In part 1 of the dbt on AWS series, we discuss data transformation pipelines using dbt on Redshift Serverless. Subsets of IMDb data are used as source and data models are developed in multiple layers according to the dbt best practices.</description><enclosure url="https://jaehyeon-kim.github.io/blog/2022-09-28-dbt-on-aws-part-1-redshift/featured.png" length="97234" type="image/png"/></item><item><title>Develop and Test Apache Spark Apps for EMR Remotely Using Visual Studio Code</title><link>https://jaehyeon-kim.github.io/blog/2022-09-07-emr-remote-dev/</link><pubDate>Wed, 07 Sep 2022 00:00:00 +0000</pubDate><guid>https://jaehyeon-kim.github.io/blog/2022-09-07-emr-remote-dev/</guid><description>This article is originally posted in the Tech Insights of Cevo Australia - Link.
When we develop a Spark application on EMR, we can use docker for local development or notebooks via EMR Studio (or EMR Notebooks). However, the local development option is not viable if the size of data is large. Also, I am not a fan of notebooks as it is not possible to utilise the features my editor supports such as syntax highlighting, autocomplete and code formatting.</description><enclosure url="https://jaehyeon-kim.github.io/blog/2022-09-07-emr-remote-dev/featured.png" length="72448" type="image/png"/></item><item><title>Manage EMR on EKS with Terraform</title><link>https://jaehyeon-kim.github.io/blog/2022-08-26-emr-on-eks-with-terraform/</link><pubDate>Fri, 26 Aug 2022 00:00:00 +0000</pubDate><guid>https://jaehyeon-kim.github.io/blog/2022-08-26-emr-on-eks-with-terraform/</guid><description>This article is originally posted in the Tech Insights of Cevo Australia - Link.
Amazon EMR on EKS is a deployment option for Amazon EMR that allows you to automate the provisioning and management of open-source big data frameworks on EKS. While eksctl is popular for working with Amazon EKS clusters, it has limitations when it comes to building infrastructure that integrates multiple AWS services. Also, it is not straightforward to update EKS cluster resources incrementally with it.</description><enclosure url="https://jaehyeon-kim.github.io/blog/2022-08-26-emr-on-eks-with-terraform/featured.png" length="67936" type="image/png"/></item><item><title>Use External Schema Registry with MSK Connect – Part 2 MSK Deployment</title><link>https://jaehyeon-kim.github.io/blog/2022-04-03-schema-registry-part2/</link><pubDate>Sun, 03 Apr 2022 00:00:00 +0000</pubDate><guid>https://jaehyeon-kim.github.io/blog/2022-04-03-schema-registry-part2/</guid><description>This article is originally posted in the Tech Insights of Cevo Australia - Link.
In the previous post, we discussed a Change Data Capture (CDC) solution with a schema registry. A local development environment is set up using Docker Compose. The Debezium and Confluent S3 connectors are deployed with the Confluent Avro converter and the Apicurio registry is used as the schema registry service. A quick example is shown to illustrate how schema evolution can be managed by the schema registry.</description><enclosure url="https://jaehyeon-kim.github.io/blog/2022-04-03-schema-registry-part2/featured.png" length="59689" type="image/png"/></item><item><title>Simplify Your Development on AWS with Terraform</title><link>https://jaehyeon-kim.github.io/blog/2022-02-06-dev-infra-terraform/</link><pubDate>Sun, 06 Feb 2022 00:00:00 +0000</pubDate><guid>https://jaehyeon-kim.github.io/blog/2022-02-06-dev-infra-terraform/</guid><description>This article is originally posted in the Tech Insights of Cevo Australia - Link.
When I wrote my data lake demo series (part 1, part 2 and part 3) recently, I used an Aurora PostgreSQL, MSK and EMR cluster. All of them were deployed to private subnets and dedicated infrastructure was created using CloudFormation. Using the infrastructure as code (IaC) tool helped a lot, but it resulted in creating 7 CloudFormation stacks, which was a bit harder to manage in the end.</description><enclosure url="https://jaehyeon-kim.github.io/blog/2022-02-06-dev-infra-terraform/featured.png" length="46253" type="image/png"/></item><item><title>Data Lake Demo using Change Data Capture (CDC) on AWS – Part 3 Implement Data Lake</title><link>https://jaehyeon-kim.github.io/blog/2021-12-19-datalake-demo-part3/</link><pubDate>Sun, 19 Dec 2021 00:00:00 +0000</pubDate><guid>https://jaehyeon-kim.github.io/blog/2021-12-19-datalake-demo-part3/</guid><description>This article is originally posted in the Tech Insights of Cevo Australia - Link.
In the previous post, we created a VPC that has private and public subnets in 2 availability zones in order to build and deploy the data lake solution on AWS. NAT instances are created to forward outbound traffic to the internet and a VPN bastion host is set up to facilitate deployment. An Aurora PostgreSQL cluster is deployed to host the source database and a Python command line app is used to create the database.</description><enclosure url="https://jaehyeon-kim.github.io/blog/2021-12-19-datalake-demo-part3/featured.png" length="164526" type="image/png"/></item><item><title>Data Lake Demo using Change Data Capture (CDC) on AWS – Part 2 Implement CDC</title><link>https://jaehyeon-kim.github.io/blog/2021-12-12-datalake-demo-part2/</link><pubDate>Sun, 12 Dec 2021 00:00:00 +0000</pubDate><guid>https://jaehyeon-kim.github.io/blog/2021-12-12-datalake-demo-part2/</guid><description>This article is originally posted in the Tech Insights of Cevo Australia - Link.
In the previous post, we discussed a data lake solution where data ingestion is performed using change data capture (CDC) and the output files are upserted to an Apache Hudi table. Being registered to Glue Data Catalog, it can be used for ad-hoc queries and report/dashboard creation. The Northwind database is used as the source database and, following the transactional outbox pattern, order-related changes are _upserted _to an outbox table by triggers.</description><enclosure url="https://jaehyeon-kim.github.io/blog/2021-12-12-datalake-demo-part2/featured.png" length="164526" type="image/png"/></item></channel></rss>