<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Kpow on Jaehyeon Kim</title><link>https://jaehyeon.me/tags/kpow/</link><description>Recent content in Kpow on Jaehyeon Kim</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>Copyright Â© 2023-2025 Jaehyeon Kim. All Rights Reserved.</copyright><lastBuildDate>Tue, 10 Jun 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://jaehyeon.me/tags/kpow/index.xml" rel="self" type="application/rss+xml"/><item><title>Flink DataStream API - Scalable Event Processing for Supplier Stats</title><link>https://jaehyeon.me/blog/2025-06-10-kotlin-getting-started-flink-datastream/</link><pubDate>Tue, 10 Jun 2025 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2025-06-10-kotlin-getting-started-flink-datastream/</guid><description><![CDATA[<p>Building on our exploration of stream processing, we now transition from Kafka&rsquo;s native library to <strong>Apache Flink</strong>, a powerful, general-purpose distributed processing engine. In this post, we&rsquo;ll dive into Flink&rsquo;s foundational <strong>DataStream API</strong>. We will tackle the same supplier statistics problem - analyzing a stream of Avro-formatted order events - but this time using Flink&rsquo;s robust features for stateful computation. This example will highlight Flink&rsquo;s sophisticated event-time processing with watermarks and its elegant, built-in mechanisms for handling late-arriving data through side outputs.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2025-06-10-kotlin-getting-started-flink-datastream/featured.png" length="142918" type="image/png"/></item><item><title>Kafka Streams - Lightweight Real-Time Processing for Supplier Stats</title><link>https://jaehyeon.me/blog/2025-06-03-kotlin-getting-started-kafka-streams/</link><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2025-06-03-kotlin-getting-started-kafka-streams/</guid><description><![CDATA[<p>In this post, we shift our focus from basic Kafka clients to real-time stream processing with <strong>Kafka Streams</strong>. We&rsquo;ll explore a Kotlin application designed to analyze a continuous stream of Avro-formatted order events, calculate supplier statistics in tumbling windows, and intelligently handle late-arriving data. This example demonstrates the power of Kafka Streams for building lightweight, yet robust, stream processing applications directly within your Kafka ecosystem, leveraging event-time processing and custom logic.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2025-06-03-kotlin-getting-started-kafka-streams/featured.png" length="131804" type="image/png"/></item><item><title>Kafka Clients with Avro - Schema Registry and Order Events</title><link>https://jaehyeon.me/blog/2025-05-27-kotlin-getting-started-kafka-avro-clients/</link><pubDate>Tue, 27 May 2025 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2025-05-27-kotlin-getting-started-kafka-avro-clients/</guid><description><![CDATA[<p>In this post, we&rsquo;ll explore a practical example of building Kafka client applications using Kotlin, Apache Avro for data serialization, and Gradle for build management. We&rsquo;ll walk through the setup of a Kafka producer that generates mock order data and a consumer that processes these orders. This example highlights best practices such as schema management with Avro, robust error handling, and graceful shutdown, providing a solid foundation for your own Kafka-based projects. We&rsquo;ll dive into the build configuration, the Avro schema definition, utility functions for Kafka administration, and the core logic of both the producer and consumer applications.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2025-05-27-kotlin-getting-started-kafka-avro-clients/featured.png" length="73988" type="image/png"/></item><item><title>Kafka Clients with JSON - Producing and Consuming Order Events</title><link>https://jaehyeon.me/blog/2025-05-20-kotlin-getting-started-kafka-json-clients/</link><pubDate>Tue, 20 May 2025 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2025-05-20-kotlin-getting-started-kafka-json-clients/</guid><description>&lt;p>This post explores a Kotlin-based Kafka project, meticulously detailing the construction and operation of both a Kafka producer application, responsible for generating and sending order data, and a Kafka consumer application, designed to receive and process these orders. We&amp;rsquo;ll delve into each component, from build configuration to message handling, to understand how they work together in an event-driven system.&lt;/p></description><enclosure url="https://jaehyeon.me/blog/2025-05-20-kotlin-getting-started-kafka-json-clients/featured.png" length="97922" type="image/png"/></item><item><title>Kafka Development with Docker - Part 2 Management App</title><link>https://jaehyeon.me/blog/2023-05-18-kafka-development-with-docker-part-2/</link><pubDate>Thu, 18 May 2023 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2023-05-18-kafka-development-with-docker-part-2/</guid><description>In the previous post, I illustrated how to create a topic and to produce/consume messages using the command utilities provided by Apache Kafka. It is not convenient, however, for example, when you consume serialised messages where their schemas are stored in a schema registry. Also, the utilities don&amp;rsquo;t support to browse or manage related resources such as connectors and schemas. Therefore, a Kafka management app can be a good companion for development, which helps monitor and manage resources on an easy-to-use user interface.</description><enclosure url="https://jaehyeon.me/blog/2023-05-18-kafka-development-with-docker-part-2/featured.png" length="59675" type="image/png"/></item></channel></rss>