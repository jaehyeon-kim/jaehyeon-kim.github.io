<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>AWS on Jaehyeon Kim</title><link>https://jaehyeon.me/tags/aws/</link><description>Recent content in AWS on Jaehyeon Kim</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>Copyright © 2023-2024 Jaehyeon Kim. All Rights Reserved.</copyright><lastBuildDate>Thu, 14 Mar 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://jaehyeon.me/tags/aws/index.xml" rel="self" type="application/rss+xml"/><item><title>Data Build Tool (dbt) Pizza Shop Demo - Part 6 ETL on Amazon Athena via Airflow</title><link>https://jaehyeon.me/blog/2024-03-14-dbt-pizza-shop-6/</link><pubDate>Thu, 14 Mar 2024 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2024-03-14-dbt-pizza-shop-6/</guid><description><![CDATA[<p>In <a href="/blog/2024-03-07-dbt-pizza-shop-5">Part 5</a>, we developed a <a href="https://docs.getdbt.com/docs/introduction" target="_blank" rel="noopener noreferrer">dbt<i class="fas fa-external-link-square-alt ms-1"></i></a> project that that targets <a href="https://iceberg.apache.org/" target="_blank" rel="noopener noreferrer">Apache Iceberg<i class="fas fa-external-link-square-alt ms-1"></i></a> where transformations are performed on <a href="https://aws.amazon.com/athena/" target="_blank" rel="noopener noreferrer">Amazon Athena<i class="fas fa-external-link-square-alt ms-1"></i></a>. Two dimension tables that keep product and user records are created as <a href="https://en.wikipedia.org/wiki/Slowly_changing_dimension" target="_blank" rel="noopener noreferrer">Type 2 slowly changing dimension (SCD Type 2)<i class="fas fa-external-link-square-alt ms-1"></i></a> tables, and one transactional fact table is built to keep pizza orders. To improve query performance, the fact table is denormalized to pre-join records from the dimension tables using the array and struct data types. In this post, we discuss how to set up an ETL process on the project using Apache Airflow.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2024-03-14-dbt-pizza-shop-6/featured.png" length="82921" type="image/png"/></item><item><title>Data Build Tool (dbt) Pizza Shop Demo - Part 5 Modelling on Amazon Athena</title><link>https://jaehyeon.me/blog/2024-03-07-dbt-pizza-shop-5/</link><pubDate>Thu, 07 Mar 2024 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2024-03-07-dbt-pizza-shop-5/</guid><description><![CDATA[<p>In <a href="%28/blog/2024-01-18-dbt-pizza-shop-1%29">Part 1</a> and <a href="%28/blog/2024-02-08-dbt-pizza-shop-3%29">Part 3</a>, we developed <a href="https://docs.getdbt.com/docs/introduction" target="_blank" rel="noopener noreferrer">data build tool (dbt)<i class="fas fa-external-link-square-alt ms-1"></i></a> projects that target <em>PostgreSQL</em> and <em>BigQuery</em> using fictional pizza shop data. The data is modelled by <a href="https://en.wikipedia.org/wiki/Slowly_changing_dimension" target="_blank" rel="noopener noreferrer">SCD type 2<i class="fas fa-external-link-square-alt ms-1"></i></a> dimension tables and one transactional fact table. While the order records should be joined with dimension tables to get complete details for <em>PostgreSQL</em>, the fact table is denormalized using <a href="https://cloud.google.com/bigquery/docs/best-practices-performance-nested" target="_blank" rel="noopener noreferrer">nested and repeated fields<i class="fas fa-external-link-square-alt ms-1"></i></a> to improve query performance for <em>BigQuery</em>.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2024-03-07-dbt-pizza-shop-5/featured.png" length="61499" type="image/png"/></item><item><title>Real Time Streaming with Kafka and Flink - Lab 6 Consume data from Kafka using Lambda</title><link>https://jaehyeon.me/blog/2023-12-14-real-time-streaming-with-kafka-and-flink-7/</link><pubDate>Thu, 14 Dec 2023 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2023-12-14-real-time-streaming-with-kafka-and-flink-7/</guid><description><![CDATA[<p>Amazon MSK can be configured as an <a href="https://docs.aws.amazon.com/lambda/latest/dg/with-msk.html" target="_blank" rel="noopener noreferrer">event source<i class="fas fa-external-link-square-alt ms-1"></i></a> of a Lambda function. Lambda internally polls for new messages from the event source and then synchronously invokes the target Lambda function. With this feature, we can develop a Kafka consumer application in serverless environment where developers can focus on application logic. In this lab, we will discuss how to create a Kafka consumer using a Lambda function.</p>
<ul>
<li><a href="/blog/2023-10-05-real-time-streaming-with-kafka-and-flink-1">Introduction</a></li>
<li><a href="/blog/2023-10-26-real-time-streaming-with-kafka-and-flink-2">Lab 1 Produce data to Kafka using Lambda</a></li>
<li><a href="/blog/2023-11-09-real-time-streaming-with-kafka-and-flink-3">Lab 2 Write data to Kafka from S3 using Flink</a></li>
<li><a href="/blog/2023-11-16-real-time-streaming-with-kafka-and-flink-4">Lab 3 Transform and write data to S3 from Kafka using Flink</a></li>
<li><a href="/blog/2023-11-23-real-time-streaming-with-kafka-and-flink-5">Lab 4 Clean, Aggregate, and Enrich Events with Flink</a></li>
<li><a href="/blog/2023-11-30-real-time-streaming-with-kafka-and-flink-6">Lab 5 Write data to DynamoDB using Kafka Connect</a></li>
<li><a href="/blog/2023-12-14-real-time-streaming-with-kafka-and-flink-7/#">Lab 6 Consume data from Kafka using Lambda</a> (this post)</li>
</ul>

<h2 id="architecture" data-numberify>Architecture<a class="anchor ms-1" href="#architecture"></a></h2>
<p>Fake taxi ride data is sent to a Kafka topic by the Kafka producer application that is discussed in <a href="/blog/2023-10-26-real-time-streaming-with-kafka-and-flink-2">Lab 1</a>. The messages of the <em>taxi-rides</em> topic are consumed by a Lambda function where the MSK cluster is configured as an event source of the function.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2023-12-14-real-time-streaming-with-kafka-and-flink-7/featured.png" length="138986" type="image/png"/></item><item><title>Real Time Streaming with Kafka and Flink - Lab 5 Write data to DynamoDB using Kafka Connect</title><link>https://jaehyeon.me/blog/2023-11-30-real-time-streaming-with-kafka-and-flink-6/</link><pubDate>Thu, 30 Nov 2023 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2023-11-30-real-time-streaming-with-kafka-and-flink-6/</guid><description><![CDATA[<p><a href="https://kafka.apache.org/documentation/#connect" target="_blank" rel="noopener noreferrer">Kafka Connect<i class="fas fa-external-link-square-alt ms-1"></i></a> is a tool for scalably and reliably streaming data between Apache Kafka and other systems. It makes it simple to quickly define connectors that move large collections of data into and out of Kafka. In this lab, we will discuss how to create a data pipeline that ingests data from a Kafka topic into a DynamoDB table using the <a href="https://camel.apache.org/camel-kafka-connector/3.20.x/reference/connectors/camel-aws-ddb-sink-kafka-sink-connector.html" target="_blank" rel="noopener noreferrer">Camel DynamoDB sink connector<i class="fas fa-external-link-square-alt ms-1"></i></a>.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2023-11-30-real-time-streaming-with-kafka-and-flink-6/featured.png" length="113252" type="image/png"/></item><item><title>Real Time Streaming with Kafka and Flink - Lab 4 Clean, Aggregate, and Enrich Events with Flink</title><link>https://jaehyeon.me/blog/2023-11-23-real-time-streaming-with-kafka-and-flink-5/</link><pubDate>Thu, 23 Nov 2023 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2023-11-23-real-time-streaming-with-kafka-and-flink-5/</guid><description>&lt;p>The value of data can be maximised when it is used without delay. With Apache Flink, we can build streaming analytics applications that incorporate the latest events with low latency. In this lab, we will create a Pyflink application that writes accumulated taxi rides data into an OpenSearch cluster. It aggregates the number of trips/passengers and trip durations by vendor ID for a window of 5 seconds. The data is then used to create a chart that monitors the status of taxi rides in the OpenSearch Dashboard.&lt;/p></description><enclosure url="https://jaehyeon.me/blog/2023-11-23-real-time-streaming-with-kafka-and-flink-5/featured.png" length="112340" type="image/png"/></item><item><title>Real Time Streaming with Kafka and Flink - Lab 3 Transform and write data to S3 from Kafka using Flink</title><link>https://jaehyeon.me/blog/2023-11-16-real-time-streaming-with-kafka-and-flink-4/</link><pubDate>Thu, 16 Nov 2023 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2023-11-16-real-time-streaming-with-kafka-and-flink-4/</guid><description><![CDATA[<p>In this lab, we will create a Pyflink application that exports Kafka topic messages into a S3 bucket. The app enriches the records by adding a new column using a user defined function and writes them via the <a href="https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/connectors/table/filesystem/" target="_blank" rel="noopener noreferrer">FileSystem SQL connector<i class="fas fa-external-link-square-alt ms-1"></i></a>. This allows us to achieve a simpler architecture compared to the <a href="https://catalog.us-east-1.prod.workshops.aws/workshops/2300137e-f2ac-4eb9-a4ac-3d25026b235f/en-US/lab-3-kdf" target="_blank" rel="noopener noreferrer">original lab<i class="fas fa-external-link-square-alt ms-1"></i></a> where the records are sent into Amazon Kinesis Data Firehose, enriched by a separate Lambda function and written to a S3 bucket afterwards. While the records are being written to the S3 bucket, a Glue table will be created to query them on Amazon Athena.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2023-11-16-real-time-streaming-with-kafka-and-flink-4/featured.png" length="160359" type="image/png"/></item><item><title>Real Time Streaming with Kafka and Flink - Lab 2 Write data to Kafka from S3 using Flink</title><link>https://jaehyeon.me/blog/2023-11-09-real-time-streaming-with-kafka-and-flink-3/</link><pubDate>Thu, 09 Nov 2023 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2023-11-09-real-time-streaming-with-kafka-and-flink-3/</guid><description>&lt;p>In this lab, we will create a Pyflink application that reads records from S3 and sends them into a Kafka topic. A custom pipeline Jar file will be created as the Kafka cluster is authenticated by IAM, and it will be demonstrated how to execute the app in a Flink cluster deployed on Docker as well as locally as a typical Python app. We can assume the S3 data is static metadata that needs to be joined into another stream, and this exercise can be useful for data enrichment.&lt;/p></description><enclosure url="https://jaehyeon.me/blog/2023-11-09-real-time-streaming-with-kafka-and-flink-3/featured.png" length="139114" type="image/png"/></item><item><title>Kafka Connect for AWS Services Integration - Part 5 Deploy Aiven OpenSearch Sink Connector</title><link>https://jaehyeon.me/blog/2023-10-30-kafka-connect-for-aws-part-5/</link><pubDate>Mon, 30 Oct 2023 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2023-10-30-kafka-connect-for-aws-part-5/</guid><description><![CDATA[<p>In the previous post, we discussed how to develop a data pipeline from Apache Kafka into OpenSearch locally using Docker. The pipeline will be deployed on AWS using <a href="https://aws.amazon.com/msk/" target="_blank" rel="noopener noreferrer">Amazon MSK<i class="fas fa-external-link-square-alt ms-1"></i></a>, <a href="https://aws.amazon.com/msk/features/msk-connect/" target="_blank" rel="noopener noreferrer">Amazon MSK Connect<i class="fas fa-external-link-square-alt ms-1"></i></a> and <a href="https://aws.amazon.com/opensearch-service/" target="_blank" rel="noopener noreferrer">Amazon OpenSearch Service<i class="fas fa-external-link-square-alt ms-1"></i></a> using <a href="https://www.terraform.io/" target="_blank" rel="noopener noreferrer">Terraform<i class="fas fa-external-link-square-alt ms-1"></i></a> in this post. First the infrastructure will be deployed that covers a Virtual Private Cloud (VPC), Virtual Private Network (VPN) server, MSK Cluster and OpenSearch domain. Then Kafka source and sink connectors will be deployed on MSK Connect, followed by performing quick data analysis.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2023-10-30-kafka-connect-for-aws-part-5/featured.png" length="85575" type="image/png"/></item><item><title>Real Time Streaming with Kafka and Flink - Lab 1 Produce data to Kafka using Lambda</title><link>https://jaehyeon.me/blog/2023-10-26-real-time-streaming-with-kafka-and-flink-2/</link><pubDate>Thu, 26 Oct 2023 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2023-10-26-real-time-streaming-with-kafka-and-flink-2/</guid><description><![CDATA[<p>In this lab, we will create a Kafka producer application using <a href="https://aws.amazon.com/lambda/" target="_blank" rel="noopener noreferrer">AWS Lambda<i class="fas fa-external-link-square-alt ms-1"></i></a>, which sends fake taxi ride data into a Kafka topic on <a href="https://aws.amazon.com/msk/" target="_blank" rel="noopener noreferrer">Amazon MSK<i class="fas fa-external-link-square-alt ms-1"></i></a>. A configurable number of the producer Lambda function will be invoked by an <a href="https://aws.amazon.com/eventbridge/" target="_blank" rel="noopener noreferrer">Amazon EventBridge<i class="fas fa-external-link-square-alt ms-1"></i></a> schedule rule. In this way we are able to generate test data concurrently based on the desired volume of messages.</p>
<ul>
<li><a href="/blog/2023-10-05-real-time-streaming-with-kafka-and-flink-1">Introduction</a></li>
<li><a href="/blog/2023-10-26-real-time-streaming-with-kafka-and-flink-2/#">Lab 1 Produce data to Kafka using Lambda</a> (this post)</li>
<li><a href="/blog/2023-11-09-real-time-streaming-with-kafka-and-flink-3">Lab 2 Write data to Kafka from S3 using Flink</a></li>
<li><a href="/blog/2023-11-16-real-time-streaming-with-kafka-and-flink-4">Lab 3 Transform and write data to S3 from Kafka using Flink</a></li>
<li><a href="/blog/2023-11-23-real-time-streaming-with-kafka-and-flink-5">Lab 4 Clean, Aggregate, and Enrich Events with Flink</a></li>
<li><a href="/blog/2023-11-30-real-time-streaming-with-kafka-and-flink-6">Lab 5 Write data to DynamoDB using Kafka Connect</a></li>
<li><a href="/blog/2023-12-14-real-time-streaming-with-kafka-and-flink-7">Lab 6 Consume data from Kafka using Lambda</a></li>
</ul>
<p>[<strong>Update 2023-11-06</strong>] Initially I planned to deploy Pyflink applications on <a href="https://aws.amazon.com/managed-service-apache-flink/" target="_blank" rel="noopener noreferrer">Amazon Managed Service for Apache Flink<i class="fas fa-external-link-square-alt ms-1"></i></a>, but I changed the plan to use a local Flink cluster deployed on Docker. The main reasons are</p>]]></description><enclosure url="https://jaehyeon.me/blog/2023-10-26-real-time-streaming-with-kafka-and-flink-2/featured.png" length="138560" type="image/png"/></item><item><title>Kafka Connect for AWS Services Integration - Part 4 Develop Aiven OpenSearch Sink Connector</title><link>https://jaehyeon.me/blog/2023-10-23-kafka-connect-for-aws-part-4/</link><pubDate>Mon, 23 Oct 2023 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2023-10-23-kafka-connect-for-aws-part-4/</guid><description><![CDATA[<p><a href="https://opensearch.org/" target="_blank" rel="noopener noreferrer">OpenSearch<i class="fas fa-external-link-square-alt ms-1"></i></a> is a popular search and analytics engine and its use cases cover log analytics, real-time application monitoring, and clickstream analysis. OpenSearch can be deployed on its own or via <a href="https://aws.amazon.com/opensearch-service/" target="_blank" rel="noopener noreferrer">Amazon OpenSearch Service<i class="fas fa-external-link-square-alt ms-1"></i></a>. <a href="https://kafka.apache.org/" target="_blank" rel="noopener noreferrer">Apache Kafka<i class="fas fa-external-link-square-alt ms-1"></i></a> is a distributed event store and stream-processing platform, and it aims to provide a unified, high-throughput, low-latency platform for handling real-time data feeds. On AWS, Apache Kafka can be deployed via <a href="https://aws.amazon.com/msk/" target="_blank" rel="noopener noreferrer">Amazon Managed Streaming for Apache Kafka (MSK)<i class="fas fa-external-link-square-alt ms-1"></i></a>.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2023-10-23-kafka-connect-for-aws-part-4/featured.png" length="61820" type="image/png"/></item><item><title>Real Time Streaming with Kafka and Flink - Introduction</title><link>https://jaehyeon.me/blog/2023-10-05-real-time-streaming-with-kafka-and-flink-1/</link><pubDate>Thu, 05 Oct 2023 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2023-10-05-real-time-streaming-with-kafka-and-flink-1/</guid><description><![CDATA[<p><a href="https://catalog.us-east-1.prod.workshops.aws/workshops/2300137e-f2ac-4eb9-a4ac-3d25026b235f/en-US" target="_blank" rel="noopener noreferrer">Real Time Streaming with Amazon Kinesis<i class="fas fa-external-link-square-alt ms-1"></i></a> is an AWS workshop that helps users build a streaming analytics application on AWS. Incoming events are stored in a number of streams of the <a href="https://aws.amazon.com/kinesis/data-streams/" target="_blank" rel="noopener noreferrer">Amazon Kinesis Data Streams<i class="fas fa-external-link-square-alt ms-1"></i></a> service, and various other AWS services and tools are used to process and analyse data.</p>
<p><a href="https://kafka.apache.org/" target="_blank" rel="noopener noreferrer">Apache Kafka<i class="fas fa-external-link-square-alt ms-1"></i></a> is a popular distributed event store and stream processing platform, and it stores incoming events in topics. As part of learning real time streaming analytics on AWS, we can rebuild the analytics applications by replacing the Kinesis streams with Kafka topics. As an introduction, this post compares the workshop architecture with the updated architecture of this series. The labs of the updated architecture will be implemented in subsequent posts.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2023-10-05-real-time-streaming-with-kafka-and-flink-1/featured.png" length="138141" type="image/png"/></item><item><title>Kafka Connect for AWS Services Integration - Part 3 Deploy Camel DynamoDB Sink Connector</title><link>https://jaehyeon.me/blog/2023-07-03-kafka-connect-for-aws-part-3/</link><pubDate>Mon, 03 Jul 2023 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2023-07-03-kafka-connect-for-aws-part-3/</guid><description><![CDATA[<p>As part of investigating how to utilize Kafka Connect effectively for AWS services integration, I demonstrated how to develop the <a href="https://camel.apache.org/camel-kafka-connector/latest/index.html" target="_blank" rel="noopener noreferrer">Camel DynamoDB sink connector<i class="fas fa-external-link-square-alt ms-1"></i></a> using Docker in <a href="/blog/2023-06-04-kafka-connect-for-aws-part-2">Part 2</a>. Fake order data was generated using the <a href="https://github.com/awslabs/amazon-msk-data-generator" target="_blank" rel="noopener noreferrer">MSK Data Generator<i class="fas fa-external-link-square-alt ms-1"></i></a> source connector, and the sink connector was configured to consume the topic messages to ingest them into a DynamoDB table. In this post, I will illustrate how to deploy the data ingestion applications using <a href="https://aws.amazon.com/msk/" target="_blank" rel="noopener noreferrer">Amazon MSK<i class="fas fa-external-link-square-alt ms-1"></i></a> and <a href="https://aws.amazon.com/msk/features/msk-connect/" target="_blank" rel="noopener noreferrer">MSK Connect<i class="fas fa-external-link-square-alt ms-1"></i></a>.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2023-07-03-kafka-connect-for-aws-part-3/featured.png" length="76240" type="image/png"/></item><item><title>Kafka Connect for AWS Services Integration - Part 2 Develop Camel DynamoDB Sink Connector</title><link>https://jaehyeon.me/blog/2023-06-04-kafka-connect-for-aws-part-2/</link><pubDate>Sun, 04 Jun 2023 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2023-06-04-kafka-connect-for-aws-part-2/</guid><description><![CDATA[<p>In <a href="/blog/2023-05-03-kafka-connect-for-aws-part-1">Part 1</a>, we reviewed Kafka connectors focusing on AWS services integration. Among the available connectors, the suite of <a href="https://camel.apache.org/camel-kafka-connector/latest/index.html" target="_blank" rel="noopener noreferrer">Apache Camel Kafka connectors<i class="fas fa-external-link-square-alt ms-1"></i></a> and the <a href="https://github.com/awslabs/kinesis-kafka-connector" target="_blank" rel="noopener noreferrer">Kinesis Kafka connector<i class="fas fa-external-link-square-alt ms-1"></i></a> from the AWS Labs can be effective for building data ingestion pipelines on AWS. In this post, I will illustrate how to develop the Camel DynamoDB sink connector using Docker. Fake order data will be generated using the <a href="https://github.com/awslabs/amazon-msk-data-generator" target="_blank" rel="noopener noreferrer">MSK Data Generator<i class="fas fa-external-link-square-alt ms-1"></i></a> source connector, and the sink connector will be configured to consume the topic messages to ingest them into a DynamoDB table.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2023-06-04-kafka-connect-for-aws-part-2/featured.png" length="87044" type="image/png"/></item><item><title>Kafka Connect for AWS Services Integration - Part 1 Introduction</title><link>https://jaehyeon.me/blog/2023-05-03-kafka-connect-for-aws-part-1/</link><pubDate>Wed, 03 May 2023 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2023-05-03-kafka-connect-for-aws-part-1/</guid><description><![CDATA[<p><a href="https://aws.amazon.com/kinesis/data-streams/" target="_blank" rel="noopener noreferrer">Amazon Kinesis Data Streams<i class="fas fa-external-link-square-alt ms-1"></i></a> and <a href="https://aws.amazon.com/msk/" target="_blank" rel="noopener noreferrer">Amazon Managed Streaming for Apache Kafka (MSK)<i class="fas fa-external-link-square-alt ms-1"></i></a> are two managed streaming services offered by AWS. Many resources on the web indicate Kinesis Data Streams is better when it comes to integrating with AWS services. However, it is not necessarily the case with the help of Kafka Connect. According to the documentation of <a href="https://kafka.apache.org/documentation/#connect" target="_blank" rel="noopener noreferrer">Apache Kafka<i class="fas fa-external-link-square-alt ms-1"></i></a>, <em>Kafka Connect is a tool for scalably and reliably streaming data between Apache Kafka and other systems. It makes it simple to quickly define connectors that move large collections of data into and out of Kafka</em>. Kafka Connect supports two types of connectors - source and sink. Source connectors are used to ingest messages from external systems into Kafka topics while messages are ingested into external systems form Kafka topics with sink connectors. In this post, I will introduce available Kafka connectors mainly for AWS services integration. Also, developing and deploying some of them will be covered in later posts.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2023-05-03-kafka-connect-for-aws-part-1/featured.png" length="22272" type="image/png"/></item><item><title>Integrate Glue Schema Registry with Your Python Kafka App</title><link>https://jaehyeon.me/blog/2023-04-12-integrate-glue-schema-registry/</link><pubDate>Wed, 12 Apr 2023 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2023-04-12-integrate-glue-schema-registry/</guid><description><![CDATA[<p>As Kafka producer and consumer apps are decoupled, they operate on Kafka topics rather than communicating with each other directly. As described in the <a href="https://docs.confluent.io/platform/current/schema-registry/index.html#sr-overview" target="_blank" rel="noopener noreferrer">Confluent document<i class="fas fa-external-link-square-alt ms-1"></i></a>, <em>Schema Registry</em> provides a centralized repository for managing and validating schemas for topic message data, and for serialization and deserialization of the data over the network. Producers and consumers to Kafka topics can use schemas to ensure data consistency and compatibility as schemas evolve. In AWS, the <a href="https://docs.aws.amazon.com/glue/latest/dg/schema-registry.html" target="_blank" rel="noopener noreferrer">Glue Schema Registry<i class="fas fa-external-link-square-alt ms-1"></i></a> supports features to manage and enforce schemas on data streaming applications using convenient integrations with Apache Kafka, <a href="https://aws.amazon.com/msk/" target="_blank" rel="noopener noreferrer">Amazon Managed Streaming for Apache Kafka<i class="fas fa-external-link-square-alt ms-1"></i></a>, <a href="https://aws.amazon.com/kinesis/data-streams/" target="_blank" rel="noopener noreferrer">Amazon Kinesis Data Streams<i class="fas fa-external-link-square-alt ms-1"></i></a>, <a href="https://aws.amazon.com/kinesis/data-analytics/" target="_blank" rel="noopener noreferrer">Amazon Kinesis Data Analytics for Apache Flink<i class="fas fa-external-link-square-alt ms-1"></i></a>, and <a href="https://aws.amazon.com/lambda/" target="_blank" rel="noopener noreferrer">AWS Lambda<i class="fas fa-external-link-square-alt ms-1"></i></a>. In this post, we will discuss how to integrate Python Kafka producer and consumer apps In AWS Lambda with the Glue Schema Registry.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2023-04-12-integrate-glue-schema-registry/featured.png" length="46040" type="image/png"/></item><item><title>Simplify Streaming Ingestion on AWS – Part 2 MSK and Athena</title><link>https://jaehyeon.me/blog/2023-03-14-simplify-streaming-ingestion-athena/</link><pubDate>Tue, 14 Mar 2023 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2023-03-14-simplify-streaming-ingestion-athena/</guid><description><![CDATA[<p>In Part 1, we discussed a streaming ingestion solution using <a href="https://aws.amazon.com/eventbridge/" target="_blank" rel="noopener noreferrer">EventBridge<i class="fas fa-external-link-square-alt ms-1"></i></a>, <a href="https://aws.amazon.com/lambda/" target="_blank" rel="noopener noreferrer">Lambda<i class="fas fa-external-link-square-alt ms-1"></i></a>, <a href="https://aws.amazon.com/msk/" target="_blank" rel="noopener noreferrer">MSK<i class="fas fa-external-link-square-alt ms-1"></i></a> and <a href="https://aws.amazon.com/redshift/redshift-serverless/" target="_blank" rel="noopener noreferrer">Redshift Serverless<i class="fas fa-external-link-square-alt ms-1"></i></a>. Athena provides the <a href="https://docs.aws.amazon.com/athena/latest/ug/connectors-msk.html" target="_blank" rel="noopener noreferrer">MSK connector<i class="fas fa-external-link-square-alt ms-1"></i></a> to enable SQL queries on Apache Kafka topics directly, and it can also facilitate the extraction of insights without setting up an additional pipeline to store data into S3. In this post, we discuss how to update the streaming ingestion solution so that data in the Kafka topic can be queried by Athena instead of Redshift.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2023-03-14-simplify-streaming-ingestion-athena/featured.png" length="43403" type="image/png"/></item><item><title>Simplify Streaming Ingestion on AWS – Part 1 MSK and Redshift</title><link>https://jaehyeon.me/blog/2023-02-08-simplify-streaming-ingestion-redshift/</link><pubDate>Wed, 08 Feb 2023 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2023-02-08-simplify-streaming-ingestion-redshift/</guid><description><![CDATA[<p><a href="https://kafka.apache.org/" target="_blank" rel="noopener noreferrer">Apache Kafka<i class="fas fa-external-link-square-alt ms-1"></i></a> is a popular distributed event store and stream processing platform. Previously loading data from Kafka into Redshift and Athena usually required Kafka connectors (e.g. <a href="https://www.confluent.io/hub/confluentinc/kafka-connect-aws-redshift" target="_blank" rel="noopener noreferrer">Amazon Redshift Sink Connector<i class="fas fa-external-link-square-alt ms-1"></i></a> and <a href="https://www.confluent.io/hub/confluentinc/kafka-connect-s3" target="_blank" rel="noopener noreferrer">Amazon S3 Sink Connector<i class="fas fa-external-link-square-alt ms-1"></i></a>). Recently these AWS services provide features to ingest data from Kafka directly, which facilitates a simpler architecture that achieves low-latency and high-speed ingestion of streaming data. In part 1 of the <em>simplify streaming ingestion on AWS</em> series, we discuss how to develop an end-to-end streaming ingestion solution using <a href="https://aws.amazon.com/eventbridge/" target="_blank" rel="noopener noreferrer">EventBridge<i class="fas fa-external-link-square-alt ms-1"></i></a>, <a href="https://aws.amazon.com/lambda/" target="_blank" rel="noopener noreferrer">Lambda<i class="fas fa-external-link-square-alt ms-1"></i></a>, <a href="https://aws.amazon.com/msk/" target="_blank" rel="noopener noreferrer">MSK<i class="fas fa-external-link-square-alt ms-1"></i></a> and <a href="https://aws.amazon.com/redshift/redshift-serverless/" target="_blank" rel="noopener noreferrer">Redshift Serverless<i class="fas fa-external-link-square-alt ms-1"></i></a> on AWS.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2023-02-08-simplify-streaming-ingestion-redshift/featured.png" length="32864" type="image/png"/></item><item><title>Data Build Tool (dbt) for Effective Data Transformation on AWS – Part 5 Athena</title><link>https://jaehyeon.me/blog/2022-12-06-dbt-on-aws-part-5-athena/</link><pubDate>Tue, 06 Dec 2022 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2022-12-06-dbt-on-aws-part-5-athena/</guid><description><![CDATA[<p>The <a href="https://docs.getdbt.com/docs/introduction" target="_blank" rel="noopener noreferrer">data build tool (dbt)<i class="fas fa-external-link-square-alt ms-1"></i></a> is an effective data transformation tool and it supports key AWS analytics services - Redshift, Glue, EMR and Athena. In the previous posts, we discussed benefits of a common data transformation tool and the potential of dbt to cover a wide range of data projects from data warehousing to data lake to data lakehouse. Demo data projects that target Redshift Serverless, Glue, EMR on EC2 and EMR on EKS are illustrated as well. In the last part of the dbt on AWS series, we discuss data transformation pipelines using dbt on <a href="https://aws.amazon.com/athena" target="_blank" rel="noopener noreferrer">Amazon Athena<i class="fas fa-external-link-square-alt ms-1"></i></a>. <a href="https://www.imdb.com/interfaces/" target="_blank" rel="noopener noreferrer">Subsets of IMDb data<i class="fas fa-external-link-square-alt ms-1"></i></a> are used as source and data models are developed in multiple layers according to the <a href="https://docs.getdbt.com/guides/best-practices/how-we-structure/1-guide-overview" target="_blank" rel="noopener noreferrer">dbt best practices<i class="fas fa-external-link-square-alt ms-1"></i></a>. A list of posts of this series can be found below.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2022-12-06-dbt-on-aws-part-5-athena/featured.png" length="91796" type="image/png"/></item><item><title>Data Build Tool (dbt) for Effective Data Transformation on AWS – Part 4 EMR on EKS</title><link>https://jaehyeon.me/blog/2022-11-01-dbt-on-aws-part-4-emr-eks/</link><pubDate>Tue, 01 Nov 2022 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2022-11-01-dbt-on-aws-part-4-emr-eks/</guid><description><![CDATA[<p>The <a href="https://docs.getdbt.com/docs/introduction" target="_blank" rel="noopener noreferrer">data build tool (dbt)<i class="fas fa-external-link-square-alt ms-1"></i></a> is an effective data transformation tool and it supports key AWS analytics services - Redshift, Glue, EMR and Athena. In the previous posts, we discussed benefits of a common data transformation tool and the potential of dbt to cover a wide range of data projects from data warehousing to data lake to data lakehouse. Demo data projects that target Redshift Serverless, Glue and EMR on EC2 are illustrated as well. In part 4 of the dbt on AWS series, we discuss data transformation pipelines using dbt on <a href="https://aws.amazon.com/emr/features/eks/" target="_blank" rel="noopener noreferrer">Amazon EMR on EKS<i class="fas fa-external-link-square-alt ms-1"></i></a>. As Spark Submit does not allow the spark thrift server to run in cluster mode on Kubernetes, a simple wrapper class is created to overcome the limitation and it makes the thrift server run indefinitely. <a href="https://www.imdb.com/interfaces/" target="_blank" rel="noopener noreferrer">Subsets of IMDb data<i class="fas fa-external-link-square-alt ms-1"></i></a> are used as source and data models are developed in multiple layers according to the <a href="https://docs.getdbt.com/guides/best-practices/how-we-structure/1-guide-overview" target="_blank" rel="noopener noreferrer">dbt best practices<i class="fas fa-external-link-square-alt ms-1"></i></a>. A list of posts of this series can be found below.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2022-11-01-dbt-on-aws-part-4-emr-eks/featured.png" length="91067" type="image/png"/></item><item><title>Data Build Tool (dbt) for Effective Data Transformation on AWS – Part 3 EMR on EC2</title><link>https://jaehyeon.me/blog/2022-10-19-dbt-on-aws-part-3-emr-ec2/</link><pubDate>Wed, 19 Oct 2022 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2022-10-19-dbt-on-aws-part-3-emr-ec2/</guid><description><![CDATA[<p>The <a href="https://docs.getdbt.com/docs/introduction" target="_blank" rel="noopener noreferrer">data build tool (dbt)<i class="fas fa-external-link-square-alt ms-1"></i></a> is an effective data transformation tool and it supports key AWS analytics services - Redshift, Glue, EMR and Athena. In the previous posts, we discussed benefits of a common data transformation tool and the potential of dbt to cover a wide range of data projects from data warehousing to data lake to data lakehouse. Demo data projects that target Redshift Serverless and Glue are illustrated as well. In part 3 of the dbt on AWS series, we discuss data transformation pipelines using dbt on <a href="https://aws.amazon.com/emr/" target="_blank" rel="noopener noreferrer">Amazon EMR<i class="fas fa-external-link-square-alt ms-1"></i></a>. <a href="https://www.imdb.com/interfaces/" target="_blank" rel="noopener noreferrer">Subsets of IMDb data<i class="fas fa-external-link-square-alt ms-1"></i></a> are used as source and data models are developed in multiple layers according to the <a href="https://docs.getdbt.com/guides/best-practices/how-we-structure/1-guide-overview" target="_blank" rel="noopener noreferrer">dbt best practices<i class="fas fa-external-link-square-alt ms-1"></i></a>. A list of posts of this series can be found below.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2022-10-19-dbt-on-aws-part-3-emr-ec2/featured.png" length="91067" type="image/png"/></item><item><title>Data Build Tool (dbt) for Effective Data Transformation on AWS – Part 2 Glue</title><link>https://jaehyeon.me/blog/2022-10-09-dbt-on-aws-part-2-glue/</link><pubDate>Sun, 09 Oct 2022 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2022-10-09-dbt-on-aws-part-2-glue/</guid><description><![CDATA[<p>The <a href="https://docs.getdbt.com/docs/introduction" target="_blank" rel="noopener noreferrer">data build tool (dbt)<i class="fas fa-external-link-square-alt ms-1"></i></a> is an effective data transformation tool and it supports key AWS analytics services - Redshift, Glue, EMR and Athena. In <a href="/blog/2022-09-28-dbt-on-aws-part-1-redshift">part 1</a>, we discussed benefits of a common data transformation tool and the potential of dbt to cover a wide range of data projects from data warehousing to data lake to data lakehouse. A demo data project that targets Redshift Serverless is illustrated as well. In part 2 of the dbt on AWS series, we discuss data transformation pipelines using dbt on <a href="https://aws.amazon.com/glue/" target="_blank" rel="noopener noreferrer">AWS Glue<i class="fas fa-external-link-square-alt ms-1"></i></a>. <a href="https://www.imdb.com/interfaces/" target="_blank" rel="noopener noreferrer">Subsets of IMDb data<i class="fas fa-external-link-square-alt ms-1"></i></a> are used as source and data models are developed in multiple layers according to the <a href="https://docs.getdbt.com/guides/best-practices/how-we-structure/1-guide-overview" target="_blank" rel="noopener noreferrer">dbt best practices<i class="fas fa-external-link-square-alt ms-1"></i></a>. A list of posts of this series can be found below.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2022-10-09-dbt-on-aws-part-2-glue/featured.png" length="90647" type="image/png"/></item><item><title>Data Build Tool (dbt) for Effective Data Transformation on AWS – Part 1 Redshift</title><link>https://jaehyeon.me/blog/2022-09-28-dbt-on-aws-part-1-redshift/</link><pubDate>Wed, 28 Sep 2022 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2022-09-28-dbt-on-aws-part-1-redshift/</guid><description><![CDATA[<p>The <a href="https://docs.getdbt.com/docs/introduction" target="_blank" rel="noopener noreferrer">data build tool (dbt)<i class="fas fa-external-link-square-alt ms-1"></i></a> is an effective data transformation tool and it supports key AWS analytics services - Redshift, Glue, EMR and Athena. In part 1 of the dbt on AWS series, we discuss data transformation pipelines using dbt on <a href="https://aws.amazon.com/redshift/redshift-serverless/" target="_blank" rel="noopener noreferrer">Redshift Serverless<i class="fas fa-external-link-square-alt ms-1"></i></a>. <a href="https://www.imdb.com/interfaces/" target="_blank" rel="noopener noreferrer">Subsets of IMDb data<i class="fas fa-external-link-square-alt ms-1"></i></a> are used as source and data models are developed in multiple layers according to the <a href="https://docs.getdbt.com/guides/best-practices/how-we-structure/1-guide-overview" target="_blank" rel="noopener noreferrer">dbt best practices<i class="fas fa-external-link-square-alt ms-1"></i></a>.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2022-09-28-dbt-on-aws-part-1-redshift/featured.png" length="97234" type="image/png"/></item><item><title>Develop and Test Apache Spark Apps for EMR Remotely Using Visual Studio Code</title><link>https://jaehyeon.me/blog/2022-09-07-emr-remote-dev/</link><pubDate>Wed, 07 Sep 2022 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2022-09-07-emr-remote-dev/</guid><description><![CDATA[<p>When we develop a Spark application on EMR, we can use <a href="/blog/2022-05-08-emr-local-dev">docker for local development</a> or notebooks via <a href="https://aws.amazon.com/emr/features/studio/" target="_blank" rel="noopener noreferrer">EMR Studio<i class="fas fa-external-link-square-alt ms-1"></i></a> (or <a href="https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-managed-notebooks.html" target="_blank" rel="noopener noreferrer">EMR Notebooks<i class="fas fa-external-link-square-alt ms-1"></i></a>). However, the local development option is not viable if the size of data is large. Also, I am not a fan of notebooks as it is not possible to utilise the features my editor supports such as syntax highlighting, autocomplete and code formatting. Moreover, it is not possible to organise code into modules and to perform unit testing properly with that option. In this post, We will discuss how to set up a remote development environment on an EMR cluster deployed in a private subnet with VPN and the <a href="https://code.visualstudio.com/docs/remote/ssh" target="_blank" rel="noopener noreferrer">VS Code remote SSH extension<i class="fas fa-external-link-square-alt ms-1"></i></a>. Typical Spark development examples will be illustrated while sharing the cluster with multiple users. Overall it brings another effective way of developing Spark apps on EMR, which improves developer experience significantly.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2022-09-07-emr-remote-dev/featured.png" length="72448" type="image/png"/></item><item><title>Manage EMR on EKS with Terraform</title><link>https://jaehyeon.me/blog/2022-08-26-emr-on-eks-with-terraform/</link><pubDate>Fri, 26 Aug 2022 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2022-08-26-emr-on-eks-with-terraform/</guid><description><![CDATA[<p><a href="https://aws.amazon.com/emr/features/eks/" target="_blank" rel="noopener noreferrer">Amazon EMR on EKS<i class="fas fa-external-link-square-alt ms-1"></i></a> is a deployment option for Amazon EMR that allows you to automate the provisioning and management of open-source big data frameworks on EKS. While <a href="https://eksctl.io/" target="_blank" rel="noopener noreferrer">eksctl<i class="fas fa-external-link-square-alt ms-1"></i></a> is popular for working with <a href="https://aws.amazon.com/eks/" target="_blank" rel="noopener noreferrer">Amazon EKS<i class="fas fa-external-link-square-alt ms-1"></i></a> clusters, it has limitations when it comes to building infrastructure that integrates multiple AWS services. Also, it is not straightforward to update EKS cluster resources incrementally with it. On the other hand <a href="https://www.terraform.io/" target="_blank" rel="noopener noreferrer">Terraform<i class="fas fa-external-link-square-alt ms-1"></i></a> can be an effective tool for managing infrastructure that includes not only EKS and EMR virtual clusters but also other AWS resources. Moreover, Terraform has a wide range of <a href="https://www.terraform.io/language/modules" target="_blank" rel="noopener noreferrer">modules<i class="fas fa-external-link-square-alt ms-1"></i></a>, and it can even be simpler to build and manage infrastructure using those compared to the CLI tool. In this post, we’ll discuss how to provision and manage Spark jobs on EMR on EKS with Terraform. <a href="https://aws-ia.github.io/terraform-aws-eks-blueprints/" target="_blank" rel="noopener noreferrer">Amazon EKS Blueprints for Terraform<i class="fas fa-external-link-square-alt ms-1"></i></a> will be used for provisioning EKS, EMR virtual cluster and related resources. Also, Spark job autoscaling will be managed by <a href="https://karpenter.sh/" target="_blank" rel="noopener noreferrer">Karpenter<i class="fas fa-external-link-square-alt ms-1"></i></a> where two Spark jobs with and without <a href="https://spark.apache.org/docs/latest/job-scheduling.html#dynamic-resource-allocation" target="_blank" rel="noopener noreferrer">Dynamic Resource Allocation (DRA)<i class="fas fa-external-link-square-alt ms-1"></i></a> will be compared.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2022-08-26-emr-on-eks-with-terraform/featured.png" length="67936" type="image/png"/></item><item><title>Revisit AWS Lambda Invoke Function Operator of Apache Airflow</title><link>https://jaehyeon.me/blog/2022-08-06-revisit-lambda-operator/</link><pubDate>Sat, 06 Aug 2022 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2022-08-06-revisit-lambda-operator/</guid><description><![CDATA[<p><a href="https://airflow.apache.org/" target="_blank" rel="noopener noreferrer">Apache Airflow<i class="fas fa-external-link-square-alt ms-1"></i></a> is a popular workflow management platform. A wide range of AWS services are integrated with the platform by <a href="https://airflow.apache.org/docs/apache-airflow-providers-amazon/stable/operators/index.html" target="_blank" rel="noopener noreferrer">Amazon AWS Operators<i class="fas fa-external-link-square-alt ms-1"></i></a>. AWS Lambda is one of the integrated services, and it can be used to develop workflows efficiently. The current <a href="https://airflow.apache.org/docs/apache-airflow-providers-amazon/stable/operators/lambda.html" target="_blank" rel="noopener noreferrer">Lambda Operator<i class="fas fa-external-link-square-alt ms-1"></i></a>, however, just invokes a Lambda function, and it can fail to report the invocation result of a function correctly and to record the exact error message from failure. In this post, we’ll discuss a custom Lambda operator that handles those limitations.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2022-08-06-revisit-lambda-operator/featured.png" length="24814" type="image/png"/></item><item><title>Serverless Application Model (SAM) for Data Professionals</title><link>https://jaehyeon.me/blog/2022-07-18-sam-for-data-professionals/</link><pubDate>Mon, 18 Jul 2022 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2022-07-18-sam-for-data-professionals/</guid><description><![CDATA[<p><a href="https://aws.amazon.com/lambda/" target="_blank" rel="noopener noreferrer">AWS Lambda<i class="fas fa-external-link-square-alt ms-1"></i></a> provides serverless computing capabilities, and it can be used for performing validation or light processing/transformation of data. Moreover, with its integration with more than 140 AWS services, it facilitates building complex systems employing <a href="https://docs.aws.amazon.com/lambda/latest/operatorguide/event-driven-architectures.html" target="_blank" rel="noopener noreferrer">event-driven architectures<i class="fas fa-external-link-square-alt ms-1"></i></a>. There are many ways to build serverless applications and one of the most efficient ways is using specialised frameworks such as the <a href="https://aws.amazon.com/serverless/sam/" target="_blank" rel="noopener noreferrer">AWS Serverless Application Model (SAM)<i class="fas fa-external-link-square-alt ms-1"></i></a> and <a href="https://www.serverless.com/framework/docs" target="_blank" rel="noopener noreferrer">Serverless Framework<i class="fas fa-external-link-square-alt ms-1"></i></a>. In this post, I’ll demonstrate how to build a serverless data processing application using SAM.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2022-07-18-sam-for-data-professionals/featured.png" length="22838" type="image/png"/></item><item><title>Data Warehousing ETL Demo with Apache Iceberg on EMR Local Environment</title><link>https://jaehyeon.me/blog/2022-06-26-iceberg-etl-demo/</link><pubDate>Sun, 26 Jun 2022 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2022-06-26-iceberg-etl-demo/</guid><description><![CDATA[<p>Unlike traditional Data Lake, new table formats (<a href="https://iceberg.apache.org/" target="_blank" rel="noopener noreferrer">Iceberg<i class="fas fa-external-link-square-alt ms-1"></i></a>, <a href="https://hudi.apache.org/" target="_blank" rel="noopener noreferrer">Hudi<i class="fas fa-external-link-square-alt ms-1"></i></a> and <a href="https://delta.io/" target="_blank" rel="noopener noreferrer">Delta Lake<i class="fas fa-external-link-square-alt ms-1"></i></a>) support <a href="https://iceberg.apache.org/docs/latest/spark-writes/" target="_blank" rel="noopener noreferrer">features<i class="fas fa-external-link-square-alt ms-1"></i></a> that can be used to apply data warehousing patterns, which can bring a way to be rescued from <a href="https://www.gartner.com/en/newsroom/press-releases/2014-07-28-gartner-says-beware-of-the-data-lake-fallacy" target="_blank" rel="noopener noreferrer">Data Swamp<i class="fas fa-external-link-square-alt ms-1"></i></a>. In this post, we&rsquo;ll discuss how to implement ETL using retail analytics data. It has two dimension data (user and product) and a single fact data (order). The dimension data sets have different ETL strategies depending on whether to track historical changes. For the fact data, the primary keys of the dimension data are added to facilitate later queries. We&rsquo;ll use Iceberg for data storage/management and Spark for data processing. Instead of provisioning an EMR cluster, a local development environment will be used. Finally, the ETL results will be queried by Athena for verification.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2022-06-26-iceberg-etl-demo/featured.png" length="43604" type="image/png"/></item><item><title>Develop and Test Apache Spark Apps for EMR Locally Using Docker</title><link>https://jaehyeon.me/blog/2022-05-08-emr-local-dev/</link><pubDate>Sun, 08 May 2022 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2022-05-08-emr-local-dev/</guid><description><![CDATA[<p>[<strong>UPDATE 2023-12-07</strong>]</p>
<ul>
<li>I wrote a <a href="/blog/2023-12-07-flink-spark-local-dev">new post</a> that simplifies the Spark configuration dramatically. Besides, the log configuration is based on Log4J2, which applies to newer Spark versions. Moreover, the container is configured to run the Spark History Server, and it allows us to debug and diagnose completed and running Spark applications. I recommend referring to the new post.</li>
</ul>
<p><a href="https://aws.amazon.com/emr/" target="_blank" rel="noopener noreferrer">Amazon EMR<i class="fas fa-external-link-square-alt ms-1"></i></a> is a managed service that simplifies running Apache Spark on AWS. It has multiple deployment options that cover EC2, <a href="https://aws.amazon.com/emr/features/eks/" target="_blank" rel="noopener noreferrer">EKS<i class="fas fa-external-link-square-alt ms-1"></i></a>, <a href="https://aws.amazon.com/emr/features/outposts/" target="_blank" rel="noopener noreferrer">Outposts<i class="fas fa-external-link-square-alt ms-1"></i></a> and <a href="https://aws.amazon.com/emr/serverless/" target="_blank" rel="noopener noreferrer">Serverless<i class="fas fa-external-link-square-alt ms-1"></i></a>. For development and testing, <a href="https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-managed-notebooks.html" target="_blank" rel="noopener noreferrer">EMR Notebooks<i class="fas fa-external-link-square-alt ms-1"></i></a> or <a href="https://aws.amazon.com/emr/features/studio/" target="_blank" rel="noopener noreferrer">EMR Studio<i class="fas fa-external-link-square-alt ms-1"></i></a> can be an option. Both provide a Jupyter Notebook environment and the former is only available for EMR on EC2. There are cases, however, that development (and learning) is performed in a local environment more efficiently. The AWS Glue team understands this demand, and they illustrate how to make use of a custom Docker image for Glue in a <a href="https://aws.amazon.com/blogs/big-data/develop-and-test-aws-glue-version-3-0-jobs-locally-using-a-docker-container/" target="_blank" rel="noopener noreferrer">recent blog post<i class="fas fa-external-link-square-alt ms-1"></i></a>. However, we don’t hear similar news from the EMR team. In order to fill the gap, we’ll discuss how to create a Spark local development environment for EMR using Docker and/or VSCode. Typical Spark development examples will be demonstrated, which covers Spark Submit, Pytest, PySpark shell, Jupyter Notebook and Spark Structured Streaming. For the Spark Submit and Jupyter Notebook examples, <a href="https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-spark-glue.html" target="_blank" rel="noopener noreferrer">Glue Catalog integration<i class="fas fa-external-link-square-alt ms-1"></i></a> will be illustrated as well. And both the cases of utilising <a href="https://code.visualstudio.com/docs/remote/containers" target="_blank" rel="noopener noreferrer">Visual Studio Code Remote - Containers<i class="fas fa-external-link-square-alt ms-1"></i></a> extension and running as an isolated container will be covered in some key examples.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2022-05-08-emr-local-dev/featured.png" length="25693" type="image/png"/></item><item><title>Use External Schema Registry with MSK Connect – Part 2 MSK Deployment</title><link>https://jaehyeon.me/blog/2022-04-03-schema-registry-part2/</link><pubDate>Sun, 03 Apr 2022 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2022-04-03-schema-registry-part2/</guid><description><![CDATA[<p>In the <a href="/blog/2022-03-07-schema-registry-part1">previous post</a>, we discussed a Change Data Capture (CDC) solution with a schema registry. A local development environment is set up using Docker Compose. The Debezium and Confluent S3 connectors are deployed with the Confluent Avro converter and the Apicurio registry is used as the schema registry service. A quick example is shown to illustrate how schema evolution can be managed by the schema registry. In this post, we&rsquo;ll build the solution on AWS using <a href="https://aws.amazon.com/msk/" target="_blank" rel="noopener noreferrer">MSK<i class="fas fa-external-link-square-alt ms-1"></i></a>, <a href="https://aws.amazon.com/msk/features/msk-connect/" target="_blank" rel="noopener noreferrer">MSK Connect<i class="fas fa-external-link-square-alt ms-1"></i></a>, <a href="https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.AuroraPostgreSQL.html" target="_blank" rel="noopener noreferrer">Aurora PostgreSQL<i class="fas fa-external-link-square-alt ms-1"></i></a> and <a href="https://aws.amazon.com/ecs/" target="_blank" rel="noopener noreferrer">ECS<i class="fas fa-external-link-square-alt ms-1"></i></a>.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2022-04-03-schema-registry-part2/featured.png" length="59689" type="image/png"/></item><item><title>Use External Schema Registry with MSK Connect – Part 1 Local Development</title><link>https://jaehyeon.me/blog/2022-03-07-schema-registry-part1/</link><pubDate>Mon, 07 Mar 2022 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2022-03-07-schema-registry-part1/</guid><description><![CDATA[<p>When we discussed a Change Data Capture (CDC) solution in <a href="/blog/2021-12-12-datalake-demo-part2">one of the earlier posts</a>, we used the JSON converter that comes with Kafka Connect. We optionally enabled the key and value schemas and the topic messages include those schemas together with payload. It seems to be convenient at first as the messages are saved into S3 on their own. However, it became cumbersome when we tried to use the <a href="https://hudi.apache.org/docs/hoodie_streaming_ingestion" target="_blank" rel="noopener noreferrer">DeltaStreamer<i class="fas fa-external-link-square-alt ms-1"></i></a> utility. Specifically it requires the scheme of the files, but unfortunately we cannot use the schema that is generated by the default JSON converter - it returns the <a href="https://www.confluent.io/blog/kafka-connect-deep-dive-converters-serialization-explained/#json-schemas" target="_blank" rel="noopener noreferrer">struct type<i class="fas fa-external-link-square-alt ms-1"></i></a>, which is not supported by the Hudi utility. In order to handle this issue, we created a schema with the <a href="https://avro.apache.org/docs/current/spec.html#schema_record" target="_blank" rel="noopener noreferrer">record type<i class="fas fa-external-link-square-alt ms-1"></i></a> using the Confluent Avro converter and used it after saving on S3. However, as we aimed to manage a long-running process, generating a schema manually was not an optimal solution because, for example, we&rsquo;re not able to handle schema evolution effectively. In this post, we&rsquo;ll discuss an improved architecture that makes use of a schema registry that resides outside the Kafka cluster and allows the producers and consumers to reference the schemas externally.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2022-03-07-schema-registry-part1/featured.png" length="59689" type="image/png"/></item><item><title>Simplify Your Development on AWS with Terraform</title><link>https://jaehyeon.me/blog/2022-02-06-dev-infra-terraform/</link><pubDate>Sun, 06 Feb 2022 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2022-02-06-dev-infra-terraform/</guid><description><![CDATA[<p>When I wrote my data lake demo series (<a href="/blog/2021-12-05-datalake-demo-part1">part 1</a>, <a href="/blog/2021-12-12-datalake-demo-part2">part 2</a> and <a href="/blog/2021-12-19-datalake-demo-part3">part 3</a>) recently, I used an Aurora PostgreSQL, MSK and EMR cluster. All of them were deployed to private subnets and dedicated infrastructure was created using CloudFormation. Using the infrastructure as code (IaC) tool helped a lot, but it resulted in creating 7 CloudFormation stacks, which was a bit harder to manage in the end. Then I looked into how to simplify building infrastructure and managing resources on AWS and decided to use Terraform instead. I find it has useful constructs (e.g. <a href="https://blog.knoldus.com/meta-arguments-in-terraform/" target="_blank" rel="noopener noreferrer">meta-arguments<i class="fas fa-external-link-square-alt ms-1"></i></a>) to make it simpler to create and manage resources. It also has a wide range of useful <a href="https://registry.terraform.io/namespaces/terraform-aws-modules" target="_blank" rel="noopener noreferrer">modules<i class="fas fa-external-link-square-alt ms-1"></i></a> that facilitate development significantly. In this post, we’ll build an infrastructure for development on AWS with Terraform. A VPN server will also be included in order to improve developer experience by accessing resources in private subnets from developer machines.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2022-02-06-dev-infra-terraform/featured.png" length="46253" type="image/png"/></item><item><title>EMR on EKS by Example</title><link>https://jaehyeon.me/blog/2022-01-17-emr-on-eks-by-example/</link><pubDate>Mon, 17 Jan 2022 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2022-01-17-emr-on-eks-by-example/</guid><description><![CDATA[<p><a href="https://aws.amazon.com/emr/features/eks/" target="_blank" rel="noopener noreferrer">EMR on EKS<i class="fas fa-external-link-square-alt ms-1"></i></a> provides a deployment option for <a href="https://aws.amazon.com/emr/" target="_blank" rel="noopener noreferrer">Amazon EMR<i class="fas fa-external-link-square-alt ms-1"></i></a> that allows you to automate the provisioning and management of open-source big data frameworks on <a href="https://aws.amazon.com/eks/" target="_blank" rel="noopener noreferrer">Amazon EKS<i class="fas fa-external-link-square-alt ms-1"></i></a>. While a wide range of open source big data components are available in EMR on EC2, only Apache Spark is available in EMR on EKS. It is more flexible, however, that applications of different EMR versions can be run in multiple availability zones on either EC2 or Fargate. Also, other types of containerized applications can be deployed on the same EKS cluster. Therefore, if you have or plan to have, for example, <a href="https://airflow.apache.org/" target="_blank" rel="noopener noreferrer">Apache Airflow<i class="fas fa-external-link-square-alt ms-1"></i></a>, <a href="https://superset.apache.org/" target="_blank" rel="noopener noreferrer">Apache Superset<i class="fas fa-external-link-square-alt ms-1"></i></a> or <a href="https://www.kubeflow.org/" target="_blank" rel="noopener noreferrer">Kubeflow<i class="fas fa-external-link-square-alt ms-1"></i></a> as your analytics toolkits, it can be an effective way to manage big data (as well as non-big data) workloads. While Glue is more for ETL, EMR on EKS can also be used for other types of tasks such as machine learning. Moreover, it allows you to build a Spark application, not a <em>Gluish</em> Spark application. For example, while you have to use custom connectors for <a href="https://aws.amazon.com/marketplace/pp/prodview-zv3vmwbkuat2e" target="_blank" rel="noopener noreferrer">Hudi<i class="fas fa-external-link-square-alt ms-1"></i></a> or <a href="https://aws.amazon.com/marketplace/pp/prodview-iicxofvpqvsio" target="_blank" rel="noopener noreferrer">Iceberg<i class="fas fa-external-link-square-alt ms-1"></i></a> for Glue, you can use their native libraries with EMR on EKS. In this post, we&rsquo;ll discuss EMR on EKS with simple and elaborated examples.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2022-01-17-emr-on-eks-by-example/featured.png" length="76740" type="image/png"/></item><item><title>Data Lake Demo using Change Data Capture (CDC) on AWS – Part 3 Implement Data Lake</title><link>https://jaehyeon.me/blog/2021-12-19-datalake-demo-part3/</link><pubDate>Sun, 19 Dec 2021 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2021-12-19-datalake-demo-part3/</guid><description><![CDATA[<p>In the <a href="/blog/2021-12-12-datalake-demo-part2">previous post</a>, we created a VPC that has private and public subnets in 2 availability zones in order to build and deploy the data lake solution on AWS. NAT instances are created to forward outbound traffic to the internet and a VPN bastion host is set up to facilitate deployment. An <a href="https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.AuroraPostgreSQL.html" target="_blank" rel="noopener noreferrer">Aurora PostgreSQL<i class="fas fa-external-link-square-alt ms-1"></i></a> cluster is deployed to host the source database and a Python command line app is used to create the database. To develop data ingestion using CDC, an <a href="https://aws.amazon.com/msk/" target="_blank" rel="noopener noreferrer">Amazon MSK<i class="fas fa-external-link-square-alt ms-1"></i></a> cluster is deployed and the <a href="https://debezium.io/documentation/reference/stable/connectors/postgresql.html" target="_blank" rel="noopener noreferrer">Debezium source<i class="fas fa-external-link-square-alt ms-1"></i></a> and <a href="https://lenses.io/blog/2020/11/new-kafka-to-S3-connector/" target="_blank" rel="noopener noreferrer">Lenses S3 sink<i class="fas fa-external-link-square-alt ms-1"></i></a> connectors are created on <a href="https://aws.amazon.com/msk/features/msk-connect/" target="_blank" rel="noopener noreferrer">MSK Connect<i class="fas fa-external-link-square-alt ms-1"></i></a>. We also confirmed the order creation and update events are captured as expected. As the last part of this series, we&rsquo;ll build an <a href="https://hudi.apache.org/docs/writing_data/#deltastreamer" target="_blank" rel="noopener noreferrer">Apache Hudi DeltaStreamer<i class="fas fa-external-link-square-alt ms-1"></i></a> app on <a href="https://aws.amazon.com/emr/" target="_blank" rel="noopener noreferrer">Amazon EMR<i class="fas fa-external-link-square-alt ms-1"></i></a> and use the resulting Hudi table with <a href="https://aws.amazon.com/athena/" target="_blank" rel="noopener noreferrer">Amazon Athena<i class="fas fa-external-link-square-alt ms-1"></i></a> and <a href="https://aws.amazon.com/quicksight/" target="_blank" rel="noopener noreferrer">Amazon Quicksight<i class="fas fa-external-link-square-alt ms-1"></i></a> to build a dashboard.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2021-12-19-datalake-demo-part3/featured.png" length="164526" type="image/png"/></item><item><title>Data Lake Demo using Change Data Capture (CDC) on AWS – Part 2 Implement CDC</title><link>https://jaehyeon.me/blog/2021-12-12-datalake-demo-part2/</link><pubDate>Sun, 12 Dec 2021 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2021-12-12-datalake-demo-part2/</guid><description><![CDATA[<p>In the <a href="/blog/2021-12-05-datalake-demo-part1">previous post</a>, we discussed a data lake solution where data ingestion is performed using <a href="https://www.redhat.com/en/topics/integration/what-is-change-data-capture#what-is-cdc" target="_blank" rel="noopener noreferrer">change data capture (CDC)<i class="fas fa-external-link-square-alt ms-1"></i></a> and the output files are <em>upserted</em> to an <a href="https://hudi.apache.org/" target="_blank" rel="noopener noreferrer">Apache Hudi<i class="fas fa-external-link-square-alt ms-1"></i></a> table. Being registered to <a href="https://docs.aws.amazon.com/glue/latest/dg/populate-data-catalog.html" target="_blank" rel="noopener noreferrer">Glue Data Catalog<i class="fas fa-external-link-square-alt ms-1"></i></a>, it can be used for ad-hoc queries and report/dashboard creation. The <a href="https://docs.yugabyte.com/latest/sample-data/northwind/" target="_blank" rel="noopener noreferrer">Northwind database<i class="fas fa-external-link-square-alt ms-1"></i></a> is used as the source database and, following the <a href="https://microservices.io/patterns/data/transactional-outbox.html" target="_blank" rel="noopener noreferrer">transactional outbox pattern<i class="fas fa-external-link-square-alt ms-1"></i></a>, order-related changes are _upserted _to an outbox table by triggers. The data ingestion is developed using Kafka connectors in the <a href="https://docs.confluent.io/platform/current/quickstart/ce-docker-quickstart.html" target="_blank" rel="noopener noreferrer">local Confluent platform<i class="fas fa-external-link-square-alt ms-1"></i></a> where the <a href="https://debezium.io/documentation/reference/stable/connectors/postgresql.html" target="_blank" rel="noopener noreferrer">Debezium for PostgreSQL<i class="fas fa-external-link-square-alt ms-1"></i></a> is used as the source connector and the <a href="https://lenses.io/blog/2020/11/new-kafka-to-S3-connector/" target="_blank" rel="noopener noreferrer">Lenses S3 sink connector<i class="fas fa-external-link-square-alt ms-1"></i></a> is used as the sink connector. We confirmed the order creation and update events are captured as expected, and it is ready for production deployment. In this post, we&rsquo;ll build the CDC part of the solution on AWS using <a href="https://aws.amazon.com/msk/" target="_blank" rel="noopener noreferrer">Amazon MSK<i class="fas fa-external-link-square-alt ms-1"></i></a> and <a href="https://aws.amazon.com/msk/features/msk-connect/" target="_blank" rel="noopener noreferrer">MSK Connect<i class="fas fa-external-link-square-alt ms-1"></i></a>.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2021-12-12-datalake-demo-part2/featured.png" length="164526" type="image/png"/></item><item><title>Data Lake Demo using Change Data Capture (CDC) on AWS – Part 1 Local Development</title><link>https://jaehyeon.me/blog/2021-12-05-datalake-demo-part1/</link><pubDate>Sun, 05 Dec 2021 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2021-12-05-datalake-demo-part1/</guid><description><![CDATA[<p><a href="https://www.redhat.com/en/topics/integration/what-is-change-data-capture#what-is-cdc" target="_blank" rel="noopener noreferrer">Change data capture (CDC)<i class="fas fa-external-link-square-alt ms-1"></i></a> is a proven data integration pattern that has a wide range of applications. Among those, data replication to data lakes is a good use case in data engineering. Coupled with <a href="https://lakefs.io/hudi-iceberg-and-delta-lake-data-lake-table-formats-compared/" target="_blank" rel="noopener noreferrer">best-in-breed data lake formats<i class="fas fa-external-link-square-alt ms-1"></i></a> such as <a href="https://hudi.apache.org/" target="_blank" rel="noopener noreferrer">Apache Hudi<i class="fas fa-external-link-square-alt ms-1"></i></a>, we can build an efficient data replication solution. This is the first post of the data lake demo series. Over time, we&rsquo;ll build a data lake that uses CDC. As a starting point, we&rsquo;ll discuss the source database and CDC streaming infrastructure in the local environment.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2021-12-05-datalake-demo-part1/featured.png" length="164526" type="image/png"/></item><item><title>Local Development of AWS Glue 3.0 and Later</title><link>https://jaehyeon.me/blog/2021-11-14-glue-3-local-development/</link><pubDate>Sun, 14 Nov 2021 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2021-11-14-glue-3-local-development/</guid><description><![CDATA[<p>In an <a href="/blog/2021-08-20-glue-local-development">earlier post</a>, I demonstrated how to set up a local development environment for AWS Glue 1.0 and 2.0 using a <a href="https://aws.amazon.com/blogs/big-data/developing-aws-glue-etl-jobs-locally-using-a-container/" target="_blank" rel="noopener noreferrer">docker image that is published by the AWS Glue team<i class="fas fa-external-link-square-alt ms-1"></i></a> and the <a href="https://code.visualstudio.com/docs/remote/containers" target="_blank" rel="noopener noreferrer">Visual Studio Code Remote – Containers<i class="fas fa-external-link-square-alt ms-1"></i></a> extension. Recently <a href="https://aws.amazon.com/about-aws/whats-new/2021/08/spark-3-1-runtime-aws-glue-3-0/" target="_blank" rel="noopener noreferrer">AWS Glue 3.0 was released<i class="fas fa-external-link-square-alt ms-1"></i></a>, but a docker image for this version is not published. In this post, I&rsquo;ll illustrate how to create a development environment for AWS Glue 3.0 (and later versions) by building a custom docker image.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2021-11-14-glue-3-local-development/featured.png" length="30923" type="image/png"/></item><item><title>AWS Glue Local Development with Docker and Visual Studio Code</title><link>https://jaehyeon.me/blog/2021-08-20-glue-local-development/</link><pubDate>Fri, 20 Aug 2021 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2021-08-20-glue-local-development/</guid><description><![CDATA[<p>As described in the product page, <a href="https://aws.amazon.com/glue" target="_blank" rel="noopener noreferrer">AWS Glue<i class="fas fa-external-link-square-alt ms-1"></i></a> is a <em>serverless</em> data integration service that makes it easy to discover, prepare, and combine data for analytics, machine learning, and application development. For development, a development endpoint is recommended, but it can be costly, inconvenient or <a href="https://docs.aws.amazon.com/glue/latest/dg/reduced-start-times-spark-etl-jobs.html" target="_blank" rel="noopener noreferrer">unavailable (for Glue 2.0)<i class="fas fa-external-link-square-alt ms-1"></i></a>. The <a href="https://aws.amazon.com/blogs/big-data/developing-aws-glue-etl-jobs-locally-using-a-container/" target="_blank" rel="noopener noreferrer">AWS Glue team published a Docker image<i class="fas fa-external-link-square-alt ms-1"></i></a> that includes the AWS Glue binaries and all the dependencies packaged together. After inspecting it, I find some modifications are necessary in order to build a development environment on it. In this post, I&rsquo;ll demonstrate how to build development environments for AWS Glue 1.0 and 2.0 using the Docker image and the <a href="https://code.visualstudio.com/docs/remote/containers" target="_blank" rel="noopener noreferrer">Visual Studio Code Remote - Containers<i class="fas fa-external-link-square-alt ms-1"></i></a> extension.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2021-08-20-glue-local-development/featured.png" length="19535" type="image/png"/></item><item><title>Thoughts on Apache Airflow AWS Lambda Operator</title><link>https://jaehyeon.me/blog/2020-04-13-airflow-lambda-operator/</link><pubDate>Mon, 13 Apr 2020 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2020-04-13-airflow-lambda-operator/</guid><description><![CDATA[<p><a href="https://airflow.apache.org/" target="_blank" rel="noopener noreferrer">Apache Airflow<i class="fas fa-external-link-square-alt ms-1"></i></a> is a popular open-source workflow management platform. Typically tasks run remotely by <a href="http://www.celeryproject.org/" target="_blank" rel="noopener noreferrer">Celery<i class="fas fa-external-link-square-alt ms-1"></i></a> workers for scalability. In AWS, however, scalability can also be achieved using serverless computing services in a simpler way. For example, the <a href="https://airflow.apache.org/docs/apache-airflow-providers-amazon/stable/operators/ecs.html" target="_blank" rel="noopener noreferrer">ECS Operator<i class="fas fa-external-link-square-alt ms-1"></i></a> allows to run <em>dockerized</em> tasks and, with the <em>Fargate</em> launch type, they can run in a serverless environment.</p>
<p>The ECS Operator alone is not sufficient because it can take up to several minutes to pull a Docker image and to set up network interface (for the case of <em>Fargate</em> launch type). Due to its latency, it is not suitable for frequently-running tasks. On the other hand, the latency of a Lambda function is negligible so that it&rsquo;s more suitable for managing such tasks.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2020-04-13-airflow-lambda-operator/featured.png" length="44994" type="image/png"/></item><item><title>AWS Local Development with LocalStack</title><link>https://jaehyeon.me/blog/2019-07-20-aws-localstack/</link><pubDate>Sat, 20 Jul 2019 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2019-07-20-aws-localstack/</guid><description><![CDATA[<p><a href="https://github.com/localstack/localstack" target="_blank" rel="noopener noreferrer">LocalStack<i class="fas fa-external-link-square-alt ms-1"></i></a> provides an easy-to-use test/mocking framework for developing AWS applications. In this post, I&rsquo;ll demonstrate how to utilize LocalStack for development using a web service.</p>
<p>Specifically a simple web service built with <a href="https://flask-restplus.readthedocs.io/en/stable/" target="_blank" rel="noopener noreferrer">Flask-RestPlus<i class="fas fa-external-link-square-alt ms-1"></i></a> is used. It supports simple CRUD operations against a database table. It is set that SQS and Lambda are used for creating and updating a record. When a <em>POST</em> or <em>PUT</em> request is made, the service sends a message to a SQS queue and directly returns <em>204</em> reponse. Once a message is received, a Lambda function is invoked and a relevant database operation is performed.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2019-07-20-aws-localstack/featured.png" length="164886" type="image/png"/></item><item><title>Serverless Data Product POC Backend Part IV - Serving R ML Model via S3</title><link>https://jaehyeon.me/blog/2017-04-17-serverless-data-product-4/</link><pubDate>Mon, 17 Apr 2017 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2017-04-17-serverless-data-product-4/</guid><description><![CDATA[<p>In the previous posts, it is discussed how to package/deploy an <a href="https://www.r-project.org/about.html" target="_blank" rel="noopener noreferrer">R<i class="fas fa-external-link-square-alt ms-1"></i></a> model with <a href="https://aws.amazon.com/lambda/details/" target="_blank" rel="noopener noreferrer">AWS Lambda<i class="fas fa-external-link-square-alt ms-1"></i></a> and to expose the Lambda function via <a href="https://aws.amazon.com/api-gateway/" target="_blank" rel="noopener noreferrer">Amazon API Gateway<i class="fas fa-external-link-square-alt ms-1"></i></a>. Main benefits of <strong>serverless architecture</strong> is cost-effectiveness and being hassle-free from provisioning/managing servers. While the API returns a predicted admission status value given <em>GRE</em>, <em>GPA</em> and <em>Rank</em>, there is an issue if it is served within a web application: <em>Cross-Origin Resource Sharing (CORS)</em>. This post discusses how to resolve this issue by updating API configuration and the Lambda function handler with a simple web application. Also it is illustrated how to host the application in a serverless environment.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2017-04-17-serverless-data-product-4/featured.png" length="225463" type="image/png"/></item><item><title>Serverless Data Product POC Backend Part III - Exposing R ML Model via APIG</title><link>https://jaehyeon.me/blog/2017-04-13-serverless-data-product-3/</link><pubDate>Thu, 13 Apr 2017 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2017-04-13-serverless-data-product-3/</guid><description><![CDATA[<p>In <a href="/blog/2017-04-08-serverless-data-product-1">Part I</a> of this series, R and necessary libraries/packages together with a Lambda function handler are packaged and saved to <a href="https://aws.amazon.com/s3/" target="_blank" rel="noopener noreferrer">Amazon S3<i class="fas fa-external-link-square-alt ms-1"></i></a>. Then, in <a href="/blog/2017-04-11-serverless-data-product-2">Part II</a>, the package is deployed at <a href="https://aws.amazon.com/lambda/" target="_blank" rel="noopener noreferrer">AWS Lambda<i class="fas fa-external-link-square-alt ms-1"></i></a> after creating and assigning a role to the Lambda function. Although the Lambda function can be called via the Invoke API, it&rsquo;ll be much more useful if the function can be called as a web service (or API). In this post, it is discussed how to expose the Lambda function via <a href="https://aws.amazon.com/api-gateway/" target="_blank" rel="noopener noreferrer">Amazon API Gateway<i class="fas fa-external-link-square-alt ms-1"></i></a>. After creating an API by integrating the Lambda function, it is protected with an API key. Finally a custom domain name is used as an alternative URL of the API.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2017-04-13-serverless-data-product-3/featured.png" length="173293" type="image/png"/></item><item><title>Serverless Data Product POC Backend Part II - Deploying R ML Model via Lambda</title><link>https://jaehyeon.me/blog/2017-04-11-serverless-data-product-2/</link><pubDate>Tue, 11 Apr 2017 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2017-04-11-serverless-data-product-2/</guid><description><![CDATA[<p>In the <a href="/blog/2017-04-08-serverless-data-product-1">previous post</a>, <strong>serverless</strong> <strong>event-driven</strong> application development is introduced. Also how to package R, necessary libraries/packages and a Lambda function handler is discussed. No need of provisioning/managing servers is one of the key benefits of the architecture. It is also a cost-effective way of delivering a data product as functions are executed <em>on-demand</em> rather than in servers that run 24/7. Furthermore <a href="https://aws.amazon.com/lambda/pricing/" target="_blank" rel="noopener noreferrer">AWS Lambda free tier<i class="fas fa-external-link-square-alt ms-1"></i></a> includes 1M free requests per month and 400,000 GB-seconds of compute time per month, which is available to both existing and new AWS customers indefinitely. (GB-seconds is applicable when execution is made with 1 GB of memory.) Lowering the size of memory increases the execution time and thus 3.2M seconds or about 37 days are free with 128 MB of memory (1 GB divided by 8) - note that CPU power is proportional to allocated memory.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2017-04-11-serverless-data-product-2/featured.png" length="139725" type="image/png"/></item><item><title>Serverless Data Product POC Backend Part I - Packaging R ML Model for Lambda</title><link>https://jaehyeon.me/blog/2017-04-08-serverless-data-product-1/</link><pubDate>Sat, 08 Apr 2017 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2017-04-08-serverless-data-product-1/</guid><description><![CDATA[<p>Let say you&rsquo;ve got a prediction model built in R and you&rsquo;d like to <em>productionize</em> it, for example, by serving it in a web application. One way is exposing the model through an API that returns the predicted result as a web service. However there are many issues. Firstly R is not a language for API development although there may be some ways - eg the <a href="https://github.com/trestletech/plumber" target="_blank" rel="noopener noreferrer">plumber<i class="fas fa-external-link-square-alt ms-1"></i></a> package. More importantly developing an API is not the end of the story as the API can&rsquo;t be served in a production system if it is not <em>deployed/managed/upgraded/patched/&hellip;</em> appropriately in a server or if it is not <em>scalable</em>, <em>protected via authentication/authorization</em> and so on. Therefore it requires quite a vast range of skill sets that cover both development and DevOps (engineering).</p>]]></description><enclosure url="https://jaehyeon.me/blog/2017-04-08-serverless-data-product-1/featured.png" length="139725" type="image/png"/></item></channel></rss>