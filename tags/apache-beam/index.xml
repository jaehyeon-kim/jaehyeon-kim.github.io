<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Apache Beam on Jaehyeon Kim</title><link>https://jaehyeon.me/tags/apache-beam/</link><description>Recent content in Apache Beam on Jaehyeon Kim</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>Copyright © 2023-2024 Jaehyeon Kim. All Rights Reserved.</copyright><lastBuildDate>Thu, 18 Jul 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://jaehyeon.me/tags/apache-beam/index.xml" rel="self" type="application/rss+xml"/><item><title>Apache Beam Python Examples - Part 2 Calculate Average Word Length with/without Fixed Look back</title><link>https://jaehyeon.me/blog/2024-07-18-beam-examples-2/</link><pubDate>Thu, 18 Jul 2024 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2024-07-18-beam-examples-2/</guid><description>In this post, we develop two Apache Beam pipelines that calculate average word lengths from input texts that are ingested by a Kafka topic. They obtain the statistics in different angles. The first pipeline emits the global average lengths whenever a new input text arrives while the latter triggers those values in a sliding time window.
Part 1 Calculate K Most Frequent Words and Max Word Length Part 2 Calculate Average Word Length with/without Fixed Look back (this post) Part 3 Build Sport Activity Tracker with/without SQL Part 4 Call RPC Service for Data Augmentation Part 5 Call RPC Service in Batch using Stateless DoFn Part 6 Call RPC Service in Batch with Defined Batch Size using Stateful DoFn Part 7 Separate Droppable Data into Side Output Part 8 Enhance Sport Activity Tracker with Runner Motivation Part 9 Develop Batch File Reader and PiSampler using Splittable DoFn Part 10 Develop Streaming File Reader using Splittable DoFn Development Environment The development environment has an Apache Flink cluster and Apache Kafka cluster and gRPC Server - gRPC server will be used in later posts.</description><enclosure url="https://jaehyeon.me/blog/2024-07-18-beam-examples-2/featured.png" length="96924" type="image/png"/></item><item><title>Apache Beam Python Examples - Part 1 Calculate K Most Frequent Words and Max Word Length</title><link>https://jaehyeon.me/blog/2024-07-04-beam-examples-1/</link><pubDate>Thu, 04 Jul 2024 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2024-07-04-beam-examples-1/</guid><description>In this series, we develop Apache Beam Python pipelines. The majority of them are from Building Big Data Pipelines with Apache Beam by Jan Lukavský. Mainly relying on the Java SDK, the book teaches fundamentals of Apache Beam using hands-on tasks, and we convert those tasks using the Python SDK. We focus on streaming pipelines, and they are deployed on a local (or embedded) Apache Flink cluster using the Apache Flink Runner.</description><enclosure url="https://jaehyeon.me/blog/2024-07-04-beam-examples-1/featured.png" length="96881" type="image/png"/></item><item><title>Deploy Python Stream Processing App on Kubernetes - Part 2 Beam Pipeline on Flink Runner</title><link>https://jaehyeon.me/blog/2024-06-06-beam-deploy-2/</link><pubDate>Thu, 06 Jun 2024 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2024-06-06-beam-deploy-2/</guid><description><![CDATA[<p>In this post, we develop an <a href="https://beam.apache.org/" target="_blank" rel="noopener noreferrer">Apache Beam<i class="fas fa-external-link-square-alt ms-1"></i></a> pipeline using the <a href="https://beam.apache.org/documentation/sdks/python/" target="_blank" rel="noopener noreferrer">Python SDK<i class="fas fa-external-link-square-alt ms-1"></i></a> and deploy it on an <a href="https://flink.apache.org/" target="_blank" rel="noopener noreferrer">Apache Flink<i class="fas fa-external-link-square-alt ms-1"></i></a> cluster via the <a href="https://beam.apache.org/documentation/runners/flink/" target="_blank" rel="noopener noreferrer">Apache Flink Runner<i class="fas fa-external-link-square-alt ms-1"></i></a>. Same as <a href="/blog/2024-05-30-beam-deploy-1">Part I</a>, we deploy a Kafka cluster using the <a href="https://strimzi.io/" target="_blank" rel="noopener noreferrer">Strimzi Operator<i class="fas fa-external-link-square-alt ms-1"></i></a> on a <a href="https://minikube.sigs.k8s.io/docs/" target="_blank" rel="noopener noreferrer">minikube<i class="fas fa-external-link-square-alt ms-1"></i></a> cluster as the pipeline uses <a href="https://kafka.apache.org/" target="_blank" rel="noopener noreferrer">Apache Kafka<i class="fas fa-external-link-square-alt ms-1"></i></a> topics for its data source and sink. Then, we develop the pipeline as a Python package and add the package to a custom Docker image so that Python user code can be executed externally. For deployment, we create a Flink session cluster via the <a href="https://nightlies.apache.org/flink/flink-kubernetes-operator-docs-main/" target="_blank" rel="noopener noreferrer">Flink Kubernetes Operator<i class="fas fa-external-link-square-alt ms-1"></i></a>, and deploy the pipeline using a Kubernetes job. Finally, we check the output of the application by sending messages to the input Kafka topic using a Python producer application.</p>]]></description><enclosure url="https://jaehyeon.me/blog/2024-06-06-beam-deploy-2/featured.png" length="58020" type="image/png"/></item><item><title>Apache Beam Local Development with Python - Part 5 Testing Pipelines</title><link>https://jaehyeon.me/blog/2024-05-09-beam-local-dev-5/</link><pubDate>Thu, 09 May 2024 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2024-05-09-beam-local-dev-5/</guid><description>We developed batch and streaming pipelines in Part 2 and Part 4. Often it is faster and simpler to identify and fix bugs on the pipeline code by performing local unit testing. Moreover, especially when it comes to creating a streaming pipeline, unit testing cases can facilitate development further by using TestStream as it allows us to advance watermarks or processing time according to different scenarios. In this post, we discuss how to perform unit testing of the batch and streaming pipelines that we developed earlier.</description><enclosure url="https://jaehyeon.me/blog/2024-05-09-beam-local-dev-5/featured.png" length="53603" type="image/png"/></item><item><title>Apache Beam Local Development with Python - Part 4 Streaming Pipelines</title><link>https://jaehyeon.me/blog/2024-05-02-beam-local-dev-4/</link><pubDate>Thu, 02 May 2024 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2024-05-02-beam-local-dev-4/</guid><description>In Part 3, we discussed the portability layer of Apache Beam as it helps understand (1) how Python pipelines run on the Flink Runner and (2) how multiple SDKs can be used in a single pipeline, followed by demonstrating local Flink and Kafka cluster creation for developing streaming pipelines. In this post, we build a streaming pipeline that aggregates page visits by user in a fixed time window of 20 seconds.</description><enclosure url="https://jaehyeon.me/blog/2024-05-02-beam-local-dev-4/featured.png" length="54556" type="image/png"/></item><item><title>Apache Beam Local Development with Python - Part 3 Flink Runner</title><link>https://jaehyeon.me/blog/2024-04-18-beam-local-dev-3/</link><pubDate>Thu, 18 Apr 2024 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2024-04-18-beam-local-dev-3/</guid><description>In this series, we discuss local development of Apache Beam pipelines using Python. In the previous posts, we mainly talked about Batch pipelines with/without Beam SQL. Beam pipelines are portable between batch and streaming semantics, and we will discuss streaming pipeline development in this and the next posts. While there are multiple Beam Runners, not every Runner supports Python or some Runners have too limited features in streaming semantics - see Beam Capability Matrix for details.</description><enclosure url="https://jaehyeon.me/blog/2024-04-18-beam-local-dev-3/featured.png" length="262307" type="image/png"/></item><item><title>Apache Beam Local Development with Python - Part 2 Batch Pipelines</title><link>https://jaehyeon.me/blog/2024-04-04-beam-local-dev-2/</link><pubDate>Thu, 04 Apr 2024 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2024-04-04-beam-local-dev-2/</guid><description>In this series, we discuss local development of Apache Beam pipelines using Python. A basic Beam pipeline was introduced in Part 1, followed by demonstrating how to utilise Jupyter notebooks, Beam SQL and Beam DataFrames. In this post, we discuss Batch pipelines that aggregate website visit log by user and time. The pipelines are developed with and without Beam SQL. Additionally, each pipeline is implemented on a Jupyter notebook for demonstration.</description><enclosure url="https://jaehyeon.me/blog/2024-04-04-beam-local-dev-2/featured.png" length="55405" type="image/png"/></item><item><title>Apache Beam Local Development with Python - Part 1 Pipeline, Notebook, SQL and DataFrame</title><link>https://jaehyeon.me/blog/2024-03-28-beam-local-dev-1/</link><pubDate>Thu, 28 Mar 2024 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2024-03-28-beam-local-dev-1/</guid><description>Apache Beam and Apache Flink are open-source frameworks for parallel, distributed data processing at scale. Flink has DataStream and Table/SQL APIs and the former has more capacity to develop sophisticated data streaming applications. The DataStream API of PyFlink, Flink&amp;rsquo;s Python API, however, is not as complete as its Java counterpart, and it doesn&amp;rsquo;t provide enough capability to extend when there are missing features in Python. Recently I had a chance to look through Apache Beam and found it supports more possibility to extend and/or customise its features.</description><enclosure url="https://jaehyeon.me/blog/2024-03-28-beam-local-dev-1/featured.png" length="88260" type="image/png"/></item></channel></rss>