<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>SparkR on Jaehyeon Kim</title><link>https://jaehyeon.me/tags/sparkr/</link><description>Recent content in SparkR on Jaehyeon Kim</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>Copyright Â© 2023-2024 Jaehyeon Kim. All Rights Reserved.</copyright><lastBuildDate>Sat, 30 Apr 2016 00:00:00 +0000</lastBuildDate><atom:link href="https://jaehyeon.me/tags/sparkr/index.xml" rel="self" type="application/rss+xml"/><item><title>Boost SparkR with Hive</title><link>https://jaehyeon.me/blog/2016-04-30-boost-sparkr-with-hive/</link><pubDate>Sat, 30 Apr 2016 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2016-04-30-boost-sparkr-with-hive/</guid><description><![CDATA[<p>In the <a href="/blog/2016-03-02-quick-start-sparkr-in-local-and-cluster-mode">previous post</a>, it is demonstrated how to start SparkR in local and cluster mode. While SparkR is in active development, it is yet to fully support Spark&rsquo;s key libraries such as MLlib and Spark Streaming. Even, as a data processing engine, this R API is still limited as it is not possible to manipulate RDDs directly but only via Spark SQL/DataFrame API. As can be checked in the <a href="http://spark.apache.org/docs/latest/api/R/index.html" target="_blank" rel="noopener noreferrer">API doc<i class="fas fa-external-link-square-alt ms-1"></i></a>, SparkR rebuilds many existing R functions to work with Spark DataFrame and notably it borrows some functions from the dplyr package. Also there are some alien functions (eg <code>from_utc_timestamp()</code>) and many of them are from <a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual" target="_blank" rel="noopener noreferrer">Hive Query Language (HiveQL)<i class="fas fa-external-link-square-alt ms-1"></i></a>. In relation to those functions from HiveQL, although some Hive user defined functions (UDFs) are ported, still many useful <a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual&#43;UDF" target="_blank" rel="noopener noreferrer">UDFs<i class="fas fa-external-link-square-alt ms-1"></i></a> and <a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual&#43;WindowingAndAnalytics" target="_blank" rel="noopener noreferrer">Window functions<i class="fas fa-external-link-square-alt ms-1"></i></a> don&rsquo;t exist.</p>]]></description></item><item><title>Quick Start SparkR in Local and Cluster Mode</title><link>https://jaehyeon.me/blog/2016-03-02-quick-start-sparkr-in-local-and-cluster-mode/</link><pubDate>Wed, 02 Mar 2016 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2016-03-02-quick-start-sparkr-in-local-and-cluster-mode/</guid><description><![CDATA[<p>In the <a href="/blog/2016-02-22-spark-cluster-setup-on-virtualbox">previous post</a>, a Spark cluster is set up using 2 VirtualBox Ubuntu guests. While this is a viable option for many, it is not always for others. For those who find setting-up such a cluster is not convenient, there&rsquo;s still another option, which is relying on the local mode of Spark. In this post, a <a href="https://bitbucket.org/jaehyeon/sparkr-test" target="_blank" rel="noopener noreferrer"><strong>BitBucket repository</strong><i class="fas fa-external-link-square-alt ms-1"></i></a> is introduced, which is a R project that includes <em>Spark 1.6.0 Pre-built for Hadoop 2.0 and later</em> and <em>hadoop-common 2.2.0</em> - the latter is necessary if it is tested on Windows. Then several initialization steps are discussed such as setting-up environment variables and library path as well as including the <a href="https://github.com/databricks/spark-csv" target="_blank" rel="noopener noreferrer">spark-csv package<i class="fas fa-external-link-square-alt ms-1"></i></a> and a JDBC driver. Finally it shows some examples of reading JSON and CSV files in the cluster mode.</p>]]></description></item><item><title>Spark Cluster Setup on VirtualBox</title><link>https://jaehyeon.me/blog/2016-02-22-spark-cluster-setup-on-virtualbox/</link><pubDate>Mon, 22 Feb 2016 00:00:00 +0000</pubDate><guid>https://jaehyeon.me/blog/2016-02-22-spark-cluster-setup-on-virtualbox/</guid><description><![CDATA[<p>We discuss how to set up a Spark cluser between 2 Ubuntu guests. Firstly it begins with machine preparation. Once a machine is baked, its image file (<em>VDI</em>) is be copied for the second one. Then how to launch a cluster by <a href="http://spark.apache.org/docs/latest/spark-standalone.html" target="_blank" rel="noopener noreferrer">standalone mode<i class="fas fa-external-link-square-alt ms-1"></i></a> is discussed. Let&rsquo;s get started.</p>

<h2 id="machine-preparation" data-numberify>Machine preparation<a class="anchor ms-1" href="#machine-preparation"></a></h2>
<p>If you haven&rsquo;t read the previous post, I recommend reading as it introduces <a href="http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html" target="_blank" rel="noopener noreferrer">Putty<i class="fas fa-external-link-square-alt ms-1"></i></a> as well. Also, as Spark need Java Development Kit (JDK), you may need to <em>apt-get</em> it first - see <a href="https://www.digitalocean.com/community/tutorials/how-to-install-java-on-ubuntu-with-apt-get" target="_blank" rel="noopener noreferrer">this tutorial<i class="fas fa-external-link-square-alt ms-1"></i></a> for further details.</p>]]></description></item></channel></rss>