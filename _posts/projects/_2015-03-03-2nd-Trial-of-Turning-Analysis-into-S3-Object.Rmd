---
layout: post
title: "2015-03-03-2nd-Trial-of-Turning-Analysis-into-S3-Object"
description: ""
category: R
tags: [rpart, caret, mlr, dplyr, programming]
---
{% include JB/setup %}

In order to avoid an error that could be caused by conflicting variable names and to keep variables in a more effective way, a trial of turning analysis into a S3 object is made in a ([previous article](http://jaehyeon-kim.github.io/r/2015/02/21/Quick-Trial-of-Turning-Analysis-into-S3-Object/)). The second trial is made and a class (*rpartDT*) that extends *rpartExt* is introduced in this article. In line with the first trial, the base class keeps key outcomes of the CART model in a list and the extended class includes outcomes of bagged trees. As the main purpose of this class is to evaluate performance of an individual tree, its bagging implementation is a bit different from the conventional one. Specifically, while unpruned trees are fit recursively in the conventional bagging so that bias-variance trade-off could be improved mainly due to lowered variance, it performs with the *cp* values set at the lowest xerror and by the 1-SE rule. Also other control variables are untouched (eg *minbucket* is 20 at default).

The class is constructed so as to produce the following outcomes.

**error (mean misclassification error or root mean squared error)**
- error distribution of each bagged tree (individual error)
-- as [Hastie et al.(2008)](http://statweb.stanford.edu/~tibs/ElemStatLearn/) explains, non-parametric bootstrap is a computer implementation of non-parametric maximum likelyhood estimation and Bayesian analysis with non-informative prior. Therefore it would be informative to see the location of a single tree's error in the distribution of individual bagged trees
- majority vote or average (cumulative error)
-- by averaging overfit and thus instable outcomes of a single tree, bagging could provide better results and comparison between them would be necessary
- oob error
-- when n records are taken with replacement, the probability that a record is not taken is $$\left(1-\frac{1}{n}\right)^n = e^{-1}$$ as n goes to infinity (about 36.8% of records are not taken) and they can be used to produce errors if there is no test data
- test error (if exists)
-- it would be even better if error can be compared to that of independent test data
**variable importance**
- to see if single tree's variable importance is far different from that of bagged trees

Before getting started, note that the source of the classes can be found in [this gist](https://gist.github.com/jaehyeon-kim/b89dcbd2fb0b84fd236e) and, together with the relevant packages (see *tags*), it requires a utility function (`bestParam()`) that can be found [here](https://gist.github.com/jaehyeon-kim/5622ae9fa982e0b46550).

```{r selection, message=FALSE, warning=FALSE, eval=FALSE}
cnt = 0
while(cnt < ntree) {
  # create resample description and task
  if(class(trainData[,res.ind]) != "factor") {
    boot.desc = makeResampleDesc(method="Bootstrap", stratify=FALSE, iters=1)
    boot.task = makeRegrTask(data=trainData,target=res.name)
  } else {
    boot.desc = makeResampleDesc(method="Bootstrap", stratify=TRUE, iters=1)
    boot.task = makeClassifTask(data=trainData,target=res.name)
  }
  # create bootstrap instance and split data - in-bag and out-of-bag
  boot.ins = makeResampleInstance(desc=boot.desc, task=boot.task) 
  trn.in = trainData[boot.ins$train.inds[[1]],]
  trn.oob = trainData[boot.ins$test.inds[[1]],]
  # fit model on in-bag sample
  mod = rpart(formula=formula, data=trn.in, control=rpart.control(cp=0))
  cp = tryCatch({
    unlist(bestParam(mod$cptable,"CP","xerror","xstd")[1,1:2])
  },
  error=function(cond) { 
    message("cp fails to be generated. The sample is discarded.")
    cp = c(0,0)
  })
  # take sample only if cp values are obtained
  if(sum(cp) != 0) {
    cnt = cnt + 1
  ...
}
```

Data is split as usual.

```{r data, message=FALSE, warning=FALSE}
## data
require(ISLR)
data(Carseats)
require(dplyr)
Carseats = Carseats %>% 
  mutate(High=factor(ifelse(Sales<=8,"No","High"),labels=c("High","No")))
data.cl = subset(Carseats, select=c(-Sales))
data.rg = subset(Carseats, select=c(-High))

# split - cl: classification, rg: regression
require(caret)
set.seed(1237)
trainIndex = createDataPartition(Carseats$High, p=0.8, list=FALSE, times=1)
trainData.cl = data.cl[trainIndex,]
testData.cl = data.cl[-trainIndex,]
```

The class is instantiated after importing the constructors.

```{r fit, message=FALSE, warning=FALSE}
## run rpartDT
# import constructors
source("src/cart.R")
set.seed(12357)
cl = cartDT(trainData.cl, testData.cl, "High ~ .", ntree=10)
```

```{r class, message=FALSE, warning=FALSE}
# class and names
class(cl)
# rpt - single tree, lst - least xerror, se - 1-SE rule, oob - out-of-bag sample, tst - test data, ind (cum) - individual (cumulative) prediction or error
names(cl)
```


```{r cp, message=FALSE, warning=FALSE}
## cp values
# cart
cl$rpt$cp[1,][[1]]
# bagging
summary(t(cl$boot.cp)[,2])
```

```{r fitted, message=FALSE, warning=FALSE}
## fitted values
# individual - each oob sample
cl$ind.oob.lst[3:6,1:8]

# cumulative - majority vote or average
# 1. not used - NA, 2. used once - get name, 3. tie - NA, 4. name at max number of labels
cl$cum.oob.lst[3:6,1:8]
```

Given a data frame of fitted values of individual trees (*fit*), the fitted values are averaged depending on the class of the respose - majority vote if *factor* or average if *numeric*. For a *numeric* response, `average()` is applied in an expanding way column wise while a vectorized function (`retVote()`) is created for a *factor* response with the following rules in order.

1. if no fitted value (table length = 0), assign *NA*
2. if a single fitted value (table length = 1), assign *name* of it
3. if there is a tie, assgin *NA*
4. finally take the *name* of the level that occupies most

```{r cum, message=FALSE, warning=FALSE, eval=FALSE}
# function to update fitted values - majority vote or average
# response kept in 1st column, should be excluded
retCum = function(fit) {
  if(ncol(fit) < 2) {
    message("no fitted values")
    cum.fit = fit
  } else {
    cum.fit = as.data.frame(apply(fit,2,function(x) { rep(0,times=(nrow(fit))) }))
    cum.fit[,1:2] = fit[,1:2]
    rownames(cum.fit) = rownames(fit)
    for(i in 3:ncol(fit)) {
      if(class(fit[,1])=="factor") {
        retVote = function(x) {
          tbls = apply(x,1,as.data.frame)
          tbls = lapply(tbls,table)
          ret = function(x) {
            if(length(x)==0) NA 
            else if(length(x)==1) names(x) 
            else if(max(x)==min(x)) NA 
            else names(x[x==max(x)])
          }
          maxVal = sapply(tbls,ret)
          maxVal
        }
        cum.fit[,i] = retVote(fit[,2:i]) # retVote already vectorized
      } else {
        cum.fit[,i] = apply(fit[,2:i],1,mean,na.rm=TRUE)
      }
    }  
  }
  cum.fit
}
```

Given a data frame of fitted values (*fit*), *mmce* or *rmse* are obtained depending on the class of the response. Note that, as the first column has response values, it is excluded.

```{r err_fcn, message=FALSE, warning=FALSE, eval=FALSE}
# function to updated errors - mmce or rmse
# response kept in 1st column, should be excluded
retErr = function(fit) {
  err = data.frame(t(rep(0,times=ncol(fit))))
  colnames(err)=colnames(fit)
  for(i in 2:ncol(fit)) {
    cmpt = complete.cases(fit[,1],fit[,i])
    if(class(fit[,1])=="factor") {
      tbl=table(fit[cmpt,1],fit[cmpt,i])
      err[i] = 1 - sum(diag(tbl))/sum(tbl)
    } else {
      err[i] = sqrt(sum(fit[cmpt,1]-fit[cmpt,i])^2/length(fit[cmpt,1]))
    }    
  }
  err[2:length(err)]
}
```

Selective individual and cumulative errors of bagged trees are shown below.

```{r err, message=FALSE, warning=FALSE}
# individual error
cl$ind.oob.lst.err[1:7]
# cumulative error
cl$cum.oob.lst.err[1:7]
```

Importance of each variable of the single and bagged trees is found below.

```{r vi, message=FALSE, warning=FALSE}
## variable importance
# cart
data.frame(variable=names(cl$rpt$mod$variable.importance)
           ,value=cl$rpt$mod$variable.importance/sum(cl$rpt$mod$variable.importance)
           ,row.names=NULL)
# bagging - cumulative
ntree = 10
data.frame(variable=rownames(cl$cum.varImp.lst)
           ,value=cl$cum.varImp.lst[,ntree])
```

In the next two articles, the CART analysis will be evaluated using data as regression and classification tasks.