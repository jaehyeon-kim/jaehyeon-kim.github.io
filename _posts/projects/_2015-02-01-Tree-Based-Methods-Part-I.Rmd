---
layout: post
title: "2015-02-01-Tree-Based-Methods-Part-I"
description: ""
category: R
tags: [dplyr, caret, rpart, rpart.plot]
---
{% include JB/setup %}

This is the first article about tree based methods using R. *Carseats* data in the chapter 8 lab of [ISLR](http://www-bcf.usc.edu/~gareth/ISL/) is used to perform classification analysis. Unlike the lab example, the **rpart** package is used to fit the CART model on the data and the **caret** package is used for tuning the pruning parameter (`cp`).

The bold-cased sections of the [tutorial](http://topepo.github.io/caret/index.html) are covered in this article.

- Visualizations
- Pre-Processing
- **Data Splitting**
- Miscellaneous Model Functions
- **Model Training and Tuning**
- Using Custom Models
- Variable Importance
- Feature Selection: RFE, Filters, GA, SA
- Other Functions
- Parallel Processing
- Adaptive Resampling

The pruning parameter in the **rpart** package is scaled so that its values are from 0 to 1. Specifically the formula is

$$
R_{cp}\left(T\right)\equiv R\left(T\right) + cp*|T|*R\left(T_{1}\right)
$$

where $$T_{1}$$ is the tree with no splits, $$\mid T\mid$$ is the number of splits for a tree and *R* is the risk.

Due to the inclusion of $$R\left(T_{1}\right)$$, when *cp=1*, the tree will result in no splits while it is not pruned when *cp=0*. On the other hand, in the original setup without the term, the pruning parameter ($$\alpha$$) can range from 0 to infinity.

Let's get started.

The following packages are used.

```{r load_pkgs, message=FALSE, warning=FALSE}
library(dplyr) # data minipulation
library(rpart) # fit tree
library(rpart.plot) # plot tree
library(caret) # tune model
```

*Carseats* data is created as following while the response (*Sales*) is converted into a binary variable.

```{r data, message=FALSE, warning=FALSE}
require(ISLR)
data(Carseats)
Carseats = Carseats %>% 
  mutate(High=factor(ifelse(Sales<=8,"No","High"),labels=c("High","No")))
# structure of predictors
str(subset(Carseats,select=c(-High,-Sales)))
# classification response summary
with(Carseats,table(High))
with(Carseats,table(High) / length(High))
```

The train and test data sets are split using `createDataPartition()`.

```{r split, message=FALSE, warning=FALSE}
## Data Splitting
set.seed(1237)
trainIndex.cl = createDataPartition(Carseats$High, p=.8, list=FALSE, times=1)
trainData.cl = subset(Carseats, select=c(-Sales))[trainIndex.cl,]
testData.cl = subset(Carseats, select=c(-Sales))[-trainIndex.cl,]
```

Stratify sampling is performed by default.

```{r check_res, message=FALSE, warning=FALSE}
# training response summary
with(trainData.cl,table(High))
with(trainData.cl,table(High) / length(High))

# test response summary
with(testData.cl,table(High))
with(testData.cl,table(High) / length(High))
```

The following resampling strategies are considered: *cross-validation*, *repeated cross-validation* and *bootstrap*.

```{r tr_ctrl, message=FALSE, warning=FALSE}
## train control
trControl.cv = trainControl(method="cv",number=10)
trControl.recv = trainControl(method="repeatedcv",number=10,repeats=5)
trControl.boot = trainControl(method="boot",number=50)
```

There are two methods in the **caret** package: `rpart` and `repart2`. The first method allows the pruning parameter to be tuned. The tune grid is not set up explicitly and it is adjusted by `tuneLength` - equally spaced **cp** values are created from 0 to 0.3 in the package.

```{r tune, message=FALSE, warning=FALSE}
set.seed(12357)
fit.cl.cv = train(High ~ .
                  ,data=trainData.cl
                  ,method="rpart"
                  ,tuneLength=20
                  ,trControl=trControl.cv)

fit.cl.recv = train(High ~ .
                    ,data=trainData.cl
                    ,method="rpart"
                    ,tuneLength=20
                    ,trControl=trControl.recv)

fit.cl.boot = train(High ~ .
                    ,data=trainData.cl
                    ,method="rpart"
                    ,tuneLength=20
                    ,trControl=trControl.boot)
```

Repeated cross-validation and bootstrap produce the same best tuned **cp** value while cross-validation returns a higher value.

```{r tune_result, message=FALSE, warning=FALSE}
# results at best tuned cp
subset(fit.cl.cv$results,subset=cp==fit.cl.cv$bestTune$cp)
subset(fit.cl.recv$results,subset=cp==fit.cl.recv$bestTune$cp)
subset(fit.cl.boot$results,subset=cp==fit.cl.boot$bestTune$cp)
```

The one from repeated cross-validation is taken to fit to the entire training data.

```{r refit, message=FALSE, warning=FALSE}
## refit the model to the entire training data
cp.cl = fit.cl.recv$bestTune$cp
fit.cl = rpart(High ~ ., data=trainData.cl, control=rpart.control(cp=cp.cl))
```

The resulting tree is shown as following. The plot shows expected losses and node probabilities in the final nodes. For example, the leftmost node has

- expected loss of 0.13 (= 8/60 (0.13333))
- node probability of 19% (= 60/321)

```{r plot_tree, message=FALSE, warning=FALSE}
# plot tree
cols <- ifelse(fit.cl$frame$yval == 1,"green4","darkred") # green if high
prp(fit.cl
    ,main="CART model tree"
    ,extra=106           # display prob of survival and percent of obs
    ,nn=TRUE             # display the node numbers
    ,fallen.leaves=TRUE  # put the leaves on the bottom of the page
    ,branch=.5           # change angle of branch lines
    ,faclen=0            # do not abbreviate factor levels
    ,trace=1             # print the automatically calculated cex
    ,shadow.col="gray"   # shadows under the leaves
    ,branch.lty=3        # draw branches using dotted lines
    ,split.cex=1.2       # make the split text larger than the node text
    ,split.prefix="is "  # put "is " before split text
    ,split.suffix="?"    # put "?" after split text
    ,col=cols, border.col=cols   # green if survived
    ,split.box.col="lightgray"   # lightgray split boxes (default is white)
    ,split.border.col="darkgray" # darkgray border on split boxes
    ,split.round=.5) 
```

```{r plot_tree_2, message=FALSE, warning=FALSE, include=FALSE}
# another plotting option - don't run
require(partykit)
plot(as.party(fit.cl))
```

The fitted model is applied to the test data.

```{r test_apply, message=FALSE, warning=FALSE}
## apply to the test data
pre.cl = predict(fit.cl, newdata=testData.cl, type="class")

# confusion matrix
cMat.cl = table(data.frame(response=pre.cl,truth=testData.cl$High))
cMat.cl

# mean misclassification error
mmse.cl = 1 - sum(diag(cMat.cl))/sum(cMat.cl)
mmse.cl
```

More models are going to be implemented/compared in the subsequent articles.