# fit models
pred.reg.caret = predict(mod.reg.caret, newdata=testData.reg)
pred.reg.rpart = predict(mod.reg.rpart, newdata=testData.reg)
# regression - caret
pred.reg.caret.cm = regCM(testData.reg$Sales, pred.reg.caret, probs=probs)
pred.reg.caret.cm
# regression - rpart
pred.reg.rpart.cm = regCM(testData.reg$Sales, pred.reg.rpart, probs=probs)
pred.reg.rpart.cm
# model by caret with lower cp produces better outcome
# 1 standard error rule is questionable on this data
pred.reg.caret.rmse = sqrt(sum(testData.reg$Sales-pred.reg.caret)^2/length(testData.reg$Sales))
pred.reg.caret.rmse
pred.reg.rpart.rmse = sqrt(sum(testData.reg$Sales-pred.reg.rpart)^2/length(testData.reg$Sales))
pred.reg.rpart.rmse
## plot actual vs prediced and resid vs fitted
mod.reg.caret.test = rpart(Sales ~ ., data=testData.reg, control=rpart.control(cp=cp.reg.caret))
predDF = data.frame(actual=testData.reg$Sales
,predicted=pred.reg.caret
,resid=resid(mod.reg.caret.test))
# correlation
cor(predDF)
# actual vs predicted
actual.plot = ggplot(predDF, aes(x=predicted,y=actual)) +
geom_point(shape=1,position=position_jitter(width=0.1,height=0.1)) +
geom_smooth(method=lm,se=FALSE)
# resid vs predicted
resid.plot = ggplot(predDF, aes(x=predicted,y=resid)) +
geom_point(shape=1,position=position_jitter(width=0.1,height=0.1)) +
geom_smooth(method=lm,se=FALSE)
grid.arrange(actual.plot, resid.plot, ncol = 2)
# plot tree
cols <- ifelse(mod.reg.caret$frame$yval > 8,"green4","darkred") # green if high
prp(mod.reg.caret
,main="CART Model Tree"
#,extra=106           # display prob of survival and percent of obs
,nn=TRUE             # display the node numbers
,fallen.leaves=TRUE  # put the leaves on the bottom of the page
,branch=.5           # change angle of branch lines
,faclen=0            # do not abbreviate factor levels
,trace=1             # print the automatically calculated cex
,shadow.col="gray"   # shadows under the leaves
,branch.lty=3        # draw branches using dotted lines
,split.cex=1.2       # make the split text larger than the node text
,split.prefix="is "  # put "is " before split text
,split.suffix="?"    # put "?" after split text
,col=cols, border.col=cols   # green if survived
,split.box.col="lightgray"   # lightgray split boxes (default is white)
,split.border.col="darkgray" # darkgray border on split boxes
,split.round=.5)
with(trainData.reg,length(Sales[Sales<=8])/length(Sales))
fit.reg.caret.rmse = sqrt(sum(trainData.reg$Sales-fit.reg.caret)^2/length(trainData.reg$Sales))
fit.reg.caret.rmse
fit.reg.rpart.rmse = sqrt(sum(trainData.reg$Sales-fit.reg.rpart)^2/length(trainData.reg$Sales))
fit.reg.rpart.rmse
?format
format(fit.reg.caret.rmse,digits=10,scientific=FALSE)
format(fit.reg.caret.rmse,scientific=FALSE)
format(fit.reg.rpart.rmse,scientific=FALSE)
fit.reg.caret.rmse
fit.reg.caret.mse = sum(trainData.reg$Sales-fit.reg.caret)^2/length(trainData.reg$Sales)
fit.reg.caret.mse
fit.reg.caret.rmse = sqrt(sum(trainData.reg$Sales-fit.reg.caret)^2/length(trainData.reg$Sales))
fit.reg.caret.rmse
fit.reg.rpart.rmse = sqrt(sum(trainData.reg$Sales-fit.reg.rpart)^2/length(trainData.reg$Sales))
fit.reg.rpart.rmse
rm(list = ls())
source("src/knitSrc.R")
knitPost("2015-02-14-Tree-Based-Methods-Part-III-Regression")
source("src/knitSrc.R")
knitPost("2015-02-14-Tree-Based-Methods-Part-III-Regression")
rm(list = ls())
source("src/knitSrc.R")
knitPost("2015-02-14-Tree-Based-Methods-Part-III-Regression")
source("src/knitSrc.R")
knitPost("2015-02-15-Tree-Based-Methods-Part-IV-Packages-Comparison")
test.perf.lst = ls()
## intro
# construct base class
# http://www.mat.uc.cl/~susana/CE/oopV2.pdf
emp = function(name) {
if(length(name) != 1 && !is.character(name))
stop("single string value of name necessary")
# assign name to result
result = name
class(result) = "employee"
return(result)
}
# extend base class
# manager 'is-a' employee
man = function(name, members) {
# create base class (employee)
result = emp(name)
# arguments should be carefully checked
if(length(sapply(members,class)[sapply(members,class)=="character"]) != length(members))
stop("a vector employees necessary")
# combine name and members as a list
result = list(name=result,members=members)
class(result)=c("manager","employee")
return(result)
}
tom = emp("Tom")
alice = emp("Alice")
nick = man("Nick",c(tom,alice))
# without print function
tom
nick
# base method can be extended
# better not to include full stop(.) in class name
print.employee = function(obj) {
cat("Employee Name:",obj,"\n")
}
print.manager = function(obj) {
cat("Employee Name:",obj$name,"\n")
cat("Members:",obj$members,"\n")
}
# methods that extends print
head(methods("print"),3)
methods("print")[grep("*employee",methods("print"))]
# methods that associated with manager class
methods(class="employee")
# with print function
tom
nick
# class can be checked/modified
class(tom) = NULL
tom # character vector
# list names and assigned classes can be checked/modified
attributes(nick)
attributes(nick) = NULL
nick
# incorrectly assigned class should be handled
foo = c(1)
class(foo) = "employee"
class(foo)
man("John",foo)
attributes(,tom)
attributes(tom)
attributes(nick)
nick
emp = function(name) {
if(length(name) != 1 && !is.character(name))
stop("single string value of name necessary")
# assign name to result
result = name
class(result) = "employee"
return(result)
}
man = function(name, members) {
# create base class (employee)
result = emp(name)
# arguments should be carefully checked
if(length(sapply(members,class)[sapply(members,class)=="character"]) != length(members))
stop("a vector employees necessary")
# combine name and members as a list
result = list(name=result,members=members)
class(result)=c("manager","employee")
return(result)
}
tom = emp("Tom")
alice = emp("Alice")
nick = man("Nick",c(tom,alice))
attributes(nick)
attributes(tom)
tom
rm(list = ls())
source("src/knitSrc.R")
knitPost("2015-02-21-Quick-Trial-of-Turning-Analysis-into-S3-Object")
## data
require(ISLR)
data(Carseats)
require(dplyr)
Carseats = Carseats %>%
mutate(High=factor(ifelse(Sales<=8,"No","High"),labels=c("High","No")))
data.cl = subset(Carseats, select=c(-Sales))
data.rg = subset(Carseats, select=c(-High))
## import constructors
source("src/cart.R")
# split - cl: classification, rg: regression
require(caret)
set.seed(1237)
trainIndex = createDataPartition(Carseats$High, p=0.8, list=FALSE, times=1)
trainData.cl = data.cl[trainIndex,]
testData.cl = data.cl[-trainIndex,]
trainData.rg = data.rg[trainIndex,]
testData.rg = data.rg[-trainIndex,]
## classification
set.seed(12357)
rpt.cl = cartRPART(trainData.cl,testData.cl,formula="High ~ .")
rpt.cl$test.lst$cm
rm(list = ls())
## data
require(ISLR)
data(Carseats)
require(dplyr)
Carseats = Carseats %>%
mutate(High=factor(ifelse(Sales<=8,"No","High"),labels=c("High","No")))
data.cl = subset(Carseats, select=c(-Sales))
data.rg = subset(Carseats, select=c(-High))
# split - cl: classification, rg: regression
require(caret)
set.seed(1237)
trainIndex = createDataPartition(Carseats$High, p=0.8, list=FALSE, times=1)
trainData.cl = data.cl[trainIndex,]
testData.cl = data.cl[-trainIndex,]
trainData.rg = data.rg[trainIndex,]
testData.rg = data.rg[-trainIndex,]
## import constructors
source("src/cart.R")
## classification
set.seed(12357)
rpt.cl = cartRPART(trainData.cl,testData.cl,formula="High ~ .")
rm(list = ls())
## data
require(ISLR)
data(Carseats)
require(dplyr)
Carseats = Carseats %>%
mutate(High=factor(ifelse(Sales<=8,"No","High"),labels=c("High","No")))
data.cl = subset(Carseats, select=c(-Sales))
data.rg = subset(Carseats, select=c(-High))
# split - cl: classification, rg: regression
require(caret)
set.seed(1237)
trainIndex = createDataPartition(Carseats$High, p=0.8, list=FALSE, times=1)
trainData.cl = data.cl[trainIndex,]
testData.cl = data.cl[-trainIndex,]
trainData.rg = data.rg[trainIndex,]
testData.rg = data.rg[-trainIndex,]
## import constructors
source("src/cart.R")
## classification
set.seed(12357)
rpt.cl = cartRPART(trainData.cl,testData.cl,formula="High ~ .")
rm(list = ls())
?sep
?seq
---
layout: post
title: "2015-02-23-Tree-Based-Methods-Part-IV-Packages-Comparison-in-S3"
description: ""
category: R
tags: [ggplot2, rpart, caret, mlr, programming]
---
{% include JB/setup %}
In the [previous article](http://jaehyeon-kim.github.io/r/2015/02/21/Quick-Trial-of-Turning-Analysis-into-S3-Object/), a brief introduction to the S3 object is made as well as a class that encapsulates the CART analysis by the **rpart** package is illustrated. Extending an [earlier article](http://jaehyeon-kim.github.io/r/2015/02/15/Tree-Based-Methods-Part-IV-Packages-Comparison/) of comparing the three packages (**rpart**, **caret** and **mlr**), this article compares them using the following 3 S3 objects: *rpartExt*, *rpartExtCrt* and *rpartExtMlr*. Like *manager* **is-a** *employee* so that it can extends the base class in the previous article, it is roughly conceptualized that the last two classes extend the first. On this setting, performance of both classification and regression tasks are compared in this article.
Before getting started, note that the source of these classes can be found in [this gist](https://gist.github.com/jaehyeon-kim/b89dcbd2fb0b84fd236e) and, together with the relevant packages, it requires 3 utility functions that can be found [here](https://gist.github.com/jaehyeon-kim/5622ae9fa982e0b46550).
Let's get started.
The data is split for both classification and regression.
```{r data, message=FALSE, warning=FALSE}
## data
require(ISLR)
data(Carseats)
require(dplyr)
Carseats = Carseats %>%
mutate(High=factor(ifelse(Sales<=8,"No","High"),labels=c("High","No")))
data.cl = subset(Carseats, select=c(-Sales))
data.rg = subset(Carseats, select=c(-High))
# split - cl: classification, rg: regression
require(caret)
set.seed(1237)
trainIndex = createDataPartition(Carseats$High, p=0.8, list=FALSE, times=1)
trainData.cl = data.cl[trainIndex,]
testData.cl = data.cl[-trainIndex,]
trainData.rg = data.rg[trainIndex,]
testData.rg = data.rg[-trainIndex,]
```
The constructors are sourced.
```{r src, message=FALSE, warning=FALSE}
## import constructors
source("src/cart.R")
```
The classification task is fit by each of the packages. Note that the constructors of the subclasses (*rpartExtCrt*, *rpartExtMlr*) has an option to fit data using the base class (*rpartExt*) and it is determined by the argument of *fitInd*. Once it is set *TRUE*, the constructor of the base class is executed (or the base class is instantiated), resulting that its outcome (named *rpt*) is kept as a member of the outcome list of its subclasses. Otherwise a null list is added as a placeholder.
```{r classif_fit, message=FALSE, warning=FALSE}
## classification
set.seed(12357)
rpt.cl = cartRPART(trainData.cl,testData.cl,formula="High ~ .")
crt.cl = cartCARET(trainData.cl,testData.cl,formula="High ~ .",fitInd=TRUE)
mlr.cl = cartMLR(trainData.cl,testData.cl,formula="High ~ .",fitInd=TRUE)
```
Class and names attributes of each object can be seen below. As the latter two are assumed to extend the first, their class attributes include the class name of the first. Also, as *fitInd* is set *TRUE*, the base class is instantiated, which can be checked that the names attributes of *rpt.cl* and *crt.cl$rpt* are the same.
```{r class_intro, message=FALSE, warning=FALSE}
## classes and attributes
data.frame(rpart=c(class(rpt.cl),""),caret=class(crt.cl),mlr=class(mlr.cl))
attributes(rpt.cl)$names
attributes(crt.cl)$names
attributes(mlr.cl)$names
attributes(crt.cl$rpt)$names
```
The performance of the classfication task is compared and, as seen earlier, the tree is not sensitive to the *cp*.
```{r classif_comparison, message=FALSE, warning=FALSE}
# comparison
perf.cl = list(rpt.cl$train.lst$error,rpt.cl$train.se$error
,rpt.cl$test.lst$error,rpt.cl$test.se$error
,crt.cl$train.lst$error,crt.cl$test.lst$error
,mlr.cl$train.lst$error,mlr.cl$test.lst$error)
err = function(perf) {
out = list(unlist(perf[[1]]))
for(i in 2:length(perf)) {
out[[length(out)+1]] = unlist(perf[[i]])
}
t(sapply(out,unlist))
}
err(perf.cl)
```
Then the data is fit as regression - the default *fitInd* value is *FALSE*
```{r reg_fit, message=FALSE, warning=FALSE}
## regression
set.seed(12357)
rpt.rg = cartRPART(trainData.rg,testData.rg,formula="Sales ~ .")
crt.rg = cartCARET(trainData.rg,testData.rg,formula="Sales ~ .")
mlr.rg = cartMLR(trainData.rg,testData.rg,formula="Sales ~ .")
```
The performance of the regression task is compared below. It is found that, unlike the classification task, the *cp* plays a more role.
Specifically
- the value (*0.0049*) at the minimum *xerror* by the **rpart** package records the least *RMSE* (*0,74*)
- the *1-SE rule* is also questionable by delivering the highest *RMSE* (*1.96*)
- the best *cp* value by the **caret** and **mlr** packages is *0* and the resulting *RMSE* (*0.95*) is higher
The last is due to the way how the grids are set up in these packages. The *tuneLength* of the **caret** package is set to *30*, which means that the grid
the grid is constructed to divide the range of *cp* values from 0 to 0.3 in 30 sub-ranges.
```{r reg_comparison, message=FALSE, warning=FALSE}
# comparison
perf.rg = list(rpt.rg$train.lst$error,rpt.rg$train.se$error
,rpt.rg$test.lst$error,rpt.rg$test.se$error
,crt.rg$train.lst$error,crt.rg$test.lst$error
,mlr.rg$train.lst$error,mlr.rg$test.lst$error)
err(perf.rg)
```
))
## data
require(ISLR)
data(Carseats)
require(dplyr)
Carseats = Carseats %>%
mutate(High=factor(ifelse(Sales<=8,"No","High"),labels=c("High","No")))
data.cl = subset(Carseats, select=c(-Sales))
data.rg = subset(Carseats, select=c(-High))
# split - cl: classification, rg: regression
require(caret)
set.seed(1237)
trainIndex = createDataPartition(Carseats$High, p=0.8, list=FALSE, times=1)
trainData.cl = data.cl[trainIndex,]
testData.cl = data.cl[-trainIndex,]
trainData.rg = data.rg[trainIndex,]
testData.rg = data.rg[-trainIndex,]
## import constructors
source("src/cart.R")
## classification
set.seed(12357)
rpt.cl = cartRPART(trainData.cl,testData.cl,formula="High ~ .")
crt.cl = cartCARET(trainData.cl,testData.cl,formula="High ~ .",fitInd=TRUE)
mlr.cl = cartMLR(trainData.cl,testData.cl,formula="High ~ .",fitInd=TRUE)
## classes and attributes
data.frame(rpart=c(class(rpt.cl),""),caret=class(crt.cl),mlr=class(crt.cl))
attributes(rpt.cl)$names
attributes(crt.cl)$names
attributes(mlr.cl)$names
attributes(crt.cl$rpt)$names
# comparison
perf.cl = list(rpt.cl$train.lst$error,rpt.cl$train.se$error
,rpt.cl$test.lst$error,rpt.cl$test.se$error
,crt.cl$train.lst$error,crt.cl$test.lst$error
,mlr.cl$train.lst$error,mlr.cl$test.lst$error)
err = function(perf) {
out = list(unlist(perf[[1]]))
for(i in 2:length(perf)) {
out[[length(out)+1]] = unlist(perf[[i]])
}
t(sapply(out,unlist))
}
err(perf.cl)
## regression
set.seed(12357)
rpt.rg = cartRPART(trainData.rg,testData.rg,formula="Sales ~ .")
crt.rg = cartCARET(trainData.rg,testData.rg,formula="Sales ~ .")
mlr.rg = cartMLR(trainData.rg,testData.rg,formula="Sales ~ .")
# comparison
perf.rg = list(rpt.rg$train.lst$error,rpt.rg$train.se$error
,rpt.rg$test.lst$error,rpt.rg$test.se$error
,crt.rg$train.lst$error,crt.rg$test.lst$error
,mlr.rg$train.lst$error,mlr.rg$test.lst$error)
err(perf.rg)
crt.rg$mod
crt.rg$mod$result
head(crt.rg$mod$result,3)
rm(list = ls())
source("src/knitSrc.R")
knitPost("2015-02-23-Tree-Based-Methods-Part-IV-Packages-Comparison-in-S3")
title: "2015-03-03-2nd-Trial-of-Turning-Analysis-into-S3-Object"
source("src/knitSrc.R")
knitPost("2015-03-03-2nd-Trial-of-Turning-Analysis-into-S3-Object")
## data
require(ISLR)
data(Carseats)
require(dplyr)
Carseats = Carseats %>%
mutate(High=factor(ifelse(Sales<=8,"No","High"),labels=c("High","No")))
data.cl = subset(Carseats, select=c(-Sales))
data.rg = subset(Carseats, select=c(-High))
## import constructors
source("src/cart.R")
# split data
require(caret)
set.seed(1237)
trainIndex = createDataPartition(Carseats$High, p=0.8, list=FALSE, times=1)
trainData.cl = data.cl[trainIndex,]
testData.cl = data.cl[-trainIndex,]
## run rpartDT
set.seed(12357)
cl = cartDT(trainData.cl, testData.cl, "High ~ .", ntree=500)
## cp values
# cart
cl$rpt$cp[1,][[1]]
# bagging
summary(t(cl$boot.cp)[,2])
## individual errors
# cart test error
crt.err = cl$rpt$test.lst$error$error
crt.err
## single tree's train and test errors are quite lower
## for oob error, 2.3 sd away / for test error, 1.4 sd away
## if the mean of oob errors approximately equals to poterior means, the train error rate of the single tree may be unreasonable
(mean(unlist(cl$ind.oob.lst.err)) - crt.err)/sd(unlist(cl$ind.oob.lst.err))
(mean(unlist(cl$ind.tst.lst.err)) - crt.err)/sd(unlist(cl$ind.tst.lst.err))
# bagging error at least xerror - se to see 1-SE rule
ind.oob.err = data.frame(type="oob",error=unlist(cl$ind.oob.lst.err))
ind.tst.err = data.frame(type="test",error=unlist(cl$ind.tst.lst.err))
ind.err = rbind(ind.oob.err,ind.tst.err)
ind.err.summary = as.data.frame(rbind(summary(ind.err$error[ind.err$type=="oob"])
,summary(ind.err$error[ind.err$type=="test"])))
rownames(ind.err.summary) <- c("oob","test")
ind.err.summary
# plot error distribution
ggplot(ind.err, aes(x=error,fill=type)) +
geom_histogram() + geom_vline(xintercept=crt.err, color="blue") +
ggtitle("Error distribution") + theme(plot.title=element_text(face="bold"))
## cumulative errors
## for oob error, the bagging error stays around 0.225, which is well over the training error of 0.16
## for test error, the bagging error converges to around 0.164, which is lower than 0.19
## bagging doesn't overestimate and provided better prediction
bgg.oob.err = data.frame(type="oob"
,ntree=1:length(cl$cum.oob.lst.err)
,error=unlist(cl$cum.oob.lst.err))
bgg.tst.err = data.frame(type="test"
,ntree=1:length(cl$cum.tst.lst.err)
,error=unlist(cl$cum.tst.lst.err))
bgg.err = rbind(bgg.oob.err,bgg.tst.err)
# plot bagging errors
ggplot(data=bgg.err,aes(x=ntree,y=error,colour=type)) +
geom_line() + geom_abline(intercept=crt.err,slope=0,color="blue") +
ggtitle("Bagging error") + theme(plot.title=element_text(face="bold"))
## variable importance
## variable importance is somewhat different - US, Urban and Education are rarely considered in the single tree
# cart
cart.varImp = data.frame(method="cart"
,variable=names(cl$rpt$mod$variable.importance)
,value=cl$rpt$mod$variable.importance/sum(cl$rpt$mod$variable.importance)
,row.names=NULL)
# bagging
ntree = length(cl$cum.varImp.lst)
bgg.varImp = data.frame(method="bagging"
,variable=rownames(cl$cum.varImp.lst)
,value=cl$cum.varImp.lst[,ntree])
# plot variable importance measure
cl.varImp = rbind(cart.varImp,bgg.varImp)
cl.varImp$variable = reorder(cl.varImp$variable, 1/cl.varImp$value)
ggplot(data=cl.varImp,aes(x=variable,y=value,fill=method)) + geom_bar(stat="identity") +
ggtitle("Variable importance") + theme(plot.title=element_text(face="bold"))
crt.err
mean(unlist(cl$ind.oob.lst.err)
)
mean(unlist(cl$ind.tst.lst.err))
sd(unlist(cl$ind.oob.lst.err))
sd(unlist(cl$ind.tst.lst.err))
round((mean(unlist(cl$ind.oob.lst.err)) - cl$rpt$test.lst$error$error)/sd(unlist(cl$ind.oob.lst.err)),2)
round((mean(unlist(cl$ind.tst.lst.err)) - cl$rpt$test.lst$error$error)/sd(unlist(cl$ind.tst.lst.err)),2)
cl$rpt$test.lst$error$error
mean(unlist(cl$ind.oob.lst.err))
round(mean(unlist(cl$ind.oob.lst.err)),2)
round(mean(unlist(cl$ind.tst.lst.err)),2)
round(unlist(rg$cum.oob.lst.err)[[1000]],3)
round(unlist(cl$cum.oob.lst.err)[[1000]],3)
round(unlist(cl$cum.oob.lst.err)[[500]],3)
round(unlist(cl$cum.tst.lst.err)[[500]],3)
round(crt.err,3)
The plots of cumulative oob and test errors are shown below. It is found that, while the cumulative test error (`r round(unlist(cl$cum.tst.lst.err)[[500]],3)`) settles just after the 300th tree and it is lower than the single tree's test error (`r round(unlist(cl$cum.oob.lst.err)[[500]],3)`), the cumulative oob error fluctuates and it is higher than that of the single tree - the error at the 500th tree is .
round(unlist(cl$cum.oob.lst.err)[[500]],3)
round(unlist(cl$cum.oob.lst.err)[[500]],3)
rm(list = ls())
source("src/knitSrc.R")
knitPost("2015-03-07-Tree-Based-Methods-Part-VI-Classification-Evaluation")
?lapply
library(parallel)
?parLapply
library(rlecuyer)
?clusterSetupRNG
library(snow)
?clusterSetupRNG
source("src/knitSrc.R")
knitPost("2015-03-14-Parallel-Processing-on-Single-Machine-Part-I")
ls()[ls()!="unloadPkg"]
ls()[ls()!="unloadPkg" || ls()!="moveFigs"]
ls()[ls()!="unloadPkg" | ls()!="moveFigs"]
ls()[ls()!="unloadPkg" && ls()!="moveFigs"]
rm(ls()[ls()!="unloadPkg" && ls()!="moveFigs"])
rm(list = ls()[ls()!="unloadPkg" && ls()!="moveFigs"])
rm(list = ls()[ls()!="unloadPkg" || ls()!="moveFigs"])
source("src/knitSrc.R")
knitPost("2015-03-14-Parallel-Processing-on-Single-Machine-Part-I")
rm(list = ls())
source("src/knitSrc.R")
knitPost("2015-03-14-Parallel-Processing-on-Single-Machine-Part-I")
