rm(list = ls())
library(dplyr)
library(ggplot2)
library(grid)
library(gridExtra)
library(rpart)
library(rpart.plot)
library(caret)
## data
require(ISLR)
data(Carseats)
# label order changed (No=1)
Carseats = Carseats %>%
mutate(High=factor(ifelse(Sales<=8,"No","High"),labels=c("No","High")))
# structure of predictors
str(subset(Carseats,select=c(-High,-Sales)))
# response summaries
res.cl.summary = with(Carseats,rbind(table(High),table(High)/length(High)))
res.cl.summary
res.reg.summary = summary(Carseats$Sales)
res.reg.summary
## split data
set.seed(1237)
trainIndex = createDataPartition(Carseats$High, p=.8, list=FALSE, times=1)
# classification
trainData.cl = subset(Carseats, select=c(-Sales))[trainIndex,]
testData.cl = subset(Carseats, select=c(-Sales))[-trainIndex,]
# regression
trainData.reg = subset(Carseats, select=c(-High))[trainIndex,]
testData.reg = subset(Carseats, select=c(-High))[-trainIndex,]
# response summary
train.res.cl.summary = with(trainData.cl,rbind(table(High),table(High)/length(High)))
test.res.cl.summary = with(testData.cl,rbind(table(High),table(High)/length(High)))
train.res.reg.summary = summary(trainData.reg$Sales)
test.res.reg.summary = summary(testData.reg$Sales)
## train model
# set up train control
trControl = trainControl(method="repeatedcv",number=10,repeats=5)
# classification
set.seed(12357)
mod.cl = train(High ~ .
,data=trainData.cl
,method="rpart"
,tuneLength=20
,trControl=trControl)
# regression - caret
source("src/mlUtils.R")
# caret
set.seed(12357)
mod.reg.caret = train(Sales ~ .
,data=trainData.reg
,method="rpart"
,tuneLength=20
,trControl=trControl)
# R squared meaningless
# Warning message:
#   In nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,  :
#     There were missing values in resampled performance measures.
# http://stackoverflow.com/questions/26828901/warning-message-missing-values-in-resampled-performance-measures-in-caret-tra
# http://stackoverflow.com/questions/10503784/caret-error-when-using-anything-but-loocv-with-rpart
# regression - rpart
set.seed(12357)
mod.reg.rpart = rpart(Sales ~ ., data=trainData.reg, control=rpart.control(cp=0))
mod.reg.rpart.param = bestParam(mod.reg.rpart$cptable,"CP","xerror","xstd")
mod.reg.rpart.param
mod.reg.caret.param = bestParam(mod.reg.caret$results,"cp","RMSE","RMSESD",isDesc=FALSE)
mod.reg.caret.param
# plot best CP
df = as.data.frame(mod.reg.rpart$cptable)
best = bestParam(mod.reg.rpart$cptable,"CP","xerror","xstd")
ubound = ifelse(best[2,1]+best[3,1]>max(df$xerror),max(df$xerror),best[2,1]+best[3,1])
lbound = ifelse(best[2,1]-best[3,1]<min(df$xerror),min(df$xerror),best[2,1]-best[3,1])
tune.plot = ggplot(data=df[3:nrow(df),], aes(x=CP,y=xerror)) +
geom_line() + geom_point() +
geom_abline(intercept=ubound,slope=0, color="blue") +
geom_abline(intercept=lbound,slope=0, color="blue") +
geom_point(aes(x=best[1,2],y=best[2,2]),color="red",size=3)
print(tune.plot)
## show best tuned cp
# classification
subset(mod.cl$results,subset=cp==mod.cl$bestTune$cp)
# regression - caret
subset(mod.reg.caret$results,subset=cp==mod.reg.caret$bestTune$cp)
# regression - rpart
mod.reg.rpart.summary = data.frame(t(mod.reg.rpart.param[,2]))
colnames(mod.reg.rpart.summary) = c("CP","xerror","xstd")
mod.reg.rpart.summary
## refit the model to the entire training data
# classification
cp.cl = mod.cl$bestTune$cp
mod.cl = rpart(High ~ ., data=trainData.cl, control=rpart.control(cp=cp.cl))
# regression - caret
cp.reg.caret = mod.reg.caret$bestTune$cp
mod.reg.caret = rpart(Sales ~ ., data=trainData.reg, control=rpart.control(cp=cp.reg.caret))
# regression - rpart
cp.reg.rpart = mod.reg.rpart.param[1,2]
mod.reg.rpart = rpart(Sales ~ ., data=trainData.reg, control=rpart.control(cp=cp.reg.rpart))
## generate confusion matrix on training data
# fit models
fit.cl = predict(mod.cl, type="class")
fit.reg.caret = predict(mod.reg.caret)
fit.reg.rpart = predict(mod.reg.rpart)
# classification
# percentile that Sales is divided by No and High
eqPercentile = with(trainData.reg,length(Sales[Sales<=8])/length(Sales))
eqPercentile
# classification
fit.cl.cm = table(data.frame(actual=trainData.cl$High,response=fit.cl))
fit.cl.cm = updateCM(fit.cl.cm,type="Fitted")
fit.cl.cm
# regression with equal percentile is not comparable
probs = eqPercentile
fit.reg.caret.cm = regCM(trainData.reg$Sales, fit.reg.caret, probs=probs, type="Fitted")
fit.reg.caret.cm
# regression with selected percentiles
probs = seq(0.2,0.8,0.2)
# regression - caret
# caret produces a better outcome on training data - note lower cp
fit.reg.caret.cm = regCM(trainData.reg$Sales, fit.reg.caret, probs=probs, type="Fitted")
fit.reg.caret.cm
# regression - rpart
fit.reg.rpart.cm = regCM(trainData.reg$Sales, fit.reg.rpart, probs=probs, type="Fitted")
fit.reg.rpart.cm
## generate confusion matrix on test data
# fit models
pred.reg.caret = predict(mod.reg.caret, newdata=testData.reg)
pred.reg.rpart = predict(mod.reg.rpart, newdata=testData.reg)
# regression - caret
pred.reg.caret.cm = regCM(testData.reg$Sales, pred.reg.caret, probs=probs)
pred.reg.caret.cm
# regression - rpart
pred.reg.rpart.cm = regCM(testData.reg$Sales, pred.reg.rpart, probs=probs)
pred.reg.rpart.cm
# model by caret with lower cp produces better outcome
# 1 standard error rule is questionable on this data
pred.reg.caret.rmse = sqrt(sum(testData.reg$Sales-pred.reg.caret)^2/length(testData.reg$Sales))
pred.reg.caret.rmse
pred.reg.rpart.rmse = sqrt(sum(testData.reg$Sales-pred.reg.rpart)^2/length(testData.reg$Sales))
pred.reg.rpart.rmse
## plot actual vs prediced and resid vs fitted
mod.reg.caret.test = rpart(Sales ~ ., data=testData.reg, control=rpart.control(cp=cp.reg.caret))
predDF = data.frame(actual=testData.reg$Sales
,predicted=pred.reg.caret
,resid=resid(mod.reg.caret.test))
# correlation
cor(predDF)
# actual vs predicted
actual.plot = ggplot(predDF, aes(x=predicted,y=actual)) +
geom_point(shape=1,position=position_jitter(width=0.1,height=0.1)) +
geom_smooth(method=lm,se=FALSE)
# resid vs predicted
resid.plot = ggplot(predDF, aes(x=predicted,y=resid)) +
geom_point(shape=1,position=position_jitter(width=0.1,height=0.1)) +
geom_smooth(method=lm,se=FALSE)
grid.arrange(actual.plot, resid.plot, ncol = 2)
# plot tree
cols <- ifelse(mod.reg.caret$frame$yval > 8,"green4","darkred") # green if high
prp(mod.reg.caret
,main="CART Model Tree"
#,extra=106           # display prob of survival and percent of obs
,nn=TRUE             # display the node numbers
,fallen.leaves=TRUE  # put the leaves on the bottom of the page
,branch=.5           # change angle of branch lines
,faclen=0            # do not abbreviate factor levels
,trace=1             # print the automatically calculated cex
,shadow.col="gray"   # shadows under the leaves
,branch.lty=3        # draw branches using dotted lines
,split.cex=1.2       # make the split text larger than the node text
,split.prefix="is "  # put "is " before split text
,split.suffix="?"    # put "?" after split text
,col=cols, border.col=cols   # green if survived
,split.box.col="lightgray"   # lightgray split boxes (default is white)
,split.border.col="darkgray" # darkgray border on split boxes
,split.round=.5)
with(trainData.reg,length(Sales[Sales<=8])/length(Sales))
fit.reg.caret.rmse = sqrt(sum(trainData.reg$Sales-fit.reg.caret)^2/length(trainData.reg$Sales))
fit.reg.caret.rmse
fit.reg.rpart.rmse = sqrt(sum(trainData.reg$Sales-fit.reg.rpart)^2/length(trainData.reg$Sales))
fit.reg.rpart.rmse
?format
format(fit.reg.caret.rmse,digits=10,scientific=FALSE)
format(fit.reg.caret.rmse,scientific=FALSE)
format(fit.reg.rpart.rmse,scientific=FALSE)
fit.reg.caret.rmse
fit.reg.caret.mse = sum(trainData.reg$Sales-fit.reg.caret)^2/length(trainData.reg$Sales)
fit.reg.caret.mse
fit.reg.caret.rmse = sqrt(sum(trainData.reg$Sales-fit.reg.caret)^2/length(trainData.reg$Sales))
fit.reg.caret.rmse
fit.reg.rpart.rmse = sqrt(sum(trainData.reg$Sales-fit.reg.rpart)^2/length(trainData.reg$Sales))
fit.reg.rpart.rmse
rm(list = ls())
source("src/knitSrc.R")
knitPost("2015-02-14-Tree-Based-Methods-Part-III-Regression")
source("src/knitSrc.R")
knitPost("2015-02-14-Tree-Based-Methods-Part-III-Regression")
rm(list = ls())
source("src/knitSrc.R")
knitPost("2015-02-14-Tree-Based-Methods-Part-III-Regression")
source("src/knitSrc.R")
knitPost("2015-02-15-Tree-Based-Methods-Part-IV-Packages-Comparison")
test.perf.lst = ls()
## intro
# construct base class
# http://www.mat.uc.cl/~susana/CE/oopV2.pdf
emp = function(name) {
if(length(name) != 1 && !is.character(name))
stop("single string value of name necessary")
# assign name to result
result = name
class(result) = "employee"
return(result)
}
# extend base class
# manager 'is-a' employee
man = function(name, members) {
# create base class (employee)
result = emp(name)
# arguments should be carefully checked
if(length(sapply(members,class)[sapply(members,class)=="character"]) != length(members))
stop("a vector employees necessary")
# combine name and members as a list
result = list(name=result,members=members)
class(result)=c("manager","employee")
return(result)
}
tom = emp("Tom")
alice = emp("Alice")
nick = man("Nick",c(tom,alice))
# without print function
tom
nick
# base method can be extended
# better not to include full stop(.) in class name
print.employee = function(obj) {
cat("Employee Name:",obj,"\n")
}
print.manager = function(obj) {
cat("Employee Name:",obj$name,"\n")
cat("Members:",obj$members,"\n")
}
# methods that extends print
head(methods("print"),3)
methods("print")[grep("*employee",methods("print"))]
# methods that associated with manager class
methods(class="employee")
# with print function
tom
nick
# class can be checked/modified
class(tom) = NULL
tom # character vector
# list names and assigned classes can be checked/modified
attributes(nick)
attributes(nick) = NULL
nick
# incorrectly assigned class should be handled
foo = c(1)
class(foo) = "employee"
class(foo)
man("John",foo)
attributes(,tom)
attributes(tom)
attributes(nick)
nick
emp = function(name) {
if(length(name) != 1 && !is.character(name))
stop("single string value of name necessary")
# assign name to result
result = name
class(result) = "employee"
return(result)
}
man = function(name, members) {
# create base class (employee)
result = emp(name)
# arguments should be carefully checked
if(length(sapply(members,class)[sapply(members,class)=="character"]) != length(members))
stop("a vector employees necessary")
# combine name and members as a list
result = list(name=result,members=members)
class(result)=c("manager","employee")
return(result)
}
tom = emp("Tom")
alice = emp("Alice")
nick = man("Nick",c(tom,alice))
attributes(nick)
attributes(tom)
tom
rm(list = ls())
source("src/knitSrc.R")
knitPost("2015-02-21-Quick-Trial-of-Turning-Analysis-into-S3-Object")
## data
require(ISLR)
data(Carseats)
require(dplyr)
Carseats = Carseats %>%
mutate(High=factor(ifelse(Sales<=8,"No","High"),labels=c("High","No")))
data.cl = subset(Carseats, select=c(-Sales))
data.rg = subset(Carseats, select=c(-High))
## import constructors
source("src/cart.R")
# split - cl: classification, rg: regression
require(caret)
set.seed(1237)
trainIndex = createDataPartition(Carseats$High, p=0.8, list=FALSE, times=1)
trainData.cl = data.cl[trainIndex,]
testData.cl = data.cl[-trainIndex,]
trainData.rg = data.rg[trainIndex,]
testData.rg = data.rg[-trainIndex,]
## classification
set.seed(12357)
rpt.cl = cartRPART(trainData.cl,testData.cl,formula="High ~ .")
rpt.cl$test.lst$cm
rm(list = ls())
## data
require(ISLR)
data(Carseats)
require(dplyr)
Carseats = Carseats %>%
mutate(High=factor(ifelse(Sales<=8,"No","High"),labels=c("High","No")))
data.cl = subset(Carseats, select=c(-Sales))
data.rg = subset(Carseats, select=c(-High))
# split - cl: classification, rg: regression
require(caret)
set.seed(1237)
trainIndex = createDataPartition(Carseats$High, p=0.8, list=FALSE, times=1)
trainData.cl = data.cl[trainIndex,]
testData.cl = data.cl[-trainIndex,]
trainData.rg = data.rg[trainIndex,]
testData.rg = data.rg[-trainIndex,]
## import constructors
source("src/cart.R")
## classification
set.seed(12357)
rpt.cl = cartRPART(trainData.cl,testData.cl,formula="High ~ .")
rm(list = ls())
## data
require(ISLR)
data(Carseats)
require(dplyr)
Carseats = Carseats %>%
mutate(High=factor(ifelse(Sales<=8,"No","High"),labels=c("High","No")))
data.cl = subset(Carseats, select=c(-Sales))
data.rg = subset(Carseats, select=c(-High))
# split - cl: classification, rg: regression
require(caret)
set.seed(1237)
trainIndex = createDataPartition(Carseats$High, p=0.8, list=FALSE, times=1)
trainData.cl = data.cl[trainIndex,]
testData.cl = data.cl[-trainIndex,]
trainData.rg = data.rg[trainIndex,]
testData.rg = data.rg[-trainIndex,]
## import constructors
source("src/cart.R")
## classification
set.seed(12357)
rpt.cl = cartRPART(trainData.cl,testData.cl,formula="High ~ .")
rm(list = ls())
?sep
?seq
---
layout: post
title: "2015-02-23-Tree-Based-Methods-Part-IV-Packages-Comparison-in-S3"
description: ""
category: R
tags: [ggplot2, rpart, caret, mlr, programming]
---
{% include JB/setup %}
In the [previous article](http://jaehyeon-kim.github.io/r/2015/02/21/Quick-Trial-of-Turning-Analysis-into-S3-Object/), a brief introduction to the S3 object is made as well as a class that encapsulates the CART analysis by the **rpart** package is illustrated. Extending an [earlier article](http://jaehyeon-kim.github.io/r/2015/02/15/Tree-Based-Methods-Part-IV-Packages-Comparison/) of comparing the three packages (**rpart**, **caret** and **mlr**), this article compares them using the following 3 S3 objects: *rpartExt*, *rpartExtCrt* and *rpartExtMlr*. Like *manager* **is-a** *employee* so that it can extends the base class in the previous article, it is roughly conceptualized that the last two classes extend the first. On this setting, performance of both classification and regression tasks are compared in this article.
Before getting started, note that the source of these classes can be found in [this gist](https://gist.github.com/jaehyeon-kim/b89dcbd2fb0b84fd236e) and, together with the relevant packages, it requires 3 utility functions that can be found [here](https://gist.github.com/jaehyeon-kim/5622ae9fa982e0b46550).
Let's get started.
The data is split for both classification and regression.
```{r data, message=FALSE, warning=FALSE}
## data
require(ISLR)
data(Carseats)
require(dplyr)
Carseats = Carseats %>%
mutate(High=factor(ifelse(Sales<=8,"No","High"),labels=c("High","No")))
data.cl = subset(Carseats, select=c(-Sales))
data.rg = subset(Carseats, select=c(-High))
# split - cl: classification, rg: regression
require(caret)
set.seed(1237)
trainIndex = createDataPartition(Carseats$High, p=0.8, list=FALSE, times=1)
trainData.cl = data.cl[trainIndex,]
testData.cl = data.cl[-trainIndex,]
trainData.rg = data.rg[trainIndex,]
testData.rg = data.rg[-trainIndex,]
```
The constructors are sourced.
```{r src, message=FALSE, warning=FALSE}
## import constructors
source("src/cart.R")
```
The classification task is fit by each of the packages. Note that the constructors of the subclasses (*rpartExtCrt*, *rpartExtMlr*) has an option to fit data using the base class (*rpartExt*) and it is determined by the argument of *fitInd*. Once it is set *TRUE*, the constructor of the base class is executed (or the base class is instantiated), resulting that its outcome (named *rpt*) is kept as a member of the outcome list of its subclasses. Otherwise a null list is added as a placeholder.
```{r classif_fit, message=FALSE, warning=FALSE}
## classification
set.seed(12357)
rpt.cl = cartRPART(trainData.cl,testData.cl,formula="High ~ .")
crt.cl = cartCARET(trainData.cl,testData.cl,formula="High ~ .",fitInd=TRUE)
mlr.cl = cartMLR(trainData.cl,testData.cl,formula="High ~ .",fitInd=TRUE)
```
Class and names attributes of each object can be seen below. As the latter two are assumed to extend the first, their class attributes include the class name of the first. Also, as *fitInd* is set *TRUE*, the base class is instantiated, which can be checked that the names attributes of *rpt.cl* and *crt.cl$rpt* are the same.
```{r class_intro, message=FALSE, warning=FALSE}
## classes and attributes
data.frame(rpart=c(class(rpt.cl),""),caret=class(crt.cl),mlr=class(mlr.cl))
attributes(rpt.cl)$names
attributes(crt.cl)$names
attributes(mlr.cl)$names
attributes(crt.cl$rpt)$names
```
The performance of the classfication task is compared and, as seen earlier, the tree is not sensitive to the *cp*.
```{r classif_comparison, message=FALSE, warning=FALSE}
# comparison
perf.cl = list(rpt.cl$train.lst$error,rpt.cl$train.se$error
,rpt.cl$test.lst$error,rpt.cl$test.se$error
,crt.cl$train.lst$error,crt.cl$test.lst$error
,mlr.cl$train.lst$error,mlr.cl$test.lst$error)
err = function(perf) {
out = list(unlist(perf[[1]]))
for(i in 2:length(perf)) {
out[[length(out)+1]] = unlist(perf[[i]])
}
t(sapply(out,unlist))
}
err(perf.cl)
```
Then the data is fit as regression - the default *fitInd* value is *FALSE*
```{r reg_fit, message=FALSE, warning=FALSE}
## regression
set.seed(12357)
rpt.rg = cartRPART(trainData.rg,testData.rg,formula="Sales ~ .")
crt.rg = cartCARET(trainData.rg,testData.rg,formula="Sales ~ .")
mlr.rg = cartMLR(trainData.rg,testData.rg,formula="Sales ~ .")
```
The performance of the regression task is compared below. It is found that, unlike the classification task, the *cp* plays a more role.
Specifically
- the value (*0.0049*) at the minimum *xerror* by the **rpart** package records the least *RMSE* (*0,74*)
- the *1-SE rule* is also questionable by delivering the highest *RMSE* (*1.96*)
- the best *cp* value by the **caret** and **mlr** packages is *0* and the resulting *RMSE* (*0.95*) is higher
The last is due to the way how the grids are set up in these packages. The *tuneLength* of the **caret** package is set to *30*, which means that the grid
the grid is constructed to divide the range of *cp* values from 0 to 0.3 in 30 sub-ranges.
```{r reg_comparison, message=FALSE, warning=FALSE}
# comparison
perf.rg = list(rpt.rg$train.lst$error,rpt.rg$train.se$error
,rpt.rg$test.lst$error,rpt.rg$test.se$error
,crt.rg$train.lst$error,crt.rg$test.lst$error
,mlr.rg$train.lst$error,mlr.rg$test.lst$error)
err(perf.rg)
```
))
## data
require(ISLR)
data(Carseats)
require(dplyr)
Carseats = Carseats %>%
mutate(High=factor(ifelse(Sales<=8,"No","High"),labels=c("High","No")))
data.cl = subset(Carseats, select=c(-Sales))
data.rg = subset(Carseats, select=c(-High))
# split - cl: classification, rg: regression
require(caret)
set.seed(1237)
trainIndex = createDataPartition(Carseats$High, p=0.8, list=FALSE, times=1)
trainData.cl = data.cl[trainIndex,]
testData.cl = data.cl[-trainIndex,]
trainData.rg = data.rg[trainIndex,]
testData.rg = data.rg[-trainIndex,]
## import constructors
source("src/cart.R")
## classification
set.seed(12357)
rpt.cl = cartRPART(trainData.cl,testData.cl,formula="High ~ .")
crt.cl = cartCARET(trainData.cl,testData.cl,formula="High ~ .",fitInd=TRUE)
mlr.cl = cartMLR(trainData.cl,testData.cl,formula="High ~ .",fitInd=TRUE)
## classes and attributes
data.frame(rpart=c(class(rpt.cl),""),caret=class(crt.cl),mlr=class(crt.cl))
attributes(rpt.cl)$names
attributes(crt.cl)$names
attributes(mlr.cl)$names
attributes(crt.cl$rpt)$names
# comparison
perf.cl = list(rpt.cl$train.lst$error,rpt.cl$train.se$error
,rpt.cl$test.lst$error,rpt.cl$test.se$error
,crt.cl$train.lst$error,crt.cl$test.lst$error
,mlr.cl$train.lst$error,mlr.cl$test.lst$error)
err = function(perf) {
out = list(unlist(perf[[1]]))
for(i in 2:length(perf)) {
out[[length(out)+1]] = unlist(perf[[i]])
}
t(sapply(out,unlist))
}
err(perf.cl)
## regression
set.seed(12357)
rpt.rg = cartRPART(trainData.rg,testData.rg,formula="Sales ~ .")
crt.rg = cartCARET(trainData.rg,testData.rg,formula="Sales ~ .")
mlr.rg = cartMLR(trainData.rg,testData.rg,formula="Sales ~ .")
# comparison
perf.rg = list(rpt.rg$train.lst$error,rpt.rg$train.se$error
,rpt.rg$test.lst$error,rpt.rg$test.se$error
,crt.rg$train.lst$error,crt.rg$test.lst$error
,mlr.rg$train.lst$error,mlr.rg$test.lst$error)
err(perf.rg)
crt.rg$mod
crt.rg$mod$result
head(crt.rg$mod$result,3)
rm(list = ls())
source("src/knitSrc.R")
knitPost("2015-02-23-Tree-Based-Methods-Part-IV-Packages-Comparison-in-S3")
