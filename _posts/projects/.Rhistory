trainData.cl = data.cl[trainIndex,]
testData.cl = data.cl[-trainIndex,]
trainData.rg = data.rg[trainIndex,]
testData.rg = data.rg[-trainIndex,]
## classification
set.seed(12357)
rpt.cl = cartRPART(trainData.cl,testData.cl,formula="High ~ .")
rpt.cl$test.lst$cm
rm(list = ls())
## data
require(ISLR)
data(Carseats)
require(dplyr)
Carseats = Carseats %>%
mutate(High=factor(ifelse(Sales<=8,"No","High"),labels=c("High","No")))
data.cl = subset(Carseats, select=c(-Sales))
data.rg = subset(Carseats, select=c(-High))
# split - cl: classification, rg: regression
require(caret)
set.seed(1237)
trainIndex = createDataPartition(Carseats$High, p=0.8, list=FALSE, times=1)
trainData.cl = data.cl[trainIndex,]
testData.cl = data.cl[-trainIndex,]
trainData.rg = data.rg[trainIndex,]
testData.rg = data.rg[-trainIndex,]
## import constructors
source("src/cart.R")
## classification
set.seed(12357)
rpt.cl = cartRPART(trainData.cl,testData.cl,formula="High ~ .")
rm(list = ls())
## data
require(ISLR)
data(Carseats)
require(dplyr)
Carseats = Carseats %>%
mutate(High=factor(ifelse(Sales<=8,"No","High"),labels=c("High","No")))
data.cl = subset(Carseats, select=c(-Sales))
data.rg = subset(Carseats, select=c(-High))
# split - cl: classification, rg: regression
require(caret)
set.seed(1237)
trainIndex = createDataPartition(Carseats$High, p=0.8, list=FALSE, times=1)
trainData.cl = data.cl[trainIndex,]
testData.cl = data.cl[-trainIndex,]
trainData.rg = data.rg[trainIndex,]
testData.rg = data.rg[-trainIndex,]
## import constructors
source("src/cart.R")
## classification
set.seed(12357)
rpt.cl = cartRPART(trainData.cl,testData.cl,formula="High ~ .")
rm(list = ls())
?sep
?seq
---
layout: post
title: "2015-02-23-Tree-Based-Methods-Part-IV-Packages-Comparison-in-S3"
description: ""
category: R
tags: [ggplot2, rpart, caret, mlr, programming]
---
{% include JB/setup %}
In the [previous article](http://jaehyeon-kim.github.io/r/2015/02/21/Quick-Trial-of-Turning-Analysis-into-S3-Object/), a brief introduction to the S3 object is made as well as a class that encapsulates the CART analysis by the **rpart** package is illustrated. Extending an [earlier article](http://jaehyeon-kim.github.io/r/2015/02/15/Tree-Based-Methods-Part-IV-Packages-Comparison/) of comparing the three packages (**rpart**, **caret** and **mlr**), this article compares them using the following 3 S3 objects: *rpartExt*, *rpartExtCrt* and *rpartExtMlr*. Like *manager* **is-a** *employee* so that it can extends the base class in the previous article, it is roughly conceptualized that the last two classes extend the first. On this setting, performance of both classification and regression tasks are compared in this article.
Before getting started, note that the source of these classes can be found in [this gist](https://gist.github.com/jaehyeon-kim/b89dcbd2fb0b84fd236e) and, together with the relevant packages, it requires 3 utility functions that can be found [here](https://gist.github.com/jaehyeon-kim/5622ae9fa982e0b46550).
Let's get started.
The data is split for both classification and regression.
```{r data, message=FALSE, warning=FALSE}
## data
require(ISLR)
data(Carseats)
require(dplyr)
Carseats = Carseats %>%
mutate(High=factor(ifelse(Sales<=8,"No","High"),labels=c("High","No")))
data.cl = subset(Carseats, select=c(-Sales))
data.rg = subset(Carseats, select=c(-High))
# split - cl: classification, rg: regression
require(caret)
set.seed(1237)
trainIndex = createDataPartition(Carseats$High, p=0.8, list=FALSE, times=1)
trainData.cl = data.cl[trainIndex,]
testData.cl = data.cl[-trainIndex,]
trainData.rg = data.rg[trainIndex,]
testData.rg = data.rg[-trainIndex,]
```
The constructors are sourced.
```{r src, message=FALSE, warning=FALSE}
## import constructors
source("src/cart.R")
```
The classification task is fit by each of the packages. Note that the constructors of the subclasses (*rpartExtCrt*, *rpartExtMlr*) has an option to fit data using the base class (*rpartExt*) and it is determined by the argument of *fitInd*. Once it is set *TRUE*, the constructor of the base class is executed (or the base class is instantiated), resulting that its outcome (named *rpt*) is kept as a member of the outcome list of its subclasses. Otherwise a null list is added as a placeholder.
```{r classif_fit, message=FALSE, warning=FALSE}
## classification
set.seed(12357)
rpt.cl = cartRPART(trainData.cl,testData.cl,formula="High ~ .")
crt.cl = cartCARET(trainData.cl,testData.cl,formula="High ~ .",fitInd=TRUE)
mlr.cl = cartMLR(trainData.cl,testData.cl,formula="High ~ .",fitInd=TRUE)
```
Class and names attributes of each object can be seen below. As the latter two are assumed to extend the first, their class attributes include the class name of the first. Also, as *fitInd* is set *TRUE*, the base class is instantiated, which can be checked that the names attributes of *rpt.cl* and *crt.cl$rpt* are the same.
```{r class_intro, message=FALSE, warning=FALSE}
## classes and attributes
data.frame(rpart=c(class(rpt.cl),""),caret=class(crt.cl),mlr=class(mlr.cl))
attributes(rpt.cl)$names
attributes(crt.cl)$names
attributes(mlr.cl)$names
attributes(crt.cl$rpt)$names
```
The performance of the classfication task is compared and, as seen earlier, the tree is not sensitive to the *cp*.
```{r classif_comparison, message=FALSE, warning=FALSE}
# comparison
perf.cl = list(rpt.cl$train.lst$error,rpt.cl$train.se$error
,rpt.cl$test.lst$error,rpt.cl$test.se$error
,crt.cl$train.lst$error,crt.cl$test.lst$error
,mlr.cl$train.lst$error,mlr.cl$test.lst$error)
err = function(perf) {
out = list(unlist(perf[[1]]))
for(i in 2:length(perf)) {
out[[length(out)+1]] = unlist(perf[[i]])
}
t(sapply(out,unlist))
}
err(perf.cl)
```
Then the data is fit as regression - the default *fitInd* value is *FALSE*
```{r reg_fit, message=FALSE, warning=FALSE}
## regression
set.seed(12357)
rpt.rg = cartRPART(trainData.rg,testData.rg,formula="Sales ~ .")
crt.rg = cartCARET(trainData.rg,testData.rg,formula="Sales ~ .")
mlr.rg = cartMLR(trainData.rg,testData.rg,formula="Sales ~ .")
```
The performance of the regression task is compared below. It is found that, unlike the classification task, the *cp* plays a more role.
Specifically
- the value (*0.0049*) at the minimum *xerror* by the **rpart** package records the least *RMSE* (*0,74*)
- the *1-SE rule* is also questionable by delivering the highest *RMSE* (*1.96*)
- the best *cp* value by the **caret** and **mlr** packages is *0* and the resulting *RMSE* (*0.95*) is higher
The last is due to the way how the grids are set up in these packages. The *tuneLength* of the **caret** package is set to *30*, which means that the grid
the grid is constructed to divide the range of *cp* values from 0 to 0.3 in 30 sub-ranges.
```{r reg_comparison, message=FALSE, warning=FALSE}
# comparison
perf.rg = list(rpt.rg$train.lst$error,rpt.rg$train.se$error
,rpt.rg$test.lst$error,rpt.rg$test.se$error
,crt.rg$train.lst$error,crt.rg$test.lst$error
,mlr.rg$train.lst$error,mlr.rg$test.lst$error)
err(perf.rg)
```
))
## data
require(ISLR)
data(Carseats)
require(dplyr)
Carseats = Carseats %>%
mutate(High=factor(ifelse(Sales<=8,"No","High"),labels=c("High","No")))
data.cl = subset(Carseats, select=c(-Sales))
data.rg = subset(Carseats, select=c(-High))
# split - cl: classification, rg: regression
require(caret)
set.seed(1237)
trainIndex = createDataPartition(Carseats$High, p=0.8, list=FALSE, times=1)
trainData.cl = data.cl[trainIndex,]
testData.cl = data.cl[-trainIndex,]
trainData.rg = data.rg[trainIndex,]
testData.rg = data.rg[-trainIndex,]
## import constructors
source("src/cart.R")
## classification
set.seed(12357)
rpt.cl = cartRPART(trainData.cl,testData.cl,formula="High ~ .")
crt.cl = cartCARET(trainData.cl,testData.cl,formula="High ~ .",fitInd=TRUE)
mlr.cl = cartMLR(trainData.cl,testData.cl,formula="High ~ .",fitInd=TRUE)
## classes and attributes
data.frame(rpart=c(class(rpt.cl),""),caret=class(crt.cl),mlr=class(crt.cl))
attributes(rpt.cl)$names
attributes(crt.cl)$names
attributes(mlr.cl)$names
attributes(crt.cl$rpt)$names
# comparison
perf.cl = list(rpt.cl$train.lst$error,rpt.cl$train.se$error
,rpt.cl$test.lst$error,rpt.cl$test.se$error
,crt.cl$train.lst$error,crt.cl$test.lst$error
,mlr.cl$train.lst$error,mlr.cl$test.lst$error)
err = function(perf) {
out = list(unlist(perf[[1]]))
for(i in 2:length(perf)) {
out[[length(out)+1]] = unlist(perf[[i]])
}
t(sapply(out,unlist))
}
err(perf.cl)
## regression
set.seed(12357)
rpt.rg = cartRPART(trainData.rg,testData.rg,formula="Sales ~ .")
crt.rg = cartCARET(trainData.rg,testData.rg,formula="Sales ~ .")
mlr.rg = cartMLR(trainData.rg,testData.rg,formula="Sales ~ .")
# comparison
perf.rg = list(rpt.rg$train.lst$error,rpt.rg$train.se$error
,rpt.rg$test.lst$error,rpt.rg$test.se$error
,crt.rg$train.lst$error,crt.rg$test.lst$error
,mlr.rg$train.lst$error,mlr.rg$test.lst$error)
err(perf.rg)
crt.rg$mod
crt.rg$mod$result
head(crt.rg$mod$result,3)
rm(list = ls())
source("src/knitSrc.R")
knitPost("2015-02-23-Tree-Based-Methods-Part-IV-Packages-Comparison-in-S3")
title: "2015-03-03-2nd-Trial-of-Turning-Analysis-into-S3-Object"
source("src/knitSrc.R")
knitPost("2015-03-03-2nd-Trial-of-Turning-Analysis-into-S3-Object")
## data
require(ISLR)
data(Carseats)
require(dplyr)
Carseats = Carseats %>%
mutate(High=factor(ifelse(Sales<=8,"No","High"),labels=c("High","No")))
data.cl = subset(Carseats, select=c(-Sales))
data.rg = subset(Carseats, select=c(-High))
## import constructors
source("src/cart.R")
# split data
require(caret)
set.seed(1237)
trainIndex = createDataPartition(Carseats$High, p=0.8, list=FALSE, times=1)
trainData.cl = data.cl[trainIndex,]
testData.cl = data.cl[-trainIndex,]
## run rpartDT
set.seed(12357)
cl = cartDT(trainData.cl, testData.cl, "High ~ .", ntree=500)
## cp values
# cart
cl$rpt$cp[1,][[1]]
# bagging
summary(t(cl$boot.cp)[,2])
## individual errors
# cart test error
crt.err = cl$rpt$test.lst$error$error
crt.err
## single tree's train and test errors are quite lower
## for oob error, 2.3 sd away / for test error, 1.4 sd away
## if the mean of oob errors approximately equals to poterior means, the train error rate of the single tree may be unreasonable
(mean(unlist(cl$ind.oob.lst.err)) - crt.err)/sd(unlist(cl$ind.oob.lst.err))
(mean(unlist(cl$ind.tst.lst.err)) - crt.err)/sd(unlist(cl$ind.tst.lst.err))
# bagging error at least xerror - se to see 1-SE rule
ind.oob.err = data.frame(type="oob",error=unlist(cl$ind.oob.lst.err))
ind.tst.err = data.frame(type="test",error=unlist(cl$ind.tst.lst.err))
ind.err = rbind(ind.oob.err,ind.tst.err)
ind.err.summary = as.data.frame(rbind(summary(ind.err$error[ind.err$type=="oob"])
,summary(ind.err$error[ind.err$type=="test"])))
rownames(ind.err.summary) <- c("oob","test")
ind.err.summary
# plot error distribution
ggplot(ind.err, aes(x=error,fill=type)) +
geom_histogram() + geom_vline(xintercept=crt.err, color="blue") +
ggtitle("Error distribution") + theme(plot.title=element_text(face="bold"))
## cumulative errors
## for oob error, the bagging error stays around 0.225, which is well over the training error of 0.16
## for test error, the bagging error converges to around 0.164, which is lower than 0.19
## bagging doesn't overestimate and provided better prediction
bgg.oob.err = data.frame(type="oob"
,ntree=1:length(cl$cum.oob.lst.err)
,error=unlist(cl$cum.oob.lst.err))
bgg.tst.err = data.frame(type="test"
,ntree=1:length(cl$cum.tst.lst.err)
,error=unlist(cl$cum.tst.lst.err))
bgg.err = rbind(bgg.oob.err,bgg.tst.err)
# plot bagging errors
ggplot(data=bgg.err,aes(x=ntree,y=error,colour=type)) +
geom_line() + geom_abline(intercept=crt.err,slope=0,color="blue") +
ggtitle("Bagging error") + theme(plot.title=element_text(face="bold"))
## variable importance
## variable importance is somewhat different - US, Urban and Education are rarely considered in the single tree
# cart
cart.varImp = data.frame(method="cart"
,variable=names(cl$rpt$mod$variable.importance)
,value=cl$rpt$mod$variable.importance/sum(cl$rpt$mod$variable.importance)
,row.names=NULL)
# bagging
ntree = length(cl$cum.varImp.lst)
bgg.varImp = data.frame(method="bagging"
,variable=rownames(cl$cum.varImp.lst)
,value=cl$cum.varImp.lst[,ntree])
# plot variable importance measure
cl.varImp = rbind(cart.varImp,bgg.varImp)
cl.varImp$variable = reorder(cl.varImp$variable, 1/cl.varImp$value)
ggplot(data=cl.varImp,aes(x=variable,y=value,fill=method)) + geom_bar(stat="identity") +
ggtitle("Variable importance") + theme(plot.title=element_text(face="bold"))
crt.err
mean(unlist(cl$ind.oob.lst.err)
)
mean(unlist(cl$ind.tst.lst.err))
sd(unlist(cl$ind.oob.lst.err))
sd(unlist(cl$ind.tst.lst.err))
round((mean(unlist(cl$ind.oob.lst.err)) - cl$rpt$test.lst$error$error)/sd(unlist(cl$ind.oob.lst.err)),2)
round((mean(unlist(cl$ind.tst.lst.err)) - cl$rpt$test.lst$error$error)/sd(unlist(cl$ind.tst.lst.err)),2)
cl$rpt$test.lst$error$error
mean(unlist(cl$ind.oob.lst.err))
round(mean(unlist(cl$ind.oob.lst.err)),2)
round(mean(unlist(cl$ind.tst.lst.err)),2)
round(unlist(rg$cum.oob.lst.err)[[1000]],3)
round(unlist(cl$cum.oob.lst.err)[[1000]],3)
round(unlist(cl$cum.oob.lst.err)[[500]],3)
round(unlist(cl$cum.tst.lst.err)[[500]],3)
round(crt.err,3)
The plots of cumulative oob and test errors are shown below. It is found that, while the cumulative test error (`r round(unlist(cl$cum.tst.lst.err)[[500]],3)`) settles just after the 300th tree and it is lower than the single tree's test error (`r round(unlist(cl$cum.oob.lst.err)[[500]],3)`), the cumulative oob error fluctuates and it is higher than that of the single tree - the error at the 500th tree is .
round(unlist(cl$cum.oob.lst.err)[[500]],3)
round(unlist(cl$cum.oob.lst.err)[[500]],3)
rm(list = ls())
source("src/knitSrc.R")
knitPost("2015-03-07-Tree-Based-Methods-Part-VI-Classification-Evaluation")
?lapply
library(parallel)
?parLapply
library(rlecuyer)
?clusterSetupRNG
library(snow)
?clusterSetupRNG
source("src/knitSrc.R")
knitPost("2015-03-14-Parallel-Processing-on-Single-Machine-Part-I")
ls()[ls()!="unloadPkg"]
ls()[ls()!="unloadPkg" || ls()!="moveFigs"]
ls()[ls()!="unloadPkg" | ls()!="moveFigs"]
ls()[ls()!="unloadPkg" && ls()!="moveFigs"]
rm(ls()[ls()!="unloadPkg" && ls()!="moveFigs"])
rm(list = ls()[ls()!="unloadPkg" && ls()!="moveFigs"])
rm(list = ls()[ls()!="unloadPkg" || ls()!="moveFigs"])
source("src/knitSrc.R")
knitPost("2015-03-14-Parallel-Processing-on-Single-Machine-Part-I")
rm(list = ls())
source("src/knitSrc.R")
knitPost("2015-03-14-Parallel-Processing-on-Single-Machine-Part-I")
library(parallel)
library(iterators)
library(foreach)
library(doParallel)
install.packages("parallel")
library(parallel)
install.packages("iterators")
install.packages("foreach")
install.packages("doParallel")
rm(list = ls())
library(parallel)
library(iterators)
library(foreach)
library(doParallel)
foreach(a=1:3) %do% sqrt(a)
matrix(0,nrow=3)
x = list()
for(i in 1:3) x[[length(x)+1]] = exp(i) # values of x is changed
x
x = foreach(i=1:3) %do% exp(i)
x
?system.time
system.time(foreach(i=1:4) %do% Sys.sleep(i))
cl = makeCluster(detectCores())
registerDoParallel(cl)
system.time(foreach(i=1:4) %dopar% Sys.sleep(i))
stopCluster(cl)
x = foreach(i=1:3, j=rep(10,3)) %do% {
message(paste("iter:",i))
i + j
}
x = foreach(i=1:3, j=rep(10,3)) %do% (i + j)
x = foreach(i=1:3, j=rep(10,3)) %do% {
# do something
i + j
}
rep(1,2)
foreach(i=rep(1,100), j=rep(10,3)) %do% a + b
foreach(i=rep(1,100), j=rep(10,3)) %do% i + j
rep(1,100)
rep(10,3)
foreach(i=rep(1,100), j=rep(10,3)) %do% i + j
foreach(i=1:100, j=rep(10,3)) %do% i + j
foreach(i=1:3, j=rep(10,3)) %do% i + j
foreach(i=rep(1,100), j=rep(10,3)) %do% (i + j)
x = foreach(i=rep(0,100), j=rep(10,3)) %do% (i + j)
sum(x)
?lapply
lapply(x,sum)
lapply(x,function(ele) sum(ele[[1]]))
sapply(x,function(ele) sum(ele[[1]]))
sapply(x,sum)
x = foreach(i=rep(0,100), j=rep(10,3)) %do% (i + j)
x
sapply(x,sum)
x
lapply(x,mean)
as.data.frame(x)
sapply(x,function(ele) {}
)
sapply(x,function(ele) ele[[1]])
sum(sapply(x,function(ele) ele[[1]]))
do.call("sum",x)
foreach(i=1:2, .combine="c") %do% exp(i)
foreach(i=1:2, .combine="rbind") %do% exp(i)
foreach(i=1:2, .combine="+") %do% exp(i)
foreach(i=1:2, .combine="min") %do% exp(i)
foreach(i=1:2, .combine="c") %do% exp(i)
foreach(i=1:2, .combine="rbind") %do% exp(i)
foreach(i=1:2, .combine="+") %do% exp(i)
foreach(i=1:2, .combine="min") %do% exp(i)
comb = function(a, b) if(a < b) a else b
foreach(i,1:2, .combine="comb") %do% exp(i)
foreach(i=1:2, .combine="min") %do% exp(i)
foreach(i=1:2, .combine="comb") %do% exp(i)
cl = makeCluster(detectCores())
registerDoParallel(cl)
foreach(i=1:4, .combine="c", .packages="MASS") %dopar% dim(Boston)[1]
stopCluster(cl)
cl = makeCluster(detectCores())
registerDoParallel(cl)
foreach(i=1:detectCores(), .combine="c", .packages="MASS") %dopar% dim(Boston)[1]
stopCluster(cl)
"Boston" %in% ls()
"Boston" %in% ls()
cl = makeCluster(detectCores())
registerDoParallel(cl)
foreach(i=1:detectCores(), .combine="c", .packages="MASS") %dopar% dim(Boston)[1]
stopCluster(cl)
require(MASS)
cl = makeCluster(detectCores())
registerDoParallel(cl)
foreach(i=1:detectCores(), .combine="c") %dopar% dim(Boston)[1]
stopCluster(cl)
cl = makeCluster(detectCores())
registerDoParallel(cl)
foreach(i=1:detectCores(), .combine="c") %dopar% {
require(MASS)
dim(Boston)[1]
NULL
}
stopCluster(cl)
cl = makeCluster(detectCores())
registerDoParallel(cl)
foreach(i=1:detectCores(), .combine="c") %dopar% {
require(MASS)
dim(Boston)[1]
}
stopCluster(cl)
?foreach
cl = makeCluster(detectCores())
registerDoParallel(cl)
foreach(i=1:detectCores(), .combine="c", .packages=c("MASS")) %dopar% dim(Boston)[1]
stopCluster(cl)
foreach(i=1:10, .combine="c") %:% when(i %% 2 == 0) %do% i
cusMin = function(a, b) if(a < b) a else b
foreach(i=1:2, .combine="cusMin") %do% exp(i)
# for
avec = c(10,20)
bvec = 1:4
mat = matrix(0, nrow=length(avec), ncol=length(bvec))
for(b in bvec) {
for(a in 1:length(avec)) {
mat[a,b] = avec[a] + bvec[b]
}
}
mat
# foreach - sequential
foreach(b=1:4, .combine="cbind") %:%
foreach(a=c(10,20), .combine="c") %do% (a + b)
# foreach - parallel
cl = makeCluster(detectCores())
registerDoParallel(cl)
foreach(b=1:4, .combine="cbind") %:%
foreach(a=c(10,20), .combine="c") %dopar% (a + b)
stopCluster(cl)
iters = iter(list(1:2, 3:4))
for(i in 1:(iters$length+1)) print(nextElem(iters))
iters = iter(list(1:2, 3:4))
for(i in 1:(iters$length+1)) print(nextElem(iters))
iters = iter(list(1:2, 3:4))
for(i in 1:iters$length) print(nextElem(iters))
?icount
?irnorm
library(doParallel)
?registerDoParallel
library(parallel)
?makeCluster
library(iterators)
iters = iter(matrix(0,nrow=10))
nextElem(iters)
rm(list = ls())
library(iterators)
iters = iter(1:10)
sapply(iters, nextElem)
foreach(i=iters, .combine="c") %do% i
library(foreach)
foreach(i=iters, .combine="c") %do% i
iters = iter(1:10)
foreach(i=iters, .combine="c") %do% i
iters = iter(1:10)
c(nextElem(iters),nextElem(iters))
library(parallel)
library(iterators)
library(foreach)
library(doParallel)
set.seed(1237)
mat = matrix(rnorm(100),nrow=1)
iters = iter(mat, by="row")
foreach(a=iters, .combine="c") %do% mean(a)
rm(list = ls())
source("src/knitSrc.R")
knitPost("2015-03-17-Parallel-Processing-on-Single-Machine-Part-II")
source("src/knitSrc.R")
knitPost("2015-03-17-Parallel-Processing-on-Single-Machine-Part-II")
?kmeans
library(randomForest)
?randomForest
source("src/knitSrc.R")
knitPost("2015-03-19-Parallel-Processing-on-Single-Machine-Part-III")
source("src/knitSrc.R")
knitPost("2015-03-24-Packaging-Analysis")
